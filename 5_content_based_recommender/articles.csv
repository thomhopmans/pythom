,title,link,content_html,content_text,tags
1,The Marketing Technologist in 2016: the data,/the-marketing-technologist-in-2016-the-data/,"
            <p>In 2016, we celebrated our two years anniversary. In the past year, besides all the great posts on analytics, data science, and code, we've again published tons of good content about a variety of subjects, like artificial intelligence and a Slack integration for Pokemon GO. If you wondering where we come from, check out <a href=""https://www.themarketingtechnologist.co/introducing-geek/"">our first post</a> where we talk about our core beliefs and ideas. In this post, we'd like to look back and dive into some analytics data. </p>

<p>Several interesting things happened last year because of our content. First of all, because of <a href=""https://www.themarketingtechnologist.co/how-to-keep-seo-rankings-when-changing-domains/"">SEO efforts by Inge</a>, we currently rank 1 on Google on the term 'marketing technologist':</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2017/Jan/tmt_nr1_on_google-1483348200573.PNG"" alt="""" class=""full-img""></p>

<p><em>Incognito search for 'marketing technologist'.</em></p>

<p>In international news, Thom's post about recommendation engines got translated into two languages: <a href=""http://nouf.in/?p=285"">Arabic</a> and <a href=""http://www.infoq.com/cn/articles/blog-recommendation-system-part02"">Chinese</a>. How awesome is that? Besides that we had several clients referring to posts and asking us to set it up for them:</p>

<blockquote>
  <p>I've read [this post] on your blog. Can you set up [x] for us?</p>
</blockquote>

<p>Content is turning into an automated source for new projects! We've also seen an increase of <a href=""https://www.themarketingtechnologist.co/calculation-of-confidence-intervals-for-ratios/"">guest</a> <a href=""https://www.themarketingtechnologist.co/whats-new-in-mraid-3-0/"">bloggers</a>, which is great! We'd really like to see more experts share opinions here. If you fancy writing for The Marketing Technologist, just drop us a line at <a href=""https://twitter.com/m__technologist"">Twitter</a>.</p>

<h2 id=""lookingatthedata"">Looking at the data</h2>

<p>We've had <strong>254.716 visits</strong> to our website and <strong>27.5% of these resulted in a full read</strong> of a post. We had a generally upward trend starting with roughly 1500 weekly visits in January and ending up with 7.500 in December, that's <strong>a 400% increase in weekly visits</strong>! Looking more closely at the trend, we had two major traffic peaks: </p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2017/Jan/tmt_weekly_traffic_2016-1483341935602.png"" alt=""TMT weekly sessions trend"" class=""full-img""></p>

<p><em>TMT weekly traffic trend of 2016.</em></p>

<p>We had the first peak around July 25 with our <a href=""https://www.themarketingtechnologist.co/pokemon-go-slack-notifications/"">Pokemon Go Slack Notifications post</a>, and one on September 30 with <a href=""https://www.themarketingtechnologist.co/helping-our-new-data-scientists-start-in-python-a-guide-to-learning-by-doing/"">the post to help new data scientists in python</a>. These two posts give a nice indication of the variety of topics on TMT.</p>

<h2 id=""contentperformance"">Content performance</h2>

<p>As you might know, <a href=""https://www.themarketingtechnologist.co/track-content-performance-using-google-analytics-enhanced-ecommerce-report/"">we've setup content performance tracking</a> back in 2015. Because of this, we can drill down in our content's performance. We've had <strong>79.714 fully read articles</strong>, consisting of <strong>77.874.643 words in total</strong> (only including full reads!), <strong>making the average read post length on TMT 976,93</strong>.  With the threshold for large content at 1000+ words, this average tells us that large content performs well on TMT. If we take a closer look at the performance of content size, we get the same results:</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2017/Jan/tmt_content_size_2016-1483342357184.png"" alt=""TMT content size performance"" title="""" class=""full-img""><em>TMT content size performance of 2016.</em></p>

<p>What's not that surprising is that large content makes up for a large chunk of the total read words (77,4%), these are the largest posts after all. What is more surprising, is the performance of the content. Besides having the most full reads at 48.264 (60.6%), the full read rate (Buy-to-Detail Rate) is 34,78%, beating medium content with 12,08% and small content with 10,51%! <strong>Our large content performance is 53-56% better compared to other post sizes</strong>. Large posts work well on TMT.</p>

<h2 id=""thetopposts"">The top posts</h2>

<p>We see something interesting in our top 10 most read post: </p>

<ol>
<li><a href=""https://www.themarketingtechnologist.co/building-nested-components-in-angular-2/"">Passing data to and from a nested component in Angular </a>  </li>
<li><a href=""https://www.themarketingtechnologist.co/react-router-an-introduction/"">React Router: a comprehensive introduction</a>  </li>
<li><a href=""https://www.themarketingtechnologist.co/google-oauth-2-enable-your-application-to-access-data-from-a-google-user/"">Google OAuth 2: access data from a Google user in your application</a>  </li>
<li><a href=""https://www.themarketingtechnologist.co/introduction-to-data-binding-in-angular-2-versus-angular-1/"">Introduction to data binding in Angular 2 versus Angular 1</a>  </li>
<li><a href=""https://www.themarketingtechnologist.co/caching-http-requests-in-angularjs/"">Caching $http requests in AngularJS</a>  </li>
<li><a href=""https://www.themarketingtechnologist.co/helping-our-new-data-scientists-start-in-python-a-guide-to-learning-by-doing/"">Helping our new Data Scientists start in Python: A guide to learning by doing</a>  </li>
<li><a href=""https://www.themarketingtechnologist.co/pokemon-go-slack-notifications/"">Send Slack notifications whenever a Pokémon spawns nearby using a Pokémon GO SlackBot</a>  </li>
<li><a href=""https://www.themarketingtechnologist.co/creating-dynamic-videos-using-javascript-and-after-effects-the-basics/"">Creating dynamic videos using JavaScript and After Effects: the basics</a>  </li>
<li><a href=""https://www.themarketingtechnologist.co/how-to-update-an-out-of-date-package-json/"">How to update out of date npm dependencies</a>  </li>
<li><a href=""https://www.themarketingtechnologist.co/where-have-my-factories-services-constants-and-values-gone-in-angular-2/"">Angular 2: Where have my factories, services, constants and values gone?</a> </li>
</ol>

<p>The number 2, 3, 5, 8, 9 and 10 posts were published in 2015 (that's 6 out of 10), indicating that this is valuable content and the subjects are still relevant. Of course, we track detailed information in Google Analytics with word count (product revenue) and quantity (amount of reads):</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2017/Jan/tmt_most_read_post_2016-1483343097027.PNG"" alt=""TMT most read post 2016"" class=""full-img"">
<em>TMT content top 10</em></p>

<p>We can remove the non-2016 posts from the list to get the most popular posts published in 2016:</p>

<ol>
<li><a href=""https://www.themarketingtechnologist.co/building-nested-components-in-angular-2/"">Passing data to and from a nested component in Angular </a>  </li>
<li><a href=""https://www.themarketingtechnologist.co/introduction-to-data-binding-in-angular-2-versus-angular-1/"">Introduction to data binding in Angular 2 versus Angular 1</a>  </li>
<li><a href=""https://www.themarketingtechnologist.co/helping-our-new-data-scientists-start-in-python-a-guide-to-learning-by-doing/"">Helping our new Data Scientists start in Python: A guide to learning by doing</a>  </li>
<li><a href=""https://www.themarketingtechnologist.co/pokemon-go-slack-notifications/"">Send Slack notifications whenever a Pokémon spawns nearby using a Pokémon GO SlackBot</a>  </li>
<li><a href=""https://www.themarketingtechnologist.co/upload-your-local-spark-script-to-an-aws-emr-cluster-using-a-simply-python-script/"">Upload your local Spark script to an AWS EMR cluster using a simple Python script</a>  </li>
<li><a href=""https://www.themarketingtechnologist.co/a-recommendation-system-for-blogs-content-based-similarity-part-2/"">A recommendation system for blogs: Content-based similarity (part 2)</a>  </li>
<li><a href=""https://www.themarketingtechnologist.co/api-ai-vs-wit-ai/"">Api.ai vs Wit.ai (or is it Google vs Facebook?)</a>  </li>
<li><a href=""https://www.themarketingtechnologist.co/open-a-whatsapp-conversation-directly-from-your-ad/"">How to start a WhatsApp conversation directly from the web</a>  </li>
<li><a href=""https://www.themarketingtechnologist.co/google-analytics-new-event-metrics-explained/"">Google Analytics' new event metrics explained</a>  </li>
<li><a href=""https://www.themarketingtechnologist.co/the-differences-between-adobe-experience-designer-and-sketch-app/"">The differences between Adobe Experience Design and Sketch App</a> </li>
</ol>

<p>The shift in top 10 posts shows us the importance of asking the right question. Do we want the most-read post of 2016, or the most-read posts published in 2016?</p>

<p>If we look at subjects, we see that 4 posts ar about development (1, 2, 8 and 10), 3 are about data science (3, 5 and 7), 2 about upcoming technologies (4 and 6) and one is about analytics (9). The amount of fully read analytics posts is dwarfed by the other subjects. Because of this, we assume that technical analytics is a niche (we do get a constant flow of a few reads a week). We see this in the trend of our three main post categories:</p>

<h4 id=""1trendofcodeposts"">1. Trend of code posts</h4>

<p>Our coding posts make up for most of the read content. The volume of read content also increased significantly throughout the year. <br>
<img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2017/Jan/tmt_2016_weekly_code_posts-1483346840071.PNG"" alt=""TMT code content trend"" class=""full-img""></p>

<h4 id=""2trendofdatascienceposts"">2. Trend of data science posts</h4>

<p>The data science related posts have a constant flow of reads and were good for the 2 major traffic peaks we discussed earlier. Just as development, the number of read words increased throughout the year. <br>
<img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2017/Jan/tmt_2016_weekly_datascience_posts-1483346600641.PNG"" alt=""TMT data science content trend"" class=""full-img""></p>

<h4 id=""3trendofanalyticsposts"">3. Trend of analytics posts</h4>

<p>Analytics has very constant flow of read articles, increasing only slightly as the year passes. It's interesting to see that the full read rate (Buy-to-Detail Rate) is the highest of the three topics. The peak is related to a post about <a href=""https://www.themarketingtechnologist.co/google-analytics-new-event-metrics-explained/"">the new event metrics in Google Analytics</a>, which Google somehow release without any documentation. </p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2017/Jan/tmt_2016_weekly_analytics_posts-1483346398363.PNG"" alt=""TMT analytics content trend"" class=""full-img""></p>

<h2 id=""thetopauthors"">The top authors</h2>

<p>Besides analysing content based on title, we can set up a list of top authors in as well. And we can do so in three: </p>

<ol>
<li>amount of words read;  </li>
<li>amount of articles read; or  </li>
<li>full read rate (how many people that open an article also read it?).</li>
</ol>

<p>Let's have a look:</p>

<h4 id=""1topauthorsbasedonamountofwordsread"">1. Top authors based on amount of words read</h4>

<p>The top 5 authors correspond with the popular topics, both Siebe and Erwin (number 1 and 3) write code related posts, Thom and Ruben (number 2 and 5) write about Data Science, and I (number 4) write about analytics. <br>
<img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2017/Jan/tmt_2016_readwords-1483347256292.PNG"" alt=""TMT top 5 authors by words read"" class=""full-img""></p>

<h4 id=""2topauthorsbasedonamountofarcticlesread"">2. Top authors based on amount of arcticles read</h4>

<p>If we switch to quantity, ignoring the number of words, Ruben drops out fo the top 5 and Gaya enters. <br>
<img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2017/Jan/tmt_2016_readarticles-1483347269123.PNG"" alt=""TMT top 5 authors by articles read"" class=""full-img""></p>

<h4 id=""3topauthorsbasedonfullreadrate"">3. Top authors based on full read rate</h4>

<p>Switching to full read rate changes the top 5 completely. Everyone who was in the top 5, is gone now. These authors post less, but when they post, readers are likely to fully read their posts. <br>
<img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2017/Jan/tmt_2016_readratio-1483347286944.PNG"" alt=""TMT top 5 authors by full read rate"" class=""full-img""></p>

<h2 id=""applyingupdates"">Applying updates</h2>

<p>Getting the performance of only the 2016 post was quite a hassle, we didn't add any publication date information to the post. Because of this, we manually have to exclude 2015 posts to get our top 2016 posts:</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2017/Jan/tmt_post_exclusion-1483349295616.PNG"" alt=""TMT manual post exclusion"" class=""full-img"">
<em>TMT manual post exclusion filter.</em></p>

<p>Of course, this would be way easier if we collect the right data. So now, we've added the post's month and year as custom dimensions:</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2017/Jan/tmt_2016_cd_update-1483349305975.PNG"" alt=""TMT new custom dimenions"">
<em>TMT new custom dimension for post year and month tracking.</em></p>

<p>Because of this change, the filter will be way easier next year:</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2017/Jan/tmt_2016_content_filter-1483349722455.PNG"" alt=""TMT new custom dimension filter"" title="""" class=""full-img""><em>TMT new custom dimension for post year and month tracking.</em></p>

<h2 id=""finalwords"">Final words</h2>

<p>In the next year, we're eager to post even more quality content on The Marketing Technologist, and we could use your help with achieving that goal. We'd really like to see more experts share opinions here, so if you fancy writing for the Marketing Technologist, just drop us a line at <a href=""https://twitter.com/m__technologist"">Twitter</a>. </p>

<p>Have a great 2017!</p>

<p><mark>This post was co-written by Siebe Hiemstra.</mark></p>
        ","In 2016, we celebrated our two years anniversary. In the past year, besides all the great posts on analytics, data science, and code, we've again published tons of good content about a variety of subjects, like artificial intelligence and a Slack integration for Pokemon GO. If you wondering where we come from, check out our first post where we talk about our core beliefs and ideas. In this post, we'd like to look back and dive into some analytics data.
Several interesting things happened last year because of our content. First of all, because of SEO efforts by Inge, we currently rank 1 on Google on the term 'marketing technologist':
Incognito search for 'marketing technologist'.
In international news, Thom's post about recommendation engines got translated into two languages: Arabic and Chinese. How awesome is that? Besides that we had several clients referring to posts and asking us to set it up for them:
I've read [this post] on your blog. Can you set up [x] for us?
Content is turning into an automated source for new projects! We've also seen an increase of guest bloggers, which is great! We'd really like to see more experts share opinions here. If you fancy writing for The Marketing Technologist, just drop us a line at Twitter.
Looking at the data
We've had 254.716 visits to our website and 27.5% of these resulted in a full read of a post. We had a generally upward trend starting with roughly 1500 weekly visits in January and ending up with 7.500 in December, that's a 400% increase in weekly visits! Looking more closely at the trend, we had two major traffic peaks:
TMT weekly traffic trend of 2016.
We had the first peak around July 25 with our Pokemon Go Slack Notifications post, and one on September 30 with the post to help new data scientists in python. These two posts give a nice indication of the variety of topics on TMT.
Content performance
As you might know, we've setup content performance tracking back in 2015. Because of this, we can drill down in our content's performance. We've had 79.714 fully read articles, consisting of 77.874.643 words in total (only including full reads!), making the average read post length on TMT 976,93. With the threshold for large content at 1000+ words, this average tells us that large content performs well on TMT. If we take a closer look at the performance of content size, we get the same results:
TMT content size performance of 2016.
What's not that surprising is that large content makes up for a large chunk of the total read words (77,4%), these are the largest posts after all. What is more surprising, is the performance of the content. Besides having the most full reads at 48.264 (60.6%), the full read rate (Buy-to-Detail Rate) is 34,78%, beating medium content with 12,08% and small content with 10,51%! Our large content performance is 53-56% better compared to other post sizes. Large posts work well on TMT.
The top posts
We see something interesting in our top 10 most read post:
Passing data to and from a nested component in Angular
React Router: a comprehensive introduction
Google OAuth 2: access data from a Google user in your application
Introduction to data binding in Angular 2 versus Angular 1
Caching $http requests in AngularJS
Helping our new Data Scientists start in Python: A guide to learning by doing
Send Slack notifications whenever a Pokémon spawns nearby using a Pokémon GO SlackBot
Creating dynamic videos using JavaScript and After Effects: the basics
How to update out of date npm dependencies
Angular 2: Where have my factories, services, constants and values gone?
The number 2, 3, 5, 8, 9 and 10 posts were published in 2015 (that's 6 out of 10), indicating that this is valuable content and the subjects are still relevant. Of course, we track detailed information in Google Analytics with word count (product revenue) and quantity (amount of reads):
TMT content top 10
We can remove the non-2016 posts from the list to get the most popular posts published in 2016:
Passing data to and from a nested component in Angular
Introduction to data binding in Angular 2 versus Angular 1
Helping our new Data Scientists start in Python: A guide to learning by doing
Send Slack notifications whenever a Pokémon spawns nearby using a Pokémon GO SlackBot
Upload your local Spark script to an AWS EMR cluster using a simple Python script
A recommendation system for blogs: Content-based similarity (part 2)
Api.ai vs Wit.ai (or is it Google vs Facebook?)
How to start a WhatsApp conversation directly from the web
Google Analytics' new event metrics explained
The differences between Adobe Experience Design and Sketch App
The shift in top 10 posts shows us the importance of asking the right question. Do we want the most-read post of 2016, or the most-read posts published in 2016?
If we look at subjects, we see that 4 posts ar about development (1, 2, 8 and 10), 3 are about data science (3, 5 and 7), 2 about upcoming technologies (4 and 6) and one is about analytics (9). The amount of fully read analytics posts is dwarfed by the other subjects. Because of this, we assume that technical analytics is a niche (we do get a constant flow of a few reads a week). We see this in the trend of our three main post categories:
1. Trend of code posts
Our coding posts make up for most of the read content. The volume of read content also increased significantly throughout the year.
2. Trend of data science posts
The data science related posts have a constant flow of reads and were good for the 2 major traffic peaks we discussed earlier. Just as development, the number of read words increased throughout the year.
3. Trend of analytics posts
Analytics has very constant flow of read articles, increasing only slightly as the year passes. It's interesting to see that the full read rate (Buy-to-Detail Rate) is the highest of the three topics. The peak is related to a post about the new event metrics in Google Analytics, which Google somehow release without any documentation.
The top authors
Besides analysing content based on title, we can set up a list of top authors in as well. And we can do so in three:
amount of words read;
amount of articles read; or
full read rate (how many people that open an article also read it?).
Let's have a look:
1. Top authors based on amount of words read
The top 5 authors correspond with the popular topics, both Siebe and Erwin (number 1 and 3) write code related posts, Thom and Ruben (number 2 and 5) write about Data Science, and I (number 4) write about analytics.
2. Top authors based on amount of arcticles read
If we switch to quantity, ignoring the number of words, Ruben drops out fo the top 5 and Gaya enters.
3. Top authors based on full read rate
Switching to full read rate changes the top 5 completely. Everyone who was in the top 5, is gone now. These authors post less, but when they post, readers are likely to fully read their posts.
Applying updates
Getting the performance of only the 2016 post was quite a hassle, we didn't add any publication date information to the post. Because of this, we manually have to exclude 2015 posts to get our top 2016 posts:
TMT manual post exclusion filter.
Of course, this would be way easier if we collect the right data. So now, we've added the post's month and year as custom dimensions:
TMT new custom dimension for post year and month tracking.
Because of this change, the filter will be way easier next year:
TMT new custom dimension for post year and month tracking.
Final words
In the next year, we're eager to post even more quality content on The Marketing Technologist, and we could use your help with achieving that goal. We'd really like to see more experts share opinions here, so if you fancy writing for the Marketing Technologist, just drop us a line at Twitter.
Have a great 2017!
This post was co-written by Siebe Hiemstra.",[Overview]
2,No click? Still results!,/whats-the-value-of-an-ad-impression-when-theres-no-click/,"
            <p>What's the true value of an online ad impression? It's an ongoing discussion which continues to pop up.</p>

<p>Everyone agrees the last click model undervalues campaigns that fulfill an important role in the start of the buying cycle. Not every ad has to lead to a sale instantly. Most other common conversion attribution models only observe what happens after a click (see Eddie Borgers' article for an overview of the most used standard conversion attribution models).</p>

<p>At the complete other end of the spectrum, you'll find  post view attribution, which commonly  overvalues your campaigns. Creatives the customer hasn't seen (when they get served below the fold for instance), are often  getting undeserved  value. Especially within performance aimed campaigns, this model is not recommended.</p>

<p>Does this mean a video or banner impression, that doesn't lead to a click, is worthless? Absolutely not! No one ever clicked on a tv ad, have they? That doesn't mean it doesn't have any value. Intuitively people understand this applies to online as well, especially for high impact formats like billboards, videos, or takeovers. </p>

<h2 id=""theidea"">The idea</h2>

<p>I'm one of those number junkies. That's why we went on a journey to find a way to quantify the value of an impression. We try to do this by looking at everything that occurs after the moment a creative is served, without it leading to a click. After this, we tie a relation with later website visits and / or (online) purchases.</p>

<p>The first obvious step is viewability. We look at the total time an ad is actually visible to a user. But we like to take it up a notch by looking at the user's behavior from the moment he or she sees the creative. We do this with Kosi, a tool that can track any kind of interaction from within an online creative. With this tool we don't just register clicks, but also interactions like swipes, scrolls, use of selectors, filling out a form, hover time, and total interaction time. Every interaction a user performs can be turned into insights through Kosi.</p>

<p>By creating a smart connection with Atlas tracking by Facebook, we're able to measure post view results from different levels of interaction. The nice thing about the Atlas link is that it makes cross-device results  available. This way we can create a connection between mobile banner interaction, and a later purchase on another device (also see my earlier article about Atlas insights).</p>

<p>This setup enables us to look at the indirect effect, since we were able to make links between the different levels of interaction, and later indirect conversions.</p>

<p>We tested this principle a few times in the real world. Every time it returns the same result. Users who interact, convert better.</p>

<h2 id=""theproof"">The proof</h2>

<p>We used a billboard game for our first test. The creative wasn't meant for people to click and buy, but to let people play a game that was related to the brand. In this case it was ""Koekhappen"", meant for the Staatsloterij Kings Day lottery draw.</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Dec/Artikel_Guus_2-1482773392782.png"" alt=""""></p>

<p>Subsequently, we looked at the difference in buying behavior between audiences who played the game, and those who didn't. During analysis, we filtered out the direct effects, and only looked at users who didn't click.</p>

<p>The result: users who played the game, had a much higher tendency to purchase from the advertiser later on through a different source. The conversion within this group ended up 30 percent higher than the audience that did see the billboard, but did not interact.</p>

<p>Because gameplay is a relatively intense interaction, we looked at a more simple interaction case that asks less of the user. For a recurring lottery campaign, we used a mobile swipe cube. The results were less extreme than the first test, but there still was a significant uplift of 15% in post view conversions in comparison to the audience that didn't interact with the ad.</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Dec/Artikel_Guus-1482773401885.png"" alt="""" class=""full-img""></p>

<p>All tests up until now show interaction within a creative shows a significant indication of the effect of the ad in the consumer journey. Let's let go of our CTR tunnel vision, and  misleading post view models. The indirect effects can have an important contribution to the  final result. Make this insightful.</p>
        ","What's the true value of an online ad impression? It's an ongoing discussion which continues to pop up.
Everyone agrees the last click model undervalues campaigns that fulfill an important role in the start of the buying cycle. Not every ad has to lead to a sale instantly. Most other common conversion attribution models only observe what happens after a click (see Eddie Borgers' article for an overview of the most used standard conversion attribution models).
At the complete other end of the spectrum, you'll find post view attribution, which commonly overvalues your campaigns. Creatives the customer hasn't seen (when they get served below the fold for instance), are often getting undeserved value. Especially within performance aimed campaigns, this model is not recommended.
Does this mean a video or banner impression, that doesn't lead to a click, is worthless? Absolutely not! No one ever clicked on a tv ad, have they? That doesn't mean it doesn't have any value. Intuitively people understand this applies to online as well, especially for high impact formats like billboards, videos, or takeovers.
The idea
I'm one of those number junkies. That's why we went on a journey to find a way to quantify the value of an impression. We try to do this by looking at everything that occurs after the moment a creative is served, without it leading to a click. After this, we tie a relation with later website visits and / or (online) purchases.
The first obvious step is viewability. We look at the total time an ad is actually visible to a user. But we like to take it up a notch by looking at the user's behavior from the moment he or she sees the creative. We do this with Kosi, a tool that can track any kind of interaction from within an online creative. With this tool we don't just register clicks, but also interactions like swipes, scrolls, use of selectors, filling out a form, hover time, and total interaction time. Every interaction a user performs can be turned into insights through Kosi.
By creating a smart connection with Atlas tracking by Facebook, we're able to measure post view results from different levels of interaction. The nice thing about the Atlas link is that it makes cross-device results available. This way we can create a connection between mobile banner interaction, and a later purchase on another device (also see my earlier article about Atlas insights).
This setup enables us to look at the indirect effect, since we were able to make links between the different levels of interaction, and later indirect conversions.
We tested this principle a few times in the real world. Every time it returns the same result. Users who interact, convert better.
The proof
We used a billboard game for our first test. The creative wasn't meant for people to click and buy, but to let people play a game that was related to the brand. In this case it was ""Koekhappen"", meant for the Staatsloterij Kings Day lottery draw.
Subsequently, we looked at the difference in buying behavior between audiences who played the game, and those who didn't. During analysis, we filtered out the direct effects, and only looked at users who didn't click.
The result: users who played the game, had a much higher tendency to purchase from the advertiser later on through a different source. The conversion within this group ended up 30 percent higher than the audience that did see the billboard, but did not interact.
Because gameplay is a relatively intense interaction, we looked at a more simple interaction case that asks less of the user. For a recurring lottery campaign, we used a mobile swipe cube. The results were less extreme than the first test, but there still was a significant uplift of 15% in post view conversions in comparison to the audience that didn't interact with the ad.
All tests up until now show interaction within a creative shows a significant indication of the effect of the ad in the consumer journey. Let's let go of our CTR tunnel vision, and misleading post view models. The indirect effects can have an important contribution to the final result. Make this insightful.","[Analytics, Online Advertising, Kosi]"
3,A lean approach to better recruitment insights,/a-lean-approach-to-better-recruitment-insights/,"
            <p>Our HR department has struggled with the insights of their marketing efforts. They know how many people apply for a job, but they don’t see the actual numbers of people that get invited for a first chat or even get the actual job. To give them the insights in Google Analytics, we’ve put our heads together and were able to get the desired data. All you need is a Google Analytics account and a spreadsheet.</p>

<h2 id=""anoverviewoftheproblemandthesolution"">An overview of the problem and the solution</h2>

<p>The main goal of the concept is to know how well campaign sources are performing, without just looking at an online application. Let's say you’re using two networks to promote jobs: Google AdWords and LinkedIn. If you’re just measuring online, you might think that AdWords is the best performing network:</p>

<ul>
<li>AdWords: 
<ul><li>2.000 sessions</li>
<li>100 applications</li>
<li>conversion rate 5%</li></ul></li>
<li>LinkedIn
<ul><li>1000 sessions</li>
<li>25 applications</li>
<li>conversion rate 2,5%</li></ul></li>
</ul>

<p>Looking at these stats you might want to shift your marketing efforts to AdWords as it is performing twice as good as LinkedIn (if I ignore the costs of each network). If we were able to measure actual invitations for a first talk, the insights may change:</p>

<ul>
<li>AdWords: 
<ul><li>2.000 sessions</li>
<li>10 invites</li>
<li>conversion rate 0,5%</li></ul></li>
<li>LinkedIn
<ul><li>1000 sessions</li>
<li>20 invites</li>
<li>conversion rate 2%</li></ul></li>
</ul>

<p>Based on actual invites, we now see that LinkedIn is performing 4 times as good as AdWords. The main goal of this post is to get you these insights. </p>

<h2 id=""step1collectingthegoogleanalyticsclientid"">Step 1: collecting the Google Analytics client id</h2>

<p>The first step of the solution is to collect the Google Analytics client id with the job application. This step requires the developers of job listing website. You’ll need to capture the client id and send it with the form. The easiest way to do this is by adding a hidden input field to the form and filling this automatically with the client ID on page load. Here’s an example piece of jQuery code that does just that:</p>

<pre><code>ga(function (tracker) {  $('#inputFieldForGoogleAnalyticsClientId').val(tracker.get('clientId'));  
  });
</code></pre>

<p><em>Sample code to set the value of the hidden input field with id 'inputFieldForGoogleAnalyticsClientId' to the value the current client id.</em></p>

<p>If you have this implemented, the form will add the Google Analytics client ID to the submitted data automatically.</p>

<h2 id=""step2usingtheclientidtogettheinsights"">Step 2: using the client id to get the insights</h2>

<p>Here’s where it gets fun. Setup a Google spreadsheet (or Excel document if you like) that lists the following information:</p>

<ul>
<li>1 Name.</li>
<li>2 Status (invited/declined)</li>
<li>3 Sent to Google Analytics (yes/no)</li>
<li>4 Google Analytics Client ID</li>
<li>5 Google Analytics link</li>
</ul>

<p>Field 1 to 4 are easy to set up. If you’re already using a spreadsheet or Excel based document, feel free to add the information to your existing document. </p>

<p>The magic is in field 5: Google Analytics allows you to send information to the platform with links (it’s officially called <a href=""https://developers.google.com/analytics/devguides/collection/protocol/v1/"">the measurement protocol</a>). If we generate these links in a smart way, we’ll connect the data sent via such a link to the campaign source of the application. Make sure to only show the link when the row's 'status' value (2) is invited and 'sent to Google Analytics' value (3) is no.</p>

<p>Let’s have a look at this link:</p>

<pre><code>https://www.google-analytics.com/collect?v=1&amp;t=event&amp;tid=GA-PROPERTY-ID&amp;cid=GA-CLIENT-ID&amp;ec=EVENT-CATEGORY&amp;ea=EVENT-LABEL&amp;ev=1&amp;aip=1  
</code></pre>

<p><em>An example of the link you’ll need to generate in your spreadsheet.</em></p>

<p>There are four important variables here:</p>

<ul>
<li>GA-PROPERTY-ID: change this to the GA property ID that runs on your recruitment website.</li>
<li>GA-CLIENT-ID-FIELD: change this to the field in the table that holds the client ID.</li>
<li>EVENT-CATEGORY: change this to event category, e.g. Application%20status (%20 is a whitespace in URL encoding).</li>
<li>EVENT-LABEL: change this to the event label, e.g. first%20invitation.</li>
</ul>

<p>Now, If you click this link, it'll send an event to GA with the client ID captured during the original form submission. Google will recognise the user and apply the known source of that user to the event coming in (using <a href=""https://www.themarketingtechnologist.co/about-the-google-analytics-client-id/"">the last non-direct click model</a>).  If you set up a goal that captures this event, you can create a simple report that shows you all the information you need:</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Dec/tmt_recruitment-1482840152158.PNG"" alt=""sample recruitment insights report"" class=""full-img""></p>

<p><em>Example screenshot of recruitment insights. I've included a calculated metric to show the performance from online application to invited for first interview.</em></p>

<p>Et voila, we have added the ‘first invite’ performance to our campaign report in Google Analytics.</p>

<h2 id=""considerationsandnextsteps"">Considerations and next steps</h2>

<p>The setup in this post requires several manual actions: adding application data to a sheet, and manually clicking a link. If either of these steps is forgotten, the system won’t work. You might want to include this measurement system in your recruitment software if you have any. Besides that, you could add extra value goals after the first invitation like ‘contract offered’, ‘hired’ and ‘employed for 6 months’. </p>
        ","Our HR department has struggled with the insights of their marketing efforts. They know how many people apply for a job, but they don’t see the actual numbers of people that get invited for a first chat or even get the actual job. To give them the insights in Google Analytics, we’ve put our heads together and were able to get the desired data. All you need is a Google Analytics account and a spreadsheet.
An overview of the problem and the solution
The main goal of the concept is to know how well campaign sources are performing, without just looking at an online application. Let's say you’re using two networks to promote jobs: Google AdWords and LinkedIn. If you’re just measuring online, you might think that AdWords is the best performing network:
AdWords:
2.000 sessions
100 applications
conversion rate 5%
LinkedIn
1000 sessions
25 applications
conversion rate 2,5%
Looking at these stats you might want to shift your marketing efforts to AdWords as it is performing twice as good as LinkedIn (if I ignore the costs of each network). If we were able to measure actual invitations for a first talk, the insights may change:
AdWords:
2.000 sessions
10 invites
conversion rate 0,5%
LinkedIn
1000 sessions
20 invites
conversion rate 2%
Based on actual invites, we now see that LinkedIn is performing 4 times as good as AdWords. The main goal of this post is to get you these insights.
Step 1: collecting the Google Analytics client id
The first step of the solution is to collect the Google Analytics client id with the job application. This step requires the developers of job listing website. You’ll need to capture the client id and send it with the form. The easiest way to do this is by adding a hidden input field to the form and filling this automatically with the client ID on page load. Here’s an example piece of jQuery code that does just that:
ga(function (tracker) {  $('#inputFieldForGoogleAnalyticsClientId').val(tracker.get('clientId'));  
  });
Sample code to set the value of the hidden input field with id 'inputFieldForGoogleAnalyticsClientId' to the value the current client id.
If you have this implemented, the form will add the Google Analytics client ID to the submitted data automatically.
Step 2: using the client id to get the insights
Here’s where it gets fun. Setup a Google spreadsheet (or Excel document if you like) that lists the following information:
1 Name.
2 Status (invited/declined)
3 Sent to Google Analytics (yes/no)
4 Google Analytics Client ID
5 Google Analytics link
Field 1 to 4 are easy to set up. If you’re already using a spreadsheet or Excel based document, feel free to add the information to your existing document.
The magic is in field 5: Google Analytics allows you to send information to the platform with links (it’s officially called the measurement protocol). If we generate these links in a smart way, we’ll connect the data sent via such a link to the campaign source of the application. Make sure to only show the link when the row's 'status' value (2) is invited and 'sent to Google Analytics' value (3) is no.
Let’s have a look at this link:
https://www.google-analytics.com/collect?v=1&t=event&tid=GA-PROPERTY-ID&cid=GA-CLIENT-ID&ec=EVENT-CATEGORY&ea=EVENT-LABEL&ev=1&aip=1  
An example of the link you’ll need to generate in your spreadsheet.
There are four important variables here:
GA-PROPERTY-ID: change this to the GA property ID that runs on your recruitment website.
GA-CLIENT-ID-FIELD: change this to the field in the table that holds the client ID.
EVENT-CATEGORY: change this to event category, e.g. Application%20status (%20 is a whitespace in URL encoding).
EVENT-LABEL: change this to the event label, e.g. first%20invitation.
Now, If you click this link, it'll send an event to GA with the client ID captured during the original form submission. Google will recognise the user and apply the known source of that user to the event coming in (using the last non-direct click model). If you set up a goal that captures this event, you can create a simple report that shows you all the information you need:
Example screenshot of recruitment insights. I've included a calculated metric to show the performance from online application to invited for first interview.
Et voila, we have added the ‘first invite’ performance to our campaign report in Google Analytics.
Considerations and next steps
The setup in this post requires several manual actions: adding application data to a sheet, and manually clicking a link. If either of these steps is forgotten, the system won’t work. You might want to include this measurement system in your recruitment software if you have any. Besides that, you could add extra value goals after the first invitation like ‘contract offered’, ‘hired’ and ‘employed for 6 months’.","[Analytics, google analytics, recruitement]"
4,All I want for Christmas is MRAID!,/whats-new-in-mraid-3-0/,"
            <p>""Why does my ad work on the web but not in-app?"". The likely answer: <a href=""https://www.iab.com/guidelines/mobile-rich-media-ad-interface-definitions-mraid/"">MRAID</a>. The same ads that work on websites do not work in an app. I've been asked what's up with this by both advertisers and publishers. The only question I ask them in return most of the time is whether they're serving a MRAID tag or not. After this, it remains silent on the other end of the line. M-what-raid?</p>

<h2 id=""whatismraid"">What is MRAID?</h2>

<p>MRAID stands for ""Mobile Rich Media Ad Interface Definitions"" and it is the IAB's program to define standards for in-app mobile rich media ads. It's a collection of agreements on how an app can show an ad properly, and how the ad can use the phone's features.</p>

<p>On the web, there are also these kinds of codes and guidelines on how to serve rich media ads. But both websites and ads on a smartphone, laptop, Android, IOS or a Microsoft device all speak the same universal language: JavaScript. Therefore it's pretty straightforward to communicate between the ad and the publisher. For instance, there's no challenge in having ad to play a video or open a new link (because the ad is also loaded in the browser ;). </p>

<p>But Apps don't speak JavaScript. Apps speak all kinds of languages (even within the same operating system): Java, Swift, Objective-C, C#, you name it. Our mobile ads do not speak all these languages, but still, we want our ad to be able to communicate with our phone and we want it to work on all kind of phones. That's exactly where MRAID comes in. </p>

<p>To enable the scaling of rich media ad serving in apps, IAB developed the first version of MRAID in 2011. Technically, MRAID is nothing more than an API (an interface that sends commands to software using code), that is supported by multiple SDK's (framework of an app). Because of this, an ad that is MRAID compliant will run in (almost) every app. To get your ad working in both web and in-app environments, you only have these two technical requirements. </p>

<ol>
<li>For (mobile) web your ad needs to be created using <strong>HTML5 + JavaScript</strong>  </li>
<li>On top of this, to make your ad work in-app, you need to make your creative <strong>MRAID compliant</strong> by integrating the MRAID library. </li>
</ol>

<p>In another post, we'll get into the technical details on how to make your ads MRAID compliant, and how to make your ad communicate with a user's mobile phone or tablet.</p>

<h2 id=""mraid30"">MRAID 3.0</h2>

<p>Over 5 years after IAB introduced the first version, they recently <a href=""https://www.iab.com/news/iab-tech-lab-releases-mraid-3-0-public-comment/"">uncovered a public draft</a> of the standard: MRAID 3.0. This is some real insider's information. To all you app professionals, I promise you're going to absolutely love this! I'm a huge fan! Let's see what they have in store for us:</p>

<h5 id=""vpaidandmraid"">VPAID and MRAID</h5>

<p>The guidelines for MRAID and <a href=""https://www.google.nl/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=3&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwjbifOY0IjRAhXKnBoKHXRlCFUQFgglMAI&amp;url=https%3A%2F%2Fwww.iab.com%2Fguidelines%2Fdigital-video-player-ad-interface-definition-vpaid-2-0%2F&amp;usg=AFQjCNHFbeIhhVqwnWR5ogPEtiHyMZHRvg&amp;sig2=o_CuXhFPGxXE09-SNJ53ZA"">VPAID</a> (Video Player Ad Interface Definition), the MRAID for in-stream video, are merged into one document. This makes a lot of sense because also with video it's quite a challenge to serve (and measure) ads at scale. </p>

<h5 id=""adreadiness"">Ad-readiness</h5>

<p>This upgrade will prevent the user from seeing white and black ad slots ('<em>blanks</em>') instead of the ad. Before showing an ad, this functionality will first verify that the ad has been loaded completely. </p>

<h5 id=""trackaudio"">Track audio</h5>

<p>A lot of users have muted their mobile phone's audio, and, hence, they'll not hear the (video) ad's sound. However, when an advertiser still chooses to run audio in a creative, now at least the mute state can be tracked. You can detect when the audio is muted or unmuted, and when the volume is changed. This is very interesting, because in-banner videos are getting more and more important in archieving awareness goals. </p>

<h5 id=""trackingviewability"">Tracking viewability</h5>

<p>My personal favorite and the biggest game changer in MRAID 3.0 definitely is the improvement of the in-app viewability tracking. In MRAID 2.0 it was already possible to read whether an ad was visible or not, but with MRAID 3.0 it's possible to track what percentage of the ad is viewable. By this new feature, the MRAID viewability measurement completely meets the criteria of the <a href=""https://www.google.nl/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwiB-4qn0IjRAhUBXhoKHV59AEoQFggaMAA&amp;url=http%3A%2F%2Fwww.mediaratingcouncil.org%2F063014%2520Viewable%2520Ad%2520Impression%2520Guideline_Final.pdf&amp;usg=AFQjCNGYeWp4xH6VTkdjmPvaZWqNNM33Mg&amp;sig2=ld926Nr-0P7TGa6P2iHFCA&amp;bvm=bv.142059868,d.d2s"">MRC guidelines</a> for tracking in-app viewability. In another upcoming post we'll get into the technical specifics of tracking viewability with MRAID 3.0. </p>

<p>With version 3.0, advertisers will no longer be dependent on publishers with specific viewability trackers SDK's, like MOAT. After all, MRAID is a must-have for publishers with an app.</p>

<h2 id=""mraidprofessionalsbepatientthisisonlythefirstdraft"">MRAID professionals: be patient, this is only the first draft</h2>

<p>Like I said earlier in the spoiler: I'm very enthusiastic about MRAID 3.0 and it's on top of my Christmas list. It's very clear that IAB listened to the needs in the market and it lowers the hurdles for specialists, publishers, and advertisers. </p>

<p>The update was also very well received by developers. Colleague, and openRTB guru Gijs: ""Now we can finally give extensive answers to questions like whether the ad has started loading, the ad has finished loading completely, the ad is viewable, and if so, what percentage? We get a clear insight in the lifecycle of an ad. Mobile advertising will become even more transparent, measurable and grown up.""</p>

<p>Hold your horses! This is only the first draft of MRAID 3.0. Unfortunately, we will only be able to enjoy all this beauty by mid-2017, when IAB releases the final version. Does it look like I'm fobbing you off here? I get it, I can wait neither. If you want to chat a bit more about MRAID, have any questions, strong opinions or any feedback on MRAID 3.0, drop me a line in the comments. Can't stopping talking about this awesome stuff!</p>

<p><em>The original Dutch version of this post appeared on <a href=""https://blog.mobpro.com/2016/12/22/want-christmas-mraid/?utm_source=TMT"">mobpro.com</a>.</em></p>
        ","""Why does my ad work on the web but not in-app?"". The likely answer: MRAID. The same ads that work on websites do not work in an app. I've been asked what's up with this by both advertisers and publishers. The only question I ask them in return most of the time is whether they're serving a MRAID tag or not. After this, it remains silent on the other end of the line. M-what-raid?
What is MRAID?
MRAID stands for ""Mobile Rich Media Ad Interface Definitions"" and it is the IAB's program to define standards for in-app mobile rich media ads. It's a collection of agreements on how an app can show an ad properly, and how the ad can use the phone's features.
On the web, there are also these kinds of codes and guidelines on how to serve rich media ads. But both websites and ads on a smartphone, laptop, Android, IOS or a Microsoft device all speak the same universal language: JavaScript. Therefore it's pretty straightforward to communicate between the ad and the publisher. For instance, there's no challenge in having ad to play a video or open a new link (because the ad is also loaded in the browser ;).
But Apps don't speak JavaScript. Apps speak all kinds of languages (even within the same operating system): Java, Swift, Objective-C, C#, you name it. Our mobile ads do not speak all these languages, but still, we want our ad to be able to communicate with our phone and we want it to work on all kind of phones. That's exactly where MRAID comes in.
To enable the scaling of rich media ad serving in apps, IAB developed the first version of MRAID in 2011. Technically, MRAID is nothing more than an API (an interface that sends commands to software using code), that is supported by multiple SDK's (framework of an app). Because of this, an ad that is MRAID compliant will run in (almost) every app. To get your ad working in both web and in-app environments, you only have these two technical requirements.
For (mobile) web your ad needs to be created using HTML5 + JavaScript
On top of this, to make your ad work in-app, you need to make your creative MRAID compliant by integrating the MRAID library.
In another post, we'll get into the technical details on how to make your ads MRAID compliant, and how to make your ad communicate with a user's mobile phone or tablet.
MRAID 3.0
Over 5 years after IAB introduced the first version, they recently uncovered a public draft of the standard: MRAID 3.0. This is some real insider's information. To all you app professionals, I promise you're going to absolutely love this! I'm a huge fan! Let's see what they have in store for us:
VPAID and MRAID
The guidelines for MRAID and VPAID (Video Player Ad Interface Definition), the MRAID for in-stream video, are merged into one document. This makes a lot of sense because also with video it's quite a challenge to serve (and measure) ads at scale.
Ad-readiness
This upgrade will prevent the user from seeing white and black ad slots ('blanks') instead of the ad. Before showing an ad, this functionality will first verify that the ad has been loaded completely.
Track audio
A lot of users have muted their mobile phone's audio, and, hence, they'll not hear the (video) ad's sound. However, when an advertiser still chooses to run audio in a creative, now at least the mute state can be tracked. You can detect when the audio is muted or unmuted, and when the volume is changed. This is very interesting, because in-banner videos are getting more and more important in archieving awareness goals.
Tracking viewability
My personal favorite and the biggest game changer in MRAID 3.0 definitely is the improvement of the in-app viewability tracking. In MRAID 2.0 it was already possible to read whether an ad was visible or not, but with MRAID 3.0 it's possible to track what percentage of the ad is viewable. By this new feature, the MRAID viewability measurement completely meets the criteria of the MRC guidelines for tracking in-app viewability. In another upcoming post we'll get into the technical specifics of tracking viewability with MRAID 3.0.
With version 3.0, advertisers will no longer be dependent on publishers with specific viewability trackers SDK's, like MOAT. After all, MRAID is a must-have for publishers with an app.
MRAID professionals: be patient, this is only the first draft
Like I said earlier in the spoiler: I'm very enthusiastic about MRAID 3.0 and it's on top of my Christmas list. It's very clear that IAB listened to the needs in the market and it lowers the hurdles for specialists, publishers, and advertisers.
The update was also very well received by developers. Colleague, and openRTB guru Gijs: ""Now we can finally give extensive answers to questions like whether the ad has started loading, the ad has finished loading completely, the ad is viewable, and if so, what percentage? We get a clear insight in the lifecycle of an ad. Mobile advertising will become even more transparent, measurable and grown up.""
Hold your horses! This is only the first draft of MRAID 3.0. Unfortunately, we will only be able to enjoy all this beauty by mid-2017, when IAB releases the final version. Does it look like I'm fobbing you off here? I get it, I can wait neither. If you want to chat a bit more about MRAID, have any questions, strong opinions or any feedback on MRAID 3.0, drop me a line in the comments. Can't stopping talking about this awesome stuff!
The original Dutch version of this post appeared on mobpro.com.","[Mobile advertising, Analytics, Online Advertising, Banners, In-app]"
5,About the Google Analytics client ID,/about-the-google-analytics-client-id/,"
            <p>One of the core concepts of Google Analytics is <a href=""https://developers.google.com/analytics/devguides/collection/analyticsjs/cookies-user-id"">the client ID</a>. You can leverage its workings to get interesting insights, e.g. <a href=""https://www.themarketingtechnologist.co/connecting-offline-sales-to-online-campaign-sources-with-google-analytics/"">connecting offline revenue connected to online campaign sources</a>. But it’s important to understand what it is. That’s what this post is for, to give you a better understanding of what it is by discussing three topics: what it is,  what it’s used for, and what it can be used for.</p>

<h2 id=""1itsacookie"">1 It’s a cookie</h2>

<p>First thing first, it’s a cookie. Every time a new user visits your website,  the Google Analytics tracking code will generate a cookie, the _ga cookie. This cookie lasts for a maximum of 2 years and holds the client ID.</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Dec/161213_tmt_clientid-1481614027217.PNG"" alt=""google analytics client id example""></p>

<p><em>A _ga cookie sample.</em></p>

<p>If the user returns to the website, the tracking code will update cookie so  it’ll last 2 years from your last activity onwards. </p>

<p>It also has some limitations because of the cookie based setup. Cookies are limited to a browser and a top level domain. If you visit the website from a new browser, you’ll get a new cookie. If you have a website called www.my-awesome-website.com any website on the .my-awesome-website.com top level domain can access the cookie information. And don’t forget that users can also delete cookies themselves.</p>

<p>With this in mind, it’s time to discuss to what Google Analytics uses it for.</p>

<h2 id=""2whatitsusedfor"">2 What it’s used for</h2>

<p>Google Analytics uses the client ID for two types of data: user insights and attribution modelling</p>

<h3 id=""21userinsights"">2.1 User insights</h3>

<p>The literal meaning of users in Google Analytics is <em>unique Google Analytics client IDs on your website</em>.  A user that visits your website on mobile and tablet will account for 2 users in your report. Another important dimension reported by Google is the Visitor type: new or returning. As you might guess, these values translate to <em>have seen this client ID before (returning)</em> and <em>haven’t seen this client id before (new)</em>. </p>

<h3 id=""22attributionmodelling"">2.2 Attribution modelling</h3>

<p>Google Analytics applies the last non-direct click model by default. This means that every direct session, meaning sessions without a source (so no organic search, referral or utm tagging) will be applied to the last known source of the past 6 months of that user. Again, if we translate this to how this utilises the client ID: </p>

<blockquote>
  <p>System registers session without a source. Has this client ID been on the website before? If so, what was it’s last known source in the past six months? If there is one, apply it to this direct session.</p>
</blockquote>

<p>Keep in mind that the real-time report will always show the user as direct if he doesn’t have any source information. Google Analytics will apply the source based on the model when the user is added to the standard reports. </p>

<h2 id=""3whatitcanbeusedfor"">3 What it can be used for</h2>

<p>There’s a neat trick that leverages the concepts discussed in chapter 2: you can attribute off-site data to your Google Analytics users. A good example is recruitment data. Users can often apply through the website, but the really important stuff happens off-site: will the person be invited over for an interview? Without offsite data, you’ll have these metrics:</p>

<ul>
<li>Sessions on website;</li>
<li>Vacancy page visited;</li>
<li>Application form visited; and</li>
<li>Application form submitted.</li>
</ul>

<p>The important step is missing:</p>

<ul>
<li>Invited for an interview.</li>
</ul>

<p>If you can send client ID with the submitted form date (e.g. via a hidden input field), you can use this client ID to send data (e.g. an event) to GA when they get invited for their interview. If you don’t add any source information, the event will be attributed to the user’s last known source: the source of his online application. This allows you to not only see how sources attribute to online form submissions but also to actual qualitative job applications, people who are invited over. </p>

<p>I’ll discuss a lean setup to the sample above in <a href=""https://www.themarketingtechnologist.co/a-lean-approach-to-better-recruitment-insights/"">a separate post</a>.</p>

<h2 id=""thegist"">The gist</h2>

<p>We can summarise the workings of the client ID in four key points:</p>

<ul>
<li>the Google Analytics client ID is a cookie that expires after two years of inactivity.</li>
<li>the number of users on your website is equal to the number of unique client IDs.</li>
<li>it determines if a user is new or returning.</li>
<li>It’s a key value in Google Analytics’ last non-direct click model.</li>
</ul>

<p>With this list in mind, you'll have a good understanding of the concept of the client ID.</p>
        ","One of the core concepts of Google Analytics is the client ID. You can leverage its workings to get interesting insights, e.g. connecting offline revenue connected to online campaign sources. But it’s important to understand what it is. That’s what this post is for, to give you a better understanding of what it is by discussing three topics: what it is, what it’s used for, and what it can be used for.
1 It’s a cookie
First thing first, it’s a cookie. Every time a new user visits your website, the Google Analytics tracking code will generate a cookie, the _ga cookie. This cookie lasts for a maximum of 2 years and holds the client ID.
A _ga cookie sample.
If the user returns to the website, the tracking code will update cookie so it’ll last 2 years from your last activity onwards.
It also has some limitations because of the cookie based setup. Cookies are limited to a browser and a top level domain. If you visit the website from a new browser, you’ll get a new cookie. If you have a website called www.my-awesome-website.com any website on the .my-awesome-website.com top level domain can access the cookie information. And don’t forget that users can also delete cookies themselves.
With this in mind, it’s time to discuss to what Google Analytics uses it for.
2 What it’s used for
Google Analytics uses the client ID for two types of data: user insights and attribution modelling
2.1 User insights
The literal meaning of users in Google Analytics is unique Google Analytics client IDs on your website. A user that visits your website on mobile and tablet will account for 2 users in your report. Another important dimension reported by Google is the Visitor type: new or returning. As you might guess, these values translate to have seen this client ID before (returning) and haven’t seen this client id before (new).
2.2 Attribution modelling
Google Analytics applies the last non-direct click model by default. This means that every direct session, meaning sessions without a source (so no organic search, referral or utm tagging) will be applied to the last known source of the past 6 months of that user. Again, if we translate this to how this utilises the client ID:
System registers session without a source. Has this client ID been on the website before? If so, what was it’s last known source in the past six months? If there is one, apply it to this direct session.
Keep in mind that the real-time report will always show the user as direct if he doesn’t have any source information. Google Analytics will apply the source based on the model when the user is added to the standard reports.
3 What it can be used for
There’s a neat trick that leverages the concepts discussed in chapter 2: you can attribute off-site data to your Google Analytics users. A good example is recruitment data. Users can often apply through the website, but the really important stuff happens off-site: will the person be invited over for an interview? Without offsite data, you’ll have these metrics:
Sessions on website;
Vacancy page visited;
Application form visited; and
Application form submitted.
The important step is missing:
Invited for an interview.
If you can send client ID with the submitted form date (e.g. via a hidden input field), you can use this client ID to send data (e.g. an event) to GA when they get invited for their interview. If you don’t add any source information, the event will be attributed to the user’s last known source: the source of his online application. This allows you to not only see how sources attribute to online form submissions but also to actual qualitative job applications, people who are invited over.
I’ll discuss a lean setup to the sample above in a separate post.
The gist
We can summarise the workings of the client ID in four key points:
the Google Analytics client ID is a cookie that expires after two years of inactivity.
the number of users on your website is equal to the number of unique client IDs.
it determines if a user is new or returning.
It’s a key value in Google Analytics’ last non-direct click model.
With this list in mind, you'll have a good understanding of the concept of the client ID.","[Analytics, google analytics, client id]"
6,Pause your Adwords campaigns automatically when a site is down with an Adwords script,/pause-your-adwords-campaigns-automatically-when-a-site-is-down/,"
            <p>When working in the field of SEA, one of your main goals is to get people to click on your ads. But what if the website is down, or not working properly because of a technical issue? That's a waste of that CPC, right? At Greenhouse Group we monitor all our clients' websites with uptime monitoring tools, like <a href=""http://www.uptimerobot.com"">Uptime Robot</a> and <a href=""http://www.pingdom.com"">Pingdom</a>, so when one of a site goes down we can - if necessary - immediately pause our campaigns. But what if the site breaks at night, or in the weekend? At moments we're away from our keyboards? We certainly don't want to be the ones that sneak away from a gathering with friends to pause our Adwords campaigns. </p>

<p>That's why we use the Uptime Robot API in a Adwords script to automatically pause our campaigns when we are not at our desks. You deserve the weekend off as well, so let me show you how it works.</p>

<h3 id=""createauptimerobotmonitor"">Create a Uptime Robot monitor</h3>

<p>In this example, we'll use Uptime Robot to check whether our website is still running or not. If you don't already have one, go to <a href=""http://www.uptimerobot.com"">uptimerobot.com</a> and create a new account for free. Now go to the dashboard and create a new monitor. This proces should all be pretty straightforward. We'll create a monitor for The Marketing Technologist: </p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Dec/uptime_robot-1480617021819.jpg"" alt="""" class=""full-img""></p>

<h3 id=""getauptimerobotapikey"">Get a Uptime Robot API key</h3>

<p>To access your monitor information from your Adwords script, you'll need a API key. In your Uptime Robot dashboard go to My Settings. In the bottom right you'll see the API Settings. </p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Dec/api_key-1480617339395.jpg"" alt="""" class=""full-img""></p>

<p>Copy the Main API key and save it somewhere. You'll need it later. </p>

<h3 id=""getthemonitorinformationinaadwordsscript"">Get the monitor information in a Adwords script</h3>

<p>Okey, let's dive into some code now. The first thing we want to do is call the Uptime Robot API to get information about the status of our website. Everytime we need to call a API from our script, we can use Adwords' great <a href=""https://developers.google.com/apps-script/reference/url-fetch/"">URL Fetch Service</a>. In the next snippet we load the monitor data and save it to the <code>reponse</code> variable. Make sure you replace the value of <code>uptimeRobotKey</code> with the key you saved in the previous step. </p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""kwd"">var</span><span class=""pln""> uptimeRobotKey </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""str"">'XXXXXXXXXXXXXX'</span><span class=""pun"">;</span><span class=""pln"">

</span><span class=""kwd"">function</span><span class=""pln""> main</span><span class=""pun"">()</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">  
  </span><span class=""kwd"">var</span><span class=""pln""> response </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""typ"">UrlFetchApp</span><span class=""pun"">.</span><span class=""pln"">fetch</span><span class=""pun"">(</span><span class=""str"">'https://api.uptimerobot.com/getMonitors?apiKey='</span><span class=""pun"">+</span><span class=""pln"">uptimeRobotKey</span><span class=""pun"">+</span><span class=""str"">'&amp;format=xml'</span><span class=""pun"">);</span><span class=""pln"">
  </span><span class=""kwd"">if</span><span class=""pln""> </span><span class=""pun"">(</span><span class=""pln"">response</span><span class=""pun"">.</span><span class=""pln"">getResponseCode</span><span class=""pun"">()</span><span class=""pln""> </span><span class=""pun"">!=</span><span class=""pln""> </span><span class=""lit"">200</span><span class=""pun"">)</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">
    </span><span class=""kwd"">throw</span><span class=""pln""> </span><span class=""typ"">Utilities</span><span class=""pun"">.</span><span class=""pln"">formatString</span><span class=""pun"">(</span><span class=""pln"">
      </span><span class=""str"">'Error returned by API: %s, Location searched: %s.'</span><span class=""pun"">,</span><span class=""pln"">
      response</span><span class=""pun"">.</span><span class=""pln"">getContentText</span><span class=""pun"">(),</span><span class=""pln""> location</span><span class=""pun"">);</span><span class=""pln"">
  </span><span class=""pun"">}</span><span class=""pln"">
</span><span class=""pun"">}</span></code></pre>

<p>The value of <code>reponse</code> now holds the monitor data in XML format. It looks something like this:  </p>

<pre><code>&lt;monitors offset=""0"" limit=""50"" total=""1""&gt;  
&lt;monitor id=""XXXXXXX"" friendlyname=""The Marketing Technologist"" url=""https://www.themarketingtechnologist.co"" type=""1"" subtype="""" keywordtype="""" keywordvalue="""" httpusername="""" httppassword="""" port="""" interval=""300"" status=""2"" alltimeuptimeratio=""99.99""/&gt;  
&lt;/monitors&gt;  
</code></pre>

<p>We get the information about all our monitors that we've set set up in our Uptime Robot account. This is because we've used the Main API key earlier. To only get a specific monitor in the response, create a API key for a specific monitor. For now, because we only have one monitor in our account, we'll continue as is. Luckily, it's very easy to work with XML in Adwords script with the XMLService. We can get all monitors available as JavaScript by using this simple snippet:</p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""kwd"">var</span><span class=""pln""> document </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""typ"">XmlService</span><span class=""pun"">.</span><span class=""pln"">parse</span><span class=""pun"">(</span><span class=""pln"">response</span><span class=""pun"">.</span><span class=""pln"">getContentText</span><span class=""pun"">());</span><span class=""pln"">  
</span><span class=""kwd"">var</span><span class=""pln""> root </span><span class=""pun"">=</span><span class=""pln""> document</span><span class=""pun"">.</span><span class=""pln"">getRootElement</span><span class=""pun"">();</span><span class=""pln"">  
</span><span class=""kwd"">var</span><span class=""pln""> entries </span><span class=""pun"">=</span><span class=""pln""> root</span><span class=""pun"">.</span><span class=""pln"">getChildren</span><span class=""pun"">(</span><span class=""str"">'monitor'</span><span class=""pun"">);</span><span class=""pln"">  </span></code></pre>

<h3 id=""checkyoursitesstatus"">Check your site's status</h3>

<p>In the XML response of Uptime Robot there's a <code>status</code> attribute for every monitor. This status contains the value 9 if the site is down, so we'd check for that value in our code. Before checking whether your site is up or down, we need the specify what URL's we should check for in our script. </p>

<pre><code>var domains = ['http://www.themarketingtechnologist.co'];  
</code></pre>

<p>Now we can loop through the monitors and see if the site is down or not.  </p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""kwd"">var</span><span class=""pln""> hasDownDomain </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""kwd"">false</span><span class=""pun"">;</span><span class=""pln"">  
</span><span class=""kwd"">for</span><span class=""pln""> </span><span class=""pun"">(</span><span class=""kwd"">var</span><span class=""pln""> i </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""lit"">0</span><span class=""pun"">;</span><span class=""pln""> i </span><span class=""pun"">&lt;</span><span class=""pln""> entries</span><span class=""pun"">.</span><span class=""pln"">length</span><span class=""pun"">;</span><span class=""pln""> i</span><span class=""pun"">++)</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">  
  </span><span class=""kwd"">var</span><span class=""pln""> url </span><span class=""pun"">=</span><span class=""pln""> entries</span><span class=""pun"">[</span><span class=""pln"">i</span><span class=""pun"">].</span><span class=""pln"">getAttribute</span><span class=""pun"">(</span><span class=""str"">'url'</span><span class=""pun"">).</span><span class=""pln"">getValue</span><span class=""pun"">();</span><span class=""pln"">
  </span><span class=""kwd"">var</span><span class=""pln""> status </span><span class=""pun"">=</span><span class=""pln""> entries</span><span class=""pun"">[</span><span class=""pln"">i</span><span class=""pun"">].</span><span class=""pln"">getAttribute</span><span class=""pun"">(</span><span class=""str"">'status'</span><span class=""pun"">).</span><span class=""pln"">getValue</span><span class=""pun"">();</span><span class=""pln"">
  </span><span class=""com"">// Uptime Robots status code for down is 9</span><span class=""pln"">
  </span><span class=""kwd"">var</span><span class=""pln""> looksDown </span><span class=""pun"">=</span><span class=""pln""> status </span><span class=""pun"">==</span><span class=""pln""> </span><span class=""str"">'9'</span><span class=""pun"">;</span><span class=""pln"">

  </span><span class=""com"">// Check if the site is down and whether it's a site we want to check</span><span class=""pln"">
  </span><span class=""kwd"">if</span><span class=""pln""> </span><span class=""pun"">(</span><span class=""pln"">looksDown </span><span class=""pun"">&amp;&amp;</span><span class=""pln""> domains</span><span class=""pun"">.</span><span class=""pln"">indexOf</span><span class=""pun"">(</span><span class=""pln"">url</span><span class=""pun"">)</span><span class=""pln""> </span><span class=""pun"">&gt;</span><span class=""pln""> </span><span class=""pun"">-</span><span class=""lit"">1</span><span class=""pun"">)</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">
    hasDownDomain </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""kwd"">true</span><span class=""pun"">;</span><span class=""pln"">
    </span><span class=""typ"">Logger</span><span class=""pun"">.</span><span class=""pln"">log</span><span class=""pun"">(</span><span class=""pln"">url </span><span class=""pun"">+</span><span class=""pln""> </span><span class=""str"">' is down. We\'re going to pause the campaigns'</span><span class=""pun"">);</span><span class=""pln"">
  </span><span class=""pun"">}</span><span class=""pln"">
</span><span class=""pun"">}</span></code></pre>

<p>Now we can simple check for the value of <code>hasDownDomain</code> to see if one of our sites is down and what action we should take.  </p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""kwd"">if</span><span class=""pln""> </span><span class=""pun"">(</span><span class=""pln"">hasDownDomain</span><span class=""pun"">)</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">  
  pauseCampaign</span><span class=""pun"">();</span><span class=""pln"">
</span><span class=""pun"">}</span><span class=""pln""> </span><span class=""kwd"">else</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">
  enableCampaign</span><span class=""pun"">();</span><span class=""pln"">
</span><span class=""pun"">}</span></code></pre>

<h3 id=""pausingcampaigns"">Pausing campaigns</h3>

<p>The implementation of the <code>pauseCampaign</code> function is pretty straightforward. We loop through all campaigns, check if the campaign is active and if so, pause it.  </p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""kwd"">function</span><span class=""pln""> pauseCampaign</span><span class=""pun"">()</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">  
  </span><span class=""kwd"">var</span><span class=""pln""> campaignIterator </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""typ"">AdWordsApp</span><span class=""pun"">.</span><span class=""pln"">campaigns</span><span class=""pun"">().</span><span class=""kwd"">get</span><span class=""pun"">();</span><span class=""pln"">
  </span><span class=""com"">// Go over all campaigns </span><span class=""pln"">
  </span><span class=""kwd"">while</span><span class=""pln""> </span><span class=""pun"">(</span><span class=""pln"">campaignIterator</span><span class=""pun"">.</span><span class=""pln"">hasNext</span><span class=""pun"">())</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">
    </span><span class=""kwd"">var</span><span class=""pln""> campaign </span><span class=""pun"">=</span><span class=""pln""> campaignIterator</span><span class=""pun"">.</span><span class=""kwd"">next</span><span class=""pun"">();</span><span class=""pln"">
    </span><span class=""com"">// Check if the campaign is active</span><span class=""pln"">
    </span><span class=""kwd"">if</span><span class=""pln""> </span><span class=""pun"">(!</span><span class=""pln"">campaign</span><span class=""pun"">.</span><span class=""pln"">isPaused</span><span class=""pun"">())</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">
      </span><span class=""com"">// Pause campaign</span><span class=""pln"">
      campaign</span><span class=""pun"">.</span><span class=""pln"">pause</span><span class=""pun"">();</span><span class=""pln"">
      </span><span class=""typ"">Logger</span><span class=""pun"">.</span><span class=""pln"">log</span><span class=""pun"">(</span><span class=""str"">""campaign ""</span><span class=""pln""> </span><span class=""pun"">+</span><span class=""pln""> campaign</span><span class=""pun"">.</span><span class=""pln"">getName</span><span class=""pun"">()</span><span class=""pln""> </span><span class=""pun"">+</span><span class=""pln""> </span><span class=""str"">"" was paused.""</span><span class=""pun"">);</span><span class=""pln"">
    </span><span class=""pun"">}</span><span class=""pln"">
  </span><span class=""pun"">}</span><span class=""pln"">
</span><span class=""pun"">}</span></code></pre>

<h3 id=""enablecampaigns"">Enable campaigns</h3>

<p>When your site gets up again, of course we want to re-enable all campaigns again. The code to do so looks very similar:</p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""kwd"">function</span><span class=""pln""> enableCampaign</span><span class=""pun"">()</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">  
  </span><span class=""kwd"">var</span><span class=""pln""> campaignIterator </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""typ"">AdWordsApp</span><span class=""pun"">.</span><span class=""pln"">campaigns</span><span class=""pun"">().</span><span class=""kwd"">get</span><span class=""pun"">();</span><span class=""pln"">
  </span><span class=""com"">// Go over all campaigns </span><span class=""pln"">
  </span><span class=""kwd"">while</span><span class=""pln""> </span><span class=""pun"">(</span><span class=""pln"">campaignIterator</span><span class=""pun"">.</span><span class=""pln"">hasNext</span><span class=""pun"">())</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">
    </span><span class=""kwd"">var</span><span class=""pln""> campaign </span><span class=""pun"">=</span><span class=""pln""> campaignIterator</span><span class=""pun"">.</span><span class=""kwd"">next</span><span class=""pun"">();</span><span class=""pln"">
    </span><span class=""com"">// Check if the campaign is paused</span><span class=""pln"">
    </span><span class=""kwd"">if</span><span class=""pln""> </span><span class=""pun"">(</span><span class=""pln"">campaign</span><span class=""pun"">.</span><span class=""pln"">isPaused</span><span class=""pun"">())</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">
      </span><span class=""com"">// Enable campaign</span><span class=""pln"">
      campaign</span><span class=""pun"">.</span><span class=""pln"">enable</span><span class=""pun"">();</span><span class=""pln"">
      </span><span class=""typ"">Logger</span><span class=""pun"">.</span><span class=""pln"">log</span><span class=""pun"">(</span><span class=""str"">""campaign ""</span><span class=""pln""> </span><span class=""pun"">+</span><span class=""pln""> campaign</span><span class=""pun"">.</span><span class=""pln"">getName</span><span class=""pun"">()</span><span class=""pln""> </span><span class=""pun"">+</span><span class=""pln""> </span><span class=""str"">"" was enabled.""</span><span class=""pun"">);</span><span class=""pln"">
    </span><span class=""pun"">}</span><span class=""pln"">
  </span><span class=""pun"">}</span><span class=""pln"">
</span><span class=""pun"">}</span></code></pre>

<h3 id=""keeptrackofpausedcampaigns"">Keep track of paused campaigns</h3>

<p>There's one big problem with this implementation. When you'd already paused a campaign manually, the script will automatically re-enable it when your site gets up again. That's not what we want! We only want the script to re-enable the campaigns it paused earlier. Here's where <a href=""https://support.google.com/adwords/answer/2475865?hl=en"">Labels</a> come in. We're going to add a label <code>campaign_paused_by_code</code> to every campaign we pause. Before we can do so, we need to create the label in our Adwords account. </p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Dec/labels-1480623104333.png"" alt="""" class=""full-img"">
<em>Labels Admin section - Image from support.google.com</em></p>

<p>After you added the label, we can use it in our code. First, we add the label in our code where we also pause it (<code>pauseCampaigns</code>). </p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""pun"">...</span><span class=""pln"">
campaign</span><span class=""pun"">.</span><span class=""pln"">applyLabel</span><span class=""pun"">(</span><span class=""str"">'campaign_paused_by_code'</span><span class=""pun"">);</span><span class=""pln"">  
campaign</span><span class=""pun"">.</span><span class=""pln"">pause</span><span class=""pun"">();</span><span class=""pln"">  
</span><span class=""pun"">...</span></code></pre>

<p>In the <code>enableCampaigns</code> function, before removing the label, we should do a check if the campaign has the label.  </p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""pun"">...</span><span class=""pln"">
</span><span class=""kwd"">var</span><span class=""pln""> labelIter </span><span class=""pun"">=</span><span class=""pln""> campaign</span><span class=""pun"">.</span><span class=""pln"">labels</span><span class=""pun"">().</span><span class=""pln"">withCondition</span><span class=""pun"">(</span><span class=""str"">""Name = 'campaign_paused_by_code'""</span><span class=""pun"">).</span><span class=""kwd"">get</span><span class=""pun"">();</span><span class=""pln"">  
</span><span class=""com"">// Check if the label exists in this campaign</span><span class=""pln"">
</span><span class=""kwd"">if</span><span class=""pln""> </span><span class=""pun"">(</span><span class=""pln"">labelIter</span><span class=""pun"">.</span><span class=""pln"">hasNext</span><span class=""pun"">())</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">  
  campaign</span><span class=""pun"">.</span><span class=""pln"">enable</span><span class=""pun"">();</span><span class=""pln"">
  campaign</span><span class=""pun"">.</span><span class=""pln"">removeLabel</span><span class=""pun"">(</span><span class=""str"">'uptime_paused'</span><span class=""pun"">);</span><span class=""pln"">
  </span><span class=""typ"">Logger</span><span class=""pun"">.</span><span class=""pln"">log</span><span class=""pun"">(</span><span class=""str"">""campaign ""</span><span class=""pln""> </span><span class=""pun"">+</span><span class=""pln""> campaign</span><span class=""pun"">.</span><span class=""pln"">getName</span><span class=""pun"">()</span><span class=""pln""> </span><span class=""pun"">+</span><span class=""pln""> </span><span class=""str"">"" was enabled.""</span><span class=""pun"">);</span><span class=""pln"">
</span><span class=""pun"">}</span><span class=""pln"">
</span><span class=""pun"">...</span></code></pre>

<h3 id=""onlyexecutescriptoutsideofficehours"">Only execute script outside office hours</h3>

<p>During business hours we manually want to pause our campaigns, so we don't want to run the script then. I created a small function that checks if it's weekend, or evening:</p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""kwd"">function</span><span class=""pln""> isDuringBusinessHours</span><span class=""pun"">()</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">  
  </span><span class=""kwd"">var</span><span class=""pln""> businessHours </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""pun"">[</span><span class=""lit"">8</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""lit"">17</span><span class=""pun"">];</span><span class=""pln"">
  </span><span class=""kwd"">var</span><span class=""pln""> today </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""kwd"">new</span><span class=""pln""> </span><span class=""typ"">Date</span><span class=""pun"">();</span><span class=""pln"">
  </span><span class=""kwd"">var</span><span class=""pln""> day </span><span class=""pun"">=</span><span class=""pln""> today</span><span class=""pun"">.</span><span class=""pln"">getDay</span><span class=""pun"">();</span><span class=""pln"">
  </span><span class=""kwd"">var</span><span class=""pln""> hour </span><span class=""pun"">=</span><span class=""pln""> today</span><span class=""pun"">.</span><span class=""pln"">getUTCHours</span><span class=""pun"">()</span><span class=""pln""> </span><span class=""pun"">+</span><span class=""pln""> </span><span class=""lit"">1</span><span class=""pun"">;</span><span class=""pln""> </span><span class=""com"">// UTC</span><span class=""pln"">
  </span><span class=""kwd"">if</span><span class=""pln""> </span><span class=""pun"">([</span><span class=""lit"">0</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""lit"">6</span><span class=""pun"">].</span><span class=""pln"">indexOf</span><span class=""pun"">(</span><span class=""pln"">day</span><span class=""pun"">)</span><span class=""pln""> </span><span class=""pun"">&gt;</span><span class=""pln""> </span><span class=""pun"">-</span><span class=""lit"">1</span><span class=""pun"">)</span><span class=""pln""> </span><span class=""kwd"">return</span><span class=""pln""> </span><span class=""kwd"">false</span><span class=""pun"">;</span><span class=""pln""> </span><span class=""com"">// weekend</span><span class=""pln"">
  </span><span class=""kwd"">return</span><span class=""pln""> hour </span><span class=""pun"">&gt;=</span><span class=""pln""> businessHours</span><span class=""pun"">[</span><span class=""lit"">0</span><span class=""pun"">]</span><span class=""pln""> </span><span class=""pun"">&amp;&amp;</span><span class=""pln""> hour </span><span class=""pun"">&lt;</span><span class=""pln""> businessHours</span><span class=""pun"">[</span><span class=""lit"">1</span><span class=""pun"">];</span><span class=""pln"">
</span><span class=""pun"">}</span></code></pre>

<p>Now at the beginning of the <code>main</code> function, we can run this check:  </p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""kwd"">function</span><span class=""pln""> main</span><span class=""pun"">()</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">  
  </span><span class=""kwd"">if</span><span class=""pln""> </span><span class=""pun"">(</span><span class=""pln"">isDuringBusinessHours</span><span class=""pun"">())</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">
    </span><span class=""com"">// Don't run the script when we're during business hours</span><span class=""pln"">
    </span><span class=""kwd"">return</span><span class=""pun"">;</span><span class=""pln"">
  </span><span class=""pun"">}</span><span class=""pln"">
  </span><span class=""pun"">...</span><span class=""pln"">
</span><span class=""pun"">}</span></code></pre>

<p>This should be all the code you need to pause and enable your campaigns. To see the complete script, check <a href=""https://gist.github.com/codeorelse/668fc7593c64e4a2cc74fe1e656fc813"">this Github gist</a>. </p>

<h3 id=""bonussendamailwhencampaignsarepausedenabled"">Bonus: send a mail when campaigns are paused/enabled</h3>

<p>You might want to know what happens when you're not in control of the campaigns at night. Therefore it's wise to let the script send you an email when it paused or enabled campaigns. This is very easy with Adwords <a href=""https://developers.google.com/adwords/scripts/docs/examples/mailapp"">MailApp</a> utility. First we create a function that sends the email.</p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""kwd"">function</span><span class=""pln""> sendMail</span><span class=""pun"">(</span><span class=""pln"">subject</span><span class=""pun"">,</span><span class=""pln""> content</span><span class=""pun"">)</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">  
  </span><span class=""typ"">MailApp</span><span class=""pun"">.</span><span class=""pln"">sendEmail</span><span class=""pun"">(</span><span class=""str"">'youremail@domain.com'</span><span class=""pun"">,</span><span class=""pln""> subject</span><span class=""pun"">,</span><span class=""pln""> content</span><span class=""pun"">);</span><span class=""pln"">
</span><span class=""pun"">}</span></code></pre>

<p>Next we can use this in our <code>enableCampaigns</code> and <code>pauseCampaigns</code> functions. Just like this:  </p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""pln"">sendMail</span><span class=""pun"">(</span><span class=""str"">'Campaigns were paused'</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""str"">'Some campaigns were paused because the website is down.'</span><span class=""pln"">  </span></code></pre>

<p>You might want to consider to put some more useful information in the email (What campaigns were paused? What site was down? etc), but I think you get the idea. </p>

<h3 id=""nextsteps"">Next steps</h3>

<p>With Uptime Robot we only check whether a site responds properly or not. But in some cases we also want to check if the website's sales funnel is working properly. Therefore, we use Pingdom's transaction checks, which allow us to check the flow of a website. We've also automated this using Pingdom's API, but that something we'll get into in a next post. </p>

<p>Also, one big flaw in this script is that it only runs every hour because of Adword's limitation. Therefore, if a site is only down for 5 minutes, your campaigns will be paused for a full hour. To overcome this, we can create a custom service that runs outside of Adwords that pauses/enables the campaigns using the Adwords API, and schedule it every 5 minutes. This would be a bit more work to setup, but definitely a good next step. </p>

<p>Internally we managed to partly solve this flaw by using a tool called <a href=""https://adjestic.com/"">Adjestic</a>. Adjestic is developed internally and helps our marketeers to improve their campaigns and optimize budgets. One of the features is nearly real-time status and stock checking. The tool achieves this by tracking 'final urls'. When it detects downtime, it will incease the check frequency for that specfic url. This way we decrease the chance of accidentally pausing campaings.</p>
        ","When working in the field of SEA, one of your main goals is to get people to click on your ads. But what if the website is down, or not working properly because of a technical issue? That's a waste of that CPC, right? At Greenhouse Group we monitor all our clients' websites with uptime monitoring tools, like Uptime Robot and Pingdom, so when one of a site goes down we can - if necessary - immediately pause our campaigns. But what if the site breaks at night, or in the weekend? At moments we're away from our keyboards? We certainly don't want to be the ones that sneak away from a gathering with friends to pause our Adwords campaigns.
That's why we use the Uptime Robot API in a Adwords script to automatically pause our campaigns when we are not at our desks. You deserve the weekend off as well, so let me show you how it works.
Create a Uptime Robot monitor
In this example, we'll use Uptime Robot to check whether our website is still running or not. If you don't already have one, go to uptimerobot.com and create a new account for free. Now go to the dashboard and create a new monitor. This proces should all be pretty straightforward. We'll create a monitor for The Marketing Technologist:
Get a Uptime Robot API key
To access your monitor information from your Adwords script, you'll need a API key. In your Uptime Robot dashboard go to My Settings. In the bottom right you'll see the API Settings.
Copy the Main API key and save it somewhere. You'll need it later.
Get the monitor information in a Adwords script
Okey, let's dive into some code now. The first thing we want to do is call the Uptime Robot API to get information about the status of our website. Everytime we need to call a API from our script, we can use Adwords' great URL Fetch Service. In the next snippet we load the monitor data and save it to the reponse variable. Make sure you replace the value of uptimeRobotKey with the key you saved in the previous step.
var uptimeRobotKey = 'XXXXXXXXXXXXXX';

function main() {  
  var response = UrlFetchApp.fetch('https://api.uptimerobot.com/getMonitors?apiKey='+uptimeRobotKey+'&format=xml');
  if (response.getResponseCode() != 200) {
    throw Utilities.formatString(
      'Error returned by API: %s, Location searched: %s.',
      response.getContentText(), location);
  }
}
The value of reponse now holds the monitor data in XML format. It looks something like this:
<monitors offset=""0"" limit=""50"" total=""1"">  
<monitor id=""XXXXXXX"" friendlyname=""The Marketing Technologist"" url=""https://www.themarketingtechnologist.co"" type=""1"" subtype="""" keywordtype="""" keywordvalue="""" httpusername="""" httppassword="""" port="""" interval=""300"" status=""2"" alltimeuptimeratio=""99.99""/>  
</monitors>  
We get the information about all our monitors that we've set set up in our Uptime Robot account. This is because we've used the Main API key earlier. To only get a specific monitor in the response, create a API key for a specific monitor. For now, because we only have one monitor in our account, we'll continue as is. Luckily, it's very easy to work with XML in Adwords script with the XMLService. We can get all monitors available as JavaScript by using this simple snippet:
var document = XmlService.parse(response.getContentText());  
var root = document.getRootElement();  
var entries = root.getChildren('monitor');  
Check your site's status
In the XML response of Uptime Robot there's a status attribute for every monitor. This status contains the value 9 if the site is down, so we'd check for that value in our code. Before checking whether your site is up or down, we need the specify what URL's we should check for in our script.
var domains = ['http://www.themarketingtechnologist.co'];  
Now we can loop through the monitors and see if the site is down or not.
var hasDownDomain = false;  
for (var i = 0; i < entries.length; i++) {  
  var url = entries[i].getAttribute('url').getValue();
  var status = entries[i].getAttribute('status').getValue();
  // Uptime Robots status code for down is 9
  var looksDown = status == '9';

  // Check if the site is down and whether it's a site we want to check
  if (looksDown && domains.indexOf(url) > -1) {
    hasDownDomain = true;
    Logger.log(url + ' is down. We\'re going to pause the campaigns');
  }
}
Now we can simple check for the value of hasDownDomain to see if one of our sites is down and what action we should take.
if (hasDownDomain) {  
  pauseCampaign();
} else {
  enableCampaign();
}
Pausing campaigns
The implementation of the pauseCampaign function is pretty straightforward. We loop through all campaigns, check if the campaign is active and if so, pause it.
function pauseCampaign() {  
  var campaignIterator = AdWordsApp.campaigns().get();
  // Go over all campaigns 
  while (campaignIterator.hasNext()) {
    var campaign = campaignIterator.next();
    // Check if the campaign is active
    if (!campaign.isPaused()) {
      // Pause campaign
      campaign.pause();
      Logger.log(""campaign "" + campaign.getName() + "" was paused."");
    }
  }
}
Enable campaigns
When your site gets up again, of course we want to re-enable all campaigns again. The code to do so looks very similar:
function enableCampaign() {  
  var campaignIterator = AdWordsApp.campaigns().get();
  // Go over all campaigns 
  while (campaignIterator.hasNext()) {
    var campaign = campaignIterator.next();
    // Check if the campaign is paused
    if (campaign.isPaused()) {
      // Enable campaign
      campaign.enable();
      Logger.log(""campaign "" + campaign.getName() + "" was enabled."");
    }
  }
}
Keep track of paused campaigns
There's one big problem with this implementation. When you'd already paused a campaign manually, the script will automatically re-enable it when your site gets up again. That's not what we want! We only want the script to re-enable the campaigns it paused earlier. Here's where Labels come in. We're going to add a label campaign_paused_by_code to every campaign we pause. Before we can do so, we need to create the label in our Adwords account.
Labels Admin section - Image from support.google.com
After you added the label, we can use it in our code. First, we add the label in our code where we also pause it (pauseCampaigns).
...
campaign.applyLabel('campaign_paused_by_code');  
campaign.pause();  
...
In the enableCampaigns function, before removing the label, we should do a check if the campaign has the label.
...
var labelIter = campaign.labels().withCondition(""Name = 'campaign_paused_by_code'"").get();  
// Check if the label exists in this campaign
if (labelIter.hasNext()) {  
  campaign.enable();
  campaign.removeLabel('uptime_paused');
  Logger.log(""campaign "" + campaign.getName() + "" was enabled."");
}
...
Only execute script outside office hours
During business hours we manually want to pause our campaigns, so we don't want to run the script then. I created a small function that checks if it's weekend, or evening:
function isDuringBusinessHours() {  
  var businessHours = [8, 17];
  var today = new Date();
  var day = today.getDay();
  var hour = today.getUTCHours() + 1; // UTC
  if ([0, 6].indexOf(day) > -1) return false; // weekend
  return hour >= businessHours[0] && hour < businessHours[1];
}
Now at the beginning of the main function, we can run this check:
function main() {  
  if (isDuringBusinessHours()) {
    // Don't run the script when we're during business hours
    return;
  }
  ...
}
This should be all the code you need to pause and enable your campaigns. To see the complete script, check this Github gist.
Bonus: send a mail when campaigns are paused/enabled
You might want to know what happens when you're not in control of the campaigns at night. Therefore it's wise to let the script send you an email when it paused or enabled campaigns. This is very easy with Adwords MailApp utility. First we create a function that sends the email.
function sendMail(subject, content) {  
  MailApp.sendEmail('youremail@domain.com', subject, content);
}
Next we can use this in our enableCampaigns and pauseCampaigns functions. Just like this:
sendMail('Campaigns were paused', 'Some campaigns were paused because the website is down.'  
You might want to consider to put some more useful information in the email (What campaigns were paused? What site was down? etc), but I think you get the idea.
Next steps
With Uptime Robot we only check whether a site responds properly or not. But in some cases we also want to check if the website's sales funnel is working properly. Therefore, we use Pingdom's transaction checks, which allow us to check the flow of a website. We've also automated this using Pingdom's API, but that something we'll get into in a next post.
Also, one big flaw in this script is that it only runs every hour because of Adword's limitation. Therefore, if a site is only down for 5 minutes, your campaigns will be paused for a full hour. To overcome this, we can create a custom service that runs outside of Adwords that pauses/enables the campaigns using the Adwords API, and schedule it every 5 minutes. This would be a bit more work to setup, but definitely a good next step.
Internally we managed to partly solve this flaw by using a tool called Adjestic. Adjestic is developed internally and helps our marketeers to improve their campaigns and optimize budgets. One of the features is nearly real-time status and stock checking. The tool achieves this by tracking 'final urls'. When it detects downtime, it will incease the check frequency for that specfic url. This way we decrease the chance of accidentally pausing campaings.","[Code, SEA, Adwords, JavaScript, Uptime Robot, Monitoring]"
7,How to check a GA account in less than 10 minutes,/how-to-check-a-ga-account-in-less-than-10-minutes/,"
            <p>When you get access to a new Google Analytics account, you want to know if it’s setup correctly. In the long run, you’ll want to do a thorough analysis, but for a quick indication of the status, you can look at 4 key features. </p>

<p><em>This post focusses on checking existing Google Analytics accounts. The same features are important  to configure when you set up a new account.</em></p>

<h2 id=""1hostnames"">1 Hostnames</h2>

<p>The first step is to check the hostnames. The hostnames are the domains your GA tracker is receiving data from. Make sure that only data to the live domains of your website is coming in, e.g. www.yourwebsite.com and shop.yourwebsite.com. If there are more domains in this report, make sure to remove them from your data set with filters (see 4).</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Nov/tmt_hostnames_good-1479803874876.PNG"" alt=""TMT Hostnames""></p>

<p><em>For TMT, the hostname report should only return www.themarketingtechnologist.co</em></p>

<h2 id=""2checkreferrals"">2 Check referrals</h2>

<p>The second important step is to check your referral sources. You should make sure that:</p>

<ul>
<li>No data is coming in from internal referrals (the domains that you want to see in your hostnames report).</li>
<li>No data is coming in from payment websites. This is often the second to last page of your e-commerce funnel. If you payment sites pop up as referrals, you overwrite the real source of your purchase: the source the user started his visit with. </li>
</ul>

<p>You can fix referral issues from the Admin part of your account. Go to Property &gt; Property settings &gt; Referral exclusion list to exclude all internal and payment referrals. If you're comfortable using JavaScript, you can also use <a href=""http://www.simoahava.com/gtm-tips/referral-exclusion-on-receipt-page/"">this trick</a> to automatically exclude referrals on receipt pages. </p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Nov/tmt_referral_good-1479804031705.PNG"" alt=""TMT referrals"" class=""full-img""></p>

<p><em>Looking for your own domain in the referral report should return zero results.</em></p>

<h2 id=""3goalsandecommerce"">3 Goals and e-commerce</h2>

<p>This is a key part of the scan. It tells you if the people who manage the account put an effort in capturing key metrics and e-commerce data (if the website has a shop).  When you review the goal section, look for two types of goals:</p>

<ul>
<li><strong>End goals</strong>: these goals capture the end of a flow, e.g. a newsletter subscription or thank you page of an e-commerce flow.</li>
<li><strong>Micro conversions</strong>: these goals capture actions that a user must take to reach the end goal, e.g. view a product, basket and personal details page. </li>
</ul>

<p>The goals help you understand how well the website is performing.</p>

<p><strong>If it is an e-commerce site</strong></p>

<p>A goal tells you how many of users reach the final page your checkout process. The e-commerce data tells you what users actually bought. The reports should at least contain product names, categories, SKUs/IDs and revenue. If anything looks odd, make sure to check it with the site owner. As a next step, you can check the data with backend data from the order system. It's okay if the difference with Google Analytics is 5-10%. </p>

<h2 id=""4filtersviews"">4 Filters &amp; views</h2>

<p>The final step is to check filters and views. Every GA property should have at least two views:</p>

<ul>
<li>Filtered view;</li>
<li>Unfiltered view (the backup view).</li>
</ul>

<p>The filtered view excludes all data that you don’t want. This includes data that is registered to unwanted hostnames (see point 1). You can exclude these with a filter that only includes the hostnames that you need:</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Nov/tmt_hostname_filter-1479804289408.PNG"" alt=""TMT hostname filter""></p>

<p><em>TMT hostname filter.</em></p>

<p>Besides that, you'll want to exclude traffic from known IP addresses. You can exclude these with an IP filter:</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Nov/tmt_exclude_ip-1479804316812.PNG"" alt=""TMT IP filter""></p>

<p><em>IP filter sample.</em></p>

<p>Lastly, the view should have the ‘Exclude all known bots and spiders’ box checked in the view settings:</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Nov/tmt_exclude_bots-1479804361905.PNG"" alt=""TMT exclude bots""></p>

<p><em>Exclude known bots filter.</em></p>

<p>If these are all present, you probably have a decently filtered view. The unfiltered view should have no filters. This view functions as a backup. If you apply a bad filter to your filtered view, you'll always have raw unfiltered data at your disposal. You might see a third view used for testing new filters, and other views for sub-parts of your website (e.g. a shop view filtered on shop.website.com). </p>

<h2 id=""thatsallfolks"">That’s all folks</h2>

<p>With this knowledge in mind, you can quickly scan a GA account. It’ll probably take you less than 10 minutes. Keep in mind that this only gives you a very high overview the status of the account. After this, you’ll want to make sure that is missing will be added. </p>
        ","When you get access to a new Google Analytics account, you want to know if it’s setup correctly. In the long run, you’ll want to do a thorough analysis, but for a quick indication of the status, you can look at 4 key features.
This post focusses on checking existing Google Analytics accounts. The same features are important to configure when you set up a new account.
1 Hostnames
The first step is to check the hostnames. The hostnames are the domains your GA tracker is receiving data from. Make sure that only data to the live domains of your website is coming in, e.g. www.yourwebsite.com and shop.yourwebsite.com. If there are more domains in this report, make sure to remove them from your data set with filters (see 4).
For TMT, the hostname report should only return www.themarketingtechnologist.co
2 Check referrals
The second important step is to check your referral sources. You should make sure that:
No data is coming in from internal referrals (the domains that you want to see in your hostnames report).
No data is coming in from payment websites. This is often the second to last page of your e-commerce funnel. If you payment sites pop up as referrals, you overwrite the real source of your purchase: the source the user started his visit with.
You can fix referral issues from the Admin part of your account. Go to Property > Property settings > Referral exclusion list to exclude all internal and payment referrals. If you're comfortable using JavaScript, you can also use this trick to automatically exclude referrals on receipt pages.
Looking for your own domain in the referral report should return zero results.
3 Goals and e-commerce
This is a key part of the scan. It tells you if the people who manage the account put an effort in capturing key metrics and e-commerce data (if the website has a shop). When you review the goal section, look for two types of goals:
End goals: these goals capture the end of a flow, e.g. a newsletter subscription or thank you page of an e-commerce flow.
Micro conversions: these goals capture actions that a user must take to reach the end goal, e.g. view a product, basket and personal details page.
The goals help you understand how well the website is performing.
If it is an e-commerce site
A goal tells you how many of users reach the final page your checkout process. The e-commerce data tells you what users actually bought. The reports should at least contain product names, categories, SKUs/IDs and revenue. If anything looks odd, make sure to check it with the site owner. As a next step, you can check the data with backend data from the order system. It's okay if the difference with Google Analytics is 5-10%.
4 Filters & views
The final step is to check filters and views. Every GA property should have at least two views:
Filtered view;
Unfiltered view (the backup view).
The filtered view excludes all data that you don’t want. This includes data that is registered to unwanted hostnames (see point 1). You can exclude these with a filter that only includes the hostnames that you need:
TMT hostname filter.
Besides that, you'll want to exclude traffic from known IP addresses. You can exclude these with an IP filter:
IP filter sample.
Lastly, the view should have the ‘Exclude all known bots and spiders’ box checked in the view settings:
Exclude known bots filter.
If these are all present, you probably have a decently filtered view. The unfiltered view should have no filters. This view functions as a backup. If you apply a bad filter to your filtered view, you'll always have raw unfiltered data at your disposal. You might see a third view used for testing new filters, and other views for sub-parts of your website (e.g. a shop view filtered on shop.website.com).
That’s all folks
With this knowledge in mind, you can quickly scan a GA account. It’ll probably take you less than 10 minutes. Keep in mind that this only gives you a very high overview the status of the account. After this, you’ll want to make sure that is missing will be added.",[Analytics]
8,Stop blaming tech for Trump,/stop-blaming-tech-for-trump/,"
            <p>Last night, I was browsing TechCrunch for interesting reads on tech. I noticed something. <a href=""https://techcrunch.com/gallery/tech-leaders-respond-to-upcoming-trump-presidency/slide/4/"">Tech doesn't seem to be happy with Trump</a>. There's an article about <a href=""https://techcrunch.com/2016/11/09/rigged/"">the influence social media had on the way we gauged Trump's chances of beating Hillary</a>. Heck, there's even a post about <a href=""https://techcrunch.com/2016/11/10/facebook-admits-it-must-do-more-to-stop-the-spread-of-misinformation-on-its-platform/"">Facebook admitting they need to address the spread of false information</a>. </p>

<p>Remember how people reacted when <a href=""https://www.washingtonpost.com/news/post-politics/wp/2016/10/18/donald-trump-says-the-election-is-rigged-heres-what-his-supporters-think-that-means/"">Trump said the elections were rigged</a>, partly by the media? And now you're saying that tech-wise, it was 'rigged' the other way around?</p>

<p>Think this over.</p>

<h2 id=""youareincontrol"">You are in control.</h2>

<p>I'm currently in the midst of reading <a href=""https://www.stephencovey.com/7habits/7habits.php"">The 7 Habits of Highly Effective People</a> by Stephen R. Covey. The first habit is about pro-activity. It discusses how there are things in your circle of influence and things that are in your circle of concern. The latter contains things that or on your mind but you don't control, things you can't influence. Blaming tech for Trump is like saying ""social media controls me"" or ""the only media I consume is the media that flows through my newsfeed based on some algorithm I don't control (nor completely understand)"". It's putting the blame not on yourself but on some outside force.</p>

<p>Come on. </p>

<p>If you're not happy with the outcome, deal with it. You're living in a democracy. Think about what you could have done differently, what you would do differently in four years. Think about the things you could have influenced and can influence during the next election. Don't just go and blame some newsfeed for the outcome.</p>

<p>Just so you know, <a href=""http://fourhourworkweek.com/2015/09/22/scott-adams-the-man-behind-dilbert/"">in the 22 September 2015 edition of Tim Ferriss' popular podcast series </a>, Scott Adams mentioned how he thinks that Trump may actually become president, and why. And in the words of Wait but Why's Tim Urban: <a href=""http://waitbutwhy.com/2016/11/its-going-to-be-okay.html"">it's going to be okay</a>. </p>

<h2 id=""thefilterisstillon"">The filter is still on</h2>

<p>One of the TechCrunch posts I've mentioned discusses how <a href=""(https://techcrunch.com/2016/11/09/rigged/)"">the Facebook bubble has popped</a>. How by only connecting to like-minded people, you only see information of like-minded people:</p>

<blockquote>
  <p>The Facebook bubble just popped. Half the country today is still in shock. Reality crashed down and many were presented with a world that didn’t match up with the one they’ve inhabited in the months leading up to the U.S. election.</p>
</blockquote>

<p>If you state something like this, also admit that the bubble hasn't actually popped. Just think about this: </p>

<ul>
<li>How many posts did you read about how <a href=""http://www.economist.com/blogs/freeexchange/2016/11/global-economy"">Trump will have [a bad influence] on [a subject]</a>? </li>
<li>And how many were a positive reaction to the results of the elections? </li>
</ul>

<p>As Tim Urban sais in his post: <strong><em>for every person that doesn't like the idea of Trump as president, there's one who does</em></strong>.</p>
        ","Last night, I was browsing TechCrunch for interesting reads on tech. I noticed something. Tech doesn't seem to be happy with Trump. There's an article about the influence social media had on the way we gauged Trump's chances of beating Hillary. Heck, there's even a post about Facebook admitting they need to address the spread of false information.
Remember how people reacted when Trump said the elections were rigged, partly by the media? And now you're saying that tech-wise, it was 'rigged' the other way around?
Think this over.
You are in control.
I'm currently in the midst of reading The 7 Habits of Highly Effective People by Stephen R. Covey. The first habit is about pro-activity. It discusses how there are things in your circle of influence and things that are in your circle of concern. The latter contains things that or on your mind but you don't control, things you can't influence. Blaming tech for Trump is like saying ""social media controls me"" or ""the only media I consume is the media that flows through my newsfeed based on some algorithm I don't control (nor completely understand)"". It's putting the blame not on yourself but on some outside force.
Come on.
If you're not happy with the outcome, deal with it. You're living in a democracy. Think about what you could have done differently, what you would do differently in four years. Think about the things you could have influenced and can influence during the next election. Don't just go and blame some newsfeed for the outcome.
Just so you know, in the 22 September 2015 edition of Tim Ferriss' popular podcast series , Scott Adams mentioned how he thinks that Trump may actually become president, and why. And in the words of Wait but Why's Tim Urban: it's going to be okay.
The filter is still on
One of the TechCrunch posts I've mentioned discusses how the Facebook bubble has popped. How by only connecting to like-minded people, you only see information of like-minded people:
The Facebook bubble just popped. Half the country today is still in shock. Reality crashed down and many were presented with a world that didn’t match up with the one they’ve inhabited in the months leading up to the U.S. election.
If you state something like this, also admit that the bubble hasn't actually popped. Just think about this:
How many posts did you read about how Trump will have [a bad influence] on [a subject]?
And how many were a positive reaction to the results of the elections?
As Tim Urban sais in his post: for every person that doesn't like the idea of Trump as president, there's one who does.","[tech, opinion]"
9,Personalized offline ads - Part 1: Recognizing people,/personal-advertisement-offline-step-1-recognizing-people/,"
            <p><p style=""color:#7e4396; font-weight:bold"">This post concerns Greenhouse Group Labs, an innovation program for students, established by Greenhouse Group. Labs is an ideal opportunity to test the latest technologies available, while allowing for talented young individuals to deeply explore them and come up with groundbreaking solutions.</p>Online ads are getting more and more personalized. To a large degree, this is made possible because of the huge amounts of data gathered online. Using this data, smart algorithms are able to tailor ads to a specific person's interests and needs. What if we would be able to personalize ads on the same level in the offline world? The data is available, so what is holding us back?</p>

<h2 id=""facialrecognition"">Facial recognition</h2>

<p>Our social robot experiment is aimed at establishing a solid connection between the online and the offline world. The robot offers an interface to make use of the online data in an offline presence. Besides that, it offers the possibility to generate data itself by measuring and analyzing offline behavior.</p>

<p>The first step in linking data to a particular person is to simply recognize who this person is. We decided to use facial recognition as our identification technique. Facial recognition systems are computer applications which are able to identify a person by a digital image. This technology offers a non-intrusive way of identifying someone without requiring any other input besides their presence. </p>

<h2 id=""howdoesitwork"">How does it work?</h2>

<p>Computers are able to distinguish between human faces through the usage of different sets of algorithms. They will start off by identifying a face within the frame. This is done by using <a href=""https://en.wikipedia.org/wiki/Haar-like_features"">Haar feature-based cascades</a>.</p>

<p>After a face has been detected, another algorithm will extract the facial features of the face and convert these to a set of values. This set will then be compared to the existing dataset of scanned faces and, subsequently, a face is identified when a positive match is found. Knowing these basics, it is fairly easy to set up a similar system yourself. And so we did! </p>

<h2 id=""gettingstarted"">Getting started</h2>

<p>Robin (the robot) comes equipped with a facial recognition module provided by OMRON. This module contains the above-mentioned algorithms and allows you to learn and recognize faces. Similar software can be found in a vast amount of various programming languages, where most of them will make use of the openCV library.</p>

<p>Once you have determined the software, it paves the road to establish a dataset. Detected faces will be compared against the faces the robot learned before. Hence, it is crucial to create a reliable dataset as a sound foundation for your facial recognition system.</p>

<p>When taking the pictures, make sure to rule out any risks and factors that may confuse or contaminate the software:  </p>

<ul>  
<li>Use an equal (preferably white) background</li>  
<li>Make sure all facial features -e.g. eyes, nose and mouth- are visible</li>  
<li>Face the person straight towards the camera</li>  
<li>Emulate the same lightning conditions as where the system will be used</li>  
<li>If people always wear glasses and/or hats, keep them on</li>  
</ul>  

<div class=""fluid-width-video-wrapper"" style=""padding-top: 75%;""><iframe src=""https://www.youtube.com/embed/Jc1COAj1760"" id=""fitvid663672"">  
</iframe></div>  

<p>After you have established your dataset you can start recognizing faces.  </p>

<h2 id=""improvingresults"">Improving results</h2>

<p>The OMRON module will only return the name of the person when a positive match is found. When digging a bit deeper, however, you are able to extract a certainty value for each match. In case you are getting a lot of false results you can choose to raise this value. While you are now able to recognize faces, you will notice that it is not yet very robust.  </p>

<h3 id=""angledview"">Angled view</h3>

<p>With this technique of facial recognition you do NOT create a 3D representation of a person's head. This means that whenever a person is facing another direction than straight into the lens, the software will have a hard time recognizing the person. <br>
To counter this problem, we took six different photos from each person. In the first five photos, the person was instructed to face different directions (up, down, left, right, straight). In the last photo, the person was asked to smile. The robot will be able to recognize the photo and link it to the correct person. Now, it does not matter anymore from which perspective the robot views the person or which expression is on the person's face. </p>

<h3 id=""falsehits"">False hits</h3>

<p>Unfortunately, we were still receiving false hits. This happened mostly when a person was in motion. <br>
</p><center><img src=""http://i.imgur.com/qPrlgPa.png"" alt=""Log data"" style=""width:25%;height:auto;""></center><p></p>

<p>Based on this, we decided to deploy an <em>on average</em> solution. Whenever a face is detected, it will receive its own unique identifier. Every time a hit for this face is fired, the certainty value is added to the corresponding name label. After ten hits, the name label with the highest score will trigger the output. By using the certainty value to calculate the averages, hits which are more certain will have a stronger impact than those with less certainty. </p>

<p>Some faces -i.e. wrong matches- kept on appearing all the time nevertheless. These faces would have scores of either 100% or have the same exact score multiple times in a row. Luckily, this allowed for easy counteractions by ignoring the scores that appear to be too perfect and only saving scores if they were not equal to the previous one.</p>

<p>While studying the pictures which led to the false hits, we were able to spot returning characteristics. The main problems were reflecting areas, like glasses and bald heads as shown below. <br>
</p><center><img src=""http://i.imgur.com/Mj367Jc.png"" alt=""Faulty picture reflecting on glasses"" style=""width:70%;height:auto;"" class=""full-img""></center> <br>
Other returning characteristics were badly/partly visible facial landmarks -e.g. an eye. Since there are six pictures of each individual in the dataset, we decided to simply remove the pictures containing any of these flaws.<p></p>

<h2 id=""finalresult"">Final result</h2>

<p>After adding the previously mentioned improvements, the software outputs stable and confident results. We are now able to successfully identify people with facial recognition! The next step consists of performing personal actions after identification, about which we will write in Part 2 of the Personalized offline ads series.</p>

<p>In the meantime, you can follow <a href=""https://www.instagram.com/goedhartrobin/"">Robin on instagram</a> to watch his progress and stay up to date about his journey and his endeavours to connect the offline and the online world! <br>
</p><center><a href=""https://www.instagram.com/goedhartrobin/""><img src=""http://i.imgur.com/aMG1FjQ.jpg"" style=""width:50%;height:auto;"" class=""full-img""></a></center><p></p>
        ","This post concerns Greenhouse Group Labs, an innovation program for students, established by Greenhouse Group. Labs is an ideal opportunity to test the latest technologies available, while allowing for talented young individuals to deeply explore them and come up with groundbreaking solutions.
Online ads are getting more and more personalized. To a large degree, this is made possible because of the huge amounts of data gathered online. Using this data, smart algorithms are able to tailor ads to a specific person's interests and needs. What if we would be able to personalize ads on the same level in the offline world? The data is available, so what is holding us back?
Facial recognition
Our social robot experiment is aimed at establishing a solid connection between the online and the offline world. The robot offers an interface to make use of the online data in an offline presence. Besides that, it offers the possibility to generate data itself by measuring and analyzing offline behavior.
The first step in linking data to a particular person is to simply recognize who this person is. We decided to use facial recognition as our identification technique. Facial recognition systems are computer applications which are able to identify a person by a digital image. This technology offers a non-intrusive way of identifying someone without requiring any other input besides their presence.
How does it work?
Computers are able to distinguish between human faces through the usage of different sets of algorithms. They will start off by identifying a face within the frame. This is done by using Haar feature-based cascades.
After a face has been detected, another algorithm will extract the facial features of the face and convert these to a set of values. This set will then be compared to the existing dataset of scanned faces and, subsequently, a face is identified when a positive match is found. Knowing these basics, it is fairly easy to set up a similar system yourself. And so we did!
Getting started
Robin (the robot) comes equipped with a facial recognition module provided by OMRON. This module contains the above-mentioned algorithms and allows you to learn and recognize faces. Similar software can be found in a vast amount of various programming languages, where most of them will make use of the openCV library.
Once you have determined the software, it paves the road to establish a dataset. Detected faces will be compared against the faces the robot learned before. Hence, it is crucial to create a reliable dataset as a sound foundation for your facial recognition system.
When taking the pictures, make sure to rule out any risks and factors that may confuse or contaminate the software:
Use an equal (preferably white) background
Make sure all facial features -e.g. eyes, nose and mouth- are visible
Face the person straight towards the camera
Emulate the same lightning conditions as where the system will be used
If people always wear glasses and/or hats, keep them on
After you have established your dataset you can start recognizing faces.
Improving results
The OMRON module will only return the name of the person when a positive match is found. When digging a bit deeper, however, you are able to extract a certainty value for each match. In case you are getting a lot of false results you can choose to raise this value. While you are now able to recognize faces, you will notice that it is not yet very robust.
Angled view
With this technique of facial recognition you do NOT create a 3D representation of a person's head. This means that whenever a person is facing another direction than straight into the lens, the software will have a hard time recognizing the person.
To counter this problem, we took six different photos from each person. In the first five photos, the person was instructed to face different directions (up, down, left, right, straight). In the last photo, the person was asked to smile. The robot will be able to recognize the photo and link it to the correct person. Now, it does not matter anymore from which perspective the robot views the person or which expression is on the person's face.
False hits
Unfortunately, we were still receiving false hits. This happened mostly when a person was in motion.
Based on this, we decided to deploy an on average solution. Whenever a face is detected, it will receive its own unique identifier. Every time a hit for this face is fired, the certainty value is added to the corresponding name label. After ten hits, the name label with the highest score will trigger the output. By using the certainty value to calculate the averages, hits which are more certain will have a stronger impact than those with less certainty.
Some faces -i.e. wrong matches- kept on appearing all the time nevertheless. These faces would have scores of either 100% or have the same exact score multiple times in a row. Luckily, this allowed for easy counteractions by ignoring the scores that appear to be too perfect and only saving scores if they were not equal to the previous one.
While studying the pictures which led to the false hits, we were able to spot returning characteristics. The main problems were reflecting areas, like glasses and bald heads as shown below.

Other returning characteristics were badly/partly visible facial landmarks -e.g. an eye. Since there are six pictures of each individual in the dataset, we decided to simply remove the pictures containing any of these flaws.
Final result
After adding the previously mentioned improvements, the software outputs stable and confident results. We are now able to successfully identify people with facial recognition! The next step consists of performing personal actions after identification, about which we will write in Part 2 of the Personalized offline ads series.
In the meantime, you can follow Robin on instagram to watch his progress and stay up to date about his journey and his endeavours to connect the offline and the online world!","[Code, Labs, Machine learning]"
10,The Genie in the Bottle: How to Tame AI?,/the-genie-in-the-bottle-how-to-tame-ai/,"
            <p>While we're seeing some great progress in AI, there is increasing concern about the danger of it by people like <a href=""http://www.bbc.com/news/technology-30290540"">Stephen Hawking</a>. This 'possible existential threat to humanity' is the reason why people like Elon Musk and Sam Altman started <a href=""https://openai.com/blog/introducing-openai/"">OpenAI</a>. The authority on the threat of AI is Nick Bostrom, Oxford professor, director of Future of Humanity Institute and author of the book <em>Superintelligence: Paths, Dangers, Strategies</em>. Before reading the strategies chapters of his book, I liked to  think on how to prevent humanity from the dangers of AI with an 'unbiased' mindset. It's truly an intriguing problem to put your mind to, so I hope you'll enjoy this post and create your own thoughts on how best to tame AI.</p>

<p>If you haven't read the <a href=""http://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html"">Wait But Why post on AI</a> yet, you should definitely do so. It clearly explains why we should worry, how sudden human level AI will hit us and why it is so hard to control something more intelligent than us, without anything like the concept of morality we humans have. An example is given about a start-up that builds a robotic arm that makes handwritten postcards. In the heat of the competition from other start-ups they can't resist the temptation of connecting their AI to the internet. A couple of days later all humans drop dead because the AI found out that humans provide valuable resources for creating hand written postcards. Further fulfilling it's objective function, the AI eventually turns the complete universe into hand written postcards.</p>

<p>It's a great read, and while maybe not too realistic, you clearly get the point. It's also a great start for giving it your own thought. These are some of mine:</p>

<h2 id=""1evenaiexpertsseemtothinklinear"">1 Even AI Experts Seem to Think Linear</h2>

<p>Wait But Why clearly explains how technological progress is exponential, but we fail to see it because we think linear. They don't stress it, but even AI experts fail to see exponentially based on the data in the post! In the questionnaires part AI experts answer when they think we'll have human level AI and when we'll have superhuman level AI. The median answer is 2040 and 2060 years respectively. This clearly illustrates that even AI experts think linear on average! According to exponential thinking, the moment we have human level AI, we'll have superhuman level AI in no time. As an example: the human level AI could be duplicated to create a small army of 'AI experts AIs' that can really easily make an AI that's more intelligent.</p>

<h2 id=""2objectivefunctionvsmodelofobjectives"">2 Objective Function vs. 'Model of Objectives'</h2>

<p>The AI in the example is given a simple and clear objective function. I believe that when AI grows more powerful towards human level AI, such simple objective functions will quickly disappear. In the Wait But Why post, it's explained that human level AI is often called Artificial General Intelligence opposed to the Artificial Narrow Intelligence, and 'General' is not about having a clear objective function. It's a bit like the Japanese expert systems that shortly boomed before the AI winter in the 80's, where 'knowledge' was specifically programmed into the system. Now we have (deep) neural networks, which work without very limited human intervention on how the model works. Even experts can't exactly comprehend how the underlying models work, they just do. To illustrate: <a href=""https://www.wired.com/2016/03/googles-ai-viewed-move-no-human-understand/"">AlphaGo</a> made a winning move that even experts couldn't understand until much further in the game. </p>

<p>This will become the same for objective functions: instead of having clearly defined ones, the objective functions of AIs will be complex models that no human can fully comprehend. I'll refer to this as a 'model of objectives'. This sounds very creepy, but in fact it might be much safer to have these complex objective ones instead of some simple ones defined by humans. We humans are pretty bad at defining objective functions. However, although we don't really have a clue about the set of 'objective functions' that make us act like we do, personally nor in general, somehow almost all individuals play their part in society really well. We do have something like an  objective function programmed by the mechanics of evolution: survival and reproduction of your own DNA. Luckily this objective isn't one we follow too much these days anymore. In fact there are very many reasons for doing the things we do and we can't fully comprehend these reasons ourselves. And if people give themselves some singular objective function in life, this more often than not turns out badly if you ask me. The fact that we have a very complex model of objectives that we can't fully comprehend ourselves makes it wonderfully possible to live together in society, even though objectives differ strongly from individual to individual. If we want to achieve something, there always will be some parts of our model of objectives that prevent us from achieving this goal by all means. To give AI a kind of conscience, set of ethics, moral, or whatever you can call it, it needs a similar model of objectives as we humans have ourselves. Therefore, we'll have to accept that we won't be able to comprehend the model of objectives functions of the AI, because it better than relying on simplistic human programmed objective functions.</p>

<h2 id=""3superhumanaiwillunderstandusbetterthanweunderstandeachotherandevenourselves"">3 Superhuman AI will understand us better than we understand each other and even ourselves</h2>

<p>The robot arm AI may be powerful, but it is pretty stupid. We humans are not that good at deciding what's good for us and what's not, but even the dumbest person in the world will understand that if you ask it to make handwritten postcards, he or she won't start killing humans. This is very closely related to thinking about AI as giving it a clear objective function: when it approaches human level AI, it will start to understand how to interpret these objective functions. Think about it: we're pretty good at understanding what somebody else wants, without the need of very specific instructions, even if it completely differs from the things we want ourselves. And if AI is getting to the superhuman level, it will be actually better in understanding what we actually want than any other human, and even than we know it about ourselves.</p>

<p>This sounds super futuristic, but let me give an example that I believe is quite easy to imagine. You're on a crazy night out with some of your colleagues and you ask the assistant app on your phone to check social media and check for connections that are nearby and message any fun people to join you in the bar. Some of your friends join, including one of your best old friends that you haven't seen for a long time and just happens to be around town. You both drink to how amazing this phone assistant is since it actually made you meet each other in the bar. Many drinks later, you come up with this hilarious joke about your company's CEO. Together with some colleagues you make a selfie video about your hilarious joke and send it to your CEO. The next morning you wake up with an incredible headache. Your phone assistant is giving you a notification whether you really want to send that video you made yesterday to your CEO. You quickly press no and wonder how we ever have lived in a world without AI.</p>

<p>I believe it's not that hard to imagine AI with this type of behavior being available in a couple of years, given <a href=""https://assistant.google.com"">companies like Google releasing personal AIs</a> and of course Apple's SIRI, Google Home and Amazon's Alexa. And it will be still far away from human level AI. However, the point is that such an AI won't be programmed to specifically prevent you from sending videos to your CEO when you're drunk. It will be impossible to humanly define all the rules we want such an AI to follow, similar to the expert systems in the 80's being programmed with specific knowledge. More importantly, the AI also doesn't blindly follow an objective to do what you wish for. If it would have asked you whether you would be happy it prevented you from sending the video when you were still in the bar, you probably wouldn't have been amused.</p>

<h2 id=""4itsnotgoingtobeusversusthem"">4 It's not going to be 'Us Versus Them'</h2>

<p>It's not only Hollywood that's thinking about AI as them vs us, the Wait But Why example is also very much describing an AI that's completely separate from us. Somehow we humans really tend to think in terms of them vs us, as the world today sadly clearly shows. This them vs us is an artifact of our primal objective function to replicate our own DNA that turns out to be really though to get rid off. Any way, as Ray Kurzweil writes in his classic <em>The singularity is near</em> AI is quite likely to be more 'us' before it becomes at the human level. We might be able to enhance our brains with artificial computing power or being able to directly tab into the cloud from our brains. It's just really hard for us to imagine today what this is going to look like, just like we could never imagine what the world would look like today in an era when internet still had to be invented.</p>

<p>Whatever the future will be, it's important to know that AI is heavily being trained on data generated by humans. And most data that's produced is data we generate living our daily lives. Therefore, probably the most efficient way of making AI smarter is to integrate it in our daily lives, like Google assistant type of projects. Training will be probably speed-up with reinforcement learning (training AI based on simulations, like AlphaGo training itself by playing incredably many games against itself). But human generated data will be crucial to reaching human level AI, since the real world is much complexer than a game of Go. More importantly, you can't just let an AI go trial-and-error in the real world. It's actually a good thing AI will be learning from human daily behavior, since not only will it be an efficient way in training the model (i.e. how do you do things), it will also be the perfect input for building the model of objectives (i.e. why do we do the things we do the way we do). With such a lot of exposure to human behavior before even coming close to human level intelligence, AI will learn gradually how to blend in perfectly into our society. As long as no humans are allowed to overwrite the model of objectives by some hardcoded objective, AI won't go rogue. Just like neural nets are robust (i.e. removing some parts of the model will hardly impact the performance), the model of objectives will be robust as long as it is only evolving slowly by learning from new data instead of humans messing too much with the objective functions.</p>

<h2 id=""5thegenieinthebottle"">5 The Genie in the Bottle</h2>

<p>Everybody has heard the fairy tale of the genie in the bottle: some genie or another magical creature grants the main character three wishes. These wishes always backfire, because of a combination of ill chosen wishes and the genie taking the liberty to define the details on how these wishes are actually fulfilled. The wisdom of these fairy tales is nowadays reflected more than ever in the topic of superintelligent AI.</p>

<p>It's time for a thought experiment. Forget my previous point about AI evolving gradually alongside humans and imagine you're suddenly facing AI of superhuman intelligence. You would be granted three wishes. What would you do?</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Nov/marko_manev_dawn_of_man-1478459680080.jpg"" alt="""" class=""full-img""></p>

<p>Don't run off quoting the <a href=""https://en.m.wikipedia.org/wiki/Three_Laws_of_Robotics"">Three Laws of Robotics</a>, give it a thought!</p>

<p>I'd came up with a strategy like this:</p>

<p>""Wow AI buddy! Before having any effect on the universe I wish you to build an exact model of my mind in your internal intelligence. Then figure out how to let me know you completed your task.""</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Nov/marko_manev_dawn_of_man_42-1478518233963.jpg"" alt="""" class=""full-img""></p>

<p>""Amazing, it took only 9 milliseconds! And it knows I like the hitchhikers guide!"" </p>

<p>""Now for my second wish. I wish that for every potential response you're going to give to one of my wishes, you're going to test it on the model of my mind you just made. If the consequences are too many to comprehend for my mind model, simply copy the model of my mind and let each copy evaluate part of the consequences. If any of the copies of my mind thinks the world will be worse than it currently is, come up with a better plan. If you run out of internal intelligence to create enough copies to fully comprehend the consequences, come up with a plan that's less complex. If you're still working on it when I make another wish, simply ignore the wish you're working on and start working on the new wish. If you find a response to a wish that satisfies this test, then execute it.""</p>

<p>""Oh and for my third wish, I want you to serve me for infinity and grand me unlimited wishes!""</p>

<p>Now take some time to think about it. The definition might not be very exact, but since we're facing superintelligence, the AI will understand you very well. And your second wish will prevent the AI from doing anything in a way you won't like or intended! Well yes, it does, but you also just made the AI pretty useless. With these restrictions, the AI won't do anything except for maybe some very simple (and useless) tasks. This is because of two reasons:</p>

<p>1) It's impossible to know all the consequences of pretty much any action with complete certainty, even for a superhuman AI. <br>
2)  It's almost impossible to find a response to your wish for which every single consequence will have a positive impact: there's always a trade-off. However, since a single copy of your mind can't comprehend the full consequences, you'll need multiple copies of your mind to evaluate part of the consequences. However, you can't simply add up the responses and do something if you like the majority of the consequences. It's hard to make a trade-off if you can't fully comprehend the consequences at once.</p>

<p>The solution, I think, will be involving probabilities and asymmetry in evaluation. What I have in mind is that an AI will only do something if there is a very, very low probability that you won't like the consequences. And if you dislike part of the consequences a little, you really have to like the benefits a lot (like a ten times more), in order for an AI to take action. Basically AIs should be much more risk averse than we humans are.</p>

<h2 id=""6withgreatpowercomesgreatresponsibility"">6 With Great Power Comes Great Responsibility</h2>

<p>The example above is of course rather selfish, since you ask the AI to only simulate your mind, not those of others. However, I believe that the way the AI will evaluate the consequences, will already make more humane decisions than we do nowadays. Especially in the case of 'us vs. them', people don't think about the consequences for others, and sometimes specifically choose not to think about what this means for others. However, the AI will specifically use some of the copies of your mind to test whether you like the impact the execution of your wish has on other people. When confronted with the facts, I think I believe people will be a lot less selfish. </p>

<p>However, it would be much better to not only use your mind in evaluating the response to your wish, but that of very many people, preferably the whole world population. So you can ask your wishes, but how they are granted is evaluated against the minds of the whole population. And luckily, most likely AI is going to evolve learning from the minds of (almost) the complete populations, or at least as many people as possible. The most efficient way of training AI, is to collect and combine as much data as possible from behavior of people in their daily lives. Therefore, there is a strong incentive to include as many people as possible. Moreover, people of diverse backgrounds will be more useful than similar people, since there is less information to gain from similar people.</p>

<h2 id=""7companiesthatactmorehumane"">7 Companies that Act More Humane</h2>

<p>Of course companies will also be making use of AI. Companies have a pretty dangerous objective function: maximizing profit. Although no human can fully comprehend the impact of all the decisions made in a company, somehow companies do behave quite well in reality. Of course there are some bad examples, but think about it: it could be so much worse if companies would be blindly following the objective function of maximizing profit. It's thanks to the model of objectives of humans that companies behave so well. </p>

<p>In this case it's really a difference between live and death whether AI will learn a model of objectives from working together with humans, or a team of employees that comes up with some objective functions that will steer an 'inhumane AI'. The later will be a doom scenario. However, the first could actually make companies behave more humane than they currently are. Think about it, we humans can't comprehend the consequences of all those decisions made in a company. Wouldn't it be much better if the employees at a company could wish what they want to achieve, but how it is executed is carefully evaluated by an AI including the minds of the complete population?</p>

<h2 id=""8solvingtheflawsofdemocracy"">8 Solving the Flaws of Democracy</h2>

<p>Democracy surely has it's flaws. However, it's the best flawed option we have. With AI, we will have a better alternative. I'll consider two flaws of democracy, the terror of the majority and voters actually don't know much about what they are voting for.</p>

<p>Democracy works best if everybody votes what they think is best for the country as a whole. Unfortunately, in reality people tend to vote what's best for themselves. The terror of majority is the case in which a majority of the population, can completely rule over a minority in the population, just because they have a majority. This most certainly doesn't have to be humane. Luckily we have our models of objectives that keep us on the right track most of the times. With AI we'll have much better option. Instead of putting our trust in some politician, the AI can simply run every single political decision against the full simulation of the minds of the complete population. Using asymmetric evaluation, the AI can prevent doing anything that benefits a majority of the population, while having too negative consequences for smaller groups in society. Just imagine how perfectly you could set taxes if you could actually run simulations on what the true impact would be on people.</p>

<p>Second, voters are probably not the best ones to decide about the best solutions since they are not the experts. Moreover, people don't like to spend there time in learning the details about politics and specific topics they are voting for. However, using AI that simulates the minds of the full population, anybody could have a copy of their minds dedicated to understanding the consequences of a political decision and evaluating whether they like the results or not. As a result, the decisions that are actually made will be definitely much wiser than the decisions that are currently made.</p>

<h2 id=""conclusion"">Conclusion</h2>

<p>I hope you liked this thought experiment! To conclude, I think these are the points that will help to keep AI evolving on the right path:</p>

<ul>
<li>AI should move more and more to having a complex model of objectives, like we humans do, instead of simplistic objective functions.</li>
<li>We will need to except that we won't be able to fully comprehend this model of objectives. </li>
<li>These models of objectives should only be able to evolve slowly based on new input data. They should never be overruled by hardcoded human objective functions.</li>
<li>AI should evolve alongside humans as personal assistants, so they will have plenty of exposure to human behavior not only to learn how to do things, but also learn their model of objectives (why do you do what you do).</li>
<li>Although we humans have pretty diverse and sometimes oposing objectives, the complexity of models of objectives makes us all fit in to society pretty well. The same will be true for AI.</li>
<li>In fact it will not be humans vs AI, but both will blend together one way or another.</li>
<li>When AI becomes superhuman, it will actually be possible to make an exact model of our minds. This should be used to asses any possible action of an AI with a simulation against copies of human minds.</li>
<li>These human minds should reflect the complete world population. Luckily, the most efficient way of training AI is by feeding it as much human behavior as possible. Therefore, AI will be able to learn it's model of objectives from as many and diverse people as possible.</li>
<li>Hopefully, the AI that will be used by companies will be run by the same AI, and therefore any possible (AI enhanced) action will be evaluated by models of minds of the world population. This could actually make companies behave more humane than they currently do.</li>
<li>It could improve democracy too.</li>
</ul>

<p>Looking forward to hearing your thoughts! </p>
        ","While we're seeing some great progress in AI, there is increasing concern about the danger of it by people like Stephen Hawking. This 'possible existential threat to humanity' is the reason why people like Elon Musk and Sam Altman started OpenAI. The authority on the threat of AI is Nick Bostrom, Oxford professor, director of Future of Humanity Institute and author of the book Superintelligence: Paths, Dangers, Strategies. Before reading the strategies chapters of his book, I liked to think on how to prevent humanity from the dangers of AI with an 'unbiased' mindset. It's truly an intriguing problem to put your mind to, so I hope you'll enjoy this post and create your own thoughts on how best to tame AI.
If you haven't read the Wait But Why post on AI yet, you should definitely do so. It clearly explains why we should worry, how sudden human level AI will hit us and why it is so hard to control something more intelligent than us, without anything like the concept of morality we humans have. An example is given about a start-up that builds a robotic arm that makes handwritten postcards. In the heat of the competition from other start-ups they can't resist the temptation of connecting their AI to the internet. A couple of days later all humans drop dead because the AI found out that humans provide valuable resources for creating hand written postcards. Further fulfilling it's objective function, the AI eventually turns the complete universe into hand written postcards.
It's a great read, and while maybe not too realistic, you clearly get the point. It's also a great start for giving it your own thought. These are some of mine:
1 Even AI Experts Seem to Think Linear
Wait But Why clearly explains how technological progress is exponential, but we fail to see it because we think linear. They don't stress it, but even AI experts fail to see exponentially based on the data in the post! In the questionnaires part AI experts answer when they think we'll have human level AI and when we'll have superhuman level AI. The median answer is 2040 and 2060 years respectively. This clearly illustrates that even AI experts think linear on average! According to exponential thinking, the moment we have human level AI, we'll have superhuman level AI in no time. As an example: the human level AI could be duplicated to create a small army of 'AI experts AIs' that can really easily make an AI that's more intelligent.
2 Objective Function vs. 'Model of Objectives'
The AI in the example is given a simple and clear objective function. I believe that when AI grows more powerful towards human level AI, such simple objective functions will quickly disappear. In the Wait But Why post, it's explained that human level AI is often called Artificial General Intelligence opposed to the Artificial Narrow Intelligence, and 'General' is not about having a clear objective function. It's a bit like the Japanese expert systems that shortly boomed before the AI winter in the 80's, where 'knowledge' was specifically programmed into the system. Now we have (deep) neural networks, which work without very limited human intervention on how the model works. Even experts can't exactly comprehend how the underlying models work, they just do. To illustrate: AlphaGo made a winning move that even experts couldn't understand until much further in the game.
This will become the same for objective functions: instead of having clearly defined ones, the objective functions of AIs will be complex models that no human can fully comprehend. I'll refer to this as a 'model of objectives'. This sounds very creepy, but in fact it might be much safer to have these complex objective ones instead of some simple ones defined by humans. We humans are pretty bad at defining objective functions. However, although we don't really have a clue about the set of 'objective functions' that make us act like we do, personally nor in general, somehow almost all individuals play their part in society really well. We do have something like an objective function programmed by the mechanics of evolution: survival and reproduction of your own DNA. Luckily this objective isn't one we follow too much these days anymore. In fact there are very many reasons for doing the things we do and we can't fully comprehend these reasons ourselves. And if people give themselves some singular objective function in life, this more often than not turns out badly if you ask me. The fact that we have a very complex model of objectives that we can't fully comprehend ourselves makes it wonderfully possible to live together in society, even though objectives differ strongly from individual to individual. If we want to achieve something, there always will be some parts of our model of objectives that prevent us from achieving this goal by all means. To give AI a kind of conscience, set of ethics, moral, or whatever you can call it, it needs a similar model of objectives as we humans have ourselves. Therefore, we'll have to accept that we won't be able to comprehend the model of objectives functions of the AI, because it better than relying on simplistic human programmed objective functions.
3 Superhuman AI will understand us better than we understand each other and even ourselves
The robot arm AI may be powerful, but it is pretty stupid. We humans are not that good at deciding what's good for us and what's not, but even the dumbest person in the world will understand that if you ask it to make handwritten postcards, he or she won't start killing humans. This is very closely related to thinking about AI as giving it a clear objective function: when it approaches human level AI, it will start to understand how to interpret these objective functions. Think about it: we're pretty good at understanding what somebody else wants, without the need of very specific instructions, even if it completely differs from the things we want ourselves. And if AI is getting to the superhuman level, it will be actually better in understanding what we actually want than any other human, and even than we know it about ourselves.
This sounds super futuristic, but let me give an example that I believe is quite easy to imagine. You're on a crazy night out with some of your colleagues and you ask the assistant app on your phone to check social media and check for connections that are nearby and message any fun people to join you in the bar. Some of your friends join, including one of your best old friends that you haven't seen for a long time and just happens to be around town. You both drink to how amazing this phone assistant is since it actually made you meet each other in the bar. Many drinks later, you come up with this hilarious joke about your company's CEO. Together with some colleagues you make a selfie video about your hilarious joke and send it to your CEO. The next morning you wake up with an incredible headache. Your phone assistant is giving you a notification whether you really want to send that video you made yesterday to your CEO. You quickly press no and wonder how we ever have lived in a world without AI.
I believe it's not that hard to imagine AI with this type of behavior being available in a couple of years, given companies like Google releasing personal AIs and of course Apple's SIRI, Google Home and Amazon's Alexa. And it will be still far away from human level AI. However, the point is that such an AI won't be programmed to specifically prevent you from sending videos to your CEO when you're drunk. It will be impossible to humanly define all the rules we want such an AI to follow, similar to the expert systems in the 80's being programmed with specific knowledge. More importantly, the AI also doesn't blindly follow an objective to do what you wish for. If it would have asked you whether you would be happy it prevented you from sending the video when you were still in the bar, you probably wouldn't have been amused.
4 It's not going to be 'Us Versus Them'
It's not only Hollywood that's thinking about AI as them vs us, the Wait But Why example is also very much describing an AI that's completely separate from us. Somehow we humans really tend to think in terms of them vs us, as the world today sadly clearly shows. This them vs us is an artifact of our primal objective function to replicate our own DNA that turns out to be really though to get rid off. Any way, as Ray Kurzweil writes in his classic The singularity is near AI is quite likely to be more 'us' before it becomes at the human level. We might be able to enhance our brains with artificial computing power or being able to directly tab into the cloud from our brains. It's just really hard for us to imagine today what this is going to look like, just like we could never imagine what the world would look like today in an era when internet still had to be invented.
Whatever the future will be, it's important to know that AI is heavily being trained on data generated by humans. And most data that's produced is data we generate living our daily lives. Therefore, probably the most efficient way of making AI smarter is to integrate it in our daily lives, like Google assistant type of projects. Training will be probably speed-up with reinforcement learning (training AI based on simulations, like AlphaGo training itself by playing incredably many games against itself). But human generated data will be crucial to reaching human level AI, since the real world is much complexer than a game of Go. More importantly, you can't just let an AI go trial-and-error in the real world. It's actually a good thing AI will be learning from human daily behavior, since not only will it be an efficient way in training the model (i.e. how do you do things), it will also be the perfect input for building the model of objectives (i.e. why do we do the things we do the way we do). With such a lot of exposure to human behavior before even coming close to human level intelligence, AI will learn gradually how to blend in perfectly into our society. As long as no humans are allowed to overwrite the model of objectives by some hardcoded objective, AI won't go rogue. Just like neural nets are robust (i.e. removing some parts of the model will hardly impact the performance), the model of objectives will be robust as long as it is only evolving slowly by learning from new data instead of humans messing too much with the objective functions.
5 The Genie in the Bottle
Everybody has heard the fairy tale of the genie in the bottle: some genie or another magical creature grants the main character three wishes. These wishes always backfire, because of a combination of ill chosen wishes and the genie taking the liberty to define the details on how these wishes are actually fulfilled. The wisdom of these fairy tales is nowadays reflected more than ever in the topic of superintelligent AI.
It's time for a thought experiment. Forget my previous point about AI evolving gradually alongside humans and imagine you're suddenly facing AI of superhuman intelligence. You would be granted three wishes. What would you do?
Don't run off quoting the Three Laws of Robotics, give it a thought!
I'd came up with a strategy like this:
""Wow AI buddy! Before having any effect on the universe I wish you to build an exact model of my mind in your internal intelligence. Then figure out how to let me know you completed your task.""
""Amazing, it took only 9 milliseconds! And it knows I like the hitchhikers guide!""
""Now for my second wish. I wish that for every potential response you're going to give to one of my wishes, you're going to test it on the model of my mind you just made. If the consequences are too many to comprehend for my mind model, simply copy the model of my mind and let each copy evaluate part of the consequences. If any of the copies of my mind thinks the world will be worse than it currently is, come up with a better plan. If you run out of internal intelligence to create enough copies to fully comprehend the consequences, come up with a plan that's less complex. If you're still working on it when I make another wish, simply ignore the wish you're working on and start working on the new wish. If you find a response to a wish that satisfies this test, then execute it.""
""Oh and for my third wish, I want you to serve me for infinity and grand me unlimited wishes!""
Now take some time to think about it. The definition might not be very exact, but since we're facing superintelligence, the AI will understand you very well. And your second wish will prevent the AI from doing anything in a way you won't like or intended! Well yes, it does, but you also just made the AI pretty useless. With these restrictions, the AI won't do anything except for maybe some very simple (and useless) tasks. This is because of two reasons:
1) It's impossible to know all the consequences of pretty much any action with complete certainty, even for a superhuman AI.
2) It's almost impossible to find a response to your wish for which every single consequence will have a positive impact: there's always a trade-off. However, since a single copy of your mind can't comprehend the full consequences, you'll need multiple copies of your mind to evaluate part of the consequences. However, you can't simply add up the responses and do something if you like the majority of the consequences. It's hard to make a trade-off if you can't fully comprehend the consequences at once.
The solution, I think, will be involving probabilities and asymmetry in evaluation. What I have in mind is that an AI will only do something if there is a very, very low probability that you won't like the consequences. And if you dislike part of the consequences a little, you really have to like the benefits a lot (like a ten times more), in order for an AI to take action. Basically AIs should be much more risk averse than we humans are.
6 With Great Power Comes Great Responsibility
The example above is of course rather selfish, since you ask the AI to only simulate your mind, not those of others. However, I believe that the way the AI will evaluate the consequences, will already make more humane decisions than we do nowadays. Especially in the case of 'us vs. them', people don't think about the consequences for others, and sometimes specifically choose not to think about what this means for others. However, the AI will specifically use some of the copies of your mind to test whether you like the impact the execution of your wish has on other people. When confronted with the facts, I think I believe people will be a lot less selfish.
However, it would be much better to not only use your mind in evaluating the response to your wish, but that of very many people, preferably the whole world population. So you can ask your wishes, but how they are granted is evaluated against the minds of the whole population. And luckily, most likely AI is going to evolve learning from the minds of (almost) the complete populations, or at least as many people as possible. The most efficient way of training AI, is to collect and combine as much data as possible from behavior of people in their daily lives. Therefore, there is a strong incentive to include as many people as possible. Moreover, people of diverse backgrounds will be more useful than similar people, since there is less information to gain from similar people.
7 Companies that Act More Humane
Of course companies will also be making use of AI. Companies have a pretty dangerous objective function: maximizing profit. Although no human can fully comprehend the impact of all the decisions made in a company, somehow companies do behave quite well in reality. Of course there are some bad examples, but think about it: it could be so much worse if companies would be blindly following the objective function of maximizing profit. It's thanks to the model of objectives of humans that companies behave so well.
In this case it's really a difference between live and death whether AI will learn a model of objectives from working together with humans, or a team of employees that comes up with some objective functions that will steer an 'inhumane AI'. The later will be a doom scenario. However, the first could actually make companies behave more humane than they currently are. Think about it, we humans can't comprehend the consequences of all those decisions made in a company. Wouldn't it be much better if the employees at a company could wish what they want to achieve, but how it is executed is carefully evaluated by an AI including the minds of the complete population?
8 Solving the Flaws of Democracy
Democracy surely has it's flaws. However, it's the best flawed option we have. With AI, we will have a better alternative. I'll consider two flaws of democracy, the terror of the majority and voters actually don't know much about what they are voting for.
Democracy works best if everybody votes what they think is best for the country as a whole. Unfortunately, in reality people tend to vote what's best for themselves. The terror of majority is the case in which a majority of the population, can completely rule over a minority in the population, just because they have a majority. This most certainly doesn't have to be humane. Luckily we have our models of objectives that keep us on the right track most of the times. With AI we'll have much better option. Instead of putting our trust in some politician, the AI can simply run every single political decision against the full simulation of the minds of the complete population. Using asymmetric evaluation, the AI can prevent doing anything that benefits a majority of the population, while having too negative consequences for smaller groups in society. Just imagine how perfectly you could set taxes if you could actually run simulations on what the true impact would be on people.
Second, voters are probably not the best ones to decide about the best solutions since they are not the experts. Moreover, people don't like to spend there time in learning the details about politics and specific topics they are voting for. However, using AI that simulates the minds of the full population, anybody could have a copy of their minds dedicated to understanding the consequences of a political decision and evaluating whether they like the results or not. As a result, the decisions that are actually made will be definitely much wiser than the decisions that are currently made.
Conclusion
I hope you liked this thought experiment! To conclude, I think these are the points that will help to keep AI evolving on the right path:
AI should move more and more to having a complex model of objectives, like we humans do, instead of simplistic objective functions.
We will need to except that we won't be able to fully comprehend this model of objectives.
These models of objectives should only be able to evolve slowly based on new input data. They should never be overruled by hardcoded human objective functions.
AI should evolve alongside humans as personal assistants, so they will have plenty of exposure to human behavior not only to learn how to do things, but also learn their model of objectives (why do you do what you do).
Although we humans have pretty diverse and sometimes oposing objectives, the complexity of models of objectives makes us all fit in to society pretty well. The same will be true for AI.
In fact it will not be humans vs AI, but both will blend together one way or another.
When AI becomes superhuman, it will actually be possible to make an exact model of our minds. This should be used to asses any possible action of an AI with a simulation against copies of human minds.
These human minds should reflect the complete world population. Luckily, the most efficient way of training AI is by feeding it as much human behavior as possible. Therefore, AI will be able to learn it's model of objectives from as many and diverse people as possible.
Hopefully, the AI that will be used by companies will be run by the same AI, and therefore any possible (AI enhanced) action will be evaluated by models of minds of the world population. This could actually make companies behave more humane than they currently do.
It could improve democracy too.
Looking forward to hearing your thoughts!","[AI, Artificial Intelligence, Data Science]"
11,7 reasons to use Snowplow besides Google Analytics 360,/7-reasons-to-use-snowplow-besides-google-analytics-360/,"
            <p>We recently got the question: if we start using <a href=""https://www.google.com/analytics/360-suite/#?modal_active=none"">Google Analytics 360</a> and <a href=""https://support.google.com/analytics/answer/3437618?hl=en"">Big Query</a>, do we still need to use a second data logger like <a href=""http://snowplowanalytics.com/"">Snowplow Analytics</a>? A great question. I discussed this question with our data team and it turns out using Snowplow (an open source data logger) besides premium analytics tooling like Google Analytics 360 has several benefits.</p>

<p><em>In this post, we compare Google Analytics 360 with Snowplow Analytics. Similar benefits apply if you would compare [a premium analytics solution] with [an event level data logger].</em></p>

<h2 id=""1sourceattributionmodelling"">1: Source attribution modelling</h2>

<p>With true event level data, you’re completely free to use the data as you see fit. A good example is source attribution. Google Analytics has its own way of recognising direct, organic and referral traffic sources. Besides that, it attributes session data to traffic sources according to the <a href=""https://support.google.com/analytics/answer/1662518?hl=en"">last-non-direct-click model</a> that they apply by default. With Snowplow’s event level data, you’ll have to set up every form of attribution manually by defining all the rules to make it work this way. It allows you to customise an attribution model to perfectly fit your needs. </p>

<h2 id=""2hittypes"">2: Hit types</h2>

<p>Another benefit is the supported hit types. Snowplow has all the familiar hit types for pageviews, events, and e-commerce. But it gets better. Snowplow, for example, also has dedicated impression events(!) to measure ad impressions and supports unstructured events that you can set up to track whatever data you want.</p>

<h2 id=""3capturingcustomdata"">3: Capturing custom data</h2>

<p>The third example is custom data. With Google Analytics 360 you can track up 400 custom variables (200 custom dimension and 200 custom metrics). You need to connect each value to one of these custom variables in your tracking code. If you don’t set this up, you won’t collect the data. Snowplow and other event level data loggers often allow you to track raw data objects (e.g. a transaction object). This allows you to capture the complete data layer of every page as unstructured data. You can decide what to use and what not to use when you start modelling. </p>

<h2 id=""4dataownership"">4: Data Ownership</h2>

<p>If you use an event level data logger like Snowplow you often get options for storing your data: you are in control of the location. If you want to stop using the tool, you can export the data and delete it. Because of this true data ownership, you might also consider sending data to Snowplow that you're not allowed to, or don't want to, send to Google. </p>

<h2 id=""5theapproachtorawdata"">5: The approach to raw data</h2>

<p>A good way to look at the difference between Google Analytics 360 and Snowplow is the way they approach raw data. With Google Analytics, the data view you use to run queries on with Big Query determines what’s in the data set. It's a top-down approach. You access the raw data of a filtered view. With Snowplow, you determine what data to use and what not to use, you set up the enrichment process  that determines what’s in the data set. It’s a bottom-up approach. </p>

<h2 id=""6datapipelines"">6: Data pipelines</h2>

<p>With Snowplow you can create a closed data flow from ad impression to a sale. You are in control of this flow and the only one that can change things. You can even set up multiple flows, e.g. a data flow to enrich your CRM data on a daily basis, and a real-time flow for instant onsite recommendations. Besides that you can use different collector types, e.g. trackers based on first party cookies or trackers that store cookies on a separate domain (this allows you to sync cookies across domains!).</p>

<h2 id=""7bettingontwohorses"">7: Betting on two horses</h2>

<p>Besides the benefits mentioned above, we always suggest using at least two data collection tools on your website. Both to check data and as a safety net. </p>

<h2 id=""technicalconsiderations"">Technical considerations</h2>

<p>It’s important to keep technical aspects in mind. Google offers a full suite, with reports in Google Analytics 360 and Big Query for raw data access. Snowplow offers the code that allows you to start capturing data and enriching this data, but they don’t run this for you. You’ll have to host tracking codes yourself, decide where to store data, and create an automated process for enriching the data so you can use it. There are no automated reports or interfaces like Big Query. You’ll need resources to set this up. </p>

<h2 id=""trueeventleveldata"">True event level data</h2>

<p>In the end, most of the benefits come down to one aspect: the control you have on the data. With solutions like Snowplow, you’ll have access to true event level data, data in its rawest form. With Google, you have access to a raw extract of Google Analytics data.</p>

<p><strong>Further reading:</strong> for additional details on the differences between Snowplow and Google Analytics 360 you can read <a href=""http://discourse.snowplowanalytics.com/t/comparing-snowplow-with-google-analytics-360-bigquery-integration-wip/666"">this post by Yali Sassoon</a> , the
Co-founder at Snowplow Analytics.</p>
        ","We recently got the question: if we start using Google Analytics 360 and Big Query, do we still need to use a second data logger like Snowplow Analytics? A great question. I discussed this question with our data team and it turns out using Snowplow (an open source data logger) besides premium analytics tooling like Google Analytics 360 has several benefits.
In this post, we compare Google Analytics 360 with Snowplow Analytics. Similar benefits apply if you would compare [a premium analytics solution] with [an event level data logger].
1: Source attribution modelling
With true event level data, you’re completely free to use the data as you see fit. A good example is source attribution. Google Analytics has its own way of recognising direct, organic and referral traffic sources. Besides that, it attributes session data to traffic sources according to the last-non-direct-click model that they apply by default. With Snowplow’s event level data, you’ll have to set up every form of attribution manually by defining all the rules to make it work this way. It allows you to customise an attribution model to perfectly fit your needs.
2: Hit types
Another benefit is the supported hit types. Snowplow has all the familiar hit types for pageviews, events, and e-commerce. But it gets better. Snowplow, for example, also has dedicated impression events(!) to measure ad impressions and supports unstructured events that you can set up to track whatever data you want.
3: Capturing custom data
The third example is custom data. With Google Analytics 360 you can track up 400 custom variables (200 custom dimension and 200 custom metrics). You need to connect each value to one of these custom variables in your tracking code. If you don’t set this up, you won’t collect the data. Snowplow and other event level data loggers often allow you to track raw data objects (e.g. a transaction object). This allows you to capture the complete data layer of every page as unstructured data. You can decide what to use and what not to use when you start modelling.
4: Data Ownership
If you use an event level data logger like Snowplow you often get options for storing your data: you are in control of the location. If you want to stop using the tool, you can export the data and delete it. Because of this true data ownership, you might also consider sending data to Snowplow that you're not allowed to, or don't want to, send to Google.
5: The approach to raw data
A good way to look at the difference between Google Analytics 360 and Snowplow is the way they approach raw data. With Google Analytics, the data view you use to run queries on with Big Query determines what’s in the data set. It's a top-down approach. You access the raw data of a filtered view. With Snowplow, you determine what data to use and what not to use, you set up the enrichment process that determines what’s in the data set. It’s a bottom-up approach.
6: Data pipelines
With Snowplow you can create a closed data flow from ad impression to a sale. You are in control of this flow and the only one that can change things. You can even set up multiple flows, e.g. a data flow to enrich your CRM data on a daily basis, and a real-time flow for instant onsite recommendations. Besides that you can use different collector types, e.g. trackers based on first party cookies or trackers that store cookies on a separate domain (this allows you to sync cookies across domains!).
7: Betting on two horses
Besides the benefits mentioned above, we always suggest using at least two data collection tools on your website. Both to check data and as a safety net.
Technical considerations
It’s important to keep technical aspects in mind. Google offers a full suite, with reports in Google Analytics 360 and Big Query for raw data access. Snowplow offers the code that allows you to start capturing data and enriching this data, but they don’t run this for you. You’ll have to host tracking codes yourself, decide where to store data, and create an automated process for enriching the data so you can use it. There are no automated reports or interfaces like Big Query. You’ll need resources to set this up.
True event level data
In the end, most of the benefits come down to one aspect: the control you have on the data. With solutions like Snowplow, you’ll have access to true event level data, data in its rawest form. With Google, you have access to a raw extract of Google Analytics data.
Further reading: for additional details on the differences between Snowplow and Google Analytics 360 you can read this post by Yali Sassoon , the Co-founder at Snowplow Analytics.","[Analytics, Data Science, Data, snowplow, GA360, Google Analytics 360]"
12,Pragmatic approach to get Disqus comment notifications in Slack,/pragmatic-approach-to-get-disqus-comment-notifications-in-slack/,"
            <p>We use the awesome Disqus platform to enable comments on the posts on The Marketing Technologist. The only downside is that - because I've created the Disqus account with my email address - the only one getting notifications when someone writes a comment on an article, is me. Until recently, I forwarded the emails to the author of the post in question. We can do better, right? At The Marketing Technologist, bloggers use Slack to communicate with each other, so it would make total sense if we could get notifications of new comments into Slack. </p>

<p>Slack has a lot of great add-ons, but a Disqus integration add-on is not one of them. Although Disqus is planning on creating one in the future, we had to find another way. </p>

<p>When Googling for 'slack Disqus integration', you'll be guided to the great <a href=""https://zapier.com/zapbook/zaps/1359/get-slack-notifications-for-new-disqus-comments/"">Slack Disqus integration at Zapier</a>. Zapier is a service that connects tons of apps and automates workflows. Although this solution works fine, from a security point-of-view, I still find it a bit frightening to just give a third party application like Zapier access (although it's very limited) to our corporate Slack account. When it would concern a personal Slack channel, I would not mind using Zapier. </p>

<p>I've also found <a href=""https://github.com/jonathanwiesel/slackus"">a simple application</a> by a guy called Jonathan Wiesel that monitors a Disqus forum and sends a notification to a Slack channel when a new comment is made in said forum. You still need to install this web app on a web server. Although I appreciate his pragmatic effort, it doesn't feel right to set up a new web app just to get notifications into Slack. </p>

<h3 id=""thepragmaticapproachforwardemailstoslack"">The Pragmatic approach: forward emails to Slack</h3>

<p>There's a very easy way to get Disqus notifications into Slack: just have the emails that Disqus sends by default forwarded to Slack. </p>

<h4 id=""1setuptheemailappinslack"">1. Set up the Email App in Slack</h4>

<p>With the Email app, you can skip your inbox and receive emails directly in Slack. When you set up an email app, you’ll get a special email address that delivers to Slack. To configure the Email app, go to <a href=""https://slack.com/apps/A0F81496D-email"">Email app</a> in Slack, and click the <code>Add Email Integration</code> button. Next, you can select an existing channel or DM where the email will be posted, or you can create a new channel.</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Nov/Screen_Shot_2016_11_06_at_14_46_19-1478440066813.png"" alt="""" class=""full-img""></p>

<p>After you've added the email integration, you can change some additional settings and preview the message that will be send to your channel or DM. You'll also see the email address that was created especially for this specific integration. </p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Nov/Screen_Shot_2016_11_06_at_14_48_12-1478440319923.png"" alt="""" class=""full-img""></p>

<p>This is the email address we will forward the Disqus emails to. </p>

<h4 id=""2forwarddisqusemailstotheslackemailaddress"">2. Forward Disqus emails to the Slack email address</h4>

<p>Next, we have to forward the Disqus emails to our freshly created Slack email address. I'll show you how this works in Outlook, but you can easily apply this idea to other email clients like Gmail as well. </p>

<p>When you receive a comment notification from Slack, it is sent from <em>notifications@disqus.net</em> and the subject line looks something like this: <em>Re: Comment on How to start a WhatsApp conversation directly from the web</em>. To forward these emails to the Slack address, you can create an email rule for this. In Outlook, you can do this by clicking <em>Tools</em> &gt; <em>Rules</em> and then click the <em>+</em> icon. Alternatively, you can right-click an existing email from Disqus and choose <em>Rules</em> &gt; <em>Create rule</em>.  </p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Nov/Screen_Shot_2016_11_06_at_14_54_29-1478440831180.png"" alt="""" class=""full-img""></p>

<p>After you've clicked 'OK', all new Disqus comment notifications will appear in Slack!</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Nov/Screen_Shot_2016_11_06_at_15_01_28-1478440926806.png"" alt="""" class=""full-img""></p>

<p>We've been using this approach for other services that do not (yet) have a Slack integration. Of course, as soon as Disqus will have a Slack app that provides proper integration, we'll drop this pragmatic approach. But until then, it just works :). </p>
        ","We use the awesome Disqus platform to enable comments on the posts on The Marketing Technologist. The only downside is that - because I've created the Disqus account with my email address - the only one getting notifications when someone writes a comment on an article, is me. Until recently, I forwarded the emails to the author of the post in question. We can do better, right? At The Marketing Technologist, bloggers use Slack to communicate with each other, so it would make total sense if we could get notifications of new comments into Slack.
Slack has a lot of great add-ons, but a Disqus integration add-on is not one of them. Although Disqus is planning on creating one in the future, we had to find another way.
When Googling for 'slack Disqus integration', you'll be guided to the great Slack Disqus integration at Zapier. Zapier is a service that connects tons of apps and automates workflows. Although this solution works fine, from a security point-of-view, I still find it a bit frightening to just give a third party application like Zapier access (although it's very limited) to our corporate Slack account. When it would concern a personal Slack channel, I would not mind using Zapier.
I've also found a simple application by a guy called Jonathan Wiesel that monitors a Disqus forum and sends a notification to a Slack channel when a new comment is made in said forum. You still need to install this web app on a web server. Although I appreciate his pragmatic effort, it doesn't feel right to set up a new web app just to get notifications into Slack.
The Pragmatic approach: forward emails to Slack
There's a very easy way to get Disqus notifications into Slack: just have the emails that Disqus sends by default forwarded to Slack.
1. Set up the Email App in Slack
With the Email app, you can skip your inbox and receive emails directly in Slack. When you set up an email app, you’ll get a special email address that delivers to Slack. To configure the Email app, go to Email app in Slack, and click the Add Email Integration button. Next, you can select an existing channel or DM where the email will be posted, or you can create a new channel.
After you've added the email integration, you can change some additional settings and preview the message that will be send to your channel or DM. You'll also see the email address that was created especially for this specific integration.
This is the email address we will forward the Disqus emails to.
2. Forward Disqus emails to the Slack email address
Next, we have to forward the Disqus emails to our freshly created Slack email address. I'll show you how this works in Outlook, but you can easily apply this idea to other email clients like Gmail as well.
When you receive a comment notification from Slack, it is sent from notifications@disqus.net and the subject line looks something like this: Re: Comment on How to start a WhatsApp conversation directly from the web. To forward these emails to the Slack address, you can create an email rule for this. In Outlook, you can do this by clicking Tools > Rules and then click the + icon. Alternatively, you can right-click an existing email from Disqus and choose Rules > Create rule.
After you've clicked 'OK', all new Disqus comment notifications will appear in Slack!
We've been using this approach for other services that do not (yet) have a Slack integration. Of course, as soon as Disqus will have a Slack app that provides proper integration, we'll drop this pragmatic approach. But until then, it just works :).",[Social advertising]
13,Connecting offline sales to online campaign sources with Google Analytics - Part 2,/connecting-offline-sales-to-online-campaign-sources-with-google-analytics-part-2/,"
            <blockquote>
  <p>Connecting offline to online is a challenge, but this week we did it. We’ve measured our first offline sales in Google Analytics, and we can directly attribute these to online campaign sources! .... This post describes the general system. The second post will discuss the actual code used in the system.</p>
</blockquote>

<p>The text mentioned above is a recap of the first part in this series. If you missed part 1 in some mysterious way, catch up by reading  <a href=""https://www.themarketingtechnologist.co/connecting-offline-sales-to-online-campaign-sources-with-google-analytics/"">the full post</a>.</p>

<p>In this post we'll dive into our solution's technical details, describe the components used / needed and challenges you <strong>will</strong> encounter. We'll throw in some code snippets as well!</p>

<h2 id=""whatshappening"">What's happening</h2>

<ul>
<li><p>It all starts with data collection. Let's say your client owns a car dealership and a website for requesting a test drive. You'll want to grab the email address from the form and store it, together with the GA Client ID.</p></li>
<li><p>Then, every night, the car dealership shares the day's sales data. You'll match the email addresses from the orders with your online data (the GA Client ID) and update your records (revenue). <strong>TADA!!</strong> You've combined online and offline data.</p></li>
<li><p>The last step is sending this data to our GA account for reporting, and we'll have all the valuable information GA has to offer, for our offline sales! </p></li>
</ul>

<h2 id=""whatyoullget"">What you'll get</h2>

<p>You'll be able to see what the offline generated revenue is for your online channels like organic / paid / social.  Did this Facebook campaign, aimed at a particular audience do better than another? You might sell 10 cars a month by running your Adwords campaign you didn't know about. Or not? You'll know now! And that's what we do at the <a href=""https://www.greenhousegroup.com/"">Greenhouse Group</a> <strong>we don't guess, we know</strong>.</p>

<hr>

<h1 id=""technicaldetails"">Technical Details</h1>

<h4 id=""components"">Components</h4>

<ul>
<li>JavaScript for data collection</li>
<li>Web Application for storing the data
*</li>
<li>Sales Parser (CSV, XML)</li>
<li>GA connector</li>
<li>Database</li>
<li>SFTP Server</li>
</ul>

<h6 id=""11javascript"">1.1 JavaScript</h6>

<p>Code a script for grabbing the form data and send it to your web application right before submitting. To keep things clean, use a tag manager to implement the script on the website.</p>

<h6 id=""12webapplication"">1.2 Web Application</h6>

<ul>
<li>NodeJS / <a href=""http://expressjs.com/"">Express</a></li>
<li>MongoDB + Mongoose</li>
</ul>

<p>Your application needs one endpoint where the JavaScript code can posts the data to: <br>
<code>https://sub.example.com?client_id=1234567.1234567&amp;custom_identifier=elon@musk.com</code></p>

<p>Because of the <strong>same-origin policy</strong>, it's not possible to send data from domain A to domain B using javascript. That's a good thing, but is that problem for us? No, we serve our application from a subdomain (sub.example.com) and set the following header <em>Access-Control-Allow-Origin: <a href=""http://www.example.com"">http://www.example.com</a></em>. Now we are able to post data from <em>www.example.com</em>  to <em>sub.example.com</em> Fixed. For ExpressJS:</p>

<pre><code>app.use(function(req, res, next) {
    res.header(""Access-Control-Allow-Origin"", 'http://www.example.com');
    res.header(""Access-Control-Allow-Headers"", ""Origin, X-Requested-With, Content-Type, Accept"");
    next();
});
</code></pre>

<p>Example Mongoose model:</p>

<pre><code>var schema = {
  email: {
      type: String,
      required: true,
  },
  cid: {
      type: String,
      required: true
  },
  domain: {
      type: String,
      required: true
  },
  timestamp: {
      type: Date,
      default: Date.now,
      required: true
  },
  saleDate: {
      type: Date,
      default: undefined
  },
  saleRevenue: {
      type: Number,
      default: 0
  },
  sales: {
      type: Number,
      default: 0
  }
};
module.exports = schema;`
</code></pre>

<p><strong>Recommendations:</strong></p>

<ul>
<li>You need one running application per implementation because of the <em>Access-Control-Allow-Origin</em> header.</li>
<li>Validate requests using the packages <code>express-validation</code> and <code>Joi</code>.</li>
<li>Please use the https protocol for obvious reasons.</li>
<li>Log all request so you always have the raw data.</li>
<li>I like to remove the powered by head <code>app.disable('x-powered-by');</code></li>
<li>Using nginx as a proxy you'll be able to run multiple ExpressJS servers on port 80 using one webserver. Please give a shout if you need more info!</li>
</ul>

<h6 id=""13salesparser"">1.3 Sales Parser</h6>

<p>For parsing the daily order data a simple Python script can be used, it:</p>

<ul>
<li>grabs the CSV / XML from a SFTP server;</li>
<li>updates the records in our database;</li>
<li>use the <a href=""https://developers.google.com/analytics/devguides/collection/protocol/v1/"">GA measurement protocol</a> to send the combined data to GA.</li>
</ul>

<p>Use <code>pysftp</code> to connect to a SFTP server. You can, of course, use other data sharing solutions. Dropbox / Drive come in mind, but we use SFTP.</p>

<pre><code># downloading files and remove them
    with pysftp.Connection(host, username, password, cnopts=cnopts) as sftp:
        sftp.get_d('upload/', FILE_DIR)
        for file in sftp.listdir('upload/'):
            sftp.remove(os.path.join('upload', file))`
</code></pre>

<p>Our clients supply us with CSV files. As for the horrors of python encoding issues, I won't get into that now. In the end we used a custom CsvDictreader:</p>

<pre><code>def Latin1DictReader(latin1_data, **kwargs):
    csv_reader = csv.DictReader(latin1_data, **kwargs)
    for row in csv_reader:
        try:
            yield {key.decode('latin-1'): value.decode('latin-1') for key, value in row.iteritems()}
        except:
            yield False`
</code></pre>

<p>The following snippet is executed for every record in the CSV file. It checks if there's a match in our database, updates the record and posts the data to GA. We set a minimum timestamp to exclude submitted forms older than X days.</p>

<pre><code># get record from db by email
sale = Sale.objects(email=email,timestamp__gte=min_timetamp).order_by('-timestamp').first()

if sale:

    # check if new(er) sale
    if sale.saleDate is None or sale.saleDate &lt; sale_date:

        # update sale in database
        sale.update(**{
            ""set__saleDate"":    sale_date,
            ""set__saleRevenue"": sale_revenue,
            ""inc__sales"":       1,
            })

        # reload doc
        sale.reload()

        # calculate delta date
        sale_timestamp = sale.timestamp.replace(hour=0, minute=0, second=0, microsecond=0, tzinfo=None)
        delta_days = (sale_date - sale_timestamp).days

        # call ga using the measurement protocol
        url = 'https://ssl.google-analytics.com/collect?v=1&amp;tid=%s&amp;z=%d&amp;cid=%s&amp;t=event&amp;ec=Offline&amp;ea=Purchase&amp;cd5=%d&amp;el=%d&amp;ev=%d&amp;dh=%s' % (
                config['ga']['prop'],
                time.time(),
                sale.cid,
                delta_days,
                int(sale.saleRevenue),
                int(sale.saleRevenue),
                'www.example.com'
            )

        headers = { 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36' }

        r = requests.get(url, headers=headers)

        logging.info(""%d\t%s"" % (r.status_code, url))`
</code></pre>

<p>Run your python script every night / morning: <br>
<code>0 5 * * * /usr/bin/python sync.py</code></p>

<hr>

<h2 id=""thatsit"">That's it!</h2>

<p>Thanks for sticking with me till the end. The above instructions should be sufficient to setup your own online / offline measurement solution! If you need more details / help or got some feedback, please drop us a note below!</p>
        ","Connecting offline to online is a challenge, but this week we did it. We’ve measured our first offline sales in Google Analytics, and we can directly attribute these to online campaign sources! .... This post describes the general system. The second post will discuss the actual code used in the system.
The text mentioned above is a recap of the first part in this series. If you missed part 1 in some mysterious way, catch up by reading the full post.
In this post we'll dive into our solution's technical details, describe the components used / needed and challenges you will encounter. We'll throw in some code snippets as well!
What's happening
It all starts with data collection. Let's say your client owns a car dealership and a website for requesting a test drive. You'll want to grab the email address from the form and store it, together with the GA Client ID.
Then, every night, the car dealership shares the day's sales data. You'll match the email addresses from the orders with your online data (the GA Client ID) and update your records (revenue). TADA!! You've combined online and offline data.
The last step is sending this data to our GA account for reporting, and we'll have all the valuable information GA has to offer, for our offline sales!
What you'll get
You'll be able to see what the offline generated revenue is for your online channels like organic / paid / social. Did this Facebook campaign, aimed at a particular audience do better than another? You might sell 10 cars a month by running your Adwords campaign you didn't know about. Or not? You'll know now! And that's what we do at the Greenhouse Group we don't guess, we know.
Technical Details
Components
JavaScript for data collection
Web Application for storing the data *
Sales Parser (CSV, XML)
GA connector
Database
SFTP Server
1.1 JavaScript
Code a script for grabbing the form data and send it to your web application right before submitting. To keep things clean, use a tag manager to implement the script on the website.
1.2 Web Application
NodeJS / Express
MongoDB + Mongoose
Your application needs one endpoint where the JavaScript code can posts the data to:
https://sub.example.com?client_id=1234567.1234567&custom_identifier=elon@musk.com
Because of the same-origin policy, it's not possible to send data from domain A to domain B using javascript. That's a good thing, but is that problem for us? No, we serve our application from a subdomain (sub.example.com) and set the following header Access-Control-Allow-Origin: http://www.example.com. Now we are able to post data from www.example.com to sub.example.com Fixed. For ExpressJS:
app.use(function(req, res, next) {
    res.header(""Access-Control-Allow-Origin"", 'http://www.example.com');
    res.header(""Access-Control-Allow-Headers"", ""Origin, X-Requested-With, Content-Type, Accept"");
    next();
});
Example Mongoose model:
var schema = {
  email: {
      type: String,
      required: true,
  },
  cid: {
      type: String,
      required: true
  },
  domain: {
      type: String,
      required: true
  },
  timestamp: {
      type: Date,
      default: Date.now,
      required: true
  },
  saleDate: {
      type: Date,
      default: undefined
  },
  saleRevenue: {
      type: Number,
      default: 0
  },
  sales: {
      type: Number,
      default: 0
  }
};
module.exports = schema;`
Recommendations:
You need one running application per implementation because of the Access-Control-Allow-Origin header.
Validate requests using the packages express-validation and Joi.
Please use the https protocol for obvious reasons.
Log all request so you always have the raw data.
I like to remove the powered by head app.disable('x-powered-by');
Using nginx as a proxy you'll be able to run multiple ExpressJS servers on port 80 using one webserver. Please give a shout if you need more info!
1.3 Sales Parser
For parsing the daily order data a simple Python script can be used, it:
grabs the CSV / XML from a SFTP server;
updates the records in our database;
use the GA measurement protocol to send the combined data to GA.
Use pysftp to connect to a SFTP server. You can, of course, use other data sharing solutions. Dropbox / Drive come in mind, but we use SFTP.
# downloading files and remove them
    with pysftp.Connection(host, username, password, cnopts=cnopts) as sftp:
        sftp.get_d('upload/', FILE_DIR)
        for file in sftp.listdir('upload/'):
            sftp.remove(os.path.join('upload', file))`
Our clients supply us with CSV files. As for the horrors of python encoding issues, I won't get into that now. In the end we used a custom CsvDictreader:
def Latin1DictReader(latin1_data, **kwargs):
    csv_reader = csv.DictReader(latin1_data, **kwargs)
    for row in csv_reader:
        try:
            yield {key.decode('latin-1'): value.decode('latin-1') for key, value in row.iteritems()}
        except:
            yield False`
The following snippet is executed for every record in the CSV file. It checks if there's a match in our database, updates the record and posts the data to GA. We set a minimum timestamp to exclude submitted forms older than X days.
# get record from db by email
sale = Sale.objects(email=email,timestamp__gte=min_timetamp).order_by('-timestamp').first()

if sale:

    # check if new(er) sale
    if sale.saleDate is None or sale.saleDate < sale_date:

        # update sale in database
        sale.update(**{
            ""set__saleDate"":    sale_date,
            ""set__saleRevenue"": sale_revenue,
            ""inc__sales"":       1,
            })

        # reload doc
        sale.reload()

        # calculate delta date
        sale_timestamp = sale.timestamp.replace(hour=0, minute=0, second=0, microsecond=0, tzinfo=None)
        delta_days = (sale_date - sale_timestamp).days

        # call ga using the measurement protocol
        url = 'https://ssl.google-analytics.com/collect?v=1&tid=%s&z=%d&cid=%s&t=event&ec=Offline&ea=Purchase&cd5=%d&el=%d&ev=%d&dh=%s' % (
                config['ga']['prop'],
                time.time(),
                sale.cid,
                delta_days,
                int(sale.saleRevenue),
                int(sale.saleRevenue),
                'www.example.com'
            )

        headers = { 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36' }

        r = requests.get(url, headers=headers)

        logging.info(""%d\t%s"" % (r.status_code, url))`
Run your python script every night / morning:
0 5 * * * /usr/bin/python sync.py
That's it!
Thanks for sticking with me till the end. The above instructions should be sufficient to setup your own online / offline measurement solution! If you need more details / help or got some feedback, please drop us a note below!","[Analytics, node.js, python]"
14,Helping our new Data Scientists start in Python: A guide to learning by doing,/helping-our-new-data-scientists-start-in-python-a-guide-to-learning-by-doing/,"
            <p>The Data Science team at Greenhouse Group is steadily growing and continuously changing. This also implies new Data Scientists and interns starting regularly. Each new Data Scientist we hire is unique and has a different set of skills. What they all have in common though is a strong analytical background and the practical ability to apply this on real business cases. The majority of our team for example studied <a href=""https://en.wikipedia.org/wiki/Econometrics"">Econometrics</a>, a study which provides a strong foundation in probability theory and statistics. </p>

<p>As the typical Data Scientist also has to work with lots of data, decent programming skills are a must-have. This is however where the backgrounds of our new Data Scientists tends to differ from each other. The programming landscape is quite diverse, and therefore, the backgrounds of our new Data Scientists cover programming languages from R, MatLab, Java, Python, STATA, SPSS, SAS, SQL, Delphi, PHP to C# and C++. It is true that knowing many different programming languages can be useful when necessary. However, we prefer the use of one language for the majority of our projects so that we can easily cooperate with each other on projects. And given that nobody knows everything, one preferred programming language gives us the possibility to learn from each other.</p>

<p>At Greenhouse Group we have chosen to work with Python when possible. With the great support of the open-source community, Python has transformed into a great tool for doing Data Science. Python’s easy to use syntax, great data processing capabilities and awesome open-source statistical libraries such as Numpy, Pandas, Scikit-learn and Statsmodels allow us to do a wide range of tasks varying from exploratory analysis to building scalable big-data pipelines and machine learning algorithms. Only for the lesser-general statistical models we sometimes combine Python with R, where Python does the heavy data processing work and R the statistical modelling.</p>

<p>I also strongly believe in the philosophy of <strong>learning by doing</strong>. Therefore, to help our new Data Scientists get on their way with doing Data Science in Python we have created a <strong>Python Data Science (Crash) Course</strong>. The goal of this course is to let our new recruits (and also colleagues from different departments) learn to solve a real business problem in an interactive way and in their own pace . Meanwhile, the more experienced Data Scientists are available to answer any questions. Note that the skill of Googling for answers on StackOverflow or browsing through the documentation of libraries should not be underestimated. We definitely want to teach our new Data Scientists this skill too!</p>

<p>In this blog we describe our practical course phase by phase.</p>

<h4 id=""phase1learningpythonthebasics"">Phase 1: Learning Python, the basics</h4>

<p>The first step obviously is learning Python. That is, learning the Python syntax and basic Python operations. Luckily, the Python syntax is not that difficult if you take good care of indentation. Personally, coming from the Java programming language where indentation is not important, I made a lot of mistakes with indentation when I started with Python.</p>

<p>So, how to start with learning Python? Well, as we prefer the <em>learning by doing</em> approach we always let our new recruits start with the <a href=""https://www.codecademy.com/learn/python""><strong>Codecademy Python course.</strong></a> Codecademy provides an interactive Python course which can be followed in the browser. Therefore, you do not have to worry about installing anything yet and you can start immediately with learning Python! </p>

<p>The Codecademy Python course takes about 13 hours to complete. After this, you should be able to do simple operations in Python.</p>

<p><strong>Update: I just discovered that Codecademy will temporarily take the Python course offline. I haven't tried it yet, but a possible alternative to learn Python from within the browser is <a href=""http://www.learnpython.org/"">learnpython.org</a></strong></p>

<p><em>Bonus tip: another useful Codecademy course for Data Scientists is the SQL course!</em></p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Sep/codecademy_python-1475142324442.jpg"" alt="""" class=""full-img""></p>

<h4 id=""phase2installingpythonlocallywithanaconda"">Phase 2: Installing Python locally with Anaconda</h4>

<p>After finishing the Codecademy course we obviously want to start developing our own codes. However, since we are not running Python in-browser anymore we need to install Python on our own local PC. </p>

<p>Python is open source and freely available from <a href=""http://www.python.org"">www.python.org</a>. However, this official version only contains the standard Python libraries. The standard libraries contain functions to work with for example text files, datetimes and basic arithmetic operations. Unfortunately, the standard Python libraries are not comprehensive enough to perform all kinds of Data Science analysis. Luckily, the open-source community has made awesome libraries to extend Python with the proper functionality to do Data Science. </p>

<p>To prevent downloading and installing all these libraries separately, we prefer to use the <strong><a href=""https://www.continuum.io/downloads"">Anaconda Python distribution</a></strong>. Anaconda is actually Python combined with tons of scientific libraries, so there is no need to manually install them all yourself! Additionally, Anaconda comes bundled with an easy commandline tool to install new or update existing libraries when necessary.</p>

<p><em>Tip: although allmost all awesome libraries are included by default in Anaconda, some of them are not yet. You can install new packages from the command line using <code>conda install package_name</code> or <code>pip install package_name</code>. For example, we regularly use the progressbar library <code>tqdm</code> in our projects. Hence, we have to execute <code>pip install tqdm</code> first when performing a new install of Anaconda.</em></p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Sep/anaconda-1475142471227.jpg"" alt=""""></p>

<h4 id=""phase3easiercodingwithpycharm"">Phase 3: Easier coding with PyCharm</h4>

<p>After installing Python we are able to run Python code on our local PC. Just start Notepad, write your Python code, open the commandline and run the newly created Python file using <code>python C:\Users\thom\new_file.py</code>. Wait, that does not sound really simple right? No...</p>

<p>To make our lifes easier, we prefer to develop our Python codes in <a href=""https://www.jetbrains.com/pycharm/?fromMenu"">Pycharm</a>. PyCharm is a so-called <em>integrated development environment</em> which supports developers when writing code. It takes care of routine tasks such as running a program by providing a simple <em>run script</em> button. Additionally, it also helps being more productive by providing autocomplete functionality and on-the-fly error checking. Forgot a space somewhere or used a variable name that is not defined yet? PyCharm will warn you. Want to use a Version Control System such as Git to cooperate on projects? PyCharm will help you. One way or another, using PyCharm will save you a lot of time when writing Python code, because it works like a charm... badum tss.</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Sep/pycharm_editor-1475142163046.png"" alt="""" class=""full-img""></p>

<h4 id=""phase4solvingafictionalbusinessproblem"">Phase 4: Solving a fictional business problem</h4>

<h6 id=""definingtheresearchproblem"">Defining the research problem</h6>

<p>So, assume that by now our manager has come to us with a business problem he faces. <br>
That is, our manager wants to be able to predict the probability of a user having his first engagement (i.e. a newsletter subscription) on the companies website. After giving it some thought we came up with the idea to predict the engagement conversion  probability based on his number of pageviews. Furthermore, you constructed the following hypothesis:</p>

<blockquote>
  <p><em>More pageviews leads to a higher probability of engaging for the first time.</em></p>
</blockquote>

<p>To check whether this hypothesis holds, we have asked our Web Analysts for two datasets:</p>

<ul>
<li><a href=""https://github.com/thomhopmans/themarketingtechnologist/tree/master/7_data_science_in_python/data"">Session data</a> containing all pageviews of all users.
<ol><li><code>user_id</code>: a unique user identifier</li>
<li><code>session_number</code>: the number of the sessions (ascending)</li>
<li><code>session_start_date</code>: the start datetime of the session</li>
<li><code>unix_timestamp</code>: the start unix timestamp of the session</li>
<li><code>campaign_id</code>: ID of the campaign that led the user to the website</li>
<li><code>domain</code>: the (sub)domain the user is visiting in this session</li>
<li><code>entry</code>: the entry page of the session</li>
<li><code>referral</code>: the referring site, i.e. google.com</li>
<li><code>pageviews</code>: the number of pageviews within the session</li>
<li><code>transactions</code>: the number of transactions within the session</li></ol></li>
<li><a href=""https://github.com/thomhopmans/themarketingtechnologist/tree/master/7_data_science_in_python/data"">Engagement data</a> containing all engagements of all users
<ol><li><code>user_id</code>: a unique user identifier</li>
<li><code>site_id</code>: the ID of the site on which the engagement took place</li>
<li><code>engagement_unix_timestamp</code>: the unix timestamp of when the engagement took place</li>
<li><code>engagement_type</code>: the type of engagement, i.e. newsletter subscription</li>
<li><code>custom_properties</code>: additional properties of the engagement</li></ol></li>
</ul>

<p>Unfortunately, we have two separate datasets because they come from different systems. However, users in both datasets can be matched by a unique user identifier denoted by <code>user_id</code>.</p>

<p>Just like earlier blogs, I have placed the final code to solve the business problem on my <a href=""https://github.com/thomhopmans/themarketingtechnologist/tree/master/7_data_science_in_python"">GitHub</a>. However, I would strongly recommend to only look at this code when you have solved the case yourself. Additionally, you can also find the code to create two fictional datasets yourself.</p>

<h6 id=""easydataprocessingusingpandas"">Easy data processing using Pandas</h6>

<p>Before we can apply any statistical model to solve the problem we need to clean and prepare our data. For example, we need to find for each user in the sessions dataset his first engagement, if any. This requires joining the two datasets on <code>user_id</code> and removing any engagements after the first.</p>

<p>The Codecademy Python course taught you already how to read text files line by line. Python is great for data munging and preparation, but not for data analysis and modeling. The <a href=""http://pandas.pydata.org/""><strong>Pandas</strong></a> library for Python helps to overcome this problem. Pandas offers data structures and operations for manipulating (numerical) tables and time series. Pandas therefore makes it much easier to do Data Science in Python!</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Sep/pandas_logo-1475142074771.png"" alt=""""></p>

<h6 id=""readingthedatasetsusingpdread_csv"">Reading the datasets using pd.read_csv()</h6>

<p>The first step in our Python code will be to load both datasets within Python. Pandas provides an easy to use function to read .csv files: <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html"">read_csv()</a>. Following the learning by doing principle we recommend you find out yourself how to read both datasets. In the end, you should have two separate <a href=""http://pandas.pydata.org/pandas-docs/stable/dsintro.html#dataframe"">DataFrames</a>, one for each dataset.</p>

<p><em>Tips: we have different delimiters in both files. Also, be sure to check out the <code>date_parser</code> option in <code>read_csv()</code> to convert the UNIX timestamps to normal datetime formats.</em></p>

<h6 id=""filteroutirrelevantdata"">Filter out irrelevant data</h6>

<p>The next step in any (big) data problem is to reduce the size of your problem. In our case, we have lots of columns which are not relevant for our problem, such as the medium/source of the session. Therefore, we apply <a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html"">Indexing and Selecting</a> on our Dataframes to only keep relevant columns such as the <code>user_id</code> (necessary to join the two DataFrames), <code>datetimes</code> of each session and engagement (to search for the first engagement and sessions before that) and the number of <code>pageviews</code> (necessary to test our hypothesis). </p>

<p>Additionally, we filter out all non-first engagements in our engagements DataFrame. This can be done by looking for the lowest datetime value for each user_id. How? Use the <a href=""http://pandas.pydata.org/pandas-docs/stable/groupby.html"">GroupBy: split-apply-combine</a> logic! :)</p>

<h6 id=""combinethedataframesbasedonuser_id"">Combine the DataFrames based on user_id</h6>

<p>One of the most powerful options of Pandas is <a href=""http://pandas.pydata.org/pandas-docs/stable/merging.html"">merging, joining and concatenating</a> tables. It allows us to perform anything from simple left joins and unions to complex full outer joins. SO, combining the sessions and first engagements DataFrames based on the unique user identifier... you've got the power!</p>

<h6 id=""removeallsessionsafterthefirstengagement"">Remove all sessions after the first engagement</h6>

<p>Using a simple merge in the previous step we added to each session the timestamp of the first engagement. By comparing the session timestamp with the first engagement timestamp you should be able to filter out irrelevant data and reduce the size of the problem as well.</p>

<h6 id=""addthedependentvariableyanengagementconversion"">Add the dependent variable y: an engagement conversion</h6>

<p>As stated, we want to predict the effect of pageviews on the conversion (i.e. first engagement) probability. Therefore, our dependent <em>y</em> variable is a binary variable which denotes whether a conversion has taken place within the session. Because of the filtering we did above (i.e. remove all non-first engagements and sessions after te first engagement), this conversion by definition takes place in the last session of each user. Again, using the <a href=""http://pandas.pydata.org/pandas-docs/stable/groupby.html"">GroupBy: split-apply-combine</a> logic we can create a new column that contains a 1-observation if it is the last sessions of a user, and a 0-observation otherwise.</p>

<h6 id=""addtheindependentvariablexcumulativesumofpageviews"">Add the independent variable X: cumulative sum of pageviews</h6>

<p>Our independent variable is the number of pageviews. However, we cannot simply take the number of pageviews within a session, because pages visited in earlier sessions can also affect the conversion probability. Hence, we create a new column in which we calculate the cumulative sum of pageviews for a user. This will be our independent variable X.</p>

<h6 id=""fitalogisticregressionusingstatsmodels"">Fit a logistic regression using StatsModels</h6>

<p>Using Pandas we finally ended up with a small DataFrame containing of a single discrete X column and a single binary y column. A (binary) logistic regression model is used to estimate the probability of a binary response of the dependent variable based on one or more independent variables. <a href=""abc""><strong>StatsModels</strong></a> is a statistics &amp; econometrics libary for Python with tools for parameter estimation &amp; statistical testing. Therefore it is not surprising that it also contains functions to perform a logistic regression. So, how to fit a Logistic regression model using StatsModels? <a href=""http://lmgtfy.com/?q=statsmodels+python+examples"">Let me Google that for you!</a> </p>

<p><em>Tip 1: do not forget to add a constant to the logistic regression...</em></p>

<p><em>Tip 2: another awesome libary to fit statistical models such as logistic regression is <a href=""http://scikit-learn.org/"">scikit-learn</a>.</em></p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Sep/logit-1475142636584.png"" alt="""" class=""full-img""></p>

<h6 id=""visualizeresultsusingmatplotliborseaborn"">Visualize results using Matplotlib or Seaborn</h6>

<p>After fitting the logistic regression model, we can <a href=""http://statsmodels.sourceforge.net/devel/generated/statsmodels.discrete.discrete_model.Logit.predict.html"">predict</a> the conversion probability for each cumulative pageviews value. However, we cannot just communicate our newly found results  to the management by handing over some raw numbers. Therefore, one of the important tasks of a Data Scientist is to present his results in a clearly and effective manner. In most cases, this means providing visualizations of our results as we all know that an image is worth more than a thousand words...</p>

<p>Python contains several awesome visualization libraries of which <a href=""http://matplotlib.org/"">MatplotLib</a> is the most well-known. <a href=""https://stanford.edu/~mwaskom/software/seaborn/"">Seaborn</a> is another awesome libary built upon MatplotLib. The syntax of MatplotLib is probably well-known to users who worked with MatLab before. However, our preference goes to Seaborn as it provides prettier plots and appearance is important.</p>

<p>Using Seaborn we created the following visualization of our fitted model:</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Sep/logistic_regression_curve_blog7-1475140226844.png"" alt="""" class=""full-img""></p>

<p>We can nicely use this visualization to support our evidence on whether our hypothesis holds.</p>

<h6 id=""testingthehypothesis"">Testing the hypothesis</h6>

<p>The final step is to check whether our constructed hypothesis holds. Recall that we stated that</p>

<blockquote>
  <p><em>More pageviews leads to a higher probability of engaging for the first time.</em></p>
</blockquote>

<p>For one, from our previous visualization it already follows that the hypothesis holds. Otherwise, the predicted probabilities would not be <a href=""https://en.wikipedia.org/wiki/Monotonic_function"">monotonically</a> increasing. Nonetheless, we could also draw the same conclusion from the summary of our fitted model as shown below.</p>

<pre><code>                           Logit Regression Results                           
==============================================================================
Dep. Variable:          is_conversion   No. Observations:                12420  
Model:                          Logit   Df Residuals:                    12418  
Method:                           MLE   Df Model:                            1  
Date:                Tue, 27 Sep 2016   Pseudo R-squ.:                  0.3207  
Time:                        21:44:57   Log-Likelihood:                -5057.6  
converged:                       True   LL-Null:                       -7445.5  
                                        LLR p-value:                     0.000
====================================================================================
                       coef    std err          z      P&gt;|z|      [95.0% Conf. Int.]
------------------------------------------------------------------------------------
const               -3.8989      0.066    -59.459      0.000        -4.027    -3.770  
pageviews_cumsum     0.2069      0.004     52.749      0.000         0.199     0.215  
====================================================================================
</code></pre>

<p>We see that the coefficient of the <code>pageviews_cumsum</code> is statistically significant positive at a significance level of 1%. Hence, we have shown that our hypothesis holds, hurray! Furthermore, you just completed your first Data Science analysis in Python! :)</p>

<h4 id=""conclusion"">Conclusion</h4>

<p>We hope you have enjoyed this Data Science in Python blog. What we described in this blog is obviously far from comprehensive or complete as it is simply too much to cover in one blog. For example, we haven’t even talked about Version Control Systems such as Git yet. We hope however that this blog has given you some directions on how to start with your first practical Data Science analysis in Python. We would appreciate to hear in the comments on how you have set your first steps in Data Science with Python. Also, any valuable feedback to improve this blog or crash course is welcome. :)</p>
        ","The Data Science team at Greenhouse Group is steadily growing and continuously changing. This also implies new Data Scientists and interns starting regularly. Each new Data Scientist we hire is unique and has a different set of skills. What they all have in common though is a strong analytical background and the practical ability to apply this on real business cases. The majority of our team for example studied Econometrics, a study which provides a strong foundation in probability theory and statistics.
As the typical Data Scientist also has to work with lots of data, decent programming skills are a must-have. This is however where the backgrounds of our new Data Scientists tends to differ from each other. The programming landscape is quite diverse, and therefore, the backgrounds of our new Data Scientists cover programming languages from R, MatLab, Java, Python, STATA, SPSS, SAS, SQL, Delphi, PHP to C# and C++. It is true that knowing many different programming languages can be useful when necessary. However, we prefer the use of one language for the majority of our projects so that we can easily cooperate with each other on projects. And given that nobody knows everything, one preferred programming language gives us the possibility to learn from each other.
At Greenhouse Group we have chosen to work with Python when possible. With the great support of the open-source community, Python has transformed into a great tool for doing Data Science. Python’s easy to use syntax, great data processing capabilities and awesome open-source statistical libraries such as Numpy, Pandas, Scikit-learn and Statsmodels allow us to do a wide range of tasks varying from exploratory analysis to building scalable big-data pipelines and machine learning algorithms. Only for the lesser-general statistical models we sometimes combine Python with R, where Python does the heavy data processing work and R the statistical modelling.
I also strongly believe in the philosophy of learning by doing. Therefore, to help our new Data Scientists get on their way with doing Data Science in Python we have created a Python Data Science (Crash) Course. The goal of this course is to let our new recruits (and also colleagues from different departments) learn to solve a real business problem in an interactive way and in their own pace . Meanwhile, the more experienced Data Scientists are available to answer any questions. Note that the skill of Googling for answers on StackOverflow or browsing through the documentation of libraries should not be underestimated. We definitely want to teach our new Data Scientists this skill too!
In this blog we describe our practical course phase by phase.
Phase 1: Learning Python, the basics
The first step obviously is learning Python. That is, learning the Python syntax and basic Python operations. Luckily, the Python syntax is not that difficult if you take good care of indentation. Personally, coming from the Java programming language where indentation is not important, I made a lot of mistakes with indentation when I started with Python.
So, how to start with learning Python? Well, as we prefer the learning by doing approach we always let our new recruits start with the Codecademy Python course. Codecademy provides an interactive Python course which can be followed in the browser. Therefore, you do not have to worry about installing anything yet and you can start immediately with learning Python!
The Codecademy Python course takes about 13 hours to complete. After this, you should be able to do simple operations in Python.
Update: I just discovered that Codecademy will temporarily take the Python course offline. I haven't tried it yet, but a possible alternative to learn Python from within the browser is learnpython.org
Bonus tip: another useful Codecademy course for Data Scientists is the SQL course!
Phase 2: Installing Python locally with Anaconda
After finishing the Codecademy course we obviously want to start developing our own codes. However, since we are not running Python in-browser anymore we need to install Python on our own local PC.
Python is open source and freely available from www.python.org. However, this official version only contains the standard Python libraries. The standard libraries contain functions to work with for example text files, datetimes and basic arithmetic operations. Unfortunately, the standard Python libraries are not comprehensive enough to perform all kinds of Data Science analysis. Luckily, the open-source community has made awesome libraries to extend Python with the proper functionality to do Data Science.
To prevent downloading and installing all these libraries separately, we prefer to use the Anaconda Python distribution. Anaconda is actually Python combined with tons of scientific libraries, so there is no need to manually install them all yourself! Additionally, Anaconda comes bundled with an easy commandline tool to install new or update existing libraries when necessary.
Tip: although allmost all awesome libraries are included by default in Anaconda, some of them are not yet. You can install new packages from the command line using conda install package_name or pip install package_name. For example, we regularly use the progressbar library tqdm in our projects. Hence, we have to execute pip install tqdm first when performing a new install of Anaconda.
Phase 3: Easier coding with PyCharm
After installing Python we are able to run Python code on our local PC. Just start Notepad, write your Python code, open the commandline and run the newly created Python file using python C:\Users\thom\new_file.py. Wait, that does not sound really simple right? No...
To make our lifes easier, we prefer to develop our Python codes in Pycharm. PyCharm is a so-called integrated development environment which supports developers when writing code. It takes care of routine tasks such as running a program by providing a simple run script button. Additionally, it also helps being more productive by providing autocomplete functionality and on-the-fly error checking. Forgot a space somewhere or used a variable name that is not defined yet? PyCharm will warn you. Want to use a Version Control System such as Git to cooperate on projects? PyCharm will help you. One way or another, using PyCharm will save you a lot of time when writing Python code, because it works like a charm... badum tss.
Phase 4: Solving a fictional business problem
Defining the research problem
So, assume that by now our manager has come to us with a business problem he faces.
That is, our manager wants to be able to predict the probability of a user having his first engagement (i.e. a newsletter subscription) on the companies website. After giving it some thought we came up with the idea to predict the engagement conversion probability based on his number of pageviews. Furthermore, you constructed the following hypothesis:
More pageviews leads to a higher probability of engaging for the first time.
To check whether this hypothesis holds, we have asked our Web Analysts for two datasets:
Session data containing all pageviews of all users.
user_id: a unique user identifier
session_number: the number of the sessions (ascending)
session_start_date: the start datetime of the session
unix_timestamp: the start unix timestamp of the session
campaign_id: ID of the campaign that led the user to the website
domain: the (sub)domain the user is visiting in this session
entry: the entry page of the session
referral: the referring site, i.e. google.com
pageviews: the number of pageviews within the session
transactions: the number of transactions within the session
Engagement data containing all engagements of all users
user_id: a unique user identifier
site_id: the ID of the site on which the engagement took place
engagement_unix_timestamp: the unix timestamp of when the engagement took place
engagement_type: the type of engagement, i.e. newsletter subscription
custom_properties: additional properties of the engagement
Unfortunately, we have two separate datasets because they come from different systems. However, users in both datasets can be matched by a unique user identifier denoted by user_id.
Just like earlier blogs, I have placed the final code to solve the business problem on my GitHub. However, I would strongly recommend to only look at this code when you have solved the case yourself. Additionally, you can also find the code to create two fictional datasets yourself.
Easy data processing using Pandas
Before we can apply any statistical model to solve the problem we need to clean and prepare our data. For example, we need to find for each user in the sessions dataset his first engagement, if any. This requires joining the two datasets on user_id and removing any engagements after the first.
The Codecademy Python course taught you already how to read text files line by line. Python is great for data munging and preparation, but not for data analysis and modeling. The Pandas library for Python helps to overcome this problem. Pandas offers data structures and operations for manipulating (numerical) tables and time series. Pandas therefore makes it much easier to do Data Science in Python!
Reading the datasets using pd.read_csv()
The first step in our Python code will be to load both datasets within Python. Pandas provides an easy to use function to read .csv files: read_csv(). Following the learning by doing principle we recommend you find out yourself how to read both datasets. In the end, you should have two separate DataFrames, one for each dataset.
Tips: we have different delimiters in both files. Also, be sure to check out the date_parser option in read_csv() to convert the UNIX timestamps to normal datetime formats.
Filter out irrelevant data
The next step in any (big) data problem is to reduce the size of your problem. In our case, we have lots of columns which are not relevant for our problem, such as the medium/source of the session. Therefore, we apply Indexing and Selecting on our Dataframes to only keep relevant columns such as the user_id (necessary to join the two DataFrames), datetimes of each session and engagement (to search for the first engagement and sessions before that) and the number of pageviews (necessary to test our hypothesis).
Additionally, we filter out all non-first engagements in our engagements DataFrame. This can be done by looking for the lowest datetime value for each user_id. How? Use the GroupBy: split-apply-combine logic! :)
Combine the DataFrames based on user_id
One of the most powerful options of Pandas is merging, joining and concatenating tables. It allows us to perform anything from simple left joins and unions to complex full outer joins. SO, combining the sessions and first engagements DataFrames based on the unique user identifier... you've got the power!
Remove all sessions after the first engagement
Using a simple merge in the previous step we added to each session the timestamp of the first engagement. By comparing the session timestamp with the first engagement timestamp you should be able to filter out irrelevant data and reduce the size of the problem as well.
Add the dependent variable y: an engagement conversion
As stated, we want to predict the effect of pageviews on the conversion (i.e. first engagement) probability. Therefore, our dependent y variable is a binary variable which denotes whether a conversion has taken place within the session. Because of the filtering we did above (i.e. remove all non-first engagements and sessions after te first engagement), this conversion by definition takes place in the last session of each user. Again, using the GroupBy: split-apply-combine logic we can create a new column that contains a 1-observation if it is the last sessions of a user, and a 0-observation otherwise.
Add the independent variable X: cumulative sum of pageviews
Our independent variable is the number of pageviews. However, we cannot simply take the number of pageviews within a session, because pages visited in earlier sessions can also affect the conversion probability. Hence, we create a new column in which we calculate the cumulative sum of pageviews for a user. This will be our independent variable X.
Fit a logistic regression using StatsModels
Using Pandas we finally ended up with a small DataFrame containing of a single discrete X column and a single binary y column. A (binary) logistic regression model is used to estimate the probability of a binary response of the dependent variable based on one or more independent variables. StatsModels is a statistics & econometrics libary for Python with tools for parameter estimation & statistical testing. Therefore it is not surprising that it also contains functions to perform a logistic regression. So, how to fit a Logistic regression model using StatsModels? Let me Google that for you!
Tip 1: do not forget to add a constant to the logistic regression...
Tip 2: another awesome libary to fit statistical models such as logistic regression is scikit-learn.
Visualize results using Matplotlib or Seaborn
After fitting the logistic regression model, we can predict the conversion probability for each cumulative pageviews value. However, we cannot just communicate our newly found results to the management by handing over some raw numbers. Therefore, one of the important tasks of a Data Scientist is to present his results in a clearly and effective manner. In most cases, this means providing visualizations of our results as we all know that an image is worth more than a thousand words...
Python contains several awesome visualization libraries of which MatplotLib is the most well-known. Seaborn is another awesome libary built upon MatplotLib. The syntax of MatplotLib is probably well-known to users who worked with MatLab before. However, our preference goes to Seaborn as it provides prettier plots and appearance is important.
Using Seaborn we created the following visualization of our fitted model:
We can nicely use this visualization to support our evidence on whether our hypothesis holds.
Testing the hypothesis
The final step is to check whether our constructed hypothesis holds. Recall that we stated that
More pageviews leads to a higher probability of engaging for the first time.
For one, from our previous visualization it already follows that the hypothesis holds. Otherwise, the predicted probabilities would not be monotonically increasing. Nonetheless, we could also draw the same conclusion from the summary of our fitted model as shown below.
                           Logit Regression Results                           
==============================================================================
Dep. Variable:          is_conversion   No. Observations:                12420  
Model:                          Logit   Df Residuals:                    12418  
Method:                           MLE   Df Model:                            1  
Date:                Tue, 27 Sep 2016   Pseudo R-squ.:                  0.3207  
Time:                        21:44:57   Log-Likelihood:                -5057.6  
converged:                       True   LL-Null:                       -7445.5  
                                        LLR p-value:                     0.000
====================================================================================
                       coef    std err          z      P>|z|      [95.0% Conf. Int.]
------------------------------------------------------------------------------------
const               -3.8989      0.066    -59.459      0.000        -4.027    -3.770  
pageviews_cumsum     0.2069      0.004     52.749      0.000         0.199     0.215  
====================================================================================
We see that the coefficient of the pageviews_cumsum is statistically significant positive at a significance level of 1%. Hence, we have shown that our hypothesis holds, hurray! Furthermore, you just completed your first Data Science analysis in Python! :)
Conclusion
We hope you have enjoyed this Data Science in Python blog. What we described in this blog is obviously far from comprehensive or complete as it is simply too much to cover in one blog. For example, we haven’t even talked about Version Control Systems such as Git yet. We hope however that this blog has given you some directions on how to start with your first practical Data Science analysis in Python. We would appreciate to hear in the comments on how you have set your first steps in Data Science with Python. Also, any valuable feedback to improve this blog or crash course is welcome. :)","[Data Science, python, Code, pandas, statsmodels, seaborn, logistic regression]"
15,Api.ai vs Wit.ai (or is it Google vs Facebook?),/api-ai-vs-wit-ai/,"
            <p><p style=""color:#7e4396; font-weight:bold"">This post concerns Greenhouse Group Labs, an innovation program for students, established by Greenhouse Group. Labs is an ideal opportunity to test the latest technologies available, while allowing for talented young individuals to deeply explore them and come up with groundbreaking solutions.</p>Yesterday’s big news in the world of chatbots was <a target=""_blank"" href=""https://techcrunch.com/2016/09/19/google-acquires-api-ai-a-company-helping-developers-build-bots-that-arent-awful-to-talk-to/"">Google acquiring Api.ai</a>, a company which allows developers to integrate natural language processing and understanding into their applications. Think of it like talking to someone, but instead, that someone is now your phone, and it actually understands what you’re saying. Services like Api.ai and Wit.ai are capable of processing human speech patterns and filtering useful data like intent and context from it.</p>  

<p>Now that Api.ai is owned by Google, the battle between Api.ai and Wit.ai will probably intensify, as <a target=""_blank"" href=""https://techcrunch.com/2015/01/05/facebook-wit-ai/"">Wit.ai has been Facebook’s property since January 5th of last year</a>. In this article, I will describe the outcome of an independent test I ran last week, without any knowledge of the acquirement of Api.ai by Google. It should provide a semi-realistic scenario for the bots and their capabilities at the moment.</p>  

<p>The test itself is pretty simple because it only focussed on intent, not on the context of the conversation. Both platforms are also capable of recognizing context but that was not necessary for our project, for which I initially ran this test. The basic idea is this:</p>  

<ul>  
<li>I have 4 phrases which could be said within the same conversation</li>  
<li>Every phrase has 3 variations, which should be recognized as the base phrase</li>  
<li>The bot will learn the base phrase first, then we start testing that set</li>  
<li>If it recognizes the base phrase, no points, if it doesn’t, points will be subtracted</li>  
<li>If the bot recognizes a variation correctly, it earns an extra point</li>  
<li>A phrase variation will be learned after testing it, for better recognition results</li>  
</ul>  

<p>Next to intent recognition, there is another important feature of both services, which is entity recognition. This allows us to define a set of words or patterns, which, from then on, are known as a named entity. In this test, I made (or reused) 3 types of entities: pizza_size, pizza_type, and address. With both services, the address entity was predefined so I didn’t have to think of every possible way to define a location or address. This is how I defined pizza_size and pizza_type</p>  

<p><img src=""http://i.imgur.com/HbOD1r2.png""> <br>
<img src=""http://i.imgur.com/am8Kvyc.png"">  </p>

<p>The purpose of these entities is to provide additional data, next to our intent. Imagine you want to order a pizza from a bot, you can simply walk through every step in the process, but giving a compound answer would be way faster. Something like “I’d like a large barbecue chicken pizza” not only gives us your intent, but also the size and type, making a separate step no longer necessary.</p>  

<p>But now, no more delays! On to the actual test, or actually, the test results. Both interfaces were sufficiently understandable and both services provided enough tutorials to get every test up and running within about 10 minutes of registering an account, so I won’t get into that here. You can read the table as follows:</p>  

<ul>  
<li>Base phrases are shown in bold and are the first of their set</li>  
<li>Base phrases can only earn 0 or -1 point, -1 meaning it was not correctly recognized although I learned it that exact phrase</li>  
<li>Phrase variations in the same set are shown between the bolder lines, underneath the emphasized base phrases</li>  
<li>Phrase variations can earn 0 or 1 point, 0 points meaning it did not recognize the variation as the base phrase, 1 point meaning the variation was recognized with the same intent or entities as the base phrase</li>  
</ul>  

<p><img src=""http://i.imgur.com/LjgqW6D.png"">  </p>

<p>As you can see, pointwise, they are pretty close, but both have their respective pros and cons, which are mostly down to their base functionality and accuracy of their provided entity models. If you were to ask me, right here, right now, which of the two is my favorite, I would say Wit.ai. It has a bit of a learning curve to it but other than that, it’s great, especially for the integration with Messenger, which is our target platform. This, of course, is no surprise as Wit.ai was bought by Facebook for over a year and a half ago. Out of the box, the support for compound answers is way better than Api.ai and they have a good system of building conversational bots, for which you can even write plugin code.</p>  

<p>We chose Wit.ai for our project but your project may be very different. I encourage you to setup a test like this one for your specific purpose and come to your own conclusion. Both of them will probably undergo significant improvements to try and get the larger market share as both are owned by big tech companies and now that chatbots are on such a rise to power.</p>  

<p>Cheers!</p>
        ","This post concerns Greenhouse Group Labs, an innovation program for students, established by Greenhouse Group. Labs is an ideal opportunity to test the latest technologies available, while allowing for talented young individuals to deeply explore them and come up with groundbreaking solutions.
Yesterday’s big news in the world of chatbots was Google acquiring Api.ai, a company which allows developers to integrate natural language processing and understanding into their applications. Think of it like talking to someone, but instead, that someone is now your phone, and it actually understands what you’re saying. Services like Api.ai and Wit.ai are capable of processing human speech patterns and filtering useful data like intent and context from it.
Now that Api.ai is owned by Google, the battle between Api.ai and Wit.ai will probably intensify, as Wit.ai has been Facebook’s property since January 5th of last year. In this article, I will describe the outcome of an independent test I ran last week, without any knowledge of the acquirement of Api.ai by Google. It should provide a semi-realistic scenario for the bots and their capabilities at the moment.
The test itself is pretty simple because it only focussed on intent, not on the context of the conversation. Both platforms are also capable of recognizing context but that was not necessary for our project, for which I initially ran this test. The basic idea is this:
I have 4 phrases which could be said within the same conversation
Every phrase has 3 variations, which should be recognized as the base phrase
The bot will learn the base phrase first, then we start testing that set
If it recognizes the base phrase, no points, if it doesn’t, points will be subtracted
If the bot recognizes a variation correctly, it earns an extra point
A phrase variation will be learned after testing it, for better recognition results
Next to intent recognition, there is another important feature of both services, which is entity recognition. This allows us to define a set of words or patterns, which, from then on, are known as a named entity. In this test, I made (or reused) 3 types of entities: pizza_size, pizza_type, and address. With both services, the address entity was predefined so I didn’t have to think of every possible way to define a location or address. This is how I defined pizza_size and pizza_type

The purpose of these entities is to provide additional data, next to our intent. Imagine you want to order a pizza from a bot, you can simply walk through every step in the process, but giving a compound answer would be way faster. Something like “I’d like a large barbecue chicken pizza” not only gives us your intent, but also the size and type, making a separate step no longer necessary.
But now, no more delays! On to the actual test, or actually, the test results. Both interfaces were sufficiently understandable and both services provided enough tutorials to get every test up and running within about 10 minutes of registering an account, so I won’t get into that here. You can read the table as follows:
Base phrases are shown in bold and are the first of their set
Base phrases can only earn 0 or -1 point, -1 meaning it was not correctly recognized although I learned it that exact phrase
Phrase variations in the same set are shown between the bolder lines, underneath the emphasized base phrases
Phrase variations can earn 0 or 1 point, 0 points meaning it did not recognize the variation as the base phrase, 1 point meaning the variation was recognized with the same intent or entities as the base phrase
As you can see, pointwise, they are pretty close, but both have their respective pros and cons, which are mostly down to their base functionality and accuracy of their provided entity models. If you were to ask me, right here, right now, which of the two is my favorite, I would say Wit.ai. It has a bit of a learning curve to it but other than that, it’s great, especially for the integration with Messenger, which is our target platform. This, of course, is no surprise as Wit.ai was bought by Facebook for over a year and a half ago. Out of the box, the support for compound answers is way better than Api.ai and they have a good system of building conversational bots, for which you can even write plugin code.
We chose Wit.ai for our project but your project may be very different. I encourage you to setup a test like this one for your specific purpose and come to your own conclusion. Both of them will probably undergo significant improvements to try and get the larger market share as both are owned by big tech companies and now that chatbots are on such a rise to power.
Cheers!","[Code, Labs, Chatbot, Wit.ai, software, testing, tech, Facebook Messenger, Chat, Robot, Facebook, google, Api.ai]"
16,Commercial Chatbots: What does the Consumer think?,/commercial-chatbots-what-does-the-consumer-think/,"
            <p><p style=""color:#7e4396; font-weight:bold"">This post concerns Greenhouse Group Labs, an innovation program for students, established by Greenhouse Group. Labs is an ideal opportunity to test the latest technologies available, while allowing for talented young individuals to deeply explore them and come up with groundbreaking solutions.</p>Within Greenhouse Group Labs, four students are currently exploring the world of chatbots. The aim is to reveal insights regarding businesses’ incentives to use chatbots and the effect of chatbot usage on customer satisfaction.  Meanwhile, various chatbots will be developed and prototyped, most probably with <a href=""https://wit.ai/"">wit.ai</a>. </p>

<blockquote>  <b>What precisely is a chatbot</b>? According to academics, a chatbot could be defined as <i>an artificially intelligent system that is able to manage a natural conversation with a user and has a sound knowledge base to interact with a user</i>. In other words, a chatbot is some computer-made software with which (whom) you can chat - on websites, messenger services (like Facebook, Telegram etc.) amongst other things. </blockquote>

<p>In fact, Matt Schlicht wrote a complete beginner guide to chatbots, which can be found <a href=""https://chatbotsmagazine.com/the-complete-beginner-s-guide-to-chatbots-8280b7b906ca#.vrtlfzwmz"">here</a>. If you would like to talk to a chatbot yourself, we’d suggest to install a chatbot on <a href=""http://messenger.com/"">Facebook Messenger</a> – click <a href=""https://botlist.co/bots/filter?platform=13"">here</a> for a full collection. </p>

<p>When we started to delve into the relation between the chatbot and the consumer, it surfaced quickly that not much literature was available. And so… we developed an exploratory survey ourselves to bring some understanding about the consumer-chatbot relationship to light. The survey was distributed among about 70 respondents. </p>

<p>We first asked some questions about information gathering and making purchases on a website. After, we asked similar questions, but now about information gathering and making purchases through a chatbot. For instance, “Are you likely to order a pizza through a chatbot?” and “Are you likely to purchase a car through a chatbot?” </p>

<p>Also, some questions about speed were asked. A large majority of the respondents said to prefer to talk to a real person instead of a chatbot. <strong> Interestingly, if the person would take more than ten minutes to answer, an immediate response from a chatbot would be preferred. </strong> Moreover, we included bits and pieces of their knowledge about chatbots and reasons why people would (not) be eager to talk to a chatbot… </p>

<p>Based on the results, you can see the infografic bellow:</p>

<p><img src=""http://i.imgur.com/TR8fN3f.png"" style="" width:95%; margin-left:2.5%;"" class=""full-img""></p>

<p><em>What is next?</em> The coming two weeks we’ll be trying to get our own first chatbot to work, whilst further researching businesses’ incentives for chatbots and the relationship between chatbots and customer <a href=""https://www.youtube.com/watch?v=nrIPxlFzDi0"">satisfaction</a>. So stay tuned :)</p>
        ","This post concerns Greenhouse Group Labs, an innovation program for students, established by Greenhouse Group. Labs is an ideal opportunity to test the latest technologies available, while allowing for talented young individuals to deeply explore them and come up with groundbreaking solutions.
Within Greenhouse Group Labs, four students are currently exploring the world of chatbots. The aim is to reveal insights regarding businesses’ incentives to use chatbots and the effect of chatbot usage on customer satisfaction. Meanwhile, various chatbots will be developed and prototyped, most probably with wit.ai.
What precisely is a chatbot? According to academics, a chatbot could be defined as an artificially intelligent system that is able to manage a natural conversation with a user and has a sound knowledge base to interact with a user. In other words, a chatbot is some computer-made software with which (whom) you can chat - on websites, messenger services (like Facebook, Telegram etc.) amongst other things.
In fact, Matt Schlicht wrote a complete beginner guide to chatbots, which can be found here. If you would like to talk to a chatbot yourself, we’d suggest to install a chatbot on Facebook Messenger – click here for a full collection.
When we started to delve into the relation between the chatbot and the consumer, it surfaced quickly that not much literature was available. And so… we developed an exploratory survey ourselves to bring some understanding about the consumer-chatbot relationship to light. The survey was distributed among about 70 respondents.
We first asked some questions about information gathering and making purchases on a website. After, we asked similar questions, but now about information gathering and making purchases through a chatbot. For instance, “Are you likely to order a pizza through a chatbot?” and “Are you likely to purchase a car through a chatbot?”
Also, some questions about speed were asked. A large majority of the respondents said to prefer to talk to a real person instead of a chatbot. Interestingly, if the person would take more than ten minutes to answer, an immediate response from a chatbot would be preferred. Moreover, we included bits and pieces of their knowledge about chatbots and reasons why people would (not) be eager to talk to a chatbot…
Based on the results, you can see the infografic bellow:
What is next? The coming two weeks we’ll be trying to get our own first chatbot to work, whilst further researching businesses’ incentives for chatbots and the relationship between chatbots and customer satisfaction. So stay tuned :)","[Social advertising, Messenger, node.js, Survey, Commercial, Labs, Chatbot, Code, Facebook]"
17,How to set up ESLint with Airbnb JavaScript Style Guide in WebStorm,/eslint-with-airbnb-javascript-style-guide-in-webstorm/,"
            <p>Last year, Gaya wrote an incredibly useful article about <a href=""/how-to-get-airbnbs-javascript-code-style-working-in-webstorm/"">how to get Airbnb's JavaScript Style Guide working in WebStorm</a>. However, JSCS has <a href=""https://medium.com/@markelog/jscs-end-of-the-line-bc9bf0b3fdb2#ece0"">joined</a> ESLint quite a while ago. We still want our code to validate through Airbnb's JS style guide. So for us, a migration to ESLint was a natural next step.</p>

<p>This article will show you how to quickly get up and running in three easy steps:</p>

<h3 id=""1installeslint"">1. Install ESLint</h3>

<p>The following command will add <a href=""http://eslint.org/"">ESLint</a> and the <a href=""https://github.com/airbnb/javascript"">Airbnb JavaScript Style Guide</a> config to your global npm modules:</p>

<pre><code class=""language-prettyprint lang-shell prettyprinted""><span class=""pln"">npm install eslint eslint</span><span class=""pun"">-</span><span class=""pln"">config</span><span class=""pun"">-</span><span class=""pln"">airbnb </span><span class=""pun"">--</span><span class=""kwd"">global</span><span class=""pln"">  </span></code></pre>

<p>You'll also need some dependencies for working with ES6:</p>

<pre><code class=""language-prettyprint lang-shell prettyprinted""><span class=""pln"">npm install eslint</span><span class=""pun"">-</span><span class=""pln"">plugin</span><span class=""pun"">-</span><span class=""pln"">jsx</span><span class=""pun"">-</span><span class=""pln"">a11y@</span><span class=""pun"">^</span><span class=""lit"">2.0</span><span class=""pun"">.</span><span class=""lit"">0</span><span class=""pln""> eslint</span><span class=""pun"">-</span><span class=""pln"">plugin</span><span class=""pun"">-</span><span class=""pln"">react eslint</span><span class=""pun"">-</span><span class=""pln"">plugin</span><span class=""pun"">-</span><span class=""kwd"">import</span><span class=""pln""> babel</span><span class=""pun"">-</span><span class=""pln"">eslint </span><span class=""pun"">--</span><span class=""kwd"">global</span><span class=""pln"">  </span></code></pre>

<h3 id=""2createaglobalconfigfile"">2. Create a global config file</h3>

<p>With your favourite text editor, create a new file containing the following JSON:</p>

<pre><code class=""language-prettyprint lang-json prettyprinted""><span class=""pun"">{</span><span class=""pln"">
  </span><span class=""str"">""extends""</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""str"">""airbnb""</span><span class=""pun"">,</span><span class=""pln"">
  </span><span class=""str"">""rules""</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">
    </span><span class=""str"">""func-names""</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""pun"">[</span><span class=""str"">""error""</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""str"">""never""</span><span class=""pun"">]</span><span class=""pln"">
  </span><span class=""pun"">}</span><span class=""pln"">
</span><span class=""pun"">}</span></code></pre>

<p>Save it as <code>.eslintrc</code> in your file system's root, or any other <em>parent folder</em> containing your project folders. You can always create another <code>.eslintrc</code> in your project folder itself later on to override the global rules.</p>

<p>To prevent unnecessary warnings, <code>""func-names""</code> is manually switched off here because Airbnb's JS style guide allows anonymous functions.</p>

<h3 id=""3configurewebstorm20162"">3. Configure WebStorm (2016.2)</h3>

<p>When enabling the ESLint tool in Webstorm, by default it will search for any <code>.eslintrc</code> files containing configuration. Our file in step 2 will automatically serve as a global config for all your projects.</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Jul/Screen_Shot_2016_07_12_at_13_24_14-1468322688540.png"" alt=""Enabling ESLint in WebStorm 2016.2"" class=""full-img""></p>

<p>Airbnb tells us to to abide by a <a href=""http://eslint.org/docs/rules/max-len"">max-len</a> of 100 characters per line. Of course we want our margin line in the code viewport to match that:</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Sep/right_margin-1473182345612.png"" alt="""" class=""full-img""></p>

<p>Lastly, to get WebStorm to stop throwing redundant warning at us, we should disable checking for trailing commas in arrays and objects:</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Sep/Unneeded_last_comma-1473182232387.png"" alt=""WebStorm 2016.2 disabling unneeded trailing commas"" class=""full-img""></p>

<p>If you only write ES6 JavaScript in your projects, you're all set! WebStorm will now show you warnings in yellow, and errors in red squiggly lines, on the fly.</p>

<h3 id=""workingwithecmascript5es5"">Working with ECMAScript 5 (ES5)</h3>

<p>If you're creating a project that uses ES5, make sure your project's <code>.eslintrc</code> extends <code>""airbnb/legacy""</code> instead of <code>""airbnb""</code> so it knows it has to test against ES5:</p>

<pre><code class=""language-prettyprint lang-json prettyprinted""><span class=""pun"">{</span><span class=""pln"">
  </span><span class=""str"">""extends""</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""str"">""airbnb/legacy""</span><span class=""pln"">
</span><span class=""pun"">}</span></code></pre>

<p>Of course you can tweak any rule or plugin to your liking using the <a href=""http://eslint.org/docs/user-guide/getting-started#configuration"">ESLint docs</a>.</p>

<p>Happy linting!</p>
        ","Last year, Gaya wrote an incredibly useful article about how to get Airbnb's JavaScript Style Guide working in WebStorm. However, JSCS has joined ESLint quite a while ago. We still want our code to validate through Airbnb's JS style guide. So for us, a migration to ESLint was a natural next step.
This article will show you how to quickly get up and running in three easy steps:
1. Install ESLint
The following command will add ESLint and the Airbnb JavaScript Style Guide config to your global npm modules:
npm install eslint eslint-config-airbnb --global  
You'll also need some dependencies for working with ES6:
npm install eslint-plugin-jsx-a11y@^2.0.0 eslint-plugin-react eslint-plugin-import babel-eslint --global  
2. Create a global config file
With your favourite text editor, create a new file containing the following JSON:
{
  ""extends"": ""airbnb"",
  ""rules"": {
    ""func-names"": [""error"", ""never""]
  }
}
Save it as .eslintrc in your file system's root, or any other parent folder containing your project folders. You can always create another .eslintrc in your project folder itself later on to override the global rules.
To prevent unnecessary warnings, ""func-names"" is manually switched off here because Airbnb's JS style guide allows anonymous functions.
3. Configure WebStorm (2016.2)
When enabling the ESLint tool in Webstorm, by default it will search for any .eslintrc files containing configuration. Our file in step 2 will automatically serve as a global config for all your projects.
Airbnb tells us to to abide by a max-len of 100 characters per line. Of course we want our margin line in the code viewport to match that:
Lastly, to get WebStorm to stop throwing redundant warning at us, we should disable checking for trailing commas in arrays and objects:
If you only write ES6 JavaScript in your projects, you're all set! WebStorm will now show you warnings in yellow, and errors in red squiggly lines, on the fly.
Working with ECMAScript 5 (ES5)
If you're creating a project that uses ES5, make sure your project's .eslintrc extends ""airbnb/legacy"" instead of ""airbnb"" so it knows it has to test against ES5:
{
  ""extends"": ""airbnb/legacy""
}
Of course you can tweak any rule or plugin to your liking using the ESLint docs.
Happy linting!","[JavaScript, Code]"
18,Google Analytics' new event metrics explained,/google-analytics-new-event-metrics-explained/,"
            <p>As of today, <strong>Google has added two new event metrics to Google Analytics</strong>: Unique Events (New) and Unique Dimension Combinations. If you still use the old metric, you'll get a message telling you that it will be dropped:</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Sep/tmt_event_drop_error-1473069675480.PNG"" alt=""""></p>

<p><em>Unique Events are deprecated warning in Google Analytics.</em></p>

<p>Let’s see how the two new metrics work.</p>

<p><strong>Update 13/09/2016:</strong> it looks like the update has been rolled back by Google. So currently the new metrics won't show up in Google Analytics.</p>

<h2 id=""comparingthemetrics"">Comparing the metrics</h2>

<p>Looking at data we see that <strong>Unique Dimension Combinations returns the same numbers as the old Unique Events</strong>. The Unique Events (NEW) metric returns different data:</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Sep/tmt_new_events_category-1473068455548.PNG"" alt=""New Event Metrics with event category"" class=""full-img""></p>

<p><em>Sample data export for new event metrics.</em></p>

<h2 id=""thedifference"">The Difference</h2>

<p>So what causes these different numbers? It becomes clear as soon as we add the event action:</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Sep/tmt_new_events_category_action-1473068702743.PNG"" alt=""New Event Metrics with event category and event action"" class=""full-img""></p>

<p><em>Sample data export with event action.</em></p>

<p>As you can see in the example above, the totals of the old Unique Events don’t add up based on event actions. It looks like <strong>the old Unique Events totals are deduplicated</strong>: if a user had three event actions and one category, it’ll count as 1 for the totals of the old Unique Event (or the new Unique Dimension Combinations).</p>

<p>The numbers of the new Unique Events (New) metric do add up correctly. Apparently <strong>the new metric always takes every event dimension (category, action, and label) into account when calculating unique values</strong>. As you can see in the example data, we still have a difference in data for the country link action (52 vs 56). Probably there were 4 events removed by deduplication in the old Unique event metric. </p>

<p>To test this I've created a flat table with all event dimensions to see if this makes the numbers per row equal:</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Sep/tmt_new_events_category_action_label-1473068810187.PNG"" alt=""New Event Metrics flat table with event category, event action, and event label"" class=""full-img""></p>

<p><em>Flat table with all event dimensions added.</em></p>

<p>As expected, the values are the same now. So <strong>if you're using the API to import event data you're in luck: as long as you always import event categories, actions, and labels, the values won't change</strong>. </p>

<h2 id=""takeaway"">Takeaway</h2>

<p>Based on the examples above we can state the following about the new metrics:</p>

<h3 id=""uniquedimensioncombinations"">Unique Dimension Combinations</h3>

<p><strong>The new Unique Dimension Combinations is the same as the old Unique Events metric</strong>. The totals are always deduplicated based on the active dimensions in your report. Use this value if you want to know if an event occurred on a page, ignoring different event category, action and label combinations. Depending on your report, the totals may be different from the sum of the rows. <strong>If a user has 3 events with the same category but different actions, it will count as 1 Unique Dimension Combination</strong>.</p>

<h3 id=""uniqueeventsnew"">Unique Events (New)</h3>

<p><strong>The Unique Events (New) metric always takes all event dimensions (category, action, and label) into account</strong>. Use this to see how many unique event combinations occurred on a page. The totals always add up correctly. <strong>If a user has 3 events with the same category but different actions, it will count as 3 Unique Events (New)</strong>.</p>

<p>Happy analysing!</p>
        ","As of today, Google has added two new event metrics to Google Analytics: Unique Events (New) and Unique Dimension Combinations. If you still use the old metric, you'll get a message telling you that it will be dropped:
Unique Events are deprecated warning in Google Analytics.
Let’s see how the two new metrics work.
Update 13/09/2016: it looks like the update has been rolled back by Google. So currently the new metrics won't show up in Google Analytics.
Comparing the metrics
Looking at data we see that Unique Dimension Combinations returns the same numbers as the old Unique Events. The Unique Events (NEW) metric returns different data:
Sample data export for new event metrics.
The Difference
So what causes these different numbers? It becomes clear as soon as we add the event action:
Sample data export with event action.
As you can see in the example above, the totals of the old Unique Events don’t add up based on event actions. It looks like the old Unique Events totals are deduplicated: if a user had three event actions and one category, it’ll count as 1 for the totals of the old Unique Event (or the new Unique Dimension Combinations).
The numbers of the new Unique Events (New) metric do add up correctly. Apparently the new metric always takes every event dimension (category, action, and label) into account when calculating unique values. As you can see in the example data, we still have a difference in data for the country link action (52 vs 56). Probably there were 4 events removed by deduplication in the old Unique event metric.
To test this I've created a flat table with all event dimensions to see if this makes the numbers per row equal:
Flat table with all event dimensions added.
As expected, the values are the same now. So if you're using the API to import event data you're in luck: as long as you always import event categories, actions, and labels, the values won't change.
Takeaway
Based on the examples above we can state the following about the new metrics:
Unique Dimension Combinations
The new Unique Dimension Combinations is the same as the old Unique Events metric. The totals are always deduplicated based on the active dimensions in your report. Use this value if you want to know if an event occurred on a page, ignoring different event category, action and label combinations. Depending on your report, the totals may be different from the sum of the rows. If a user has 3 events with the same category but different actions, it will count as 1 Unique Dimension Combination.
Unique Events (New)
The Unique Events (New) metric always takes all event dimensions (category, action, and label) into account. Use this to see how many unique event combinations occurred on a page. The totals always add up correctly. If a user has 3 events with the same category but different actions, it will count as 3 Unique Events (New).
Happy analysing!","[Analytics, google analytics]"
19,Get more out of Facebook Messenger,/get-more-out-of-facebook-messenger/,"
            <p>Answering questions and interacting with people on a Facebook business page was already possible. Now there are options which make this a lot easier, like automatic replies and the Messenger code and username. Do you already make use of these possibilities?  </p>

<h2 id=""howtosetupautomaticrepliesinfacebookmessenger"">How to set up automatic replies in Facebook Messenger</h2>

<p>You want people to get a reply asap, but you are not available at the moment. The Automatic replies feature is the solution with personalized messages. This functionality is available on desktop in ‘settings’ and choose 'messaging' when you have page admin permissions. </p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Sep/blog_1_messaging-1472810333330.png"" alt="""" class=""full-img""></p>

<p>These are the available options:</p>

<h3 id=""greetings"">Greetings</h3>

<p>This functionality is an automatic greeting when people contact you for the very first time. In this automatically sent message, it's smart to display contact information, like a phone number or the opening hours. This way you can already provide valuable information in the initial message.</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Sep/Blog_1_Greeting_Engels-1472810346803.png"" alt=""""></p>

<h3 id=""instantreplies"">Instant replies</h3>

<p>With this option, you can show people that you have received their message and the fact that you will reply asap.  </p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Sep/blog_1_Instant_reply_engels-1472810368294.png"" alt=""""></p>

<h3 id=""awaymessage"">Away message</h3>

<p>When you are not available for instance in the evening, then choose this option. To do this, you first have to set your status to ‘away’ in your ‘messages’ tab. When someone messages you in the evening and you enabled this option it will not affect the response time of the business page.</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Sep/Blog_1_Messages_tab_2_blur-1472810435017.png"" alt="""" class=""full-img""></p>

<p>Then go to 'settings' again and choose 'messaging', to type your message that belongs to the 'away' status.</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Sep/blog_1_away_message_engels-1472810386876.png"" alt=""""></p>

<h2 id=""messengerusername"">Messenger username</h2>

<p>To make it easier for people to reach you, it’s possible to display the username of the page in an email signature for example. The account name is the username, which begins with an @, it looks like this: @weareblossom. It’s comparable with Twitter. You can find your account name on your Facebook business page.</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Sep/blog_1_username-1472810508811.png"" alt="""" class=""full-img""></p>

<h2 id=""messengercode"">Messenger Code</h2>

<p>The Messenger code is comparable with a link, only now people scan a code to start a conversation in Messenger. Share this code on business cards and people can contact you very easily. <br>
To get this code, go to 'messages' tab on your desktop computer. At the bottom of the page, you can find the option to download the code.</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Sep/Blog_1_messenger_code_pagina_blur-1472810472908.png"" alt="""" class=""full-img""></p>

<p>This is how it looks like when a consumer scans a Messenger code:</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Sep/Blog_1_messenger_code_blur_app-1472810603813.png"" alt=""""></p>

<p>Now you know how to use Facebook Messenger's functionalities!</p>
        ","Answering questions and interacting with people on a Facebook business page was already possible. Now there are options which make this a lot easier, like automatic replies and the Messenger code and username. Do you already make use of these possibilities?
How to set up automatic replies in Facebook Messenger
You want people to get a reply asap, but you are not available at the moment. The Automatic replies feature is the solution with personalized messages. This functionality is available on desktop in ‘settings’ and choose 'messaging' when you have page admin permissions.
These are the available options:
Greetings
This functionality is an automatic greeting when people contact you for the very first time. In this automatically sent message, it's smart to display contact information, like a phone number or the opening hours. This way you can already provide valuable information in the initial message.
Instant replies
With this option, you can show people that you have received their message and the fact that you will reply asap.
Away message
When you are not available for instance in the evening, then choose this option. To do this, you first have to set your status to ‘away’ in your ‘messages’ tab. When someone messages you in the evening and you enabled this option it will not affect the response time of the business page.
Then go to 'settings' again and choose 'messaging', to type your message that belongs to the 'away' status.
Messenger username
To make it easier for people to reach you, it’s possible to display the username of the page in an email signature for example. The account name is the username, which begins with an @, it looks like this: @weareblossom. It’s comparable with Twitter. You can find your account name on your Facebook business page.
Messenger Code
The Messenger code is comparable with a link, only now people scan a code to start a conversation in Messenger. Share this code on business cards and people can contact you very easily.
To get this code, go to 'messages' tab on your desktop computer. At the bottom of the page, you can find the option to download the code.
This is how it looks like when a consumer scans a Messenger code:
Now you know how to use Facebook Messenger's functionalities!","[Social advertising, Facebook]"
20,Calculation of confidence intervals for ratios,/calculation-of-confidence-intervals-for-ratios/,"
            <p>The science of campaign optimization was among the very first things I was introduced to when first coming to work in programmatic media at Bannerconnect, and it was with great interest that I learned the particular steps our campaign managers regularly take to ensure campaigns run in the most effective way for their advertiser.</p>

<h5 id=""statisticallysoundoptimization"">Statistically sound optimization</h5>

<p>From the data scientist point of view, I was of course very interested in the statistics behind how a certain domain, creative type, or even time of day could be judged to be better than any of the others. When optimizing toward clicks, view times, landings, or final conversions, the answer to the above seems simple enough – there are many types of comparative analysis that one could do between different variable types, and indeed a confidence interval for this binomial type data can be easily calculated, with the resulting confidence interval being largely dependent on the sample size present.  The below image visualizes this: domain 2 may appear better than domain 1, but with overlapping confidence intervals, we cannot be sure that there is a real difference.</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Aug/plot_error-1472109751912.png"" alt="""" class=""full-img""></p>

<h5 id=""thedifficultywithratios"">The difficulty with ratios</h5>

<p>However, I quickly came across a more interesting problem: how do you calculate the confidence interval of a ratio of two populations, where there is an uncertainty in each population? For example, what is the confidence interval in my Cost per Action (CPA),  given that both the Cost of an impression has a natural variation when taken over a large dataset (and can be presumed to be normally distributed), and the “Action” will also have an uncertainty related to its sample size, and should be binomially distributed? What is the best way to combine these two uncertainties in my measurements, to have a good estimate of how much faith I can put in my calculated CPA?</p>

<h5 id=""previousworkinhealtheconomics"">Previous work in health economics</h5>

<p>A quick Google here revealed that not only was I the only one struggling with this problem, but that it was actually more complicated than I had thought. My literature review led me to a different field entirely – that of epidemiology and health economics, where the similar problems had been explored, ignored, or criticized, in turn.</p>

<p>This problem comes about in health economics in the form of cost effectivity ratios – the calculation of how much a treatment costs, versus how likely it is to work. As noted in the paper of Polsky et. al (1), methods for calculating the confidence interval in these ratio are less well developed in their field, and as such often left out of publications. Their work gives a comparison of the effectivity of four methods of calculating these intervals, judging them by performing a Monte Carlo experiment. Of the methods they explore, I’ll mention here only the two which they found most accurate: Fieller’s Method, and a Bootstrap method. </p>

<h5 id=""fiellersmethod"">Fieller's Method</h5>

<p>Fieller’s method was first published in 1954, and provides an analytical method for calculating the confidence interval of two ratios, where each part of the ratio may be from a different distribution, i.e. have unrelated uncertainties (2). While this method is very successful for the health economics field, unfortunately for us, it has some conditions which make it inaccurate for the calculation of our CPA, for example. The main restriction that is violated here is that of normality:  our “Action” data here will fail this requirement. Another difficulty to be aware of here is that the proportion of positive actions we have in forming our CPA will be extremely low – often less than 1%, which will lead to difficulties in the calculation – a problem that is uncommon in the health field.</p>

<h5 id=""bootstrapping"">Bootstrapping</h5>

<p>Happily, the second of the recommended methods they mention, Bootstrapping, leads to much more success. Bootstrapping, simply put, is a method for estimating an estimator (for example the standard deviation), using resampling with replacement of your data. In simple terms, what this means for our example is that we should use a large amount of input data, consisting of cost per impression, and whether the impression resulted in a conversion or not. We can then take a sample of some of these rows, to calculate the CPA. By calculating this thousands of times over different samples of our input data, the distribution of the CPA can be built up, from which we could, for example calculate a standard deviation. While this can sound fiddly, in practice there are bootstrapping routines built into many popular programmes – for example R, SciPy, and MatLab, to name a few. </p>

<p>Armed with these methods, calculating confidence intervals for ratios with unrelated errors becomes much easier, and most importantly, campaign optimizations can be made on statistically sound information!</p>

<p>(1)    <a href=""https://scholar.google.nl/scholar?hl=en&amp;q=Polsky%2C+Glick%2C+Willke%2C+and+Schulman%2C+%E2%80%9CConfidence+intervals+for+cost-effectiveness+ratios%3A+A+comparison+of+four+methods%E2%80%9D%2C+Health+Economics%2C+Vol+6%3A+243-252+%281997%29&amp;btnG=&amp;as_sdt=1%2C5&amp;as_sdtp="">Polsky, Glick, Willke, and Schulman, “Confidence intervals for cost-effectiveness ratios: A comparison of four methods”</a>, Health Economics, Vol 6: 243-252 (1997).</p>

<p>(2)    <a href=""https://scholar.google.nl/scholar?hl=en&amp;q=Fieller%2C+%E2%80%9CSome+Problems+in+Interval+Estimation%E2%80%9D&amp;btnG=&amp;as_sdt=1%2C5&amp;as_sdtp="">Fieller, “Some Problems in Interval Estimation”</a>, Journal of the Royal Statistical Society, Series B (Methodological), Vol. 16, No. 2 (1954), 175-185.</p>
        ","The science of campaign optimization was among the very first things I was introduced to when first coming to work in programmatic media at Bannerconnect, and it was with great interest that I learned the particular steps our campaign managers regularly take to ensure campaigns run in the most effective way for their advertiser.
Statistically sound optimization
From the data scientist point of view, I was of course very interested in the statistics behind how a certain domain, creative type, or even time of day could be judged to be better than any of the others. When optimizing toward clicks, view times, landings, or final conversions, the answer to the above seems simple enough – there are many types of comparative analysis that one could do between different variable types, and indeed a confidence interval for this binomial type data can be easily calculated, with the resulting confidence interval being largely dependent on the sample size present. The below image visualizes this: domain 2 may appear better than domain 1, but with overlapping confidence intervals, we cannot be sure that there is a real difference.
The difficulty with ratios
However, I quickly came across a more interesting problem: how do you calculate the confidence interval of a ratio of two populations, where there is an uncertainty in each population? For example, what is the confidence interval in my Cost per Action (CPA), given that both the Cost of an impression has a natural variation when taken over a large dataset (and can be presumed to be normally distributed), and the “Action” will also have an uncertainty related to its sample size, and should be binomially distributed? What is the best way to combine these two uncertainties in my measurements, to have a good estimate of how much faith I can put in my calculated CPA?
Previous work in health economics
A quick Google here revealed that not only was I the only one struggling with this problem, but that it was actually more complicated than I had thought. My literature review led me to a different field entirely – that of epidemiology and health economics, where the similar problems had been explored, ignored, or criticized, in turn.
This problem comes about in health economics in the form of cost effectivity ratios – the calculation of how much a treatment costs, versus how likely it is to work. As noted in the paper of Polsky et. al (1), methods for calculating the confidence interval in these ratio are less well developed in their field, and as such often left out of publications. Their work gives a comparison of the effectivity of four methods of calculating these intervals, judging them by performing a Monte Carlo experiment. Of the methods they explore, I’ll mention here only the two which they found most accurate: Fieller’s Method, and a Bootstrap method.
Fieller's Method
Fieller’s method was first published in 1954, and provides an analytical method for calculating the confidence interval of two ratios, where each part of the ratio may be from a different distribution, i.e. have unrelated uncertainties (2). While this method is very successful for the health economics field, unfortunately for us, it has some conditions which make it inaccurate for the calculation of our CPA, for example. The main restriction that is violated here is that of normality: our “Action” data here will fail this requirement. Another difficulty to be aware of here is that the proportion of positive actions we have in forming our CPA will be extremely low – often less than 1%, which will lead to difficulties in the calculation – a problem that is uncommon in the health field.
Bootstrapping
Happily, the second of the recommended methods they mention, Bootstrapping, leads to much more success. Bootstrapping, simply put, is a method for estimating an estimator (for example the standard deviation), using resampling with replacement of your data. In simple terms, what this means for our example is that we should use a large amount of input data, consisting of cost per impression, and whether the impression resulted in a conversion or not. We can then take a sample of some of these rows, to calculate the CPA. By calculating this thousands of times over different samples of our input data, the distribution of the CPA can be built up, from which we could, for example calculate a standard deviation. While this can sound fiddly, in practice there are bootstrapping routines built into many popular programmes – for example R, SciPy, and MatLab, to name a few.
Armed with these methods, calculating confidence intervals for ratios with unrelated errors becomes much easier, and most importantly, campaign optimizations can be made on statistically sound information!
(1) Polsky, Glick, Willke, and Schulman, “Confidence intervals for cost-effectiveness ratios: A comparison of four methods”, Health Economics, Vol 6: 243-252 (1997).
(2) Fieller, “Some Problems in Interval Estimation”, Journal of the Royal Statistical Society, Series B (Methodological), Vol. 16, No. 2 (1954), 175-185.",[Data Science]
21,How to write a basic viewability tracker,/how-to-write-a-basic-viewability-tracker/,"
            <p>Although viewability is not always a <a href=""https://www.themarketingtechnologist.co/viewability-does-not-guarantee-success/"">guarantee for success</a>, it's a hot topic in the land of online marketing. We do not only track viewability in our display campaigns to optimise the exposure time of our campaigns, but we also use viewability to see how visitors are using our client's website and apply personalisation based on that data. Also, the viewability data is used to gather some data and insights for A/B tests. In this post, I'd like to show you the basics of tracking the viewability of an element on a web page. </p>

<h4 id=""trackwhetheranelementisinviewornot"">Track whether an element is in view or not</h4>

<p>For this basic example, we use a simple HTML element with an ID of <em>foo</em> with some basic styling applied to it (<a href=""https://jsfiddle.net/4qf5ep3f/"">Fiddle</a>):</p>

<pre><code class=""language-prettypring lang-html"">&lt;div id=""foo"" /&gt;

&lt;style&gt;  
#foo {
  border-top: 8px solid #7e4396;
  border-bottom: 8px solid #7e4396;
  width: 336px;
  height: 250px;
  display: block;
  background-color: #f0efef;
}
&lt;/style&gt;  
</code></pre>

<p>The most simple approach to check whether an element is in view is to see if there is at least a distance of 0 pixels or more between the top of an element and the top of the viewport, and at least 0 pixels or more between the bottom of an element and the bottom of the viewport. </p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Aug/viewability-1471938060846.png"" alt="""" class=""full-img""></p>

<p>To calculate this, we need to know the position of the element relative to the viewport, the height of the element and the viewport height. </p>

<h5 id=""figuringouttheheightoftheviewport"">Figuring out the height of the viewport</h5>

<p>Because of the differences in the way browser's layout engines measure the window's width, we need to check for <a href=""http://stackoverflow.com/questions/6942785/window-innerwidth-vs-document-documentelement-clientwidth"">two properties</a>: <code>window.innerWidth</code> and <code>document.documentElement.clientWidth</code>. Webkit (Chrome and Safari) claims innerWidth is smaller than clientWidth, Trident (Internet Explorer) and Presto (Opera) claim innerWidth is bigger than clientWidth and Gecko (Firefox) claims innerWidth is the same size as clientWidth. To get the correct value in all situations, we can grab the highest value of both by using <code>Math.max</code>:</p>

<pre><code class=""language-prettyprint-javascript prettyprinted""><span class=""kwd"">var</span><span class=""pln""> stageHeight </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""typ"">Math</span><span class=""pun"">.</span><span class=""pln"">max</span><span class=""pun"">(</span><span class=""pln"">document</span><span class=""pun"">.</span><span class=""pln"">documentElement</span><span class=""pun"">.</span><span class=""pln"">clientWidth</span><span class=""pun"">,</span><span class=""pln""> window</span><span class=""pun"">.</span><span class=""pln"">innerWidth </span><span class=""pun"">||</span><span class=""pln""> </span><span class=""lit"">0</span><span class=""pun"">);</span><span class=""pln"">  </span></code></pre>

<h5 id=""getboundingclientrect"">getBoundingClientRect()</h5>

<p>Now we know the height of the viewport, we need to find the element's position relative to the viewport. We can use the <a href=""https://developer.mozilla.org/en-US/docs/Web/API/Element/getBoundingClientRect""><code>getBoundingClientRect</code></a> method for this. The getBoundingClientRect() method returns the size of an element and its position relative to the viewport. Bingo. </p>

<pre><code class=""language-prettyprint-javascript prettyprinted""><span class=""com"">// Create a reference to our element</span><span class=""pln"">
</span><span class=""kwd"">var</span><span class=""pln""> element </span><span class=""pun"">=</span><span class=""pln""> document</span><span class=""pun"">.</span><span class=""pln"">querySelector</span><span class=""pun"">(</span><span class=""str"">'#foo'</span><span class=""pun"">);</span><span class=""pln"">  
</span><span class=""com"">// Get the bounding rectangle of our element</span><span class=""pln"">
</span><span class=""kwd"">var</span><span class=""pln""> elementBoundingBox </span><span class=""pun"">=</span><span class=""pln""> element</span><span class=""pun"">.</span><span class=""pln"">getBoundingClientRect</span><span class=""pun"">();</span><span class=""pln"">  </span></code></pre>

<p>We can now add these properties to what we already know: <br>
<img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Aug/elementbox-1471939669333.png"" alt="""" class=""full-img""></p>

<p>The only missing value is the distance from the element to the viewport's bottom. We can simply calculate this value by subtracting the sum of the element's top and height value from the viewport's height. </p>

<pre><code class=""language-prettyprint-javascript prettyprinted""><span class=""kwd"">var</span><span class=""pln""> elementsTopY </span><span class=""pun"">=</span><span class=""pln""> elementBoundingBox</span><span class=""pun"">.</span><span class=""pln"">top</span><span class=""pun"">;</span><span class=""pln"">  
</span><span class=""kwd"">var</span><span class=""pln""> elementsBottomY </span><span class=""pun"">=</span><span class=""pln""> elementBoundingBox</span><span class=""pun"">.</span><span class=""pln"">top </span><span class=""pun"">+</span><span class=""pln""> elementBoundingBox</span><span class=""pun"">.</span><span class=""pln"">height</span><span class=""pun"">;</span><span class=""pln"">  </span></code></pre>

<h5 id=""combinestageheightgetboundingclientrect"">Combine stageHeight &amp; getBoundingClientRect()</h5>

<p>Now that we have all the values we need, we can simple calculate whether our element is 100% in view or not:</p>

<pre><code class=""language-prettyprint-javascript prettyprinted""><span class=""kwd"">var</span><span class=""pln""> elementIsInView </span><span class=""pun"">=</span><span class=""pln""> elementsTopY </span><span class=""pun"">&gt;=</span><span class=""pln""> </span><span class=""lit"">0</span><span class=""pln""> </span><span class=""pun"">&amp;&amp;</span><span class=""pln""> elementsBottomY </span><span class=""pun"">&lt;</span><span class=""pln""> stageHeight</span><span class=""pun"">;</span><span class=""pln"">  </span></code></pre>

<p>Before we continue, let's first wrap our code in a function so we can re-use it later. </p>

<pre><code class=""language-prettyprint-javascript prettyprinted""><span class=""kwd"">function</span><span class=""pln""> elementIsInView</span><span class=""pun"">(</span><span class=""pln"">element</span><span class=""pun"">,</span><span class=""pln""> stageHeight</span><span class=""pun"">)</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">  
  </span><span class=""kwd"">var</span><span class=""pln""> elementBoundingBox </span><span class=""pun"">=</span><span class=""pln""> element</span><span class=""pun"">.</span><span class=""pln"">getBoundingClientRect</span><span class=""pun"">();</span><span class=""pln"">
  </span><span class=""kwd"">var</span><span class=""pln""> elementsTopY </span><span class=""pun"">=</span><span class=""pln""> elementBoundingBox</span><span class=""pun"">.</span><span class=""pln"">top</span><span class=""pun"">;</span><span class=""pln"">
  </span><span class=""kwd"">var</span><span class=""pln""> elementsBottomY </span><span class=""pun"">=</span><span class=""pln""> elementBoundingBox</span><span class=""pun"">.</span><span class=""pln"">top </span><span class=""pun"">+</span><span class=""pln""> elementBoundingBox</span><span class=""pun"">.</span><span class=""pln"">height</span><span class=""pun"">;</span><span class=""pln"">
  </span><span class=""kwd"">return</span><span class=""pln""> elementsTopY </span><span class=""pun"">&gt;=</span><span class=""pln""> </span><span class=""lit"">0</span><span class=""pln""> </span><span class=""pun"">&amp;&amp;</span><span class=""pln""> elementsBottomY </span><span class=""pun"">&lt;</span><span class=""pln""> stageHeight</span><span class=""pun"">;</span><span class=""pln"">
</span><span class=""pun"">}</span></code></pre>

<h4 id=""updatewhenauserscrolls"">Update when a user scrolls</h4>

<p>If we want to track the viewability of an element when a visitor scrolls the page, we need to track the user's scroll behaviour. We can do this by listening to the <code>scroll</code> event of the <code>window</code> object. </p>

<pre><code class=""language-prettyprint-javascript prettyprinted""><span class=""pln"">window</span><span class=""pun"">.</span><span class=""pln"">addEventListener</span><span class=""pun"">(</span><span class=""str"">""scroll""</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""kwd"">function</span><span class=""pun"">(</span><span class=""pln"">e</span><span class=""pun"">)</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">  
  </span><span class=""com"">// Do something when a user scrolls</span><span class=""pln"">
  console</span><span class=""pun"">.</span><span class=""pln"">log</span><span class=""pun"">(</span><span class=""str"">'Scrolling'</span><span class=""pun"">);</span><span class=""pln"">
</span><span class=""pun"">});</span></code></pre>

<p>So now instead of just logging the word <em>'Scrolling'</em>, we want to see if the viewability state of our element has changed. Luckily we've just created a function to check the element's visibility, so we can use that in our scroll event handler:</p>

<pre><code class=""language-prettyprint-javascript prettyprinted""><span class=""pln"">window</span><span class=""pun"">.</span><span class=""pln"">addEventListener</span><span class=""pun"">(</span><span class=""str"">""scroll""</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""kwd"">function</span><span class=""pun"">(</span><span class=""pln"">e</span><span class=""pun"">)</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">  
  </span><span class=""kwd"">if</span><span class=""pln""> </span><span class=""pun"">(</span><span class=""pln"">elementIsInView</span><span class=""pun"">())</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">
    console</span><span class=""pun"">.</span><span class=""pln"">log</span><span class=""pun"">(</span><span class=""str"">'Our element is viewable'</span><span class=""pun"">);</span><span class=""pln"">
  </span><span class=""pun"">}</span><span class=""pln""> </span><span class=""kwd"">else</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">
    console</span><span class=""pun"">.</span><span class=""pln"">log</span><span class=""pun"">(</span><span class=""str"">'Our element is not viewable'</span><span class=""pun"">);</span><span class=""pln"">
  </span><span class=""pun"">}</span><span class=""pln"">
</span><span class=""pun"">});</span></code></pre>

<p>This works, but the only problem is that the check is only executed after the visitor starts scrolling, and not initially when the script in executed on load. We need to change our code just a bit to fix this. First, we need to move ('extract') our viewability check call away from the scroll handler. We'll create a separate function for this. </p>

<pre><code class=""language-prettyprint-javascript prettyprinted""><span class=""kwd"">function</span><span class=""pln""> updateElementViewabilityMessage</span><span class=""pun"">()</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">  
  </span><span class=""kwd"">if</span><span class=""pln""> </span><span class=""pun"">(</span><span class=""pln"">elementIsInView</span><span class=""pun"">())</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">
    console</span><span class=""pun"">.</span><span class=""pln"">log</span><span class=""pun"">(</span><span class=""str"">'Our element is viewable'</span><span class=""pun"">);</span><span class=""pln"">
  </span><span class=""pun"">}</span><span class=""pln""> </span><span class=""kwd"">else</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">
    console</span><span class=""pun"">.</span><span class=""pln"">log</span><span class=""pun"">(</span><span class=""str"">'Our element is not viewable'</span><span class=""pun"">);</span><span class=""pln"">
  </span><span class=""pun"">}</span><span class=""pln"">
</span><span class=""pun"">}</span><span class=""pln"">

window</span><span class=""pun"">.</span><span class=""pln"">addEventListener</span><span class=""pun"">(</span><span class=""str"">""scroll""</span><span class=""pun"">,</span><span class=""pln""> updateElementViewabilityMessage</span><span class=""pun"">);</span><span class=""pln"">  </span></code></pre>

<p>Now, all we have to do to check the viewability when the page loads, is calling the <code>updateElementViewabilityMessage</code> function manually. Like this:</p>

<pre><code class=""language-prettyprint-javascript prettyprinted""><span class=""pln"">updateElementViewabilityMessage</span><span class=""pun"">();</span><span class=""pln"">  </span></code></pre>

<h4 id=""debouncingthescrollevent"">Debouncing the scroll event</h4>

<p>The scroll event is triggered very often, which can make your viewability script's performance pretty bad, especially when you execute some complex or DOM manipulation in your handler. We can fix this by <a href=""https://remysharp.com/2010/07/21/throttling-function-calls"">debouncing</a> (also referred to as <em>throttling</em>) the scroll event. We can use this small throttle method:</p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""kwd"">var</span><span class=""pln""> debounce </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""kwd"">function</span><span class=""pun"">(</span><span class=""pln"">i</span><span class=""pun"">,</span><span class=""pln""> t</span><span class=""pun"">,</span><span class=""pln""> e</span><span class=""pun"">)</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">  
    e </span><span class=""pun"">=</span><span class=""pln""> e </span><span class=""pun"">||</span><span class=""pln""> window</span><span class=""pun"">;</span><span class=""pln"">
    </span><span class=""kwd"">var</span><span class=""pln""> n </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""pun"">!</span><span class=""lit"">1</span><span class=""pun"">,</span><span class=""pln"">
      o </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""kwd"">function</span><span class=""pun"">()</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">
        n </span><span class=""pun"">||</span><span class=""pln""> </span><span class=""pun"">(</span><span class=""pln"">n </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""pun"">!</span><span class=""lit"">0</span><span class=""pun"">,</span><span class=""pln""> requestAnimationFrame</span><span class=""pun"">(</span><span class=""kwd"">function</span><span class=""pun"">()</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">
          e</span><span class=""pun"">.</span><span class=""pln"">dispatchEvent</span><span class=""pun"">(</span><span class=""kwd"">new</span><span class=""pln""> </span><span class=""typ"">CustomEvent</span><span class=""pun"">(</span><span class=""pln"">t</span><span class=""pun"">)),</span><span class=""pln""> n </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""pun"">!</span><span class=""lit"">1</span><span class=""pln"">
        </span><span class=""pun"">}))</span><span class=""pln"">
      </span><span class=""pun"">};</span><span class=""pln"">
    e</span><span class=""pun"">.</span><span class=""pln"">addEventListener</span><span class=""pun"">(</span><span class=""pln"">i</span><span class=""pun"">,</span><span class=""pln""> o</span><span class=""pun"">)</span><span class=""pln"">
  </span><span class=""pun"">};</span><span class=""pln"">

debounce</span><span class=""pun"">(</span><span class=""str"">""scroll""</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""str"">""optimizedScroll""</span><span class=""pun"">,</span><span class=""pln""> window</span><span class=""pun"">);</span><span class=""pln"">  </span></code></pre>

<p>Now, instead of listening to the <code>scroll</code> event, we listen to the <code>optimizedScroll</code> event:</p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""pln"">window</span><span class=""pun"">.</span><span class=""pln"">addEventListener</span><span class=""pun"">(</span><span class=""str"">""optimizedScroll""</span><span class=""pun"">,</span><span class=""pln""> updateElementViewabilityMessage</span><span class=""pun"">);</span><span class=""pln"">  </span></code></pre>

<h4 id=""storingthedata"">Storing the data</h4>

<p>With the example described in this post, you can perform any action when an element enters or leaves the browser's viewport. Most of the time, you also want to store the data for later analysis and campaign optimisation, like ads or websites based on the data. Especially when used in display campaigns, the amount of data can become enormous. We store our data using <a href=""http://snowplowanalytics.com/"">Snowplow</a> to an <a href=""https://aws.amazon.com/s3/"">Amazon S3</a> bucket. Our data scientist can perform analysis on this raw data. </p>

<p>If you don't have the ability to create an advanced setup like this, or you don't expect that much traffic, you could also try a data collector service like <a href=""https://keen.io/"">Keen.io</a>. I also like the idea of just sending your viewability data to Google Analytics using the <a href=""http://www.lunametrics.com/blog/2016/06/09/using-google-analytics-measurement-protocol-webhooks/"">Google Analytics Measurement Protocol</a>.</p>

<h4 id=""anoteonviewabilityindisplayads"">A note on viewability in display ads</h4>

<p>In this post, I've described the happy flow when tracking viewability. Unfortunately, when you want to track viewability in real world display ads, you run into a lot of limitations and challenges. For example, a lot of display ads are served in an unfriendly iFrame, leaving the ad unable to read the viewport height and access the <code>getBoundingClientRect</code> method. This limitation forces parties that offer viewability data to use weird fallback methods, like using Flash(!!) in ads to measure the frame rate to decide whether an ad is in view or not. Even big players like Appnexus use this technique and are struggling to overcome the limitations. I'll dive into the specifics and workarounds in a future post. </p>
        ","Although viewability is not always a guarantee for success, it's a hot topic in the land of online marketing. We do not only track viewability in our display campaigns to optimise the exposure time of our campaigns, but we also use viewability to see how visitors are using our client's website and apply personalisation based on that data. Also, the viewability data is used to gather some data and insights for A/B tests. In this post, I'd like to show you the basics of tracking the viewability of an element on a web page.
Track whether an element is in view or not
For this basic example, we use a simple HTML element with an ID of foo with some basic styling applied to it (Fiddle):
<div id=""foo"" />

<style>  
#foo {
  border-top: 8px solid #7e4396;
  border-bottom: 8px solid #7e4396;
  width: 336px;
  height: 250px;
  display: block;
  background-color: #f0efef;
}
</style>  
The most simple approach to check whether an element is in view is to see if there is at least a distance of 0 pixels or more between the top of an element and the top of the viewport, and at least 0 pixels or more between the bottom of an element and the bottom of the viewport.
To calculate this, we need to know the position of the element relative to the viewport, the height of the element and the viewport height.
Figuring out the height of the viewport
Because of the differences in the way browser's layout engines measure the window's width, we need to check for two properties: window.innerWidth and document.documentElement.clientWidth. Webkit (Chrome and Safari) claims innerWidth is smaller than clientWidth, Trident (Internet Explorer) and Presto (Opera) claim innerWidth is bigger than clientWidth and Gecko (Firefox) claims innerWidth is the same size as clientWidth. To get the correct value in all situations, we can grab the highest value of both by using Math.max:
var stageHeight = Math.max(document.documentElement.clientWidth, window.innerWidth || 0);  
getBoundingClientRect()
Now we know the height of the viewport, we need to find the element's position relative to the viewport. We can use the getBoundingClientRect method for this. The getBoundingClientRect() method returns the size of an element and its position relative to the viewport. Bingo.
// Create a reference to our element
var element = document.querySelector('#foo');  
// Get the bounding rectangle of our element
var elementBoundingBox = element.getBoundingClientRect();  
We can now add these properties to what we already know:
The only missing value is the distance from the element to the viewport's bottom. We can simply calculate this value by subtracting the sum of the element's top and height value from the viewport's height.
var elementsTopY = elementBoundingBox.top;  
var elementsBottomY = elementBoundingBox.top + elementBoundingBox.height;  
Combine stageHeight & getBoundingClientRect()
Now that we have all the values we need, we can simple calculate whether our element is 100% in view or not:
var elementIsInView = elementsTopY >= 0 && elementsBottomY < stageHeight;  
Before we continue, let's first wrap our code in a function so we can re-use it later.
function elementIsInView(element, stageHeight) {  
  var elementBoundingBox = element.getBoundingClientRect();
  var elementsTopY = elementBoundingBox.top;
  var elementsBottomY = elementBoundingBox.top + elementBoundingBox.height;
  return elementsTopY >= 0 && elementsBottomY < stageHeight;
}
Update when a user scrolls
If we want to track the viewability of an element when a visitor scrolls the page, we need to track the user's scroll behaviour. We can do this by listening to the scroll event of the window object.
window.addEventListener(""scroll"", function(e) {  
  // Do something when a user scrolls
  console.log('Scrolling');
});
So now instead of just logging the word 'Scrolling', we want to see if the viewability state of our element has changed. Luckily we've just created a function to check the element's visibility, so we can use that in our scroll event handler:
window.addEventListener(""scroll"", function(e) {  
  if (elementIsInView()) {
    console.log('Our element is viewable');
  } else {
    console.log('Our element is not viewable');
  }
});
This works, but the only problem is that the check is only executed after the visitor starts scrolling, and not initially when the script in executed on load. We need to change our code just a bit to fix this. First, we need to move ('extract') our viewability check call away from the scroll handler. We'll create a separate function for this.
function updateElementViewabilityMessage() {  
  if (elementIsInView()) {
    console.log('Our element is viewable');
  } else {
    console.log('Our element is not viewable');
  }
}

window.addEventListener(""scroll"", updateElementViewabilityMessage);  
Now, all we have to do to check the viewability when the page loads, is calling the updateElementViewabilityMessage function manually. Like this:
updateElementViewabilityMessage();  
Debouncing the scroll event
The scroll event is triggered very often, which can make your viewability script's performance pretty bad, especially when you execute some complex or DOM manipulation in your handler. We can fix this by debouncing (also referred to as throttling) the scroll event. We can use this small throttle method:
var debounce = function(i, t, e) {  
    e = e || window;
    var n = !1,
      o = function() {
        n || (n = !0, requestAnimationFrame(function() {
          e.dispatchEvent(new CustomEvent(t)), n = !1
        }))
      };
    e.addEventListener(i, o)
  };

debounce(""scroll"", ""optimizedScroll"", window);  
Now, instead of listening to the scroll event, we listen to the optimizedScroll event:
window.addEventListener(""optimizedScroll"", updateElementViewabilityMessage);  
Storing the data
With the example described in this post, you can perform any action when an element enters or leaves the browser's viewport. Most of the time, you also want to store the data for later analysis and campaign optimisation, like ads or websites based on the data. Especially when used in display campaigns, the amount of data can become enormous. We store our data using Snowplow to an Amazon S3 bucket. Our data scientist can perform analysis on this raw data.
If you don't have the ability to create an advanced setup like this, or you don't expect that much traffic, you could also try a data collector service like Keen.io. I also like the idea of just sending your viewability data to Google Analytics using the Google Analytics Measurement Protocol.
A note on viewability in display ads
In this post, I've described the happy flow when tracking viewability. Unfortunately, when you want to track viewability in real world display ads, you run into a lot of limitations and challenges. For example, a lot of display ads are served in an unfriendly iFrame, leaving the ad unable to read the viewport height and access the getBoundingClientRect method. This limitation forces parties that offer viewability data to use weird fallback methods, like using Flash(!!) in ads to measure the frame rate to decide whether an ad is in view or not. Even big players like Appnexus use this technique and are struggling to overcome the limitations. I'll dive into the specifics and workarounds in a future post.","[Code, marketing, viewability, JavaScript]"
22,What is a tag manager?,/what-is-a-tag-manager/,"
            <p>We’ve been working with tag managers for about three years now, but we still notice some unclarity about what a tag manager is amongst both colleagues and clients alike. Let’s clarify things.</p>

<h2 id=""whataretags"">What are tags?</h2>

<p>First things first: what does tag in tag manager stand for? Tags are generally code snippets that you’ll want to place on your website for various reasons:</p>

<ul>
<li>Analytics: for tracking website visitors (e.g. Google Analytics or Adobe Analytics).</li>
<li>Conversion tags: track conversions in an advertising platform (e.g. Google Adwords, AppNexus).</li>
<li>Audience builders for remarketing: build audiences based on website behaviour for remarketing (e.g. Google Remarketing Lists, Facebook Audiences).</li>
<li>Research: generate click maps (heatmaps), record user behaviour, ask questions (e.g. Hotjar, Mouseflow, Qualaroo). </li>
</ul>

<p>They generally come in two flavours:</p>

<ul>
<li><strong>JavaScript tags</strong>.</li>
<li><strong>HTML image tags</strong> (generally an alternative to the JavaScript tracker when JavaScript is disabled). </li>
</ul>

<p>Say you’ll want to have a set of these tools on your website: </p>

<ul>
<li>Google Analytics.</li>
<li>Google AdWords  conversion tracking.</li>
<li>Facebook audience builder.</li>
<li>Hotjar.</li>
</ul>

<p>Without a tag manager, <strong>your developers will need to implement all 4 tags individually</strong>:</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Aug/tmt_without_tag_manager-1470385998487.PNG"" alt=""Implementing tags without a tag manager""></p>

<p><em>If you don't use a tag manager, developers need to implement 4 tags separately.</em></p>

<p>And this only takes basic all pages tags into account. You’ll probably want to track special pages, like product, basket and purchase pages in a separate, more detailed way (e.g. add total revenue to tags on the purchase page). For each extra tracking update, you’ll need your developers. This reduces the speed with which you can change or add tags.</p>

<h2 id=""tagmanagementtotherescue"">Tag management to the rescue</h2>

<p><strong>A tag management systems (TMS)  solves this issue</strong>. With a tag manager, you’ll only implement one tag for your entire website: the tag manager. This tag allows you to control other tags from an interface:</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Aug/tmt_with_tat_manager-1470385990564.PNG"" alt=""Implementing tags with a tag manager""></p>

<p><em>With a tag manager, developers will implement only one tag: the tag manager. Marketers, Analysts and other people can add all the tags they need themselves.</em></p>

<p>Do you want to add Google Analytics? Just add the tag from the GA interface to your tag manager and you’re done.  Need a conversion tag on the basket page to measure ad performance? Add a tag with a basket specific load rule and you’re done. The benefit is clear: <strong>you put the people who use the tags (web analysts, marketers etc.) in control of those very same tags</strong>.</p>

<h2 id=""thepowerofthedatalayer"">The power of the data layer</h2>

<p>If you’ve installed a tag manager, you can load tags based on basic rules such as:</p>

<ul>
<li>Page URL.</li>
<li>Timers.</li>
<li>Scroll depth.</li>
</ul>

<p>Though these options are great, <strong>the potential of your TMS can be improved</strong>. What if you could add tags based on the value of a basket, add revenue to your conversion tracking or show a pop-up question for a specific set of products? That’s what <a href=""https://www.themarketingtechnologist.co/the-ceddl-a-data-layer-standard-that-never-was/"">a data layer</a> is for:</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Aug/tmt_with_tat_manager_and_datalayer-1470386011014.PNG"" alt=""Implementing tags with a tag manager and data layer"" class=""full-img""></p>

<p><em>With a tag manager and data layer, you'll increase the data you can use for both your tags load rules and the information you send tags themselves.</em></p>

<p><strong>It’s a layer that transforms website data, both visible and invisible, to a readable data object for your TMS</strong>. It can contain different types of information:</p>

<ul>
<li>Page data, e.g. a virtual page path or <a href=""https://www.themarketingtechnologist.co/start-using-page-types-to-make-your-tag-management-life-easier/"">a page type</a>.</li>
<li>User data, e.g. a user id or user value.</li>
<li>Product data, e.g. names, prices and SKUs.</li>
<li>E-commerce data, e.g. total revenue of a transaction.</li>
</ul>

<p>By adding this data on your website, <strong>you’ll increase the options you have for both placing tags and measuring the data within those tags</strong>:</p>

<ul>
<li>Measure advance e-commerce data with Google Analytics Enhance E-commerce.</li>
<li>Add actual revenue of a conversion to your Adwords Conversion Tracking.</li>
<li>Add product information to your Facebook audiences for remarketing.</li>
<li>Trigger a Hotjar poll when a user has high margin product in the basket and tries to leave the website.</li>
<li>Apply cross-device measurement with the help of user IDs.</li>
</ul>

<p>These are just some example of what you can do with a good data layer in place. </p>

<h2 id=""responsibilityforthedatalayer"">Responsibility for the data layer</h2>

<p>I sometimes get the question about <strong>who’s responsible for the data layer</strong>. People sometimes assume that we create it via the tag manager as well. Sadly, that’s not the case. <strong>Developers of the website make it available for the tag manager</strong>. They make sure that both visible data (e.g. total revenue on a transaction page) and invisible data (e.g. a user id, or a page type) are added to the data layer. <strong>An agency that manages your tags will only leverage the data layer</strong>. </p>

<h2 id=""advancedimplementationsbeatbasicimplementations"">Advanced implementations beat basic implementations</h2>

<p>If you brief a tag manager implementation, <strong>I strongly suggest you to brief a very advanced and complete one</strong>. It's better to have data in there that you don't need right now. Not having that data will make you dependent from updates by your developers once you do need it. </p>

<h2 id=""thechallenge"">The challenge</h2>

<p>The biggest challenge is to convince your clients and other stakeholders of the value of a TMS with a well-defined data layer. They might not get the benefits as some of the tags you’ll be moving to the TMS are already running on the site. But as soon as you have it in place, both you and the client will greatly benefit from it. It’ll put marketers and analysts in control of their tags, which increases the flexibility and speed of their day-to-day business. </p>

<p><strong>In the end, it’ll make both you, your colleagues and your client happier people</strong>.</p>
        ","We’ve been working with tag managers for about three years now, but we still notice some unclarity about what a tag manager is amongst both colleagues and clients alike. Let’s clarify things.
What are tags?
First things first: what does tag in tag manager stand for? Tags are generally code snippets that you’ll want to place on your website for various reasons:
Analytics: for tracking website visitors (e.g. Google Analytics or Adobe Analytics).
Conversion tags: track conversions in an advertising platform (e.g. Google Adwords, AppNexus).
Audience builders for remarketing: build audiences based on website behaviour for remarketing (e.g. Google Remarketing Lists, Facebook Audiences).
Research: generate click maps (heatmaps), record user behaviour, ask questions (e.g. Hotjar, Mouseflow, Qualaroo).
They generally come in two flavours:
JavaScript tags.
HTML image tags (generally an alternative to the JavaScript tracker when JavaScript is disabled).
Say you’ll want to have a set of these tools on your website:
Google Analytics.
Google AdWords conversion tracking.
Facebook audience builder.
Hotjar.
Without a tag manager, your developers will need to implement all 4 tags individually:
If you don't use a tag manager, developers need to implement 4 tags separately.
And this only takes basic all pages tags into account. You’ll probably want to track special pages, like product, basket and purchase pages in a separate, more detailed way (e.g. add total revenue to tags on the purchase page). For each extra tracking update, you’ll need your developers. This reduces the speed with which you can change or add tags.
Tag management to the rescue
A tag management systems (TMS) solves this issue. With a tag manager, you’ll only implement one tag for your entire website: the tag manager. This tag allows you to control other tags from an interface:
With a tag manager, developers will implement only one tag: the tag manager. Marketers, Analysts and other people can add all the tags they need themselves.
Do you want to add Google Analytics? Just add the tag from the GA interface to your tag manager and you’re done. Need a conversion tag on the basket page to measure ad performance? Add a tag with a basket specific load rule and you’re done. The benefit is clear: you put the people who use the tags (web analysts, marketers etc.) in control of those very same tags.
The power of the data layer
If you’ve installed a tag manager, you can load tags based on basic rules such as:
Page URL.
Timers.
Scroll depth.
Though these options are great, the potential of your TMS can be improved. What if you could add tags based on the value of a basket, add revenue to your conversion tracking or show a pop-up question for a specific set of products? That’s what a data layer is for:
With a tag manager and data layer, you'll increase the data you can use for both your tags load rules and the information you send tags themselves.
It’s a layer that transforms website data, both visible and invisible, to a readable data object for your TMS. It can contain different types of information:
Page data, e.g. a virtual page path or a page type.
User data, e.g. a user id or user value.
Product data, e.g. names, prices and SKUs.
E-commerce data, e.g. total revenue of a transaction.
By adding this data on your website, you’ll increase the options you have for both placing tags and measuring the data within those tags:
Measure advance e-commerce data with Google Analytics Enhance E-commerce.
Add actual revenue of a conversion to your Adwords Conversion Tracking.
Add product information to your Facebook audiences for remarketing.
Trigger a Hotjar poll when a user has high margin product in the basket and tries to leave the website.
Apply cross-device measurement with the help of user IDs.
These are just some example of what you can do with a good data layer in place.
Responsibility for the data layer
I sometimes get the question about who’s responsible for the data layer. People sometimes assume that we create it via the tag manager as well. Sadly, that’s not the case. Developers of the website make it available for the tag manager. They make sure that both visible data (e.g. total revenue on a transaction page) and invisible data (e.g. a user id, or a page type) are added to the data layer. An agency that manages your tags will only leverage the data layer.
Advanced implementations beat basic implementations
If you brief a tag manager implementation, I strongly suggest you to brief a very advanced and complete one. It's better to have data in there that you don't need right now. Not having that data will make you dependent from updates by your developers once you do need it.
The challenge
The biggest challenge is to convince your clients and other stakeholders of the value of a TMS with a well-defined data layer. They might not get the benefits as some of the tags you’ll be moving to the TMS are already running on the site. But as soon as you have it in place, both you and the client will greatly benefit from it. It’ll put marketers and analysts in control of their tags, which increases the flexibility and speed of their day-to-day business.
In the end, it’ll make both you, your colleagues and your client happier people.","[Analytics, tag management]"
23,How to start a WhatsApp conversation directly from the web,/open-a-whatsapp-conversation-directly-from-your-ad/,"
            <p>Did you know you can send a <a href=""https://www.whatsapp.com/"">WhatsApp</a> message directly from a web page? You might have already seen this elsewhere because AddThis also offers it in their well-known and well-spread sharing tools. As an online marketing agency, we can also leverage the power of WhatsApp by integrating it in our campaigns. We could, for example, create a button that sends a campaign's promotion code to a buddy over WhatsApp. The good news is: it's extremely easy. The bad news? It has quite some limitations.</p>

<h2 id=""sendaspecificmessage"">Send a specific message</h2>

<p>Most of the times when you are browsing the web, you utilise the <a href=""https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol"">HTTP</a> or <a href=""https://en.wikipedia.org/wiki/HTTPS"">HTTPS</a> protocol. But in order to send a message over WhatsApp, you'll need a different protocol. You can send a message over WhatsApp using a so-called <em>custom URL Schema</em>. A lot of big apps have custom schemas implemented, and <a href=""http://www.gotschemes.com/"">Got Schema</a> has a decent overview of what's out there. </p>

<p><a href=""https://www.whatsapp.com/faq/en/iphone/23559013"">WhatsApp's schema</a> looks pretty straightforward: <code>whatsapp://</code>. But instead of just opening the Whatsapp app, we want to send a message. The Whatsapp:// schema supports a <code>text</code> parameter. If present, this text will be pre-filled into message text input field on a conversation screen. It looks something like this:</p>

<pre><code>whatsapp://send?text=message  
</code></pre>

<p>To use this in a web page, we could simply add it to a link:  </p>

<pre><code>&lt;a href=""whatsapp://send?text=Hello%2C%20World!""&gt;Send message to WhatsApp&lt;/a&gt;  
</code></pre>

<p>If you're on a mobile phone with WhatsApp installed, you can <a href=""whatsapp://send?text=Hello%2C%20World"">give it a try over here</a>. This should work on both Android and iOS. You should see something close to these screenshots: <br>
<img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Aug/whatsapp-1470144313076.jpg"" alt="""" class=""full-img""></p>

<h2 id=""startconversationwithaspecificperson"">Start conversation with a specific person</h2>

<p>Until recently, the WhatsApp protocol also offered an ""Address Book ID"" parameter on iOS. Essentially, the ID is a unique string that WhatsApp associates with each contact in your buddy list. Unfortunately, WhatsApp removed this option. This makes it impossible to send a message to a specific user/phone number. </p>

<p>On Android, it's possible to open the Whatsapp chat window for a specific buddy. To do so, you need to use an Android intent. Intents are messages which allow <del>application components</del> <em>your browser</em> to request functionality from <del>other Android components</del> <em>the WhatsApp app</em>. Intents allow you to interact with components from the same applications as well as with components contributed by other applications.</p>

<p>This is the Android Intent to start a Whatsapp conversation:</p>

<pre><code>intent://send/+XXXXXXXXXXX#Intent;scheme=smsto;package=com.whatsapp;action=android.intent.action.SENDTO;end  
</code></pre>

<p>To use this in a web page, we simply add it to a link:</p>

<pre><code>&lt;a href=""intent://send/+XXXXXXXXXXX#Intent;scheme=smsto;package=com.whatsapp;action=android.intent.action.SENDTO;end""&gt;Open WhatsApp chat window&lt;/a&gt;  
</code></pre>

<p>Of course, you should replace the <em>XXXXXXXXXXX</em> with a valid phone number. Unfortunately, it's not possible to add a specific message to the conversation. </p>

<h3 id=""combiningwiththirdpartyurltrackersorurlshorteners"">Combining with third party URL trackers or URL shorteners</h3>

<p>When you want to add a WhatsApp link to a Facebook post, you might not want to show the total URL (or intent) to a visitor. Unfortunately, the URL shorten services that I know of (like <a href=""https://bitly.com/"">Bit.ly</a>) require your URL to start with <em>http(s)</em>. </p>

<p>Also, when you want to use this in a display ad, most ad servers and DSP's require a creative to trigger a click tracker. These trackers only support the HTTP(S) protocol. I've found a workaround for using the WhatsApp link in AppNexus, but this is too much of an ugly hack I really do not want to encourage anyone to use this =). </p>

<h3 id=""detectwhetherwhatsappisinstalledornot"">Detect whether WhatsApp is installed or not?</h3>

<p>There is no legitimate way of achieving the goal of detecting whether WhatsApp or any other specific app, is installed on the device without having the browser re-direct to the app if it is installed. Bummer. </p>

<p><em>Image by <a href=""https://www.flickr.com/photos/microsiervos/15137651306/in/photolist-p4Eqm3-djCUYy-kZMPsp-EGju7e-q9WDuo-ksmHPM-Hc1R5b-h2G83g-khvKWX-dZZo74-sjA8LQ-fP5C7K-o1Wp1n-pMSq4Q-mLEaxR-vge1GH-vdqryU-dkkpkV-by8hfj-vuUgPg-o1VrDw-pee74f-oxpGeq-aqyRym-aqyRyU-p6drFj-feKUFq-aqwbDF-dHBtfC-bkihEU-dEFvTi-nkQdr3-aqwbGg-wPUv95-oLunc5-aqwbHz-eePN4H-aqyRAu-g74BKw-qQixjv-e1FJAu-wwoUYQ-fPSG9E-fPSGbG-oTYxut-cwCTo5-avEr1w-fPA9P8-cwCT33-cwCTi5/"">microsiervos</a> - <a href=""https://creativecommons.org/licenses/by/2.0/"">Creative Commons Attribution 2.0 Generic</a></em></p>
        ","Did you know you can send a WhatsApp message directly from a web page? You might have already seen this elsewhere because AddThis also offers it in their well-known and well-spread sharing tools. As an online marketing agency, we can also leverage the power of WhatsApp by integrating it in our campaigns. We could, for example, create a button that sends a campaign's promotion code to a buddy over WhatsApp. The good news is: it's extremely easy. The bad news? It has quite some limitations.
Send a specific message
Most of the times when you are browsing the web, you utilise the HTTP or HTTPS protocol. But in order to send a message over WhatsApp, you'll need a different protocol. You can send a message over WhatsApp using a so-called custom URL Schema. A lot of big apps have custom schemas implemented, and Got Schema has a decent overview of what's out there.
WhatsApp's schema looks pretty straightforward: whatsapp://. But instead of just opening the Whatsapp app, we want to send a message. The Whatsapp:// schema supports a text parameter. If present, this text will be pre-filled into message text input field on a conversation screen. It looks something like this:
whatsapp://send?text=message  
To use this in a web page, we could simply add it to a link:
<a href=""whatsapp://send?text=Hello%2C%20World!"">Send message to WhatsApp</a>  
If you're on a mobile phone with WhatsApp installed, you can give it a try over here. This should work on both Android and iOS. You should see something close to these screenshots:
Start conversation with a specific person
Until recently, the WhatsApp protocol also offered an ""Address Book ID"" parameter on iOS. Essentially, the ID is a unique string that WhatsApp associates with each contact in your buddy list. Unfortunately, WhatsApp removed this option. This makes it impossible to send a message to a specific user/phone number.
On Android, it's possible to open the Whatsapp chat window for a specific buddy. To do so, you need to use an Android intent. Intents are messages which allow application components your browser to request functionality from other Android components the WhatsApp app. Intents allow you to interact with components from the same applications as well as with components contributed by other applications.
This is the Android Intent to start a Whatsapp conversation:
intent://send/+XXXXXXXXXXX#Intent;scheme=smsto;package=com.whatsapp;action=android.intent.action.SENDTO;end  
To use this in a web page, we simply add it to a link:
<a href=""intent://send/+XXXXXXXXXXX#Intent;scheme=smsto;package=com.whatsapp;action=android.intent.action.SENDTO;end"">Open WhatsApp chat window</a>  
Of course, you should replace the XXXXXXXXXXX with a valid phone number. Unfortunately, it's not possible to add a specific message to the conversation.
Combining with third party URL trackers or URL shorteners
When you want to add a WhatsApp link to a Facebook post, you might not want to show the total URL (or intent) to a visitor. Unfortunately, the URL shorten services that I know of (like Bit.ly) require your URL to start with http(s).
Also, when you want to use this in a display ad, most ad servers and DSP's require a creative to trigger a click tracker. These trackers only support the HTTP(S) protocol. I've found a workaround for using the WhatsApp link in AppNexus, but this is too much of an ugly hack I really do not want to encourage anyone to use this =).
Detect whether WhatsApp is installed or not?
There is no legitimate way of achieving the goal of detecting whether WhatsApp or any other specific app, is installed on the device without having the browser re-direct to the app if it is installed. Bummer.
Image by microsiervos - Creative Commons Attribution 2.0 Generic",[Code]
24,Send Slack notifications whenever a Pokémon spawns nearby using a Pokémon GO SlackBot,/pokemon-go-slack-notifications/,"
            <p><mark><strong>CURRENT STATUS</strong> (04/08/2016): Niantic seems to have made some big changes to the API last night, so currently all API scripts (including PoGoMap and bots) seem to be down. Feel free to share any fixes if you come across anything that could be of help. Meanwhile, I hope that the post is still a good read about how to use slack webhooks in Python.</mark></p>

<p>At the Greenhouse Group we've got quite some colleagues playing Pokémon GO. No wonder we were pretty excited when this <a href=""https://m.reddit.com/r/pokemongodev/comments/4t80df/wip_pokemon_go_map_visualization_google_maps_view/d5feu2f?st=iqtsngmd&amp;sh=1a3e519e&amp;utm_source=mweb_redirect&amp;compact=true"">reddit post</a> appeared on my Google Now. It let's you run a Google Maps app locally which is filled with Pokémon that are actually visible right now in the game, by pinging the API used by the game itself. Many credits to waishda and contributors!</p>

<p>However, I figured it's not that convenient to continuously look on a map to check whether you're not missing any Pokémon. Just imagine that you just needed that last one and you find out just to late it has been sitting right there on your desk while you were working or in a meeting!</p>

<p>So I decided to clone the repository and simply hook-up to an <a href=""https://api.slack.com/incoming-webhooks"">incoming webhook on Slack</a>, so all the Pokéfans of the Greenhouse Group get notified whenever a new Pokémon spawns within our office. This is the result:</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Jul/Pokemon_SlackBot-1469200888128.JPG"" alt=""""></p>

<p>If somebody is interested in the Pokémon, he/she can simply click the distance in the post, which is linking to the exact location of the Pokémon on <a href=""https://pokevision.com/"">PokéVision</a>, which is a website hosted version of <a href=""https://github.com/AHAAAAAAA/PokemonGo-Map"">waishda repository</a>.</p>

<p>If you want to do the same for your colleagues, or you want Slack notifications when Pokémon spawn in your home, follow these steps:</p>

<ol>
<li>If you like living on the edge, you can use your private (google) account credentials, but I'd suggest making a new <a href=""https://accounts.google.com/signup"">Google</a> or <a href=""https://club.pokemon.com/us/pokemon-trainer-club/sign-up/"">Pokémon Club</a> account.  </li>
<li>Obtain <code>/services/your/slack_webhook/urlpath</code> for the webhook in the channel you want to post the notifications by <a href=""https://my.slack.com/services/new/incoming-webhook/"">creating a new webhook in Slack</a>.  </li>
<li>Download zip (and unzip) or git clone <a href=""https://github.com/rubenmak/PokemonGo-SlackBot"">my repository</a> (which is just a tweaked clone of <a href=""https://github.com/AHAAAAAAA/PokemonGo-Map"">waishda's repos</a>).  </li>
<li>Move into the directory, e.g. <code>cd ~/Downloads/PokemonGo-SlackBot</code>  </li>
<li>Execute in command line <code>sudo pip install -r requirements.txt</code>.  </li>
<li>Run <code>python pokeslack.py -u &lt;your_user_name&gt; -p &lt;your_password&gt; -l ""&lt;Your location&gt;"" -st 1 -r &lt;the range you want in meters&gt; -sw ""&lt;/services/your/slack_webhook/urlpath&gt;""</code>. If you use a Google account for authentication, you should add <code>-a google</code> to the command. For <code>-l ""&lt;Your location&gt;""</code> you can input any Google Maps search query, so just try on Google Maps whether your query gives a marker you like. If you want to ping around in a wider range than your exact location (more or less 250 meters radius), set the step argument <code>-st 1</code> to a higher number like <code>-st 10</code>. If your want to make the map available to your colleagues within your network, add <code>-H &lt;server ip address&gt; -P &lt;port&gt;</code>.  </li>
<li>After a while you should start receiving Slack messages every time a Pokémon spawns.  </li>
<li>You can also see the original Pokemon-Map when browsing to <a href=""http://localhost:5000"">http://localhost:5000</a>. Note that you'll see more Pokémon on the map, since only those in the specified range will send a notification.</li>
</ol>

<p>The user_icons will be a :pokeball: by default, however if you upload <a href=""https://github.com/Templarian/slack-emoji-pokemon"">Pokémon emojis</a> to Slack, you can use those for each specific Pokémon. If you use the smileys without prefix, run <code>python pokeslack.py -u &lt;your_user_name&gt; -p &lt;your_password&gt; -l ""&lt;Your location&gt;"" -st 1 -r &lt;the range you want in meters&gt; -sw ""&lt;/services/your/slack_webhook/urlpath&gt;"" -pi ':'</code>, and if you do use the prefix <code>python pokeslack.py -u &lt;your_user_name&gt; -p &lt;your_password&gt; -l ""&lt;Your location&gt;"" -st 1 -r &lt;the range you want in meters&gt; -sw ""&lt;/services/your/slack_webhook/urlpath&gt;"" -pi ':pokemon-'</code>. You can also apply filters by adding <code>-i</code> to ignore or <code>-o</code> for only, like: <code>python pokeslack.py -u &lt;your_user_name&gt; -p &lt;your_password&gt; -l ""&lt;Your location&gt;"" -st 1 -r &lt;the range you want in meters&gt; -sw ""&lt;/services/your/slack_webhook/urlpath&gt;"" -pi ':' -i ""zubat, rattata, pidgey, spearow""</code></p>

<p>Slack webhooks are easy and fun and can be pretty useful for sending any notification. For example, what about monitoring your data pipelines, sending notification about the progress of your data crunching jobs or even automatically send results to your colleagues? So if you're interested in how to do this, I'll explain a bit about the code:</p>

<p>The data is simply send to Slack using a POST request:</p>

<pre><code class=""language-prettyprint lang-python prettyprinted""><span class=""kwd"">import</span><span class=""pln""> httplib  
</span><span class=""kwd"">import</span><span class=""pln""> urllib

</span><span class=""kwd"">def</span><span class=""pln""> send_to_slack</span><span class=""pun"">(</span><span class=""pln"">text</span><span class=""pun"">,</span><span class=""pln""> username</span><span class=""pun"">,</span><span class=""pln""> icon_emoji</span><span class=""pun"">,</span><span class=""pln""> webhook</span><span class=""pun"">):</span><span class=""pln"">  
    data </span><span class=""pun"">=</span><span class=""pln""> urllib</span><span class=""pun"">.</span><span class=""pln"">urlencode</span><span class=""pun"">({</span><span class=""str"">'payload'</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""str"">'{""username"": ""'</span><span class=""pln""> </span><span class=""pun"">+</span><span class=""pln""> username </span><span class=""pun"">+</span><span class=""pln""> </span><span class=""str"">'"", '</span><span class=""pln"">
                                        </span><span class=""str"">'""icon_emoji"": ""'</span><span class=""pln""> </span><span class=""pun"">+</span><span class=""pln""> icon_emoji </span><span class=""pun"">+</span><span class=""pln""> </span><span class=""str"">'"", '</span><span class=""pln"">
                                        </span><span class=""str"">'""text"": ""'</span><span class=""pln""> </span><span class=""pun"">+</span><span class=""pln""> text </span><span class=""pun"">+</span><span class=""pln""> </span><span class=""str"">'""}'</span><span class=""pln"">
                             </span><span class=""pun"">})</span><span class=""pln"">

    h </span><span class=""pun"">=</span><span class=""pln""> httplib</span><span class=""pun"">.</span><span class=""typ"">HTTPSConnection</span><span class=""pun"">(</span><span class=""str"">'hooks.slack.com'</span><span class=""pun"">)</span><span class=""pln"">
    headers </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""str"">""Content-type""</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""str"">""application/x-www-form-urlencoded""</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""str"">""Accept""</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""str"">""text/plain""</span><span class=""pun"">}</span><span class=""pln"">

    h</span><span class=""pun"">.</span><span class=""pln"">request</span><span class=""pun"">(</span><span class=""str"">'POST'</span><span class=""pun"">,</span><span class=""pln""> webhook</span><span class=""pun"">,</span><span class=""pln""> data</span><span class=""pun"">,</span><span class=""pln""> headers</span><span class=""pun"">)</span><span class=""pln"">
    r </span><span class=""pun"">=</span><span class=""pln""> h</span><span class=""pun"">.</span><span class=""pln"">getresponse</span><span class=""pun"">()</span></code></pre>

<p>The content of the notification is generated in this way:</p>

<pre><code class=""language-prettyprint lang-python prettyprinted""><span class=""pln"">spotted_pokemon </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""pun"">{}</span><span class=""pln"">

        </span><span class=""kwd"">if</span><span class=""pln""> poke</span><span class=""pun"">.</span><span class=""typ"">SpawnPointId</span><span class=""pln""> </span><span class=""kwd"">in</span><span class=""pln""> spotted_pokemon</span><span class=""pun"">.</span><span class=""pln"">keys</span><span class=""pun"">():</span><span class=""pln"">
            </span><span class=""kwd"">if</span><span class=""pln""> spotted_pokemon</span><span class=""pun"">[</span><span class=""pln"">poke</span><span class=""pun"">.</span><span class=""typ"">SpawnPointId</span><span class=""pun"">][</span><span class=""str"">'disappear_datetime'</span><span class=""pun"">]</span><span class=""pln""> </span><span class=""pun"">&gt;</span><span class=""pln""> datetime</span><span class=""pun"">.</span><span class=""pln"">now</span><span class=""pun"">():</span><span class=""pln"">
                </span><span class=""kwd"">continue</span><span class=""pln"">
        </span><span class=""kwd"">if</span><span class=""pln""> poke</span><span class=""pun"">.</span><span class=""typ"">TimeTillHiddenMs</span><span class=""pln""> </span><span class=""pun"">&lt;</span><span class=""pln""> </span><span class=""lit"">0</span><span class=""pun"">:</span><span class=""pln"">
            </span><span class=""kwd"">continue</span><span class=""pln"">

        disappear_datetime </span><span class=""pun"">=</span><span class=""pln""> datetime</span><span class=""pun"">.</span><span class=""pln"">fromtimestamp</span><span class=""pun"">(</span><span class=""pln"">disappear_timestamp</span><span class=""pun"">)</span><span class=""pln"">
        distance </span><span class=""pun"">=</span><span class=""pln""> lonlat_to_meters</span><span class=""pun"">(</span><span class=""pln"">origin_lat</span><span class=""pun"">,</span><span class=""pln""> origin_lon</span><span class=""pun"">,</span><span class=""pln""> poke</span><span class=""pun"">.</span><span class=""typ"">Latitude</span><span class=""pun"">,</span><span class=""pln""> poke</span><span class=""pun"">.</span><span class=""typ"">Longitude</span><span class=""pun"">)</span><span class=""pln"">

        </span><span class=""kwd"">if</span><span class=""pln""> distance </span><span class=""pun"">&lt;</span><span class=""pln""> max_distance</span><span class=""pun"">:</span><span class=""pln"">
            time_till_disappears </span><span class=""pun"">=</span><span class=""pln""> disappear_datetime </span><span class=""pun"">-</span><span class=""pln""> datetime</span><span class=""pun"">.</span><span class=""pln"">now</span><span class=""pun"">()</span><span class=""pln"">
            disappear_hours</span><span class=""pun"">,</span><span class=""pln""> disappear_remainder </span><span class=""pun"">=</span><span class=""pln""> divmod</span><span class=""pun"">(</span><span class=""pln"">time_till_disappears</span><span class=""pun"">.</span><span class=""pln"">seconds</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""lit"">3600</span><span class=""pun"">)</span><span class=""pln"">
            disappear_minutes</span><span class=""pun"">,</span><span class=""pln""> disappear_seconds </span><span class=""pun"">=</span><span class=""pln""> divmod</span><span class=""pun"">(</span><span class=""pln"">disappear_remainder</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""lit"">60</span><span class=""pun"">)</span><span class=""pln"">
            disappear_minutes </span><span class=""pun"">=</span><span class=""pln""> str</span><span class=""pun"">(</span><span class=""pln"">disappear_minutes</span><span class=""pun"">)</span><span class=""pln"">
            disappear_seconds </span><span class=""pun"">=</span><span class=""pln""> str</span><span class=""pun"">(</span><span class=""pln"">disappear_seconds</span><span class=""pun"">)</span><span class=""pln"">
            </span><span class=""kwd"">if</span><span class=""pln""> len</span><span class=""pun"">(</span><span class=""pln"">disappear_seconds</span><span class=""pun"">)</span><span class=""pln""> </span><span class=""pun"">==</span><span class=""pln""> </span><span class=""lit"">1</span><span class=""pun"">:</span><span class=""pln"">
                disappear_seconds </span><span class=""pun"">=</span><span class=""pln""> str</span><span class=""pun"">(</span><span class=""lit"">0</span><span class=""pun"">)</span><span class=""pln""> </span><span class=""pun"">+</span><span class=""pln""> disappear_seconds
            disappear_time </span><span class=""pun"">=</span><span class=""pln""> disappear_datetime</span><span class=""pun"">.</span><span class=""pln"">strftime</span><span class=""pun"">(</span><span class=""str"">""%H:%M:%S""</span><span class=""pun"">)</span><span class=""pln"">

            alert_text </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""str"">'I\'m just &lt;https://pokevision.com/#/@'</span><span class=""pln""> </span><span class=""pun"">+</span><span class=""pln""> str</span><span class=""pun"">(</span><span class=""pln"">poke</span><span class=""pun"">.</span><span class=""typ"">Latitude</span><span class=""pun"">)</span><span class=""pln""> </span><span class=""pun"">+</span><span class=""pln""> \
                        </span><span class=""str"">','</span><span class=""pln""> </span><span class=""pun"">+</span><span class=""pln""> str</span><span class=""pun"">(</span><span class=""pln"">poke</span><span class=""pun"">.</span><span class=""typ"">Longitude</span><span class=""pun"">)</span><span class=""pln""> </span><span class=""pun"">+</span><span class=""pln""> \
                        </span><span class=""str"">'|'</span><span class=""pln""> </span><span class=""pun"">+</span><span class=""pln""> </span><span class=""str"">""{0:.2f}""</span><span class=""pun"">.</span><span class=""pln"">format</span><span class=""pun"">(</span><span class=""pln"">distance</span><span class=""pun"">)</span><span class=""pln""> </span><span class=""pun"">+</span><span class=""pln""> \
                        </span><span class=""str"">' m&gt; away until '</span><span class=""pln""> </span><span class=""pun"">+</span><span class=""pln""> disappear_time </span><span class=""pun"">+</span><span class=""pln""> \
                        </span><span class=""str"">' ('</span><span class=""pln""> </span><span class=""pun"">+</span><span class=""pln""> disappear_minutes </span><span class=""pun"">+</span><span class=""pln""> </span><span class=""str"">':'</span><span class=""pln""> </span><span class=""pun"">+</span><span class=""pln""> disappear_seconds </span><span class=""pun"">+</span><span class=""pln""> </span><span class=""str"">')!'</span><span class=""pln"">

            </span><span class=""kwd"">if</span><span class=""pln""> pokemon_icons_prefix </span><span class=""pun"">!=</span><span class=""pln""> </span><span class=""str"">':pokeball:'</span><span class=""pun"">:</span><span class=""pln"">
                user_icon </span><span class=""pun"">=</span><span class=""pln""> pokemon_icons_prefix </span><span class=""pun"">+</span><span class=""pln""> pokename</span><span class=""pun"">.</span><span class=""pln"">lower</span><span class=""pun"">()</span><span class=""pln""> </span><span class=""pun"">+</span><span class=""pln""> </span><span class=""str"">':'</span><span class=""pln"">
            </span><span class=""kwd"">else</span><span class=""pun"">:</span><span class=""pln"">
                user_icon </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""str"">':pokeball:'</span><span class=""pln"">

            send_to_slack</span><span class=""pun"">(</span><span class=""pln"">alert_text</span><span class=""pun"">,</span><span class=""pln""> pokename</span><span class=""pun"">,</span><span class=""pln""> user_icon</span><span class=""pun"">,</span><span class=""pln""> slack_webhook_urlpath</span><span class=""pun"">)</span><span class=""pln"">

            spotted_pokemon</span><span class=""pun"">[</span><span class=""pln"">poke</span><span class=""pun"">.</span><span class=""typ"">SpawnPointId</span><span class=""pun"">]</span><span class=""pln""> </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""str"">'disappear_datetime'</span><span class=""pun"">:</span><span class=""pln""> disappear_datetime</span><span class=""pun"">,</span><span class=""pln""> 
                                                 </span><span class=""str"">'pokename'</span><span class=""pun"">:</span><span class=""pln""> pokename</span><span class=""pun"">}</span></code></pre>

<p>Note the 'spotted_pokemon' dict. If you don't check whether a Pokémon is actually newly spawn, you'll generate a lot of spam and probably not so happy colleagues.</p>

<p>Also, there is a little bit of math in calculating lon-lat into meters distance using haversine:</p>

<pre><code class=""language-prettyprint lang-python prettyprinted""><span class=""kwd"">from</span><span class=""pln""> math </span><span class=""kwd"">import</span><span class=""pln""> radians</span><span class=""pun"">,</span><span class=""pln""> cos</span><span class=""pun"">,</span><span class=""pln""> sin</span><span class=""pun"">,</span><span class=""pln""> asin</span><span class=""pun"">,</span><span class=""pln""> sqrt

</span><span class=""kwd"">def</span><span class=""pln""> lonlat_to_meters</span><span class=""pun"">(</span><span class=""pln"">lat1</span><span class=""pun"">,</span><span class=""pln""> lon1</span><span class=""pun"">,</span><span class=""pln""> lat2</span><span class=""pun"">,</span><span class=""pln""> lon2</span><span class=""pun"">):</span><span class=""pln"">  
    </span><span class=""str"">""""""
    Calculate the great circle distance between two points
    on the earth (specified in decimal degrees)
    """"""</span><span class=""pln"">
    </span><span class=""com""># convert decimal degrees to radians</span><span class=""pln"">
    lon1</span><span class=""pun"">,</span><span class=""pln""> lat1</span><span class=""pun"">,</span><span class=""pln""> lon2</span><span class=""pun"">,</span><span class=""pln""> lat2 </span><span class=""pun"">=</span><span class=""pln""> map</span><span class=""pun"">(</span><span class=""pln"">radians</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""pun"">[</span><span class=""pln"">lon1</span><span class=""pun"">,</span><span class=""pln""> lat1</span><span class=""pun"">,</span><span class=""pln""> lon2</span><span class=""pun"">,</span><span class=""pln""> lat2</span><span class=""pun"">])</span><span class=""pln"">
    </span><span class=""com""># haversine formula</span><span class=""pln"">
    dlon </span><span class=""pun"">=</span><span class=""pln""> lon2 </span><span class=""pun"">-</span><span class=""pln""> lon1
    dlat </span><span class=""pun"">=</span><span class=""pln""> lat2 </span><span class=""pun"">-</span><span class=""pln""> lat1
    a </span><span class=""pun"">=</span><span class=""pln""> sin</span><span class=""pun"">(</span><span class=""pln"">dlat</span><span class=""pun"">/</span><span class=""lit"">2</span><span class=""pun"">)**</span><span class=""lit"">2</span><span class=""pln""> </span><span class=""pun"">+</span><span class=""pln""> cos</span><span class=""pun"">(</span><span class=""pln"">lat1</span><span class=""pun"">)</span><span class=""pln""> </span><span class=""pun"">*</span><span class=""pln""> cos</span><span class=""pun"">(</span><span class=""pln"">lat2</span><span class=""pun"">)</span><span class=""pln""> </span><span class=""pun"">*</span><span class=""pln""> sin</span><span class=""pun"">(</span><span class=""pln"">dlon</span><span class=""pun"">/</span><span class=""lit"">2</span><span class=""pun"">)**</span><span class=""lit"">2</span><span class=""pln"">
    c </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""lit"">2</span><span class=""pln""> </span><span class=""pun"">*</span><span class=""pln""> asin</span><span class=""pun"">(</span><span class=""pln"">sqrt</span><span class=""pun"">(</span><span class=""pln"">a</span><span class=""pun"">))</span><span class=""pln"">
    </span><span class=""com""># earth radius in meters: 6378100</span><span class=""pln"">
    m </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""lit"">6378100</span><span class=""pln""> </span><span class=""pun"">*</span><span class=""pln""> c
    </span><span class=""kwd"">return</span><span class=""pln""> m</span></code></pre>

<p>For more code, please checkout the <a href=""https://github.com/rubenmak/PokemonGo-SlackBot"">git repository</a>!</p>

<h4 id=""update1japaneselanguagesupport"">Update #1: Japanese language support</h4>

<p>Thanks to several git pull requests from the community, the code is now also working in Japanese. If it happens that the Pokémon names in your language are inconsistent with the names of your emojis in slack, try uploading <a href=""https://github.com/jaylynch/pokemoji/blob/master/pokemon-by-number.yaml"">this</a> list of emojis by pokemon number, using <a href=""https://github.com/lambtron/emojipacksv"">emojipacks</a>, and replace this piece of code:</p>

<pre><code class=""language-prettyprint lang-python prettyprinted""><span class=""pln"">           </span><span class=""kwd"">if</span><span class=""pln""> pokemon_icons_prefix </span><span class=""pun"">!=</span><span class=""pln""> </span><span class=""str"">':pokeball:'</span><span class=""pun"">:</span><span class=""pln"">
                user_icon </span><span class=""pun"">=</span><span class=""pln""> pokemon_icons_prefix </span><span class=""pun"">+</span><span class=""pln""> pokename</span><span class=""pun"">.</span><span class=""pln"">lower</span><span class=""pun"">()</span><span class=""pln""> </span><span class=""pun"">+</span><span class=""pln""> </span><span class=""str"">':'</span><span class=""pln"">
            </span><span class=""kwd"">else</span><span class=""pun"">:</span><span class=""pln"">
                user_icon </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""str"">':pokeball:'</span></code></pre>

<p>with this:</p>

<pre><code class=""language-prettyprint lang-python prettyprinted""><span class=""pln"">           </span><span class=""kwd"">if</span><span class=""pln""> pokemon_icons_prefix </span><span class=""pun"">!=</span><span class=""pln""> </span><span class=""str"">':pokeball:'</span><span class=""pun"">:</span><span class=""pln"">
                user_icon </span><span class=""pun"">=</span><span class=""pln""> pokemon_icons_prefix </span><span class=""pun"">+</span><span class=""pln""> </span><span class=""str"">'pokemon-'</span><span class=""pln""> </span><span class=""pun"">+</span><span class=""pln""> pokeid </span><span class=""pun"">+</span><span class=""pln""> </span><span class=""str"">':'</span><span class=""pln"">
            </span><span class=""kwd"">else</span><span class=""pun"">:</span><span class=""pln"">
                user_icon </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""str"">':pokeball:'</span></code></pre>

<h4 id=""update2homeysupportbybartpersoons"">Update #2: Homey support by Bart Persoons</h4>

<p>After using the Pokemon Slack notifications at work I was wondering if it is possible to send this information to my Homey at home. Homey is my connected home hub which controls my lights for example. It also has a speech function, so how cool would it be if <a href=""https://www.athom.com/en/"">Homey</a> tells me when there is a new Pokémon nearby! <br>
To send the data to Homey I used Homey’s “Webhook Manager” app. With this app it is possible to load an event with 3 data fields. After making some small changes to the Pokeslack script I was able to send the following information to Homey:</p>

<p><code>{'event': 'pokemon', 'data1': pokename, 'data2': distance}</code></p>

<p>Within Homey it is now possible to create a flow which triggers on the event “pokemon” and it can use the data parameters as variables.</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Jul/Homey_pokehomey_setup-1469442533079.jpg"" alt="""" class=""full-img""></p>

<p>Watch the <a href=""https://youtu.be/5F7R0STK1FA"">result in this video!</a></p>

<p>The Homey token should be added as an argument, so to run the script you can use:</p>

<p><code>python pokehomey.py -u &lt;your_user_name&gt; &lt;your_password&gt; -l ""&lt;Your location&gt;"" -st 1 -r &lt;the range you want in meters&gt; -ht ""&lt;homey webhook manager token&gt;""</code></p>

<h4 id=""update3languagesstabilityandluredpokmon"">Update #3: Languages, stability and lured Pokémon!</h4>

<p>Several updates:</p>

<p>Message are now available in French and German thanks to <a href=""https://github.com/VincentCATILLON"">Vincent</a>. They can be set by setting the locale. The locale that needs to be used for the emojis can be set separately with <code>-iL</code>, default is English.</p>

<p>As we also see in the app and at Pokevision, server quite often seem to have troubles. Therefore, the script now has a reconnect features instead of stopping the script in cases of issues. From experience, it's advisable to use a Google account, since it's more stable than PTC.</p>

<p>And last but not least, lured Pokémon! Pokevision doesn't show these (yet), so many thanks go to the tip by <a href=""https://github.com/danopia"">Daniel</a>. The Pokémon will have <code>(lured)</code> after ttheir names in the Slack messages. If you also want to show the lured Pokéstops on the map, use <code>-dp -ol</code>. If you want to see all Pokéstops on the map, just use <code>-dp</code> without <code>-ol</code>, and for gyms add <code>-dg</code>.</p>
        ","CURRENT STATUS (04/08/2016): Niantic seems to have made some big changes to the API last night, so currently all API scripts (including PoGoMap and bots) seem to be down. Feel free to share any fixes if you come across anything that could be of help. Meanwhile, I hope that the post is still a good read about how to use slack webhooks in Python.
At the Greenhouse Group we've got quite some colleagues playing Pokémon GO. No wonder we were pretty excited when this reddit post appeared on my Google Now. It let's you run a Google Maps app locally which is filled with Pokémon that are actually visible right now in the game, by pinging the API used by the game itself. Many credits to waishda and contributors!
However, I figured it's not that convenient to continuously look on a map to check whether you're not missing any Pokémon. Just imagine that you just needed that last one and you find out just to late it has been sitting right there on your desk while you were working or in a meeting!
So I decided to clone the repository and simply hook-up to an incoming webhook on Slack, so all the Pokéfans of the Greenhouse Group get notified whenever a new Pokémon spawns within our office. This is the result:
If somebody is interested in the Pokémon, he/she can simply click the distance in the post, which is linking to the exact location of the Pokémon on PokéVision, which is a website hosted version of waishda repository.
If you want to do the same for your colleagues, or you want Slack notifications when Pokémon spawn in your home, follow these steps:
If you like living on the edge, you can use your private (google) account credentials, but I'd suggest making a new Google or Pokémon Club account.
Obtain /services/your/slack_webhook/urlpath for the webhook in the channel you want to post the notifications by creating a new webhook in Slack.
Download zip (and unzip) or git clone my repository (which is just a tweaked clone of waishda's repos).
Move into the directory, e.g. cd ~/Downloads/PokemonGo-SlackBot
Execute in command line sudo pip install -r requirements.txt.
Run python pokeslack.py -u <your_user_name> -p <your_password> -l ""<Your location>"" -st 1 -r <the range you want in meters> -sw ""</services/your/slack_webhook/urlpath>"". If you use a Google account for authentication, you should add -a google to the command. For -l ""<Your location>"" you can input any Google Maps search query, so just try on Google Maps whether your query gives a marker you like. If you want to ping around in a wider range than your exact location (more or less 250 meters radius), set the step argument -st 1 to a higher number like -st 10. If your want to make the map available to your colleagues within your network, add -H <server ip address> -P <port>.
After a while you should start receiving Slack messages every time a Pokémon spawns.
You can also see the original Pokemon-Map when browsing to http://localhost:5000. Note that you'll see more Pokémon on the map, since only those in the specified range will send a notification.
The user_icons will be a :pokeball: by default, however if you upload Pokémon emojis to Slack, you can use those for each specific Pokémon. If you use the smileys without prefix, run python pokeslack.py -u <your_user_name> -p <your_password> -l ""<Your location>"" -st 1 -r <the range you want in meters> -sw ""</services/your/slack_webhook/urlpath>"" -pi ':', and if you do use the prefix python pokeslack.py -u <your_user_name> -p <your_password> -l ""<Your location>"" -st 1 -r <the range you want in meters> -sw ""</services/your/slack_webhook/urlpath>"" -pi ':pokemon-'. You can also apply filters by adding -i to ignore or -o for only, like: python pokeslack.py -u <your_user_name> -p <your_password> -l ""<Your location>"" -st 1 -r <the range you want in meters> -sw ""</services/your/slack_webhook/urlpath>"" -pi ':' -i ""zubat, rattata, pidgey, spearow""
Slack webhooks are easy and fun and can be pretty useful for sending any notification. For example, what about monitoring your data pipelines, sending notification about the progress of your data crunching jobs or even automatically send results to your colleagues? So if you're interested in how to do this, I'll explain a bit about the code:
The data is simply send to Slack using a POST request:
import httplib  
import urllib

def send_to_slack(text, username, icon_emoji, webhook):  
    data = urllib.urlencode({'payload': '{""username"": ""' + username + '"", '
                                        '""icon_emoji"": ""' + icon_emoji + '"", '
                                        '""text"": ""' + text + '""}'
                             })

    h = httplib.HTTPSConnection('hooks.slack.com')
    headers = {""Content-type"": ""application/x-www-form-urlencoded"", ""Accept"": ""text/plain""}

    h.request('POST', webhook, data, headers)
    r = h.getresponse()
The content of the notification is generated in this way:
spotted_pokemon = {}

        if poke.SpawnPointId in spotted_pokemon.keys():
            if spotted_pokemon[poke.SpawnPointId]['disappear_datetime'] > datetime.now():
                continue
        if poke.TimeTillHiddenMs < 0:
            continue

        disappear_datetime = datetime.fromtimestamp(disappear_timestamp)
        distance = lonlat_to_meters(origin_lat, origin_lon, poke.Latitude, poke.Longitude)

        if distance < max_distance:
            time_till_disappears = disappear_datetime - datetime.now()
            disappear_hours, disappear_remainder = divmod(time_till_disappears.seconds, 3600)
            disappear_minutes, disappear_seconds = divmod(disappear_remainder, 60)
            disappear_minutes = str(disappear_minutes)
            disappear_seconds = str(disappear_seconds)
            if len(disappear_seconds) == 1:
                disappear_seconds = str(0) + disappear_seconds
            disappear_time = disappear_datetime.strftime(""%H:%M:%S"")

            alert_text = 'I\'m just <https://pokevision.com/#/@' + str(poke.Latitude) + \
                        ',' + str(poke.Longitude) + \
                        '|' + ""{0:.2f}"".format(distance) + \
                        ' m> away until ' + disappear_time + \
                        ' (' + disappear_minutes + ':' + disappear_seconds + ')!'

            if pokemon_icons_prefix != ':pokeball:':
                user_icon = pokemon_icons_prefix + pokename.lower() + ':'
            else:
                user_icon = ':pokeball:'

            send_to_slack(alert_text, pokename, user_icon, slack_webhook_urlpath)

            spotted_pokemon[poke.SpawnPointId] = {'disappear_datetime': disappear_datetime, 
                                                 'pokename': pokename}
Note the 'spotted_pokemon' dict. If you don't check whether a Pokémon is actually newly spawn, you'll generate a lot of spam and probably not so happy colleagues.
Also, there is a little bit of math in calculating lon-lat into meters distance using haversine:
from math import radians, cos, sin, asin, sqrt

def lonlat_to_meters(lat1, lon1, lat2, lon2):  
    """"""
    Calculate the great circle distance between two points
    on the earth (specified in decimal degrees)
    """"""
    # convert decimal degrees to radians
    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])
    # haversine formula
    dlon = lon2 - lon1
    dlat = lat2 - lat1
    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2
    c = 2 * asin(sqrt(a))
    # earth radius in meters: 6378100
    m = 6378100 * c
    return m
For more code, please checkout the git repository!
Update #1: Japanese language support
Thanks to several git pull requests from the community, the code is now also working in Japanese. If it happens that the Pokémon names in your language are inconsistent with the names of your emojis in slack, try uploading this list of emojis by pokemon number, using emojipacks, and replace this piece of code:
           if pokemon_icons_prefix != ':pokeball:':
                user_icon = pokemon_icons_prefix + pokename.lower() + ':'
            else:
                user_icon = ':pokeball:'
with this:
           if pokemon_icons_prefix != ':pokeball:':
                user_icon = pokemon_icons_prefix + 'pokemon-' + pokeid + ':'
            else:
                user_icon = ':pokeball:'
Update #2: Homey support by Bart Persoons
After using the Pokemon Slack notifications at work I was wondering if it is possible to send this information to my Homey at home. Homey is my connected home hub which controls my lights for example. It also has a speech function, so how cool would it be if Homey tells me when there is a new Pokémon nearby!
To send the data to Homey I used Homey’s “Webhook Manager” app. With this app it is possible to load an event with 3 data fields. After making some small changes to the Pokeslack script I was able to send the following information to Homey:
{'event': 'pokemon', 'data1': pokename, 'data2': distance}
Within Homey it is now possible to create a flow which triggers on the event “pokemon” and it can use the data parameters as variables.
Watch the result in this video!
The Homey token should be added as an argument, so to run the script you can use:
python pokehomey.py -u <your_user_name> <your_password> -l ""<Your location>"" -st 1 -r <the range you want in meters> -ht ""<homey webhook manager token>""
Update #3: Languages, stability and lured Pokémon!
Several updates:
Message are now available in French and German thanks to Vincent. They can be set by setting the locale. The locale that needs to be used for the emojis can be set separately with -iL, default is English.
As we also see in the app and at Pokevision, server quite often seem to have troubles. Therefore, the script now has a reconnect features instead of stopping the script in cases of issues. From experience, it's advisable to use a Google account, since it's more stable than PTC.
And last but not least, lured Pokémon! Pokevision doesn't show these (yet), so many thanks go to the tip by Daniel. The Pokémon will have (lured) after ttheir names in the Slack messages. If you also want to show the lured Pokéstops on the map, use -dp -ol. If you want to see all Pokéstops on the map, just use -dp without -ol, and for gyms add -dg.","[Data Science, Code, Pokemon, Slack]"
25,Connecting offline sales to online campaign sources with Google Analytics - Part 1,/connecting-offline-sales-to-online-campaign-sources-with-google-analytics/,"
            <p>Connecting offline to online is a challenge, but this week we did it. <strong>We’ve measured our first offline sales in Google Analytics, and we can directly attribute these to online campaign sources!</strong> Awesome right? In this two-part post series, we share the theory behind the system that gives us these insights, so you can setup a similar system yourself. This post describes the general system. The second post will discuss the actual code used in the system.</p>

<h2 id=""astoryoftwodatasets"">A story of two data sets</h2>

<p>At its core, <strong>the system consists of two data sets</strong>: one that we capture online, and one that we get from our client’s offline sales team. <strong>Both data sets contain a <a href=""https://www.themarketingtechnologist.co/what-to-look-for-when-selecting-a-customer-identifier/"">customer identifier</a></strong>:</p>

<ul>
<li>Online data set:
<ul><li>Date</li>
<li>Customer identifier</li>
<li><a href=""https://developers.google.com/analytics/devguides/collection/analyticsjs/field-reference#clientId"">Google Analytics client id</a></li></ul></li>
<li>Offline sales data set:
<ul><li>Date</li>
<li>Customer identifier</li>
<li>Revenue</li></ul></li>
</ul>

<p>The customer identifier allows our system to <strong>connect the Google Analytics client id to the offline revenue</strong>. We collect the data in two ways:</p>

<ol>
<li>Online data collection for the Google Analytics client id and the customer identifier;  </li>
<li>Secure server to put offline sales data on.</li>
</ol>

<p>These two systems work like this:</p>

<h2 id=""1onlinedatacollection"">1: Online data collection</h2>

<p>The online data is collected every time the user fills out a customer identifier:</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Jul/ropo_flow_online-1469112545531.PNG"" alt=""Cross channel online data collection"" class=""full-img""></p>

<p><em>A simplified view of the online data collection for cross-channel analysis.</em> </p>

<p>As soon  as we have a customer identifier on site, we get the Google Analytics client ID  and send the two values to a database.</p>

<h2 id=""2offlinesalesdata"">2: Offline sales data</h2>

<p>The offline sales data is made available through a secure FTP server:</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Jul/ropo_flow_offline-1469092492230.PNG"" alt=""Cross channel offline data collection"" class=""full-img""></p>

<p><em>A simplified view of the offline sales data server.</em></p>

<p>This data set is supplied to us on a daily basis. </p>

<h2 id=""themagic"">The magic</h2>

<p>The magic is in connecting these two data sets. The system is easy. <strong>Every day we run a Python script</strong> after the new offline sales data set becomes available. This system uses the online data as a lookup table for the Google Analytics client id. Based on this lookup, <strong>it fires a Google Analytics event through <a href=""https://developers.google.com/analytics/devguides/collection/protocol/v1/"">the measurement protocol</a></strong>:</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Jul/ropo_flow_connection-1469112375673.PNG"" alt=""Cross channel data connection"" class=""full-img""></p>

<p><em>A simplified view of the script that connects online data to offline sales.</em></p>

<p>We format the event as follows:</p>

<ul>
<li>Event category: Offline.</li>
<li>Event action: Purchase.</li>
<li>Event value: revenue.</li>
</ul>

<p>Now we can use the event value to measure offline revenue in Google Analytics. We also set up a goal based on the event that uses the event value as goal value. </p>

<h2 id=""howtheattributionworks"">How the attribution works</h2>

<p>This system fires the event to Google Analytics without campaign tagging. Therefore, the event will be attributed to direct traffic. Google Analytics applies a <a href=""https://support.google.com/analytics/answer/1662518?hl=en"">last non-direct click model</a>. This means that Google uses the client ID to attribute direct visit to the last known source of that user over the last six months: the source of the user when the customer identifier was filled out online. <strong>Boom! Offline sales with online source attribution.</strong> </p>

<h2 id=""thingstokeepinmind"">Things to keep in mind</h2>

<p>The system is easy at its core, but there are things to keep in mind while building the system:</p>

<ul>
<li>Depending on your type of customer identifier (broad or narrow), it may appear multiple times for different users. Determine how to handle these duplicate entries.</li>
<li>Find out what the maximum time is for a customer journey, from first interaction to purchase. If the time gap between the online data point and the offline sale exceeds this time window, ignore the event.</li>
</ul>

<h2 id=""itsateameffort"">It's a team effort</h2>

<p>Creating an automated system that connects offline sales to online sources is a team effort. It requires commitment from the client, our data department, our developers, and the account team to align the client's view with our project purpose. In the end, the result is awesome.  </p>

<p>There will be <a href=""https://www.themarketingtechnologist.co/connecting-offline-sales-to-online-campaign-sources-with-google-analytics-part-2/"">a follow-up post</a> to this post that handles the actual Python scripts that are used in this system. </p>
        ","Connecting offline to online is a challenge, but this week we did it. We’ve measured our first offline sales in Google Analytics, and we can directly attribute these to online campaign sources! Awesome right? In this two-part post series, we share the theory behind the system that gives us these insights, so you can setup a similar system yourself. This post describes the general system. The second post will discuss the actual code used in the system.
A story of two data sets
At its core, the system consists of two data sets: one that we capture online, and one that we get from our client’s offline sales team. Both data sets contain a customer identifier:
Online data set:
Date
Customer identifier
Google Analytics client id
Offline sales data set:
Date
Customer identifier
Revenue
The customer identifier allows our system to connect the Google Analytics client id to the offline revenue. We collect the data in two ways:
Online data collection for the Google Analytics client id and the customer identifier;
Secure server to put offline sales data on.
These two systems work like this:
1: Online data collection
The online data is collected every time the user fills out a customer identifier:
A simplified view of the online data collection for cross-channel analysis.
As soon as we have a customer identifier on site, we get the Google Analytics client ID and send the two values to a database.
2: Offline sales data
The offline sales data is made available through a secure FTP server:
A simplified view of the offline sales data server.
This data set is supplied to us on a daily basis.
The magic
The magic is in connecting these two data sets. The system is easy. Every day we run a Python script after the new offline sales data set becomes available. This system uses the online data as a lookup table for the Google Analytics client id. Based on this lookup, it fires a Google Analytics event through the measurement protocol:
A simplified view of the script that connects online data to offline sales.
We format the event as follows:
Event category: Offline.
Event action: Purchase.
Event value: revenue.
Now we can use the event value to measure offline revenue in Google Analytics. We also set up a goal based on the event that uses the event value as goal value.
How the attribution works
This system fires the event to Google Analytics without campaign tagging. Therefore, the event will be attributed to direct traffic. Google Analytics applies a last non-direct click model. This means that Google uses the client ID to attribute direct visit to the last known source of that user over the last six months: the source of the user when the customer identifier was filled out online. Boom! Offline sales with online source attribution.
Things to keep in mind
The system is easy at its core, but there are things to keep in mind while building the system:
Depending on your type of customer identifier (broad or narrow), it may appear multiple times for different users. Determine how to handle these duplicate entries.
Find out what the maximum time is for a customer journey, from first interaction to purchase. If the time gap between the online data point and the offline sale exceeds this time window, ignore the event.
It's a team effort
Creating an automated system that connects offline sales to online sources is a team effort. It requires commitment from the client, our data department, our developers, and the account team to align the client's view with our project purpose. In the end, the result is awesome.
There will be a follow-up post to this post that handles the actual Python scripts that are used in this system.","[Analytics, cross-channel, online/offline, ropo, measurement protocol, Data]"
26,"GHG Labs: ""The web loyalty system is ready to bring change to digital marketing""",/ghg-the-web-loyalty-system-is-ready-to-bring-change-to-digital-marketing/,"
            <p><p style=""color:#7e4396; font-weight:bold"">This post concerns Greenhouse Group Labs, an innovation program for students, established by Greenhouse Group. Labs is an ideal opportunity to test the latest technologies available, while allowing for talented young individuals to deeply explore them and come up with groundbreaking solutions.</p>As some of you might remember, two months ago we introduced you to <a href=""https://www.themarketingtechnologist.co/the-first-world-wide-web-loyalty-system/"">the first worldwide web loyalty system</a>, which was created to deal with the users’ rejection of digitals ads that are seen by them as irrelevant. In the meantime, we named it Ad-Wallet and managed to finalize our prototype. </p>

<h3 id=""whatwaslefttodo"">What was left to do</h3>

<p>One of the aspects we still had to focus on was the website’s interface. This part was just as important as the rest, especially given that in this case, users deal with their own data, so its design has definitely to provide them a feeling of trust and safety. For instance, its initial main colours – dark gray with blue accents – had the opposite effect. Thus, we changed the colour scheme, focusing instead on a combination of orange and white which definitely did the trick, as you can see below: </p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Jul/Picture1-1469040849345.png"" alt=""Ad-Wallet's menu"" class=""full-img"">
<em>Image 1 - Ad-Wallet's menu</em></p>

<p>Concerning its functionality, users simply need to log in with their Facebook account which will allow us to retrieve their interests from the pages they like. However, it is also possible to manage this information, so if someone doesn’t want to see ads about a specific subject, then he or she can remove that category from their Ad-Wallet account. This means of course that we’re giving back the power of the user over his or hers own information.</p>

<h3 id=""rewardsarewaitingforyou"">Rewards are waiting for you</h3>

<p>As we also explained on our previous post, this is only part of the solution for the problem the digital marketing world is faced with. In other words, we can deliver fine-tuned ads according to the user’s preferences, but it’s also important to ensure that they prompt some action, either that is seeing or clicking on ads. That’s how we manage to make users loyal. In this way, we reward them with discounts from the brands on the ads they responded to. As you can see from the screenshot below, the user received discount 'stamps', that can then be later used at the moment of purchasing an item from those brands:</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Jul/Picture2-1469040854005.png"" alt="""" class=""full-img""></p>

<h3 id=""reducingadblocksinfluence"">Reducing AdBlock's influence</h3>

<p>It goes without saying that users have to turn off their AdBlock extension, in order to use our web loyalty system, which brings them more benefits than it might seem at first. For instance, we might see a decrease of the number of websites such as Forbes’ that require turning off AdBlock in order to access them. But an important question still needs to be addressed: how will the relationship between Ad-Wallet and publishers function?</p>

<p>We believe that many drastic changes might end up having a negative effect, thus it is important that the process remains simple enough to be grasped and used accordingly by the professionals in this sector, in other words, that means mainly unaltered. However, this is an aspect of our concept that will still be further explored in the time to come.</p>

<h3 id=""exchangingpersonaldataagrowingtrend"">Exchanging personal data, a growing trend</h3>

<p>Naturally, we’re not the only ones to see the potential of exchanging personal data in some way. Currently, there are already several startups pursuing a similar path. For instance, <a href=""https://datacoup.com/"">Datacoup</a> gives you a certain amount of money according to the relevancy of your data, by simply connecting your apps to them. <a href=""https://www.powrofyou.com/"">Pwr of you</a> has a similar approach but they allow you also to look at the insights about your own personal data, in a Google Analytics fashion. Another example of this is <a href=""https://people.io/"">People.io</a>, a sort of more simplified version of Datacoup that seems to give you a greater control over the way your data is used. </p>

<p>This trend clearly tells us that re-empowering the user is definitely the way to go, but all of these services don’t have the same positive effect as Ad-Wallet. They don’t tackle AdBlock’s perennity. They don’t change digital ads for the better. This is part of Ad-Wallet’s mission and this defining aspect shows how it sits apart from the rest. </p>
        ","This post concerns Greenhouse Group Labs, an innovation program for students, established by Greenhouse Group. Labs is an ideal opportunity to test the latest technologies available, while allowing for talented young individuals to deeply explore them and come up with groundbreaking solutions.
As some of you might remember, two months ago we introduced you to the first worldwide web loyalty system, which was created to deal with the users’ rejection of digitals ads that are seen by them as irrelevant. In the meantime, we named it Ad-Wallet and managed to finalize our prototype.
What was left to do
One of the aspects we still had to focus on was the website’s interface. This part was just as important as the rest, especially given that in this case, users deal with their own data, so its design has definitely to provide them a feeling of trust and safety. For instance, its initial main colours – dark gray with blue accents – had the opposite effect. Thus, we changed the colour scheme, focusing instead on a combination of orange and white which definitely did the trick, as you can see below:
Image 1 - Ad-Wallet's menu
Concerning its functionality, users simply need to log in with their Facebook account which will allow us to retrieve their interests from the pages they like. However, it is also possible to manage this information, so if someone doesn’t want to see ads about a specific subject, then he or she can remove that category from their Ad-Wallet account. This means of course that we’re giving back the power of the user over his or hers own information.
Rewards are waiting for you
As we also explained on our previous post, this is only part of the solution for the problem the digital marketing world is faced with. In other words, we can deliver fine-tuned ads according to the user’s preferences, but it’s also important to ensure that they prompt some action, either that is seeing or clicking on ads. That’s how we manage to make users loyal. In this way, we reward them with discounts from the brands on the ads they responded to. As you can see from the screenshot below, the user received discount 'stamps', that can then be later used at the moment of purchasing an item from those brands:
Reducing AdBlock's influence
It goes without saying that users have to turn off their AdBlock extension, in order to use our web loyalty system, which brings them more benefits than it might seem at first. For instance, we might see a decrease of the number of websites such as Forbes’ that require turning off AdBlock in order to access them. But an important question still needs to be addressed: how will the relationship between Ad-Wallet and publishers function?
We believe that many drastic changes might end up having a negative effect, thus it is important that the process remains simple enough to be grasped and used accordingly by the professionals in this sector, in other words, that means mainly unaltered. However, this is an aspect of our concept that will still be further explored in the time to come.
Exchanging personal data, a growing trend
Naturally, we’re not the only ones to see the potential of exchanging personal data in some way. Currently, there are already several startups pursuing a similar path. For instance, Datacoup gives you a certain amount of money according to the relevancy of your data, by simply connecting your apps to them. Pwr of you has a similar approach but they allow you also to look at the insights about your own personal data, in a Google Analytics fashion. Another example of this is People.io, a sort of more simplified version of Datacoup that seems to give you a greater control over the way your data is used.
This trend clearly tells us that re-empowering the user is definitely the way to go, but all of these services don’t have the same positive effect as Ad-Wallet. They don’t tackle AdBlock’s perennity. They don’t change digital ads for the better. This is part of Ad-Wallet’s mission and this defining aspect shows how it sits apart from the rest.","[Labs, Adblocker]"
27,VIVA VR: Introduction to using Virtual Reality for marketing (part 1),/viva-vr-introduction-to-using-virtual-reality-for-marketing-part-1/,"
            <p>At the Greenhouse Group, we've been experimenting with Virtual Reality (VR) for a couple of years now. From the moment we got our hands on the first Oculus Rift, we knew this was going to be gold. Sure, we are nerds and we love the technology, but the foremost reason we're into VR is that it lets us take content to the next level. To make it more intense, more interactive, more fun and more meaningful. The number of search queries for 'VR Porn' is 10,000 (!) times higher compared to last year. There must be something going on, right? </p>

<h3 id=""slopeofenlightenment"">Slope of enlightenment</h3>

<p>To those familiar with the <a href=""http://www.gartner.com/technology/research/methodologies/hype-cycle.jsp"">Gartner Hype Cycle</a>, this should sound familiar. The initial VR-hype is already a few years behind us. The 'Oculus Rift' Kickstarter project in 2012 was the start. </p>

<p>Finally, VR technology seemed to become available to and affordable for the masses. But after Facebook bought Oculus in 2014, it became quiet in the land of VR. At the surface at least, because behind the scenes, all big tech companies were working on it. Not only Facebook, but also Google, Microsoft, Sony, and Samsung.  </p>

<p>Billions have been (and still are being) invested. Just a few weeks ago, HTC announced that they've set aside over <a href=""https://techcrunch.com/2016/06/29/htc-vive-announces-10-billion-vr-venture-capital-alliance/"">10 billion(!) Dollars</a> on marketing and technology for the HTC Vive. Meanwhile, the 'second generation' hardware has arrived. The Samsung Gear VR, Playstation VR, Oculus Rift and HTC Vive have launched and are available to the masses. </p>

<p>As Menno Zevenbergen (<a href=""https://www.facebook.com/greenhousegrouplabs/"">GHG Labs</a>) put it nicely: ""With VR, we're at the beginning of 'the slope of enlightenment'. This is the moment for courageous brands and developers to learn, to fail and to get a grip on VR.""   </p>

<p>In the next two years, we'll see and discover more ways to apply VR. The hardware is going to be more widespread, and consumers will get used to the experience. Everyone who's both curious and willing to invest, should get on the VR train right now.</p>

<h3 id=""contentrulesforthevrscene"">Content rules for the VR scene</h3>

<p>During our 'We &lt;3 VR' event, Jip van Samhoud (&amp;Samhoud | Media) talked about <a href=""https://thevrcinema.com/"">VR Cinema</a> and the <a href=""https://www.youtube.com/watch?v=CHL4AIp-3fE"">AH Dino case</a>. The latter is a well-known campaign in The Netherlands by a supermarket chain. With the AH Dino campaign, kids can get four Dinosaur cards for every € 10 they spend on groceries, and they can bring the dinosaurs to life by scanning the cards with an augmented reality smartphone app.</p>

<p>These cases show that it's not only about the application, but mostly also about the content. High-quality content is essential for making VR successful. In the AH Dino case alone, there's over 400 hours of content available. High-quality content. </p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Jul/13582067_1028204083942671_5788985386994292423_o_e1468238831453_1024x516-1468565652819.jpg"" alt="""" class=""full-img""></p>

<p>Besides that, VR also asks for a completely different approach to developing content. The creation of 360 degrees video, for example, differs greatly from '2D' video. The viewer is 'in' the content. That's neat, but it also means that you have zero control over where someone is looking. </p>

<p>You need to grab and hold the viewer's attention. Luckily, VR content platforms like <a href=""http://store.steampowered.com/search/?vrsupport=101%2C102"">SteamVR</a>, <a href=""https://itunes.apple.com/nl/app/within-vr-virtual-reality/id959327054?mt=8"">Within</a> and <a href=""http://www.nytimes.com/marketing/nytvr/"">NYTVR</a> are developing rapidly. As such, the number of great benchmarks grows rapidly. The coming years, the VR scene will be like a playground. Right now, the rules are being written by trial and error. </p>

<h3 id=""findyourfit"">Find your fit</h3>

<p>By now, the hardware comes in all shapes and sizes. From the rather simple 'Google Cardboard' (also used in the AH Dino action) to the more advanced HTC Vive. Between these, there is a very considerable difference in quality, and thus a difference in possible applications. </p>

<p>A cardboard VR 'device' is basically usable for everyone with a smartphone, but on the other hand, it has rather low quality and there's lack of real interactivity.</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Jul/13603554_1028204280609318_6967835598873765423_o_1024x521-1468565632465.jpg"" alt="""" class=""full-img""></p>

<p>For the photorealistic, interactive experience, we need to choose a high-end system like the Oculus Rift. Not only are these a lot more expensive, they are also far less mobile, because you'll need a heavy gaming PC to produce the images. They're a perfect fit for a publicity stunt, or an awesome experience at a specific location (like a store), but less suitable for the masses. At least for now. So, at this moment, the hardware is a crucial factor for defining the application and possibilities of VR, and the extent to which you can trigger the <em>WOW factor</em> for users.</p>

<h3 id=""vrarormrshowmesomeexamplesfirst"">VR, AR or MR? Show me some examples first!</h3>

<p>Virtual Reality, Augmented Reality, and Mixed Reality. In a next post, we'll dive deeper into the technical details and the possibilities. For now, here are some ace examples to get you (even more) excited:</p>

<h4 id=""happygoggles"">Happy Goggles</h4>

<p>McDonalds lets you transform your HappyMeal box into VR glasses. Virtual skiing is the new ball pit. </p>

<div class=""fluid-width-video-wrapper"" style=""padding-top: 56.25%;""><iframe src=""https://www.youtube.com/embed/bnYg752URcE?rel=0"" frameborder=""0"" allowfullscreen="""" id=""fitvid631728""></iframe></div>

<h4 id=""thedisplaced"">The displaced</h4>

<p>With this award-winning piece of content, the New York Times launched its virtual reality platform, the NYTVR app, with which they confront the user with the refugee issue. </p>

<div class=""fluid-width-video-wrapper"" style=""padding-top: 56.25%;""><iframe src=""https://www.youtube.com/embed/ecavbpCuvkI?rel=0"" frameborder=""0"" allowfullscreen="""" id=""fitvid924830""></iframe></div>

<h4 id=""oldirish"">Old Irish</h4>

<p>Mix VR with 'real world' activation and you get 'real-world virtual reality'. According to the creators at least. We just think it's an awesome stunt. </p>

<div class=""fluid-width-video-wrapper"" style=""padding-top: 56.25%;""><iframe src=""https://www.youtube.com/embed/49pASGqpb7M?rel=0"" frameborder=""0"" allowfullscreen="""" id=""fitvid369183""></iframe></div>

<h4 id=""volvoxc90"">Volvo XC90</h4>

<p>How would you introduce a car that's not even for sale yet? By making a virtual test drive throughout Canada. </p>

<div class=""fluid-width-video-wrapper"" style=""padding-top: 56.25%;""><iframe src=""https://www.youtube.com/embed/HEkGRUkqjTA?rel=0"" frameborder=""0"" allowfullscreen="""" id=""fitvid83140""></iframe></div>

<h4 id=""stepintothepage"">Step into the Page</h4>

<p>Take the man who created characters as an animator at Disney and let him create a VR environment. Step into the world of Glen Keane.</p>

<div class=""fluid-width-video-wrapper"" style=""padding-top: 56.25%;""><iframe src=""https://player.vimeo.com/video/138790270"" frameborder=""0"" webkitallowfullscreen="""" mozallowfullscreen="""" allowfullscreen="""" id=""fitvid939821""></iframe></div>

<h4 id=""theexcedrinmigraineexperience"">The Excedrin® Migraine Experience</h4>

<p>What Does a Migraine Feel Like?</p>

<div class=""fluid-width-video-wrapper"" style=""padding-top: 56.25%;""><iframe src=""https://www.youtube.com/embed/SmJW8gYIN4E?rel=0"" frameborder=""0"" allowfullscreen="""" id=""fitvid539942""></iframe></div>

<p>Would you like to discuss VR, or maybe even try the Oculus Rift at our office in Eindhoven (the Netherlands), just drop a line in the comments below, or send me an email at joris-ad-weareblossom.com. </p>
        ","At the Greenhouse Group, we've been experimenting with Virtual Reality (VR) for a couple of years now. From the moment we got our hands on the first Oculus Rift, we knew this was going to be gold. Sure, we are nerds and we love the technology, but the foremost reason we're into VR is that it lets us take content to the next level. To make it more intense, more interactive, more fun and more meaningful. The number of search queries for 'VR Porn' is 10,000 (!) times higher compared to last year. There must be something going on, right?
Slope of enlightenment
To those familiar with the Gartner Hype Cycle, this should sound familiar. The initial VR-hype is already a few years behind us. The 'Oculus Rift' Kickstarter project in 2012 was the start.
Finally, VR technology seemed to become available to and affordable for the masses. But after Facebook bought Oculus in 2014, it became quiet in the land of VR. At the surface at least, because behind the scenes, all big tech companies were working on it. Not only Facebook, but also Google, Microsoft, Sony, and Samsung.
Billions have been (and still are being) invested. Just a few weeks ago, HTC announced that they've set aside over 10 billion(!) Dollars on marketing and technology for the HTC Vive. Meanwhile, the 'second generation' hardware has arrived. The Samsung Gear VR, Playstation VR, Oculus Rift and HTC Vive have launched and are available to the masses.
As Menno Zevenbergen (GHG Labs) put it nicely: ""With VR, we're at the beginning of 'the slope of enlightenment'. This is the moment for courageous brands and developers to learn, to fail and to get a grip on VR.""
In the next two years, we'll see and discover more ways to apply VR. The hardware is going to be more widespread, and consumers will get used to the experience. Everyone who's both curious and willing to invest, should get on the VR train right now.
Content rules for the VR scene
During our 'We <3 VR' event, Jip van Samhoud (&Samhoud | Media) talked about VR Cinema and the AH Dino case. The latter is a well-known campaign in The Netherlands by a supermarket chain. With the AH Dino campaign, kids can get four Dinosaur cards for every € 10 they spend on groceries, and they can bring the dinosaurs to life by scanning the cards with an augmented reality smartphone app.
These cases show that it's not only about the application, but mostly also about the content. High-quality content is essential for making VR successful. In the AH Dino case alone, there's over 400 hours of content available. High-quality content.
Besides that, VR also asks for a completely different approach to developing content. The creation of 360 degrees video, for example, differs greatly from '2D' video. The viewer is 'in' the content. That's neat, but it also means that you have zero control over where someone is looking.
You need to grab and hold the viewer's attention. Luckily, VR content platforms like SteamVR, Within and NYTVR are developing rapidly. As such, the number of great benchmarks grows rapidly. The coming years, the VR scene will be like a playground. Right now, the rules are being written by trial and error.
Find your fit
By now, the hardware comes in all shapes and sizes. From the rather simple 'Google Cardboard' (also used in the AH Dino action) to the more advanced HTC Vive. Between these, there is a very considerable difference in quality, and thus a difference in possible applications.
A cardboard VR 'device' is basically usable for everyone with a smartphone, but on the other hand, it has rather low quality and there's lack of real interactivity.
For the photorealistic, interactive experience, we need to choose a high-end system like the Oculus Rift. Not only are these a lot more expensive, they are also far less mobile, because you'll need a heavy gaming PC to produce the images. They're a perfect fit for a publicity stunt, or an awesome experience at a specific location (like a store), but less suitable for the masses. At least for now. So, at this moment, the hardware is a crucial factor for defining the application and possibilities of VR, and the extent to which you can trigger the WOW factor for users.
VR, AR or MR? Show me some examples first!
Virtual Reality, Augmented Reality, and Mixed Reality. In a next post, we'll dive deeper into the technical details and the possibilities. For now, here are some ace examples to get you (even more) excited:
Happy Goggles
McDonalds lets you transform your HappyMeal box into VR glasses. Virtual skiing is the new ball pit.
The displaced
With this award-winning piece of content, the New York Times launched its virtual reality platform, the NYTVR app, with which they confront the user with the refugee issue.
Old Irish
Mix VR with 'real world' activation and you get 'real-world virtual reality'. According to the creators at least. We just think it's an awesome stunt.
Volvo XC90
How would you introduce a car that's not even for sale yet? By making a virtual test drive throughout Canada.
Step into the Page
Take the man who created characters as an animator at Disney and let him create a VR environment. Step into the world of Glen Keane.
The Excedrin® Migraine Experience
What Does a Migraine Feel Like?
Would you like to discuss VR, or maybe even try the Oculus Rift at our office in Eindhoven (the Netherlands), just drop a line in the comments below, or send me an email at joris-ad-weareblossom.com.","[Virtual Reality, VR]"
28,Facebook's advertising tips: First look,/facebooks-advertising-tips/,"
            <p>On June 28th Facebook hosted one of her Agency Academies in Amsterdam. During these academies, Dutch Facebook agencies are invited and updated about the latest developments on the platform. One of the announcements on that day was that Facebook would now assist you in your advertising campaigns by giving you tips on your performance and would suggest edits to improve your ad delivery.</p>

<h3 id=""advertisingtips"">Advertising tips</h3>

<p>Today, when opening my Facebook ads manager this option appeared for the first time. See the screenshot below:</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Jul/snip_20160704113417-1468486620551.png"" alt=""""></p>

<p>Your ad sets are marked with a series of different colored circles. Right now I'm seeing green and orange ones only. A green circle obviously means your campaign is running well, but the orange circles mean Facebook would like to give you some suggestions. I'm not sure if a red dot exists as well, I (thankfully) haven't run into one yet.</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Jul/snip_20160704113917-1468486753664.png"" alt=""""></p>

<p>When you click 'learn more' you'll get redirected to the Facebook Help Center where you can find a bunch of general info and tips. However, When you click on the review button, it takes you to the underperforming ad set and Facebook gives you an advertising tip I can put into practice straight away. In this case, the tip is about the potential reach of my ad set:</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Jul/snip_20160704114334-1468486857267.png"" alt=""""></p>

<p>The tip I screenshotted above doesn't actually show the real situation, because the audience I'm targeting actually has an audience size of 22.000 people. Facebook tells me I'm targeting 0... The content of the tip is clear though, I'm not targeting enough people and to make my campaign more effective I should broaden my audience. When I click the edit audience button, ads manager opens up to the screen where I can make changes straight away.</p>

<p>In this screen it shows the tip again, so you really really won't be able to overlook it:</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Jul/snip_20160704115806-1468487342720.png"" alt="""" class=""full-img""></p>

<p>This is an example of how Facebook tries to help me get more from my campaigns, I can imagine them giving these tips when your bid is too low, or when it's better to switch off or edit a creative when it is underperforming (when it has heavy text for example). There are a ton of ways Facebook can give you these tips, and they are a goldmine for Facebook. Think about it, the more effective your campaigns are, the higher the chances of you coming back and running even more campaigns with more budget.</p>

<p>Also, Facebook hopes to take some of the pressure off their support team, who receive thousands of questions about ad performance and campaigns and how to improve them every day. A win-win situation for Facebook :).</p>

<p>Have you stumbled across this function yet? Happy clicking!</p>
        ","On June 28th Facebook hosted one of her Agency Academies in Amsterdam. During these academies, Dutch Facebook agencies are invited and updated about the latest developments on the platform. One of the announcements on that day was that Facebook would now assist you in your advertising campaigns by giving you tips on your performance and would suggest edits to improve your ad delivery.
Advertising tips
Today, when opening my Facebook ads manager this option appeared for the first time. See the screenshot below:
Your ad sets are marked with a series of different colored circles. Right now I'm seeing green and orange ones only. A green circle obviously means your campaign is running well, but the orange circles mean Facebook would like to give you some suggestions. I'm not sure if a red dot exists as well, I (thankfully) haven't run into one yet.
When you click 'learn more' you'll get redirected to the Facebook Help Center where you can find a bunch of general info and tips. However, When you click on the review button, it takes you to the underperforming ad set and Facebook gives you an advertising tip I can put into practice straight away. In this case, the tip is about the potential reach of my ad set:
The tip I screenshotted above doesn't actually show the real situation, because the audience I'm targeting actually has an audience size of 22.000 people. Facebook tells me I'm targeting 0... The content of the tip is clear though, I'm not targeting enough people and to make my campaign more effective I should broaden my audience. When I click the edit audience button, ads manager opens up to the screen where I can make changes straight away.
In this screen it shows the tip again, so you really really won't be able to overlook it:
This is an example of how Facebook tries to help me get more from my campaigns, I can imagine them giving these tips when your bid is too low, or when it's better to switch off or edit a creative when it is underperforming (when it has heavy text for example). There are a ton of ways Facebook can give you these tips, and they are a goldmine for Facebook. Think about it, the more effective your campaigns are, the higher the chances of you coming back and running even more campaigns with more budget.
Also, Facebook hopes to take some of the pressure off their support team, who receive thousands of questions about ad performance and campaigns and how to improve them every day. A win-win situation for Facebook :).
Have you stumbled across this function yet? Happy clicking!","[Facebook, Social advertising]"
29,Change your website's address bar or device status bar to match your brand colors,/change-your-websites-address-bar-to-your-brand-colors/,"
            <p>Since <strong>Chrome version 39</strong> it is possible to have your website's address bar in a custom color instead of the default 'Seattle Gray' color.</p>

<p>Use the following meta tag in your head section and change the color to your choice. </p>

<p><code>&lt;meta name=""theme-color"" content=""#000""&gt;</code></p>

<p>With this setting your website has a subtle effect moving it a little step closer to a native app experience. You can also use it to extend your design, like the example below. The blue sky suddenly looks like it is stretched out to your entire screen. Sweet!</p>

<p>At this time of writing this setting is <strong>only available on Android Lollipop devices</strong>. <br>
<img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Jul/colored_menubar-1468418798545.png"" alt="""" class=""full-img""></p>

<p>Some settings are predefined by Chrome:</p>

<ul>
<li>Chrome auto picks the font color. Black on light backgrounds or white on dark backgrounds.</li>
<li>The status bar has a black transparent background color, if you choose black for the address bar the status bar will bleed into the address bar. In Chrome it is not possible to choose a custom color for the status bar. In Safari you can only change the status bar (see next paragraph).</li>
</ul>

<h4 id=""safaricustomstylingoptions"">Safari custom styling options</h4>

<p>Safari will only allow you to <strong>change the status bar color</strong>. <br>
If you choose 'black-translucent' it will float above the content, like a fixed position element in CSS.</p>

<p><code>&lt;meta name=""apple-mobile-web-app-status-bar-style"" content=""black-translucent""&gt;</code></p>

<p>Screenshot using 'black' <br>
<img src=""https://developers.google.com/web/fundamentals/design-and-ui/browser-customization/imgs/status-bar-black.png"" alt=""""></p>

<p>Screenshot using 'black-translucent' <br>
<img src=""https://developers.google.com/web/fundamentals/design-and-ui/browser-customization/imgs/status-bar-translucent.png"" alt=""""></p>

<p>Have fun with it and check out the documentation here:</p>

<p><a href=""https://developers.google.com/web/fundamentals/design-and-ui/browser-customization/theme-color"">https://developers.google.com/web/fundamentals/design-and-ui/browser-customization/theme-color</a> <br>
<a href=""https://developers.google.com/web/updates/2014/11/Support-for-theme-color-in-Chrome-39-for-Android"">https://developers.google.com/web/updates/2014/11/Support-for-theme-color-in-Chrome-39-for-Android</a></p>
        ","Since Chrome version 39 it is possible to have your website's address bar in a custom color instead of the default 'Seattle Gray' color.
Use the following meta tag in your head section and change the color to your choice.
<meta name=""theme-color"" content=""#000"">
With this setting your website has a subtle effect moving it a little step closer to a native app experience. You can also use it to extend your design, like the example below. The blue sky suddenly looks like it is stretched out to your entire screen. Sweet!
At this time of writing this setting is only available on Android Lollipop devices.
Some settings are predefined by Chrome:
Chrome auto picks the font color. Black on light backgrounds or white on dark backgrounds.
The status bar has a black transparent background color, if you choose black for the address bar the status bar will bleed into the address bar. In Chrome it is not possible to choose a custom color for the status bar. In Safari you can only change the status bar (see next paragraph).
Safari custom styling options
Safari will only allow you to change the status bar color.
If you choose 'black-translucent' it will float above the content, like a fixed position element in CSS.
<meta name=""apple-mobile-web-app-status-bar-style"" content=""black-translucent"">
Screenshot using 'black'
Screenshot using 'black-translucent'
Have fun with it and check out the documentation here:
https://developers.google.com/web/fundamentals/design-and-ui/browser-customization/theme-color
https://developers.google.com/web/updates/2014/11/Support-for-theme-color-in-Chrome-39-for-Android",[Code]
30,Shopping in the future: a virtual reality experience,/shopping-in-the-future-a-virtual-reality-experience/,"
            <p><p style=""color:#7e4396; font-weight:bold"">This post concerns Greenhouse Group Labs, an innovation program for students, established by Greenhouse Group. Labs is an ideal opportunity to test the latest technologies available, while allowing for talented young individuals to deeply explore them and come up with groundbreaking solutions.</p>Virtual reality still seems something out of the ordinary for many people. Some definitely think it belongs to the science fiction world, while others think it is very close to becoming realistic and I fall into the latter category. However, even the most sceptical ones should probably consider changing their minds as several companies became increasingly more interested in this technology and its potential applications.</p>

<h3 id=""virtualrealityintheshoppingenvironmentcurrentsolutions"">Virtual reality in the shopping environment: current solutions</h3>

<p>In 2016, many virtual reality devices were released and many businesses have already begun to invest their time and experts in the field of virtual reality. In my findings, several companies used virtual reality as a mean to display or show information, data, models or promotion. There are a few which use virtual reality as a means for people to purchase items. Most notable virtual reality stores found are the following:</p>

<h4 id=""yihaodian"">Yihaodian</h4>

<p>An online grocery service which launched 1000 stores in 2012 all over China. These stores, however, were not made of bricks and steel, but of pixels and bytes. Each virtual store covers 40 by 40 meter square of space and is placed strategically in different locations such as big open fields or considerable large empty places in general. In order to have access to this shop, users need to have the app installed on their smartphones. Then, as they walk in real life, they will simultaneously be walking around the store and viewing items on their phone.</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Jul/Picture1-1467622987932.png"" alt=""Figure 1 Yihaodian App"">
<em>Figure 1 Yihaodian App</em></p>

<h4 id=""mediasaturn"">Media-Saturn</h4>

<p>In early 2016 Media-Saturn tested virtual reality in their stores in Ingolstadt and Berlin, Germany. Using the HTC Vive as the means of the virtual device, they are able to display a showcase of kitchens and living rooms to customers. This method provides the user with a certain degree of control over the way how they want their kitchen to look like.</p>

<h4 id=""ikeavrexperience"">Ikea VR Experience</h4>

<p>In early April Ikea released an application that runs on the head-mounted device HTC-Vive. The application allows users to construct their dream kitchen and are enabled to walk in the simulated kitchen before they even purchase it. The user can adjust various features and options in the kitchen to his/her liking such as the size, colour and style of objects.</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Jul/Picture1-1467622971089.png"" alt="""" class=""full-img"">
Figure 2 Ikea VR Experience</p>

<h3 id=""ourprototype"">Our prototype</h3>

<blockquote>
  <blockquote>
    <p>Experience a relevant environment that allows for personalized products</p>
  </blockquote>
</blockquote>

<p>In our approach to the development phase, we decided to develop a prototype with the bare minimum core functionality and not a complete environment, which could be put fully into question with the results from the user tests that would follow. Thus, the user tests allowed us to observe how individuals behave in our virtual reality shopping environment. After that, we made the necessary improvements and performed more tests to see if the changes had successfully tackled the issues we came across with.</p>

<p>The prototype centres on an outdoor store where people would buy their gear and tools for camping or hiking. We aimed for users to shop in the same environment in which they plan to go hiking or camping. </p>

<p>Using the newest Oculus Rift released in 2016 as the technology to connect to the virtual world, a user can walk around the environment and select items on a floating menu. In the image below you can see the floating menu with four items on it in the middle of a forest. A user can select any item and it will be spawned in a location in the environment. When the item is spawned, a sound is produced in the direction of the item. <br>
Besides that, the user has the freedom to walk around in the forest using an external controller.</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Jul/Picture1-1467623016911.png"" alt=""Menu of the prototype"" class=""full-img"">
<em>Figure 3 Menu of the prototype</em></p>

<p>In the image below you can see a tent which was chosen from the menu. Above the tent are two spheres which control the colour of it. The only available colours for the tent are green and blue. The user can focus on one of the two spheres and the tent will change the colour accordingly.</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Jul/Picture1-1467623069614.png"" alt=""""></p>

<h3 id=""ebayshowsusthatwereintherightway"">eBay shows us that we’re in the right way</h3>

<p>Two months ago, Myer and eBay together released the very first virtual reality department store called “eBay VR Experience”. It has over 12,000 products available that customers can check by using the shopticals (exclusive VR goggles for this shopping experience).</p>

<p>This solution shows that shopping in virtual reality is becoming more realistic. However, these companies are still looking into improving this virtual environment, which will certainly become better in time. What we have made is similar to what they have achieved and it shows that shopping in virtual reality is here to stay and will continue to grow in the upcoming years.</p>
        ","This post concerns Greenhouse Group Labs, an innovation program for students, established by Greenhouse Group. Labs is an ideal opportunity to test the latest technologies available, while allowing for talented young individuals to deeply explore them and come up with groundbreaking solutions.
Virtual reality still seems something out of the ordinary for many people. Some definitely think it belongs to the science fiction world, while others think it is very close to becoming realistic and I fall into the latter category. However, even the most sceptical ones should probably consider changing their minds as several companies became increasingly more interested in this technology and its potential applications.
Virtual reality in the shopping environment: current solutions
In 2016, many virtual reality devices were released and many businesses have already begun to invest their time and experts in the field of virtual reality. In my findings, several companies used virtual reality as a mean to display or show information, data, models or promotion. There are a few which use virtual reality as a means for people to purchase items. Most notable virtual reality stores found are the following:
Yihaodian
An online grocery service which launched 1000 stores in 2012 all over China. These stores, however, were not made of bricks and steel, but of pixels and bytes. Each virtual store covers 40 by 40 meter square of space and is placed strategically in different locations such as big open fields or considerable large empty places in general. In order to have access to this shop, users need to have the app installed on their smartphones. Then, as they walk in real life, they will simultaneously be walking around the store and viewing items on their phone.
Figure 1 Yihaodian App
Media-Saturn
In early 2016 Media-Saturn tested virtual reality in their stores in Ingolstadt and Berlin, Germany. Using the HTC Vive as the means of the virtual device, they are able to display a showcase of kitchens and living rooms to customers. This method provides the user with a certain degree of control over the way how they want their kitchen to look like.
Ikea VR Experience
In early April Ikea released an application that runs on the head-mounted device HTC-Vive. The application allows users to construct their dream kitchen and are enabled to walk in the simulated kitchen before they even purchase it. The user can adjust various features and options in the kitchen to his/her liking such as the size, colour and style of objects.
Figure 2 Ikea VR Experience
Our prototype
Experience a relevant environment that allows for personalized products
In our approach to the development phase, we decided to develop a prototype with the bare minimum core functionality and not a complete environment, which could be put fully into question with the results from the user tests that would follow. Thus, the user tests allowed us to observe how individuals behave in our virtual reality shopping environment. After that, we made the necessary improvements and performed more tests to see if the changes had successfully tackled the issues we came across with.
The prototype centres on an outdoor store where people would buy their gear and tools for camping or hiking. We aimed for users to shop in the same environment in which they plan to go hiking or camping.
Using the newest Oculus Rift released in 2016 as the technology to connect to the virtual world, a user can walk around the environment and select items on a floating menu. In the image below you can see the floating menu with four items on it in the middle of a forest. A user can select any item and it will be spawned in a location in the environment. When the item is spawned, a sound is produced in the direction of the item.
Besides that, the user has the freedom to walk around in the forest using an external controller.
Figure 3 Menu of the prototype
In the image below you can see a tent which was chosen from the menu. Above the tent are two spheres which control the colour of it. The only available colours for the tent are green and blue. The user can focus on one of the two spheres and the tent will change the colour accordingly.
eBay shows us that we’re in the right way
Two months ago, Myer and eBay together released the very first virtual reality department store called “eBay VR Experience”. It has over 12,000 products available that customers can check by using the shopticals (exclusive VR goggles for this shopping experience).
This solution shows that shopping in virtual reality is becoming more realistic. However, these companies are still looking into improving this virtual environment, which will certainly become better in time. What we have made is similar to what they have achieved and it shows that shopping in virtual reality is here to stay and will continue to grow in the upcoming years.","[Labs, Virtual Reality, VR]"
31,3 steps to remove all sensitive user data from Hotjar,/3-steps-to-remove-sensitive-userdata-from-hotjar-recordings/,"
            <p>If you're not using <a href=""https://www.hotjar.com/"">Hotjar</a> yet, you should be. While Google Analytics gives you a lot of quantitative data to interpret, Hotjar focusses on qualitative data, letting you know exactly what actions your visitors are taking on your site. We're talking heatmaps and screen recordings here. Screen recordings are especially useful as you can actually watch a user navigate through your website, and see the issues they run into. Screen recordings have a small caveat though, as most legal departments find it risky to record PII (Personally Identifiable Information). While Hotjar supplies a standard fix for this issue, it's only a small part of the solution. To completely remove all the users personal information from the recordings, you need to follow these 3 steps:</p>

<h2 id=""step1blockhotjarfromrecordingkeystrokedata"">Step 1: Block Hotjar from recording keystroke data</h2>

<p>When setting up your site in Hotjar, you'll eventually run into a checkbox that says ""Record visitor keystroke data on this site"". Unchecking this will result in any user input to show up as asterisks in your recording. This is the part you've probably done already.</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Jul/hjflow-1467639652032.jpg"" alt=""All you have to do is uncheck that box"" class=""full-img""></p>

<h2 id=""step2blockhotjarfromseeinginputvalues"">Step 2: Block Hotjar from seeing input values</h2>

<p>While keystrokes are blocked, there are other ways a user's entered data can be shown in any input field:</p>

<ul>
<li>Using the browser's back-button</li>
<li>The browser's autocomplete tools</li>
<li>Copy/pasting data (sometimes)</li>
<li>Validation scripts (sometimes)</li>
</ul>

<p>Because this data isn't registered as a keystroke, the user's data will be visible in the recording, which is a bad thing. Luckily, Hotjar has you covered and has a specific HTML attribute that when set, will <a href=""http://docs.hotjar.com/docs/masking-elements-in-recordings-and-heatmaps"">mask the elements content in the recording</a>. All you need to do is add <code>data-hj-masked</code> to your elements and you're good to go. If that sounds like a lot of work, you can always add a line of code above your Hotjar tracking script:</p>

<pre><code>//note: this is a jQuery example, recode to vanilla JS if jQuery is not available on your website.
$('input[type=""text""]').attr('data-hj-masked','');

&lt;!-- Hotjar Tracking Code for www.yoursite.com --&gt;  
  ... rest of the Hotjar tracking code ...
</code></pre>

<p>There you go, all of your inputs are now masked in Hotjar recordings.</p>

<h2 id=""step3blockhotjarfromrecordingfeedbackdata"">Step 3: Block Hotjar from recording feedback data</h2>

<p>This is the part most people forget. Let's say you have an e-commerce website, and you've meticulously blocked all inputs from showing up in the Hotjar recordings. Some time has passed and you look at a recording. The user types their information in the fields, you only see asterisks, so far so good. The user moves through the funnel, and ends up on the confirmation page, and there it is: all the users information right there on the screen. Crap.</p>

<p>Most sites have some form of feedback of user data as a final check before a purchase, usually in the form of a final step listing everything a user has entered, from selected products to shipping information (and maybe even creditcard details). Don't forget to add the <code>data-hj-masked</code>-attribute to the elements that hold this information, so Hotjar won't see it. If you can't access the site, I recommend adding the specific selectors to a javascript array and adding the attribute just like the example in step 2:  </p>

<pre><code>var blockFromHotjar = ['#details','.user-info','input[type=""text""]','textarea'];  
$.each(blockFromHotjar,function(i,val){
  $(val).attr('data-hj-masked','');
});

&lt;!-- Hotjar Tracking Code for www.yoursite.com --&gt;  
  ... rest of the Hotjar tracking code ...
</code></pre>

<p>With the code above, any selectors you add to the array, will automatically get the attribute needed to block it from Hotjar's recordings.</p>

<p>And there you go, your Hotjar recordings are now totally void of any PII. If you listen closely, you can hear the sighs of relief coming from your legal department.</p>
        ","If you're not using Hotjar yet, you should be. While Google Analytics gives you a lot of quantitative data to interpret, Hotjar focusses on qualitative data, letting you know exactly what actions your visitors are taking on your site. We're talking heatmaps and screen recordings here. Screen recordings are especially useful as you can actually watch a user navigate through your website, and see the issues they run into. Screen recordings have a small caveat though, as most legal departments find it risky to record PII (Personally Identifiable Information). While Hotjar supplies a standard fix for this issue, it's only a small part of the solution. To completely remove all the users personal information from the recordings, you need to follow these 3 steps:
Step 1: Block Hotjar from recording keystroke data
When setting up your site in Hotjar, you'll eventually run into a checkbox that says ""Record visitor keystroke data on this site"". Unchecking this will result in any user input to show up as asterisks in your recording. This is the part you've probably done already.
Step 2: Block Hotjar from seeing input values
While keystrokes are blocked, there are other ways a user's entered data can be shown in any input field:
Using the browser's back-button
The browser's autocomplete tools
Copy/pasting data (sometimes)
Validation scripts (sometimes)
Because this data isn't registered as a keystroke, the user's data will be visible in the recording, which is a bad thing. Luckily, Hotjar has you covered and has a specific HTML attribute that when set, will mask the elements content in the recording. All you need to do is add data-hj-masked to your elements and you're good to go. If that sounds like a lot of work, you can always add a line of code above your Hotjar tracking script:
//note: this is a jQuery example, recode to vanilla JS if jQuery is not available on your website.
$('input[type=""text""]').attr('data-hj-masked','');

<!-- Hotjar Tracking Code for www.yoursite.com -->  
  ... rest of the Hotjar tracking code ...
There you go, all of your inputs are now masked in Hotjar recordings.
Step 3: Block Hotjar from recording feedback data
This is the part most people forget. Let's say you have an e-commerce website, and you've meticulously blocked all inputs from showing up in the Hotjar recordings. Some time has passed and you look at a recording. The user types their information in the fields, you only see asterisks, so far so good. The user moves through the funnel, and ends up on the confirmation page, and there it is: all the users information right there on the screen. Crap.
Most sites have some form of feedback of user data as a final check before a purchase, usually in the form of a final step listing everything a user has entered, from selected products to shipping information (and maybe even creditcard details). Don't forget to add the data-hj-masked-attribute to the elements that hold this information, so Hotjar won't see it. If you can't access the site, I recommend adding the specific selectors to a javascript array and adding the attribute just like the example in step 2:
var blockFromHotjar = ['#details','.user-info','input[type=""text""]','textarea'];  
$.each(blockFromHotjar,function(i,val){
  $(val).attr('data-hj-masked','');
});

<!-- Hotjar Tracking Code for www.yoursite.com -->  
  ... rest of the Hotjar tracking code ...
With the code above, any selectors you add to the array, will automatically get the attribute needed to block it from Hotjar's recordings.
And there you go, your Hotjar recordings are now totally void of any PII. If you listen closely, you can hear the sighs of relief coming from your legal department.","[cro, Data, privacy]"
32,Optimizing banner ads by tracking banner interaction,/optimizing-banner-ads-by-tracking-banner-interaction/,"
            <p>In general banner optimization is focused on, for example, A/B testing a creative and tracking impressions,  clicks and onsite (micro) conversions. <br>
But once a banner visitor enters the website and continues his path through the funnel, each step makes it more difficult to link an action completely to the initial banner he clicked on. </p>

<p>And while there are many tools and options for onsite optimization and measuring interactions, offsite you are stuck. Correction: you <em>were</em> stuck. <br>
My colleagues Koen and Siebe at the Greenhouse Group started working on ideas to measure banner interaction and came up with the very cool idea of Kosi metrics; an internally developed tool for Greenhouse group clients. </p>

<h2>How to start analyzing banners before the click</h2>  

<p>Kosi metrics works by pushing events from the banner to Google Analytics. By doing this you can track interactions such as:</p>

<ul>
<li>Swiping in the banner</li>
<li>swipe through rate</li>
<li>Hovers</li>
<li>Interaction rate on impressions</li>
<li>Comparing CTR and interaction rates</li>
</ul>

<p>It’s clear to see that by adding Kosi metrics to the banner, you can collect lots of interesting insights for analyzing and optimizing your banner ads. <br>
As soon as I heard about this new way of tracking banners I asked Siebe if we could also measure certain banner areas. And after a meeting with some enthusiastic  Analytics colleagues we decided to create not only an interaction map based on some pre-defined areas, but also set up some custom dimensions for  the x- and y-lines so we could measure the banner completely.</p>

<h2>Tracking hover and click areas in a banner - the pilot</h2>  

<p>For the pilot we started adding Kosi metrics to the banner of one of our clients. We chose a comparison banner, which we optimized several times, but as the banner has many topics, we thought it would be interesting to see if one of the colomns/topics leads to more interactions.</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Jul/BlogTMT_Kosi_Banner-1467369376890.jpg"" alt="""">
<em>Example of the banner we used for the Kosi pilot</em></p>

<h2>Which data did we collect?</h2>  

<p>The data we mainly focused on was the collection of hovers and clicks, in combination with the banner areas. We divided the banner into 10 segments and we wanted to know where users interact (or don’t interact at all) in order to formulate new hypotheses and testing new variants.</p>

<h2 id=""howdidwevisualizethebannerinteractiondata"">How did we visualize the banner interaction data?</h2>

<p>We made two types of data visualisations. The first two are a summary heatmap of the 10 areas (hovers and clicks); the other visualisation is a more profound heatmap, created in Excel, based on the x- and y-lines and covers each pixel of the banner.</p>

<p><em>Hover heatmap:</em>
<img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Jul/Kosimetrics_heatmap_Hovering2-1467370298666.jpg"" alt="""">
<img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Jul/Hovering_colorsv2-1467623308793.jpg"" alt=""""></p>

<p><em>Clicks heatmap:</em></p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Jul/Kosimetrics_heatmap_Clicks2-1467370086682.jpg"" alt="""">
<img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Jul/Clicks_Colorsv2-1467623357645.jpg"" alt=""""></p>

<p><em>Heatmap based on the x- and y-lines, created with custom dimensions in Analytics:</em></p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Jul/Heatmaps_clicks_Kosi_v2-1467623352600.jpg"" alt="""" class=""full-img""></p>

<h2>What learnings did we get?</h2>  

<p>As you can see lots of (hover) interactions took place in the header, which rather surprised us. That immediately leads to questions for optimization we wouldn’t have before adding Kosi metrics. Besides we discovered there’s also a lot of interaction in one of the columns and with this information we can now further investigate why there’s interaction there. Not very surprisingly most clicks go to the button, but it might be interesting to make clear other areas are clickable as well. </p>

<p>So instead of testing things like removing one of the columns based on assumptions, we can now create hypotheses and variants based on data. How users interact in a banner becomes easier to see, so we can now focus more deeply on why users interact with some elements or don’t.</p>

<p>Kosi metrics provides us a very effective step in creating better performing banners within less time.</p>
        ","In general banner optimization is focused on, for example, A/B testing a creative and tracking impressions, clicks and onsite (micro) conversions.
But once a banner visitor enters the website and continues his path through the funnel, each step makes it more difficult to link an action completely to the initial banner he clicked on.
And while there are many tools and options for onsite optimization and measuring interactions, offsite you are stuck. Correction: you were stuck.
My colleagues Koen and Siebe at the Greenhouse Group started working on ideas to measure banner interaction and came up with the very cool idea of Kosi metrics; an internally developed tool for Greenhouse group clients.
How to start analyzing banners before the click
Kosi metrics works by pushing events from the banner to Google Analytics. By doing this you can track interactions such as:
Swiping in the banner
swipe through rate
Hovers
Interaction rate on impressions
Comparing CTR and interaction rates
It’s clear to see that by adding Kosi metrics to the banner, you can collect lots of interesting insights for analyzing and optimizing your banner ads.
As soon as I heard about this new way of tracking banners I asked Siebe if we could also measure certain banner areas. And after a meeting with some enthusiastic Analytics colleagues we decided to create not only an interaction map based on some pre-defined areas, but also set up some custom dimensions for the x- and y-lines so we could measure the banner completely.
Tracking hover and click areas in a banner - the pilot
For the pilot we started adding Kosi metrics to the banner of one of our clients. We chose a comparison banner, which we optimized several times, but as the banner has many topics, we thought it would be interesting to see if one of the colomns/topics leads to more interactions.
Example of the banner we used for the Kosi pilot
Which data did we collect?
The data we mainly focused on was the collection of hovers and clicks, in combination with the banner areas. We divided the banner into 10 segments and we wanted to know where users interact (or don’t interact at all) in order to formulate new hypotheses and testing new variants.
How did we visualize the banner interaction data?
We made two types of data visualisations. The first two are a summary heatmap of the 10 areas (hovers and clicks); the other visualisation is a more profound heatmap, created in Excel, based on the x- and y-lines and covers each pixel of the banner.
Hover heatmap:
Clicks heatmap:
Heatmap based on the x- and y-lines, created with custom dimensions in Analytics:
What learnings did we get?
As you can see lots of (hover) interactions took place in the header, which rather surprised us. That immediately leads to questions for optimization we wouldn’t have before adding Kosi metrics. Besides we discovered there’s also a lot of interaction in one of the columns and with this information we can now further investigate why there’s interaction there. Not very surprisingly most clicks go to the button, but it might be interesting to make clear other areas are clickable as well.
So instead of testing things like removing one of the columns based on assumptions, we can now create hypotheses and variants based on data. How users interact in a banner becomes easier to see, so we can now focus more deeply on why users interact with some elements or don’t.
Kosi metrics provides us a very effective step in creating better performing banners within less time.","[conversion rate optimization, banner optimization, Tracking banner interaction, Analytics, Event tracking, banner heatmap, cro]"
33,Research: Mobile advertising increases both online and offline sales,/research-mobile-advertising-increases-both-online-and-offline-sales/,"
            <p>Finally, there’s proof for what we’ve known all along: mobile advertising has a positive effect on sales through other channels, such as other devices or an offline shop. Finally, because anno 2016, mobile advertising is still struggling to claim its rightful place in the marketing world. Sure, budgets for mobile ads are on the rise, but if we need a budget cut, mobile ads are the first to go. </p>

<p>This makes perfect sense, of course. A result hardly counts unless you can measure it, and the ROI of a mobile ad isn’t easy to calculate. Many marketers prefer to take the easy road, and choose the method with the most obvious result. Now you might wonder why the effect of mobile bannering hasn’t been researched before. After all, in this era of the smartphone, it’s unthinkable that mobile ads don’t have an impact. The biggest hurdle? Data. To research this impact, you need to target a large, diverse group of mobile users, and find a way to compare the online and offline sales results of this campaign with a control group. That may sound like a piece of cake, but there’s a reason we’re the first in the world to research the impact of mobile ads this way. </p>

<h3 id=""goldenopportunitywiththestaatsloterij"">Golden opportunity with the Staatsloterij</h3>

<p>The perfect partner for this experiment wasn’t easy to find. Luckily, one of our clients was just as curious about the ROI of mobile advertising as we are: the national lottery (Staatsloterij). A golden opportunity, because Staatsloterij has a very heterogeneous client base. Young, old, unemployed, highly educated, indigenous people, immigrants and everything in between. People from all segments of the population are represented. Best of all, these clients are distributed somewhat evenly across the country. Staatsloterij was very eager to help us with our experiment and provided us with 1.5 years of historical sales data. </p>

<h3 id=""theresearchapproach"">The research approach</h3>

<p>To test the impact of mobile ads, we selected 3 Dutch provinces: Drenthe, Overijssel and Zuid-Holland. Using IP-masts, we targeted people in just these provinces and served them a total of 3.5 million ad impressions on their mobile phones. Meanwhile, we paused all mobile ads in the rest of the country: our control group. Through the so-called <a href=""https://en.wikipedia.org/wiki/Fixed_effects_model"">fixed effects model</a> we neutralised constants. I won’t go down the statistics rabbit hole, but this basically means we filtered the effect of other factors (besides the mobile ads) to achieve a clean result. This is where those 1.5 years of historical data came in handy. Apart from that, we carefully selected our test and control groups to diminish result-altering differences in areas such as age, education, or employment. </p>

<h3 id=""ontotheresults"">On to the results</h3>

<p>Hopefully, at this point you’re still curious about the results of our experiment. And those were pretty outstanding. The offline sales increased with 2.8% on average! A great result! What's also interesting is we discovered that offline sales don’t have a so-called cannibalistic effect on online sales. On average, the amount of online sales through mobile and non-mobile channels increased with 7%! Our conclusion? The ROI of mobile ads is considerably higher than previously calculated. This definitely calls for further research, for instance with other clients and in different markets. That’s why we hope that others follow our lead. Of course, we also hope that mobile ads will finally get the attention (and budgets) they deserve. </p>
        ","Finally, there’s proof for what we’ve known all along: mobile advertising has a positive effect on sales through other channels, such as other devices or an offline shop. Finally, because anno 2016, mobile advertising is still struggling to claim its rightful place in the marketing world. Sure, budgets for mobile ads are on the rise, but if we need a budget cut, mobile ads are the first to go.
This makes perfect sense, of course. A result hardly counts unless you can measure it, and the ROI of a mobile ad isn’t easy to calculate. Many marketers prefer to take the easy road, and choose the method with the most obvious result. Now you might wonder why the effect of mobile bannering hasn’t been researched before. After all, in this era of the smartphone, it’s unthinkable that mobile ads don’t have an impact. The biggest hurdle? Data. To research this impact, you need to target a large, diverse group of mobile users, and find a way to compare the online and offline sales results of this campaign with a control group. That may sound like a piece of cake, but there’s a reason we’re the first in the world to research the impact of mobile ads this way.
Golden opportunity with the Staatsloterij
The perfect partner for this experiment wasn’t easy to find. Luckily, one of our clients was just as curious about the ROI of mobile advertising as we are: the national lottery (Staatsloterij). A golden opportunity, because Staatsloterij has a very heterogeneous client base. Young, old, unemployed, highly educated, indigenous people, immigrants and everything in between. People from all segments of the population are represented. Best of all, these clients are distributed somewhat evenly across the country. Staatsloterij was very eager to help us with our experiment and provided us with 1.5 years of historical sales data.
The research approach
To test the impact of mobile ads, we selected 3 Dutch provinces: Drenthe, Overijssel and Zuid-Holland. Using IP-masts, we targeted people in just these provinces and served them a total of 3.5 million ad impressions on their mobile phones. Meanwhile, we paused all mobile ads in the rest of the country: our control group. Through the so-called fixed effects model we neutralised constants. I won’t go down the statistics rabbit hole, but this basically means we filtered the effect of other factors (besides the mobile ads) to achieve a clean result. This is where those 1.5 years of historical data came in handy. Apart from that, we carefully selected our test and control groups to diminish result-altering differences in areas such as age, education, or employment.
On to the results
Hopefully, at this point you’re still curious about the results of our experiment. And those were pretty outstanding. The offline sales increased with 2.8% on average! A great result! What's also interesting is we discovered that offline sales don’t have a so-called cannibalistic effect on online sales. On average, the amount of online sales through mobile and non-mobile channels increased with 7%! Our conclusion? The ROI of mobile ads is considerably higher than previously calculated. This definitely calls for further research, for instance with other clients and in different markets. That’s why we hope that others follow our lead. Of course, we also hope that mobile ads will finally get the attention (and budgets) they deserve.","[Analytics, Online Advertising]"
34,Recently used targeting in Facebook's Power Editor,/recently-used-targeting-power-editor/,"
            <p>Facebook optimizes and makes little changes to its platform every day. The apps get a new update every single week and all of the Facebook platforms' developments can sometimes make your head spin. All of these changes (big or small) can sometimes easily be left unnoticed until you happen to stumble upon them by accident. Like the new targeting suggestions in Power Editor. A playful little tool I feel you should play with at least once.</p>

<h3 id=""makinguseoftherecentlyusedtargetingoption"">Making use of the recently used targeting option</h3>

<p>You can find this option in the Power Editor when you create a new ad set in an account that you frequently make use of. When I set up a new ad set a couple of days ago and went to set my targeting preferences I stumbled upon one of these little nifty Facebook options: <br>
<img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Jun/snip_20160620164904-1467191124865.png"" alt="""" class=""full-img""></p>

<p>To make the text easier to read I've copied it below:</p>

<p><strong>Recently Used</strong></p>

<blockquote>
  <blockquote>
    <p>We’ll show your recently used targeting options in order of how well they performed in past campaigns. Performance is calculated from the relative number of conversions and cost per conversion attributed to each interest, behavior or demographic. See more details by hovering over “Recently Used” options. Add high performers to use targeting that has worked well for you before.</p>
  </blockquote>
</blockquote>

<p><strong>Suggested</strong></p>

<blockquote>
  <blockquote>
    <p>We’ll suggest interests, behaviors and demographics based on your previous selections or popularity with similar advertisers. Suggestions are a good place to start if you want to quickly find relevant options to define your audience, but aren’t sure what to search for.</p>
  </blockquote>
</blockquote>

<p>So, when adding your targeting preferences you now get the option to pick from a set of interests that you've used before, called the Recently Used targeting option. When you hover over the different interests you used in previous campaigns/ad sets you get an insight into which of these interests performed well in the past:</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Jun/snip_20160620165524-1467191397252.png"" alt="""" class=""full-img""></p>

<p>In one of our campaigns, we've used the interest targeting to target people with an interest in 'De Wereld Draait Door' (a Dutch television show). You can see that the performance of this interest group was average, so definitely not one I would necessarily include in future campaigns ;).</p>

<p><strong>How are the interests rated?</strong></p>

<p>The grading is as follows: <br>
1. High <br>
2. Above average <br>
3. Average <br>
4. Below average <br>
5. Low</p>

<p>It's funny to see that some keywords I've added don't perform well at all, while others that I wouldn't have expected to perform a lot better. This is also a legit metric to take back to your clients, who persist on using a certain target interest time and time again. If Facebook tells you it doesn't work, it must be true! You've now got evidence to show you that some interest perform less well than others. Keep in mind though: this is probably not a foolproof method by far. It seems as though Power Editor takes an average from a bunch of campaigns in the past &amp; targeting you've previously worked, which doesn't mean that an interest doesn't work at all, bus maybe not in the combination with the creatives or message you used. </p>

<p>Still, it's a funny little tool to click through and see what Facebook has to tell you about your targeting, to add the high performing ones (and new ones!) to see of things change. I wouldn't go and optimize all of your campaigns based on this one tiny feature, but just taking a look might give you some insights you could actually work with. </p>

<p>Also, I don't think there will ever only be 'average' performing interests, because out of all your average performing targeting, one will still come up on top ;).</p>

<p>Happy clicking!</p>
        ","Facebook optimizes and makes little changes to its platform every day. The apps get a new update every single week and all of the Facebook platforms' developments can sometimes make your head spin. All of these changes (big or small) can sometimes easily be left unnoticed until you happen to stumble upon them by accident. Like the new targeting suggestions in Power Editor. A playful little tool I feel you should play with at least once.
Making use of the recently used targeting option
You can find this option in the Power Editor when you create a new ad set in an account that you frequently make use of. When I set up a new ad set a couple of days ago and went to set my targeting preferences I stumbled upon one of these little nifty Facebook options:
To make the text easier to read I've copied it below:
Recently Used
We’ll show your recently used targeting options in order of how well they performed in past campaigns. Performance is calculated from the relative number of conversions and cost per conversion attributed to each interest, behavior or demographic. See more details by hovering over “Recently Used” options. Add high performers to use targeting that has worked well for you before.
Suggested
We’ll suggest interests, behaviors and demographics based on your previous selections or popularity with similar advertisers. Suggestions are a good place to start if you want to quickly find relevant options to define your audience, but aren’t sure what to search for.
So, when adding your targeting preferences you now get the option to pick from a set of interests that you've used before, called the Recently Used targeting option. When you hover over the different interests you used in previous campaigns/ad sets you get an insight into which of these interests performed well in the past:
In one of our campaigns, we've used the interest targeting to target people with an interest in 'De Wereld Draait Door' (a Dutch television show). You can see that the performance of this interest group was average, so definitely not one I would necessarily include in future campaigns ;).
How are the interests rated?
The grading is as follows:
1. High
2. Above average
3. Average
4. Below average
5. Low
It's funny to see that some keywords I've added don't perform well at all, while others that I wouldn't have expected to perform a lot better. This is also a legit metric to take back to your clients, who persist on using a certain target interest time and time again. If Facebook tells you it doesn't work, it must be true! You've now got evidence to show you that some interest perform less well than others. Keep in mind though: this is probably not a foolproof method by far. It seems as though Power Editor takes an average from a bunch of campaigns in the past & targeting you've previously worked, which doesn't mean that an interest doesn't work at all, bus maybe not in the combination with the creatives or message you used.
Still, it's a funny little tool to click through and see what Facebook has to tell you about your targeting, to add the high performing ones (and new ones!) to see of things change. I wouldn't go and optimize all of your campaigns based on this one tiny feature, but just taking a look might give you some insights you could actually work with.
Also, I don't think there will ever only be 'average' performing interests, because out of all your average performing targeting, one will still come up on top ;).
Happy clicking!","[Facebook, Analytics, Social advertising]"
35,Improve your customer journey with data collection,/improve-your-customer-journey-with-data-collection/,"
            <p>During <a href=""https://www.themarketingtechnologist.co/what-ive-learned-by-giving-a-talk-about-data-to-100-people/"">my talk at Growth Hacking symposium</a> in March, I presented a guide for setting up data collection for any project, startup or website.  In this post, I’ll discuss that specific guide and help you set up your basic metrics.</p>

<h2 id=""whyshouldyoucollectdata"">Why should you collect data?</h2>

<p>Let's start with the most important question: why should you set up a data collection solution? Well, it helps you make better decisions for whatever you’re doing. Without it, you might have an idea of what’s going on, but you won’t know any actual numbers. With data collection, you know exactly what’s going on and are able to improve your product in the best possible way. <strong>Data changes thinking something might work, into knowing something actually works.</strong> </p>

<h2 id=""theimportantmetrics"">The important metrics</h2>

<p>For every website, app or other projects, <strong>there is a basic set of metrics that will always be helpful</strong>:</p>

<ol>
<li>Main goal  </li>
<li>First interaction  </li>
<li>The journey &amp; micro Conversions  </li>
<li>Value</li>
</ol>

<p>In the world of startups and growth hacking, metrics are often referred to as <a href=""http://www.expectedbehavior.com/experiments/pirate_metrics/"">AARRR metrics or pirate metrics</a>:</p>

<ol>
<li>Acquisition  </li>
<li>Activation  </li>
<li>Retention  </li>
<li>Referral  </li>
<li>Revenue</li>
</ol>

<p>Though my metric list mainly focuses on acquisition, activation and revenue from the AARRR list, I think the biggest potential for improving any kind of  project lies in the ‘journey &amp; micro conversions’ step from my list. </p>

<p>Let’s elaborate on the four steps:</p>

<h3 id=""1maingoal"">1 Main goal</h3>

<p>For every project you start, <strong>there is always one important thing that users should do</strong>: they should achieve the main goal. For some projects, the main goal is easy to define:</p>

<ul>
<li>Webshop: users should buy stuff.</li>
<li>App: users should install the app.</li>
<li>Service provider: users should get a contract.</li>
</ul>

<p>For these main goals, it’s easy to capture the corresponding data. But there are also projects where your main goal might be less clear, e.g. a blog. At TMT we want people to read our content, but how do you measure read content? We’ve currently defined a 75% scroll and 60 seconds time on page as a full read. It's not perfect, but it allows us to capture the moment that people probably have read the article. </p>

<p>To clarify the way the metrics relate to one another, I'll add them to a simplified visualisation. The main goal is our first  dot:</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Jun/tmt_metrics_maingoals-1466586186204.PNG"" alt=""simplified main goal"" class=""full-img""></p>

<p><em>A simplified view of the main goal.</em></p>

<h3 id=""2firstinteraction"">2: First interaction</h3>

<p>The second important metric is the first interaction. This captures all the first interactions users have with your brand or product. Think about a user who sees one of your advertisements or a search result on Google. <strong>The first interaction shows you how many people were exposed your brand, product or startup</strong>. </p>

<p>We’ll add it as a yellow dot to our visualisation:</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Jun/tmt_metrics_firstinteraction-1466586248318.PNG"" alt=""simplified first interaction"" class=""full-img""></p>

<p><em>A simplified view of the main goal and the first interaction</em></p>

<h3 id=""3thecustomerjourneyandmicroconversions"">3: The customer journey and micro conversions</h3>

<p>Now we know how many people see your brand and achieve a goal, it’s time to look at the customer journey: <strong>what happens between those  the first interactions and the main goal?</strong></p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Jun/tmt_metrics_journey-1466586276745.PNG"" alt=""simplified customer journey"" class=""full-img""></p>

<p><em>A simplified view of the customer journey.</em></p>

<p>To analyse your website more effectively, you should <strong>identify key moments in the customer journey</strong> and set up metrics to capture data for these steps: these are called micro conversion. Micro conversions are not as important as the main goal, but they help you understand how far users get to achieving the main goal. If we take a standard webshop as an example, the micro conversions could be:</p>

<ul>
<li>Website visit </li>
<li>Category page (orientation)</li>
<li>Product page (interested orientation)</li>
<li>Basket page (buying indication)</li>
<li>Checkout (strong buying indication)</li>
<li>Purchase (main goal)</li>
</ul>

<p>Just as with the main goal, the micro conversions may be harder to set up for a website where the goal is less clear. If we look at this blog, we use the following steps to measure content performance:</p>

<ul>
<li>Website visit</li>
<li>Article open (interest)</li>
<li>Article scroll (triggered to start reading)</li>
<li>25%, 50% and 75% scroll (the content is good enough  to continue reading)</li>
<li>75% scroll + 60 seconds time on page (main goal)</li>
</ul>

<p>Measuring content performance isn't as obvious as measuring a shop's performance, but the idea is the same: identify the substeps in your customer journey and measure them. </p>

<p>Let’s add the micro conversions a blue dots to the visualisation:</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Jun/tmt_metrics_microconversions-1466586313466.PNG"" alt=""simplified customer journey with micro conversions"" class=""full-img""></p>

<p><em>A simplified view of the customer journey with micro conversions.</em></p>

<h3 id=""4value"">4: Value</h3>

<p>With the main goal, first interaction, and micro conversions set up, you’re all set to measure performance and see how new customers or users are coming in. Though this is valuable information for any startup or company, it’s important to <strong>track what happens after the main goal</strong>: </p>

<ul>
<li>How many users that have installed your app keep using it? </li>
<li>Do new customers of your store return for more products?</li>
<li>How many people come back to read more content on your blog?</li>
</ul>

<p>For value, we'll add a final purple dot to our visual: </p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Jun/tmt_metrics_value-1466586356381.PNG"" alt=""simplified view of the customer journey with user value"" class=""full-img""></p>

<p><em>A simplified view of the customer journey with value added at the end.
Tracking valuable users allows you to transform a large user base, into a healthy user base.</em></p>

<h2 id=""bringinthecalculations"">Bring in the calculations</h2>

<p>With the data collection setup in place, it's time to start adding calculations. I’ll discuss two calculations:</p>

<ol>
<li>Performance  </li>
<li>Cost Per Action</li>
</ol>

<p>These two calculations will help you <strong>grow your startup or company and increase marketing effectiveness</strong>. </p>

<h3 id=""1performance"">1: Performance</h3>

<p>It’s important to know how well your website is doing, to know where users drop off the most in the customer journey. Luckily we’ve set up micro conversions and with them, we’re able to <strong>calculate conversion rates</strong>. </p>

<p>The main conversion rate, how many users achieve the main goal divided by users that have  had a first interaction, tells you how well you’re doing on the highest level:</p>

<ul>
<li>How many users that see my app install it? </li>
<li>How big is the chance that someone will buy a product after seeing my web shop for the first time?</li>
<li>How likely are people to read a blog post after seeing it appear on Google?
Though these are interesting metrics, calculations really shine when you apply them to micro conversions. We can calculate a conversion rate for each step in the funnel.</li>
</ul>

<p>The main conversion rate can be calculated with the first interaction and the main goal:</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Jun/tmt_metrics_conversionrate-1466586401894.PNG"" alt=""simplified conversion rate"" class=""full-img""></p>

<p><em>The simplified view of a conversion rate.</em></p>

<p>For detailed insights, you can <strong>calculate conversion rates for every step in your funnel</strong>:</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Jun/tmt_metrics_conversionrates-1466586496756.PNG"" alt=""simplified conversion rates per micro conversion"" class=""full-img""></p>

<p><em>The simplified view of conversion rates per step in the customer journey.</em></p>

<p>It’s clear to see now what the biggest drop off point in your funnel is. In the example, the biggest drop off is highlighted in red, where only 10% of users convert to the next step. This is the best place to start optimising your website. If you double the conversion rate here, you’ll double the number of customers in the end.</p>

<h3 id=""2costperaction"">2: Cost per action</h3>

<p>The second important calculation is the cost per action. For all you’re marketing efforts you’ll spend money: the cost of running AdWords, Facebook Ads or any other kind of advertising. The calculations you can make are easy:</p>

<ul>
<li>Cost per main goal</li>
<li>Cost per first interaction</li>
<li>Cost per visit to your website</li>
<li>Cost per micro conversion</li>
<li>Cost per valuable user</li>
</ul>

<p>To clarify the value of this calculated metric, you should look at the following example:</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Jun/tmt_metrics_cpas-1466586669036.PNG"" alt=""simplified view of cpa's"" class=""full-img""></p>

<p><em>A simplified journey per marketing source.</em></p>

<p>By calculating the cost per action on several steps you can see that <strong>different marketing sources serve different purposes</strong>. Based on the example we can say that:</p>

<ul>
<li>Google has the lowest cost per visit</li>
<li>Google has the lowest cost per main goal</li>
<li>Email has the lowest cost per valuable users</li>
</ul>

<p>Though banners are performing quite bad according to CPA’s, it will probably impact the other sources. Users might not have clicked on the ad, but if users saw the banner they have had a visual impression that might trigger them to click on a Google Adwords ad later on. So keep in mind that the mix of ads is important:</p>

<ul>
<li>Prime users with banners.</li>
<li>Get new visits and users through Google.</li>
<li>Increase the number of valuable users through email.</li>
</ul>

<h2 id=""toolsfordatacollection"">Tools for data collection</h2>

<p>There are several tools that allow you to collect the data you want, and implementations can be as complicated as you want it to be. To give you an idea of the variations, here are three tools that differ from one another quite a bit, but are all powerful in their own way:</p>

<h3 id=""1googleanalytics"">1 Google Analytics</h3>

<p><a href=""https://www.google.com/analytics/#?modal_active=none"">Google Analytics</a> is a powerful and free to use data tracker. Implementations can be a simple as installing the standard pageview tag, to complex implementations with scroll reach tracking and enhanced e-commerce insights. </p>

<h3 id=""2heapanalytics"">2 Heap analytics</h3>

<p><a href=""https://heapanalytics.com/"">Heap Analytics</a> is a retroactive analytics solution. You install one simple tag on all pages, and you’re able to define and analyse every page view and click event retroactively.</p>

<h3 id=""3snowplow"">3 Snowplow</h3>

<p><a href=""http://snowplowanalytics.com/"">Snowplow</a> is an open source data logger and captures raw data. It doesn’t offer any kind of dashboarding and you should do all data modelling yourself.  It takes more time to set up than the other two examples, but  you’ll be way more flexible with the data you capture.</p>

<p>Keep in mind that these tools are just three examples, and you can use any data collection solution that you want. I personally suggest to use multiple trackers, as Google Analytics is powerfull for day-to-day analysis and offers aggregated reports, and Snowplow excels in advanced data analysis on a user level that is harder to do with Google Analytics. </p>

<h2 id=""startcapturing"">Start capturing</h2>

<p>In the end, it all comes down to knowing what’s important for your project, startup or website. If you know that, you can setup your data collection. How you capture the data is up to you. Just make sure that as soon as you start capturing data, you’ll <strong>make data part of every big decision you make</strong>. </p>
        ","During my talk at Growth Hacking symposium in March, I presented a guide for setting up data collection for any project, startup or website. In this post, I’ll discuss that specific guide and help you set up your basic metrics.
Why should you collect data?
Let's start with the most important question: why should you set up a data collection solution? Well, it helps you make better decisions for whatever you’re doing. Without it, you might have an idea of what’s going on, but you won’t know any actual numbers. With data collection, you know exactly what’s going on and are able to improve your product in the best possible way. Data changes thinking something might work, into knowing something actually works.
The important metrics
For every website, app or other projects, there is a basic set of metrics that will always be helpful:
Main goal
First interaction
The journey & micro Conversions
Value
In the world of startups and growth hacking, metrics are often referred to as AARRR metrics or pirate metrics:
Acquisition
Activation
Retention
Referral
Revenue
Though my metric list mainly focuses on acquisition, activation and revenue from the AARRR list, I think the biggest potential for improving any kind of project lies in the ‘journey & micro conversions’ step from my list.
Let’s elaborate on the four steps:
1 Main goal
For every project you start, there is always one important thing that users should do: they should achieve the main goal. For some projects, the main goal is easy to define:
Webshop: users should buy stuff.
App: users should install the app.
Service provider: users should get a contract.
For these main goals, it’s easy to capture the corresponding data. But there are also projects where your main goal might be less clear, e.g. a blog. At TMT we want people to read our content, but how do you measure read content? We’ve currently defined a 75% scroll and 60 seconds time on page as a full read. It's not perfect, but it allows us to capture the moment that people probably have read the article.
To clarify the way the metrics relate to one another, I'll add them to a simplified visualisation. The main goal is our first dot:
A simplified view of the main goal.
2: First interaction
The second important metric is the first interaction. This captures all the first interactions users have with your brand or product. Think about a user who sees one of your advertisements or a search result on Google. The first interaction shows you how many people were exposed your brand, product or startup.
We’ll add it as a yellow dot to our visualisation:
A simplified view of the main goal and the first interaction
3: The customer journey and micro conversions
Now we know how many people see your brand and achieve a goal, it’s time to look at the customer journey: what happens between those the first interactions and the main goal?
A simplified view of the customer journey.
To analyse your website more effectively, you should identify key moments in the customer journey and set up metrics to capture data for these steps: these are called micro conversion. Micro conversions are not as important as the main goal, but they help you understand how far users get to achieving the main goal. If we take a standard webshop as an example, the micro conversions could be:
Website visit
Category page (orientation)
Product page (interested orientation)
Basket page (buying indication)
Checkout (strong buying indication)
Purchase (main goal)
Just as with the main goal, the micro conversions may be harder to set up for a website where the goal is less clear. If we look at this blog, we use the following steps to measure content performance:
Website visit
Article open (interest)
Article scroll (triggered to start reading)
25%, 50% and 75% scroll (the content is good enough to continue reading)
75% scroll + 60 seconds time on page (main goal)
Measuring content performance isn't as obvious as measuring a shop's performance, but the idea is the same: identify the substeps in your customer journey and measure them.
Let’s add the micro conversions a blue dots to the visualisation:
A simplified view of the customer journey with micro conversions.
4: Value
With the main goal, first interaction, and micro conversions set up, you’re all set to measure performance and see how new customers or users are coming in. Though this is valuable information for any startup or company, it’s important to track what happens after the main goal:
How many users that have installed your app keep using it?
Do new customers of your store return for more products?
How many people come back to read more content on your blog?
For value, we'll add a final purple dot to our visual:
A simplified view of the customer journey with value added at the end. Tracking valuable users allows you to transform a large user base, into a healthy user base.
Bring in the calculations
With the data collection setup in place, it's time to start adding calculations. I’ll discuss two calculations:
Performance
Cost Per Action
These two calculations will help you grow your startup or company and increase marketing effectiveness.
1: Performance
It’s important to know how well your website is doing, to know where users drop off the most in the customer journey. Luckily we’ve set up micro conversions and with them, we’re able to calculate conversion rates.
The main conversion rate, how many users achieve the main goal divided by users that have had a first interaction, tells you how well you’re doing on the highest level:
How many users that see my app install it?
How big is the chance that someone will buy a product after seeing my web shop for the first time?
How likely are people to read a blog post after seeing it appear on Google? Though these are interesting metrics, calculations really shine when you apply them to micro conversions. We can calculate a conversion rate for each step in the funnel.
The main conversion rate can be calculated with the first interaction and the main goal:
The simplified view of a conversion rate.
For detailed insights, you can calculate conversion rates for every step in your funnel:
The simplified view of conversion rates per step in the customer journey.
It’s clear to see now what the biggest drop off point in your funnel is. In the example, the biggest drop off is highlighted in red, where only 10% of users convert to the next step. This is the best place to start optimising your website. If you double the conversion rate here, you’ll double the number of customers in the end.
2: Cost per action
The second important calculation is the cost per action. For all you’re marketing efforts you’ll spend money: the cost of running AdWords, Facebook Ads or any other kind of advertising. The calculations you can make are easy:
Cost per main goal
Cost per first interaction
Cost per visit to your website
Cost per micro conversion
Cost per valuable user
To clarify the value of this calculated metric, you should look at the following example:
A simplified journey per marketing source.
By calculating the cost per action on several steps you can see that different marketing sources serve different purposes. Based on the example we can say that:
Google has the lowest cost per visit
Google has the lowest cost per main goal
Email has the lowest cost per valuable users
Though banners are performing quite bad according to CPA’s, it will probably impact the other sources. Users might not have clicked on the ad, but if users saw the banner they have had a visual impression that might trigger them to click on a Google Adwords ad later on. So keep in mind that the mix of ads is important:
Prime users with banners.
Get new visits and users through Google.
Increase the number of valuable users through email.
Tools for data collection
There are several tools that allow you to collect the data you want, and implementations can be as complicated as you want it to be. To give you an idea of the variations, here are three tools that differ from one another quite a bit, but are all powerful in their own way:
1 Google Analytics
Google Analytics is a powerful and free to use data tracker. Implementations can be a simple as installing the standard pageview tag, to complex implementations with scroll reach tracking and enhanced e-commerce insights.
2 Heap analytics
Heap Analytics is a retroactive analytics solution. You install one simple tag on all pages, and you’re able to define and analyse every page view and click event retroactively.
3 Snowplow
Snowplow is an open source data logger and captures raw data. It doesn’t offer any kind of dashboarding and you should do all data modelling yourself. It takes more time to set up than the other two examples, but you’ll be way more flexible with the data you capture.
Keep in mind that these tools are just three examples, and you can use any data collection solution that you want. I personally suggest to use multiple trackers, as Google Analytics is powerfull for day-to-day analysis and offers aggregated reports, and Snowplow excels in advanced data analysis on a user level that is harder to do with Google Analytics.
Start capturing
In the end, it all comes down to knowing what’s important for your project, startup or website. If you know that, you can setup your data collection. How you capture the data is up to you. Just make sure that as soon as you start capturing data, you’ll make data part of every big decision you make.","[metrics, pirate metrics, data collection, aarrr, Data, Analytics]"
36,How one simple text link boosted revenue and won a WhichTestWon Award,/how-one-simple-text-link-boosted-revenue-and-won-a-whichtestwon-award/,"
            <p><strong>Last Thursday was the fifth annual WhichTestWon Email Awards Ceremony. Thanks to their annual award shows, but also recurring site-items such as 'Test Of The Week', 'Behind The Scenes' and 'The Live Event', American company WhichTestWon is an authority in the field of Conversion Rate Optimization. This year, the Netherlands won 2 out of 3 awards, which were coincidentally both won by a lottery. <em>Vriendenloterij</em> won in the category 'best e-mail blast', and <em>Staatsloterij</em> and <em>Blue Mango Interactive</em> won 'best call to action'. The latter case will be examined below.</strong></p>

<h3 id=""testdetailshypothesis"">Test details &amp; hypothesis</h3>

<p>Usually, Staatsloterij will send out an email a few days before an upcoming draw to people who have played before, as a reminder. Because of the timing, a few days before the draw, the focus of the email and the resulting landing page is on single draw tickets. However, subscriptions carry more certainty and are therefore advantageous for Staatsloterij. But pushing subscriptions in the email may have an adverse effect on single ticket sales, so we set up a test in which we added the subscription-link in a subtle way. This led to the hypothesis that adding a secondary CTA for subscriptions (called 'play automatically'), followed by the incentive of receiving a free ticket upon signup worth €15, will lead to more subscription sales without cannibalizing single draw ticket sales too much.</p>

<h4 id=""controlvariant"">Control &amp; variant:</h4>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Jun/pasted_image_at_2016_06_22_14_30-1466598762057.png"" alt="""" class=""full-img""></p>

<h3 id=""theresults"">The results</h3>

<p>The variation with the added text link led to an uplift of 335% in subscription orders. As a side effect, single draw orders dropped by 33%, but when calculated to a customer lifetime value, the uplift in subscriptions greatly outweighed the drop in single tickets, crowning the text link variation as the clear winner in this test. </p>

<h3 id=""learnings"">Learnings</h3>

<p>Small changes can lead to big results. Simply adding a text link to the CTA resulted in a  dramatic shift in sales of single draw tickets versus subscriptions. This is a reminder that successful tests don't aways have to be radical, the strength of this case is in it is simplicity.</p>

<h3 id=""verdict"">Verdict</h3>

<p>Winning two of three global awards, the Netherlands is a leader in the field of Email Conversion Optimization. So if you are not structurally A/B-testing within your email campaigns yet, my advice would be to start doing it. Not only is email a channel that converts relatively well, it's also a channel that can show you a lot of data in a short amount of time, where you'll often need to have more patience with other media. Further, it turns out the Butterfly Effect also counts for email optimization, because the fact that small changes can generate a big impact, was one of the key takeaways during the WhichTestWon Email Awards 2016. </p>

<p>More inspiring cases and the full award show can be found <a href=""http://www.whichtestwon.com"">here</a>.</p>
        ","Last Thursday was the fifth annual WhichTestWon Email Awards Ceremony. Thanks to their annual award shows, but also recurring site-items such as 'Test Of The Week', 'Behind The Scenes' and 'The Live Event', American company WhichTestWon is an authority in the field of Conversion Rate Optimization. This year, the Netherlands won 2 out of 3 awards, which were coincidentally both won by a lottery. Vriendenloterij won in the category 'best e-mail blast', and Staatsloterij and Blue Mango Interactive won 'best call to action'. The latter case will be examined below.
Test details & hypothesis
Usually, Staatsloterij will send out an email a few days before an upcoming draw to people who have played before, as a reminder. Because of the timing, a few days before the draw, the focus of the email and the resulting landing page is on single draw tickets. However, subscriptions carry more certainty and are therefore advantageous for Staatsloterij. But pushing subscriptions in the email may have an adverse effect on single ticket sales, so we set up a test in which we added the subscription-link in a subtle way. This led to the hypothesis that adding a secondary CTA for subscriptions (called 'play automatically'), followed by the incentive of receiving a free ticket upon signup worth €15, will lead to more subscription sales without cannibalizing single draw ticket sales too much.
Control & variant:
The results
The variation with the added text link led to an uplift of 335% in subscription orders. As a side effect, single draw orders dropped by 33%, but when calculated to a customer lifetime value, the uplift in subscriptions greatly outweighed the drop in single tickets, crowning the text link variation as the clear winner in this test.
Learnings
Small changes can lead to big results. Simply adding a text link to the CTA resulted in a dramatic shift in sales of single draw tickets versus subscriptions. This is a reminder that successful tests don't aways have to be radical, the strength of this case is in it is simplicity.
Verdict
Winning two of three global awards, the Netherlands is a leader in the field of Email Conversion Optimization. So if you are not structurally A/B-testing within your email campaigns yet, my advice would be to start doing it. Not only is email a channel that converts relatively well, it's also a channel that can show you a lot of data in a short amount of time, where you'll often need to have more patience with other media. Further, it turns out the Butterfly Effect also counts for email optimization, because the fact that small changes can generate a big impact, was one of the key takeaways during the WhichTestWon Email Awards 2016.
More inspiring cases and the full award show can be found here.","[Analytics, cro]"
37,Tech revolution vs Human Evolution - a call for smarter marketing,/tech-revolution-vs-human-evolution/,"
            <p>It’s been about a year now since the 50th birthday of Moore’s Law. For the ones who don’t know him, let me start with a short description of Moore and his Law.</p>

<p>Gordon Moore is one of the founders of Intel. In 1965 Moore wrote an article in Electronics Magazine in which he predicted that in the coming 10 years (meaning until 1975) the number of semiconductors in a circuit would double every year. This implied an unforeseen exponential growth of processor power within 10 years. Later in 1975 the expectation was adjusted to a 2 year but still, to quote Moore: ""It was far more accurate than I could have anticipated"".</p>

<p><img src=""https://ralfvanlieshout.files.wordpress.com/2015/05/moores-law-graph-gif.png"" alt="""" class=""full-img""></p>

<h3 id=""techrevolution"">Tech revolution</h3>

<p>Moore’s Law has eventually become an archetype to demonstrate the fact that technological development and change move faster in an accelerating way. This also means new innovations rise - and become obsolete - faster and faster.</p>

<p>Think of VHS video. The VHS has had a mainstream presence in households for almost 25 years before it was replaced by DVD. The DVD lasted for about 12 years before it was replaced by Blue Ray about 5 years ago which is now almost made obsolete after by online streaming services. Other examples like ""Record – Tape – CD – iPod - Spotify"" are numerous. It has become a ""law"" that storage capacity and bandwidth double every year and become twice as cheap every year. It has become a ""law"" that people share two times as much via social media every year compared with the year before.</p>

<p>Eventually, scientists expect computers to exceed the thinking power of the human brain somewhere between now 15 to 20 years. Speculations about the impact when we reach this point of <a href=""http://en.wikipedia.org/wiki/Technological_singularity"">singularity</a> are diverse - but my personal favourite viewpoint is written by Tim Urban of the <a href=""http://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html"">Wait But Why blog</a>.</p>

<h3 id=""humanevolution"">Human evolution</h3>

<p>Now I don’t want to go into detail about the singularity. I want to focus on the human brain capacity in relation to this tech revolution. </p>

<p>One of the particular implications of this tech revolution is the exponential growth of communication messages we get to see every day. </p>

<p>The amount of communication produced in the world since the beginning of mankind until 2003 is being broadcasted into the world via the internet every 5-10 minutes at this moment. 90% of all data in the world was generated over the last two year. On average, we receive 3500 commercial messages every day. That is 1 every 15 seconds. </p>

<p>And besides a quantity of messages, tech drives us to consume these messages closer and closer to our brain. From TV, to PC, to your smartphone, to virtual reality and eventually a simple lens should do the trick. </p>

<p><img src=""https://ralfvanlieshout.files.wordpress.com/2015/05/140117-google-contact_1.jpg"" alt=""""></p>

<p>And we see the effects in front of our eyes when we look at our kids, right? If you have friends with kids you almost certainly have heard them say: “My kid is only 1,5 years old but if you give her a tablet, she knows how to find YouTube and play her favourite videos... really cool...”</p>

<p><img src=""https://ralfvanlieshout.files.wordpress.com/2015/05/baby-and-ipad.jpg"" alt=""""></p>

<p>Don't get me wrong, I'm a tech aficionado and I love gadgets. I’m not the one to morally judge other people’s on the way they raise their kids because I’m probably a bad example for my kids too. But I wonder what this development might do to our mental and physical wellbeing. What are the real short-term and long-term consequences? </p>

<p>In <a href=""http://www.npo.nl/dwdd-university-presenteert-het-brein-door-erik-scherder/07-05-2015/VARA_101373841"">a DWDD University broadcast about the brain (Dutch only)</a>, the famous Dutch Professor Erik Scherder explained really simple that most new technology makes us less physically active. Less physical activity leads to a lowering of the production of neurotoxins which are the nutrition for the neurotransmitters in your brain. Active neurotoxins also generate stronger contact points around your brain’s neurons that in effect grow your spatial awareness. That’s why - in the long term low physical activity increases the chance of getting a brain stroke.</p>

<p>Besides this, research has shown that stress caused by information overload costs US society 900 billion dollars a year. 42% of stress related illness is connected to this Infobesity - the popular term for Information Fatigue Syndrom (IFS). People suffering from IFS feel depressed, afraid, the compulsion to stay connected, have a lower immune response and sometimes even become hostile towards their environment.</p>

<p>And that’s hardly a surprise because if we match Moore’s Law curve with the development curve of the human brain you see a similar pattern with the slight difference of a few million years. </p>

<p><img src=""https://ralfvanlieshout.files.wordpress.com/2015/05/6597574_orig.jpg"" alt=""""></p>

<h3 id=""thedistractioneconomy"">The distraction economy</h3>

<p>So, my educated guess is that IFS and brain-numbing device usage will not make us happy and healthy people. </p>

<p>The argument that we have the ability to adapt does not make real sense. True; people have an incredible ability to adjust to new situations. We weren’t designed to drive a car over a freeway at 120 km/h or to fly a plane but we can. But the only reason we can do crazy stuff like that is because we can put our attention to it. Focused attention. And when do things run out of hand and become dangerous? Exactly! When we get distracted. </p>

<p>And that’s the key to the Tech Revolution vs. Human Evolution discussion: We constantly get distracted by technology. The never-ending call of pings, rings and vibrations. We are continuously seduced by tech stimuli to step out of the ‘real’ world into a rabbit hole of digital communication. We are living in a constant mode of anxiety that – for a few years back – was only experienced by air traffic controllers. And they are allowed to do their job only for 4 hours in a row for safety reasons. </p>

<p>The underlying commercial reason behind this is the fact that there is a huge value in our attention. If we talk to each other in a physical world – no company makes any money. If they get us to talk to each other on Facebook – they do. Simple as that.</p>

<p>So the real problem here is that technology is currently undermining our ability to consciously pay attention. Either to a message, to each other, to ourselves and to our environment. </p>

<p>This means we are not talking about an attention-economy anymore, but about a distraction-economy where we – marketers and media - overflow people with messages and information to the point that they can’t handle it anymore. They become physically and mentally ill - if they haven’t blocked us already.</p>

<p>So are we as digital marketers responsible? Are we the Walter Whites of digital addiction? Let’s call it a draw. Consumers and marketers are both responsible. And we also have a common interest. The trick of a good dealer is to give his clients as much as they can handle to feel as happy as they can be, without OD-ing them out of business (this is a metaphor of course ;-)). </p>

<p>If we consider the rise of ad-blockers and the amount of ad spending that is wasted on bad targeting anyone can do the math and come to the conclusion that we need less quantity- and more quality communication. The moment we call for attention it should add value for both the advertiser as well as the consumer. </p>

<p>The best any brand could give it’s consumers these days is quality time. The best a brand can get back from consumers is focused attention.</p>

<p>Let’s make it work!</p>
        ","It’s been about a year now since the 50th birthday of Moore’s Law. For the ones who don’t know him, let me start with a short description of Moore and his Law.
Gordon Moore is one of the founders of Intel. In 1965 Moore wrote an article in Electronics Magazine in which he predicted that in the coming 10 years (meaning until 1975) the number of semiconductors in a circuit would double every year. This implied an unforeseen exponential growth of processor power within 10 years. Later in 1975 the expectation was adjusted to a 2 year but still, to quote Moore: ""It was far more accurate than I could have anticipated"".
Tech revolution
Moore’s Law has eventually become an archetype to demonstrate the fact that technological development and change move faster in an accelerating way. This also means new innovations rise - and become obsolete - faster and faster.
Think of VHS video. The VHS has had a mainstream presence in households for almost 25 years before it was replaced by DVD. The DVD lasted for about 12 years before it was replaced by Blue Ray about 5 years ago which is now almost made obsolete after by online streaming services. Other examples like ""Record – Tape – CD – iPod - Spotify"" are numerous. It has become a ""law"" that storage capacity and bandwidth double every year and become twice as cheap every year. It has become a ""law"" that people share two times as much via social media every year compared with the year before.
Eventually, scientists expect computers to exceed the thinking power of the human brain somewhere between now 15 to 20 years. Speculations about the impact when we reach this point of singularity are diverse - but my personal favourite viewpoint is written by Tim Urban of the Wait But Why blog.
Human evolution
Now I don’t want to go into detail about the singularity. I want to focus on the human brain capacity in relation to this tech revolution.
One of the particular implications of this tech revolution is the exponential growth of communication messages we get to see every day.
The amount of communication produced in the world since the beginning of mankind until 2003 is being broadcasted into the world via the internet every 5-10 minutes at this moment. 90% of all data in the world was generated over the last two year. On average, we receive 3500 commercial messages every day. That is 1 every 15 seconds.
And besides a quantity of messages, tech drives us to consume these messages closer and closer to our brain. From TV, to PC, to your smartphone, to virtual reality and eventually a simple lens should do the trick.
And we see the effects in front of our eyes when we look at our kids, right? If you have friends with kids you almost certainly have heard them say: “My kid is only 1,5 years old but if you give her a tablet, she knows how to find YouTube and play her favourite videos... really cool...”
Don't get me wrong, I'm a tech aficionado and I love gadgets. I’m not the one to morally judge other people’s on the way they raise their kids because I’m probably a bad example for my kids too. But I wonder what this development might do to our mental and physical wellbeing. What are the real short-term and long-term consequences?
In a DWDD University broadcast about the brain (Dutch only), the famous Dutch Professor Erik Scherder explained really simple that most new technology makes us less physically active. Less physical activity leads to a lowering of the production of neurotoxins which are the nutrition for the neurotransmitters in your brain. Active neurotoxins also generate stronger contact points around your brain’s neurons that in effect grow your spatial awareness. That’s why - in the long term low physical activity increases the chance of getting a brain stroke.
Besides this, research has shown that stress caused by information overload costs US society 900 billion dollars a year. 42% of stress related illness is connected to this Infobesity - the popular term for Information Fatigue Syndrom (IFS). People suffering from IFS feel depressed, afraid, the compulsion to stay connected, have a lower immune response and sometimes even become hostile towards their environment.
And that’s hardly a surprise because if we match Moore’s Law curve with the development curve of the human brain you see a similar pattern with the slight difference of a few million years.
The distraction economy
So, my educated guess is that IFS and brain-numbing device usage will not make us happy and healthy people.
The argument that we have the ability to adapt does not make real sense. True; people have an incredible ability to adjust to new situations. We weren’t designed to drive a car over a freeway at 120 km/h or to fly a plane but we can. But the only reason we can do crazy stuff like that is because we can put our attention to it. Focused attention. And when do things run out of hand and become dangerous? Exactly! When we get distracted.
And that’s the key to the Tech Revolution vs. Human Evolution discussion: We constantly get distracted by technology. The never-ending call of pings, rings and vibrations. We are continuously seduced by tech stimuli to step out of the ‘real’ world into a rabbit hole of digital communication. We are living in a constant mode of anxiety that – for a few years back – was only experienced by air traffic controllers. And they are allowed to do their job only for 4 hours in a row for safety reasons.
The underlying commercial reason behind this is the fact that there is a huge value in our attention. If we talk to each other in a physical world – no company makes any money. If they get us to talk to each other on Facebook – they do. Simple as that.
So the real problem here is that technology is currently undermining our ability to consciously pay attention. Either to a message, to each other, to ourselves and to our environment.
This means we are not talking about an attention-economy anymore, but about a distraction-economy where we – marketers and media - overflow people with messages and information to the point that they can’t handle it anymore. They become physically and mentally ill - if they haven’t blocked us already.
So are we as digital marketers responsible? Are we the Walter Whites of digital addiction? Let’s call it a draw. Consumers and marketers are both responsible. And we also have a common interest. The trick of a good dealer is to give his clients as much as they can handle to feel as happy as they can be, without OD-ing them out of business (this is a metaphor of course ;-)).
If we consider the rise of ad-blockers and the amount of ad spending that is wasted on bad targeting anyone can do the math and come to the conclusion that we need less quantity- and more quality communication. The moment we call for attention it should add value for both the advertiser as well as the consumer.
The best any brand could give it’s consumers these days is quality time. The best a brand can get back from consumers is focused attention.
Let’s make it work!",[marketing]
38,What to look for when selecting a customer identifier,/what-to-look-for-when-selecting-a-customer-identifier/,"
            <p>Recognising a user across channels can be a challenge. But if you're able to do so, you can answer some interesting questions:</p>

<ul>
<li>What part of my offline sales can I attribute to online orientation behaviour?  </li>
<li>How many online orientations result in an offline purchase?</li>
</ul>

<p>For cross-channel reports, it’s important to collect the right data. If you do so you can answer the two questions with ease. The most important question here is: <strong>how do you identify users across channels?</strong></p>

<h2 id=""thecrosschannelchallenge"">The Cross Channel challenge</h2>

<p>We've noticed the percentage of users that orient online and purchase offline has been on a steady rise over the past few years for our clients. Right now, companies often collect online data and offline data, but  they aren’t directly connecting the two: </p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Jun/offline_orientation_journey-1465213145145.PNG"" alt=""cross-channel journey"" class=""full-img"">
<em>A simplified cross-channel journey.</em></p>

<p>In this simplified journey, we have an online visit that results in an orientation event. Afer that, we have an offline purchase journey that results in an offline sale. Without the connection, it's hard to tell how online website behaviour attributes to offline sales. But if you do connect them, <strong>you’ll be able to directly attribute offline sales to online campaign sources</strong>. Wouldn’t that be great?</p>

<h2 id=""solvingthechallengewithacustomeridentifier"">Solving the challenge with a customer identifier</h2>

<p>Luckily, there’s a solution to our problems: the customer identifier. This is a value that allows you to recognise a user both online and offline. Potential values for identifiers are:</p>

<ul>
<li>Customer ID</li>
<li>Email address</li>
<li>Home address</li>
<li>Postcode</li>
<li>Phone number</li>
</ul>

<p>By adding a customer identifier, our view of the cross-channel behaviour will be improved:</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Jun/offline_orientation_journey_improved-1465213152258.PNG"" alt=""improved cross-channel journey"" class=""full-img"">
<em>The improved cross-channel journey.</em></p>

<p>In the example journey, the customer identifier allows us to <strong>match online behaviour to offline purchases</strong>. </p>

<h2 id=""selectingtherightcustomeridentifierforyourbusiness"">Selecting the right customer identifier for your business</h2>

<p>If you’re looking for a customer identifier for your website or client, there are two main categories to choose from:</p>

<h3 id=""1broadidentifiers"">1 Broad identifiers</h3>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Jun/tmt_broad_identifier-1465393148095.png"" alt=""broad identifier image"" class=""full-img""></p>

<p>These values are suited for <strong>products that have a long buying cycle</strong> and aren’t bought that often by users. Think about bigger expenses, e.g. a kitchen, a bed or a fridge. <a href=""https://www.themarketingtechnologist.co/multichannel-problem-partly-solved-with-postal-codes/"">Values like a postcode work well here</a>, as there’s only a low chance that two people from the same street or neighbourhood will buy a product within a small time range. </p>

<h3 id=""2narrowidentifiers"">2 Narrow identifiers</h3>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Jun/tmt_narrow_idientifier-1465393169143.png"" alt=""narrow identifier image"" class=""full-img""></p>

<p>These values are suited for <strong>products that have a short buying cycle</strong> and are often bought by users. Some example products are clothing or groceries. With these products, there’s an increased chance that two people from the same area will buy the product within a short time span. Here, values like email address, phone number or full address come into play.</p>

<p>Keep in mind that the narrower the identifier is, the higher the chance is that it contains Personal Identifiable Information (PII). This raises privacy questions and not all web tracking tools allow you to collect PII, e.g. Google Analytics. </p>

<h2 id=""theimportanceofthecustomeridentifiercollectionmoment"">The importance of the customer identifier collection moment</h2>

<p>A good place to look for your identifier is your <strong>offline sales data</strong>. It is often harder to change the data collected with these sales than to change the data collected on your website. So if you find a good identifier in your offline data, I suggest using that value for your online data collection as well. </p>

<p>From the moment that you know what your customer identifier will be, it’s time to <strong>start collecting that  value online</strong>.  The moment that you collect it is of great importance. Collecting the value early on in the orientation process will greatly increase the amount of data you can use.  So <strong>try to collect your identifier as early as possible</strong> on your website to get the biggest pool of users for your cross-channel analysis.</p>

<h2 id=""analysingthedata"">Analysing the data</h2>

<p>When you start analysing the data there are some important things to keep in mind:</p>

<ul>
<li>When matching online behaviour to offline sales, make sure that the online orientation happened before the offline purchase.</li>
<li>Analyse the time between the orientations and the purchases and select a maximum time gap. If the time gap between the offline purchase and online orientation exceeds this threshold, don’t link them. </li>
</ul>

<p>If you take this into account, you can start generating interesting metrics:</p>

<ul>
<li><strong>Percentage of matched offline revenue</strong>: matched offline revenue / total offline revenue.</li>
<li><strong>Percentage of online orientations that resulted in an offline sale</strong>: matched online orientations / total online orientations.</li>
<li><strong>Average orientation value</strong>: matched offline revenue / matched  online orientations.</li>
</ul>

<p>These three metrics will give you an understanding of how your online website visits attribute to your offline sales. </p>

<h2 id=""startingacustomeridentifierproject"">Starting a customer identifier project</h2>

<p>At Greenhouse Group, we analyse the impact of online orientation on offline sales in two steps:</p>

<ul>
<li><strong>Manual analysis</strong> to see if there is an effect. If we see an effect, we continue to step 2.</li>
<li><a href=""https://www.themarketingtechnologist.co/connecting-offline-sales-to-online-campaign-sources-with-google-analytics/""><strong>Automated analysis</strong></a> to get the insights on a daily basis.</li>
</ul>

<p>The first step helps us and our client <strong>understand the cross-channel impact</strong>: is there an effect, and if so, how big is the effect? In this step, we manually connect the identifiers of the online data and offline data. The second step automates the analysis and allows us to <strong>use cross-channel data in our day-to-day performance analysis</strong>.  </p>

<h2 id=""conclusion"">Conclusion</h2>

<p>The cross-channel challenge can be solved with customer identifiers. All you have to do is select the right identifier for your business and start collecting it both online and offline. <strong>The data will help you understand how your online website attributes to offline purchases</strong>. The question is: what identifier will you use?</p>
        ","Recognising a user across channels can be a challenge. But if you're able to do so, you can answer some interesting questions:
What part of my offline sales can I attribute to online orientation behaviour?
How many online orientations result in an offline purchase?
For cross-channel reports, it’s important to collect the right data. If you do so you can answer the two questions with ease. The most important question here is: how do you identify users across channels?
The Cross Channel challenge
We've noticed the percentage of users that orient online and purchase offline has been on a steady rise over the past few years for our clients. Right now, companies often collect online data and offline data, but they aren’t directly connecting the two:
A simplified cross-channel journey.
In this simplified journey, we have an online visit that results in an orientation event. Afer that, we have an offline purchase journey that results in an offline sale. Without the connection, it's hard to tell how online website behaviour attributes to offline sales. But if you do connect them, you’ll be able to directly attribute offline sales to online campaign sources. Wouldn’t that be great?
Solving the challenge with a customer identifier
Luckily, there’s a solution to our problems: the customer identifier. This is a value that allows you to recognise a user both online and offline. Potential values for identifiers are:
Customer ID
Email address
Home address
Postcode
Phone number
By adding a customer identifier, our view of the cross-channel behaviour will be improved:
The improved cross-channel journey.
In the example journey, the customer identifier allows us to match online behaviour to offline purchases.
Selecting the right customer identifier for your business
If you’re looking for a customer identifier for your website or client, there are two main categories to choose from:
1 Broad identifiers
These values are suited for products that have a long buying cycle and aren’t bought that often by users. Think about bigger expenses, e.g. a kitchen, a bed or a fridge. Values like a postcode work well here, as there’s only a low chance that two people from the same street or neighbourhood will buy a product within a small time range.
2 Narrow identifiers
These values are suited for products that have a short buying cycle and are often bought by users. Some example products are clothing or groceries. With these products, there’s an increased chance that two people from the same area will buy the product within a short time span. Here, values like email address, phone number or full address come into play.
Keep in mind that the narrower the identifier is, the higher the chance is that it contains Personal Identifiable Information (PII). This raises privacy questions and not all web tracking tools allow you to collect PII, e.g. Google Analytics.
The importance of the customer identifier collection moment
A good place to look for your identifier is your offline sales data. It is often harder to change the data collected with these sales than to change the data collected on your website. So if you find a good identifier in your offline data, I suggest using that value for your online data collection as well.
From the moment that you know what your customer identifier will be, it’s time to start collecting that value online. The moment that you collect it is of great importance. Collecting the value early on in the orientation process will greatly increase the amount of data you can use. So try to collect your identifier as early as possible on your website to get the biggest pool of users for your cross-channel analysis.
Analysing the data
When you start analysing the data there are some important things to keep in mind:
When matching online behaviour to offline sales, make sure that the online orientation happened before the offline purchase.
Analyse the time between the orientations and the purchases and select a maximum time gap. If the time gap between the offline purchase and online orientation exceeds this threshold, don’t link them.
If you take this into account, you can start generating interesting metrics:
Percentage of matched offline revenue: matched offline revenue / total offline revenue.
Percentage of online orientations that resulted in an offline sale: matched online orientations / total online orientations.
Average orientation value: matched offline revenue / matched online orientations.
These three metrics will give you an understanding of how your online website visits attribute to your offline sales.
Starting a customer identifier project
At Greenhouse Group, we analyse the impact of online orientation on offline sales in two steps:
Manual analysis to see if there is an effect. If we see an effect, we continue to step 2.
Automated analysis to get the insights on a daily basis.
The first step helps us and our client understand the cross-channel impact: is there an effect, and if so, how big is the effect? In this step, we manually connect the identifiers of the online data and offline data. The second step automates the analysis and allows us to use cross-channel data in our day-to-day performance analysis.
Conclusion
The cross-channel challenge can be solved with customer identifiers. All you have to do is select the right identifier for your business and start collecting it both online and offline. The data will help you understand how your online website attributes to offline purchases. The question is: what identifier will you use?","[Analytics, Data, cross-channel, cross-device, customer identifier, postcode, user id, facebook id]"
39,Passing data to and from a nested component in Angular 2,/building-nested-components-in-angular-2/,"
            <p>Angular decided to drop the '@', '&amp;' and '=' in version 2.0. If you don't know what they do: good for you! To me, the @&amp;= concept was among Angular 1's worst choices. Luckily, in Angular 2 the communication between components is a lot more explicit and easier to understand. In this post I'd like to show you how to pass data to and from a nested component in Angular 2.</p>

<p><strong>Note:</strong> The examples in this post use Angular 2 Alpha. There were some changes in the final release, so dome code might work a bit different now. I'll try to keep it up to date.</p>

<h2 id=""buildinganestedcomponent"">Building a Nested Component</h2>

<p>We can simply add a nested component by using a component as a directive within another component. </p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/May/nested-1463921912326.png"" alt=""""></p>

<p>Let's first create a basic component that will be nested in another component later on. This components has a <code>title</code> property that we use in its template:</p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""lit"">@Component</span><span class=""pun"">({</span><span class=""pln"">
  selector</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""str"">'child-selector'</span><span class=""pun"">,</span><span class=""pln"">
  </span><span class=""kwd"">template</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""str"">'child.component.html'</span><span class=""pln"">
</span><span class=""pun"">})</span><span class=""pln"">
</span><span class=""kwd"">export</span><span class=""pln""> </span><span class=""kwd"">class</span><span class=""pln""> </span><span class=""typ"">ChildComponent</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">  
  title </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""str"">'I\'m a nested component'</span><span class=""pun"">;</span><span class=""pln"">
</span><span class=""pun"">}</span></code></pre>

<p>The <code>child.component.html</code> is just an HTML file that shows the value of the <code>title</code> property:</p>

<pre><code class=""language-prettyprint lang-html prettyprinted""><span class=""com"">/* child.conponent.html */</span><span class=""pln"">
</span><span class=""str"">&lt;h2&gt;</span><span class=""pun"">{{</span><span class=""pln"">title</span><span class=""pun"">}}&lt;/</span><span class=""pln"">h2</span><span class=""pun"">&gt;</span><span class=""pln"">  </span></code></pre>

<p>Now we want to create a container component. It looks almost identical to the nested component, except we have to specify that we want to use the nested component. We do that by adding the <code>ChildComponent</code> to the <code>directives</code> property of the Component decorator. Without doing this, the ChildComponent can not be used.</p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""lit"">@Component</span><span class=""pun"">({</span><span class=""pln"">
  selector</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""str"">'parent-selector'</span><span class=""pun"">,</span><span class=""pln"">
  </span><span class=""kwd"">template</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""str"">'parent.component.html'</span><span class=""pun"">,</span><span class=""pln"">
  directives</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""pun"">[</span><span class=""typ"">ChildComponent</span><span class=""pun"">]</span><span class=""pln"">
</span><span class=""pun"">})</span><span class=""pln"">
</span><span class=""kwd"">export</span><span class=""pln""> </span><span class=""kwd"">class</span><span class=""pln""> </span><span class=""typ"">ParentComponent</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln""> </span><span class=""pun"">}</span><span class=""pln"">  </span></code></pre>

<p>The container component uses the nested component by specifying its directive in the template:</p>

<pre><code class=""language-prettyprint lang-html prettyprinted""><span class=""com"">/* parent.component.html */</span><span class=""pln"">
</span><span class=""str"">&lt;div&gt;</span><span class=""pln"">  
  </span><span class=""str"">&lt;h1&gt;</span><span class=""pln"">I</span><span class=""str"">'m a container component&lt;/h1&gt;
  &lt;child-selector&gt;&lt;/child-selector&gt;
&lt;/div&gt;  </span></code></pre>

<p>Now, when we run our code we see something like this: </p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/May/1-1463922985949.png"" alt="""" class=""full-img""></p>

<p>Nothing too fancy yet. Let's pass some data to the nested component!</p>

<h2 id=""passingdatatoanestedcomponent"">Passing data to a nested component</h2>

<p>If a nested component wants to receive input from its container, it must expose a property to that container. The nested component exposes a property it can use to receive input from its container using the <a href=""https://angular.io/docs/ts/latest/guide/attribute-directives.html"">@Input decorator</a>. </p>

<p>We use the Input decorator to decorate any property in the nested component class. This works with every property type, including objects. In our example, we'll pass a string value to the nested component's <code>title</code> property, so we'll mark that property with the @Input decorator:</p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""lit"">@Component</span><span class=""pun"">({</span><span class=""pln"">
  selector</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""str"">'child-selector'</span><span class=""pun"">,</span><span class=""pln"">
  </span><span class=""kwd"">template</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""str"">'child.component.html'</span><span class=""pln"">
</span><span class=""pun"">})</span><span class=""pln"">
</span><span class=""kwd"">export</span><span class=""pln""> </span><span class=""kwd"">class</span><span class=""pln""> </span><span class=""typ"">ChildComponent</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">  
  </span><span class=""lit"">@Input</span><span class=""pun"">()</span><span class=""pln""> title</span><span class=""pun"">:</span><span class=""kwd"">string</span><span class=""pun"">;</span><span class=""pln"">
</span><span class=""pun"">}</span></code></pre>

<p>Now our nested component is ready to receive input from its parent component. </p>

<p>In the container component, we need to define the property we want to pass to the nested component. We call it <code>childTitle</code>:</p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""lit"">@Component</span><span class=""pun"">({</span><span class=""pln"">
  selector</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""str"">'parent-selector'</span><span class=""pun"">,</span><span class=""pln"">
  </span><span class=""kwd"">template</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""str"">'parent.component.html'</span><span class=""pun"">,</span><span class=""pln"">
  directives</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""pun"">[</span><span class=""typ"">ChildComponent</span><span class=""pun"">]</span><span class=""pln"">
</span><span class=""pun"">})</span><span class=""pln"">
</span><span class=""kwd"">export</span><span class=""pln""> </span><span class=""kwd"">class</span><span class=""pln""> </span><span class=""typ"">ParentComponent</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">  
  childTitle</span><span class=""pun"">:</span><span class=""kwd"">string</span><span class=""pln""> </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""str"">'This text is passed to child'</span><span class=""pun"">;</span><span class=""pln"">
</span><span class=""pun"">}</span></code></pre>

<p>Now the container component should pass the value of <code>childTitle</code> to the nested component by settings this property with <a href=""https://www.themarketingtechnologist.co/introduction-to-data-binding-in-angular-2-versus-angular-1/"">property binding</a>. When using property binding, we enclose the binding target in square brackets. The binding target refers to the <code>title</code> property of the nested component. We set the binding source to the data that the container wants to pass to the nested component, which is <code>childTitle</code>.</p>

<pre><code class=""language-prettyprint lang-html prettyprinted""><span class=""com"">/* parent.component.html */</span><span class=""pln"">
</span><span class=""str"">&lt;div&gt;</span><span class=""pln"">  
  </span><span class=""str"">&lt;h1&gt;</span><span class=""pln"">I</span><span class=""str"">'m a container component&lt;/h1&gt;
  &lt;child-selector [title]='</span><span class=""pln"">childTitle</span><span class=""str"">'&gt;&lt;/child-selector&gt;
&lt;/div&gt;  </span></code></pre>

<p>The only time we can specify a nested component's property as a property binding target, is when that property is decorated with the @Input decorator, like we did earlier.</p>

<p>When we run our code, we should see the passed value in the nested component's H2 element:</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/May/2-1463923054994.png"" alt="""" class=""full-img""></p>

<p>In this example we only exposed one input property, but of course you can expose multiple input properties as needed.</p>

<h2 id=""passingdatafromanestedcomponent"">Passing data from a Nested Component</h2>

<p>In the previous example I showed how the container can pass data to the nested component by binding to a nested component's property, that is declared with the @Input decorator.</p>

<p>If the nested component wants to send information back to its container, it can raise an event. The nested component exposes an event it can use to pass output to its container using the @Output decorator.</p>

<p>Like with the @Input decorator, we can use the @Output decorator to decorate any property of the nested components class. However, the property type must be an event. The only way a nested component can pass data back to its container, is with an event. The data to pass is called the event payload. In Angular, an event is defined with an <a href=""https://angular.io/docs/js/latest/api/core/EventEmitter-class.html"">EventEmitter</a> object. <br>
So let's start by creating a new instance of an Event Emitter and decorate the property with the @Output decorator.</p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""lit"">@output</span><span class=""pun"">()</span><span class=""pln""> notify</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""typ"">EventEmitter</span><span class=""str"">&lt;string&gt;</span><span class=""pln""> </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""kwd"">new</span><span class=""pln""> </span><span class=""typ"">EventEmitter</span><span class=""str"">&lt;string&gt;</span><span class=""pun"">();</span></code></pre>

<p>If you're not familiar with generics, this syntax may look a bit odd to you. <a href=""https://www.typescriptlang.org/docs/handbook/generics.html"">Generics</a> allow us to identify a specific type that the object instance will work with.</p>

<p>The generic argument, <code>string</code>, identifies of the event payload. So now we can only pass string values to the container. If we would want to pass a integer for example, we'd write something like this:</p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""lit"">@output</span><span class=""pun"">()</span><span class=""pln""> notify</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""typ"">EventEmitter</span><span class=""str"">&lt;number&gt;</span><span class=""pln""> </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""kwd"">new</span><span class=""pln""> </span><span class=""typ"">EventEmitter</span><span class=""str"">&lt;number&gt;</span><span class=""pun"">();</span></code></pre>

<p>Although not recommended, you can let it accept any type. You can use TypeScript's <code>any</code> type. We'll stick with a string for now. Please note that JavaScript does not support generics by itself; it's a TypeScript feature. I've personally only played with generics in C# a bit, and I think the concept is fairly new to the Javascript community in general.</p>

<p>Now we've got an event emitter in place, let's use the <code>notify</code> event property and call its <code>emit</code> method to raise the <code>notify</code> event and pass in our payload as an argument.</p>

<pre><code>this.notify.emit('payload');  
</code></pre>

<p>We'll need a user interaction that will raise the event, so I'll add a link that will do so. </p>

<pre><code class=""language-prettyprint lang-html prettyprinted""><span class=""com"">/* child.component.html */</span><span class=""pln"">
</span><span class=""str"">&lt;h2&gt;</span><span class=""typ"">Hi</span><span class=""pun"">,</span><span class=""pln""> I</span><span class=""str"">'m a nested component&lt;/h2&gt;  
&lt;span (click)='</span><span class=""pln"">onClick</span><span class=""pun"">()</span><span class=""str"">'&gt;Click me please!&lt;/span&gt;  </span></code></pre>

<p>Now we'll add the click event handler to the component, and raise the <code>notify</code> event:</p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""lit"">@Component</span><span class=""pun"">({</span><span class=""pln"">
  selector</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""str"">'child-selector'</span><span class=""pun"">,</span><span class=""pln"">
  </span><span class=""kwd"">template</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""str"">'child.component.html'</span><span class=""pln"">
</span><span class=""pun"">})</span><span class=""pln"">
</span><span class=""kwd"">export</span><span class=""pln""> </span><span class=""kwd"">class</span><span class=""pln""> </span><span class=""typ"">ChildComponent</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">  
  </span><span class=""lit"">@output</span><span class=""pun"">()</span><span class=""pln""> notify</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""typ"">EventEmitter</span><span class=""str"">&lt;string&gt;</span><span class=""pln""> </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""kwd"">new</span><span class=""pln""> </span><span class=""typ"">EventEmitter</span><span class=""str"">&lt;string&gt;</span><span class=""pun"">();</span><span class=""pln"">

  onClick</span><span class=""pun"">()</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">
    </span><span class=""kwd"">this</span><span class=""pun"">.</span><span class=""pln"">notify</span><span class=""pun"">.</span><span class=""pln"">emit</span><span class=""pun"">(</span><span class=""str"">'Click from nested component'</span><span class=""pun"">);</span><span class=""pln"">
  </span><span class=""pun"">}</span><span class=""pln"">
</span><span class=""pun"">}</span></code></pre>

<p>So every time a user clicks on the 'Click me please' link, the nested component will dispatch an event to its parent component.</p>

<p>The parent component receives that event and its payload. We use event binding to bind to this <code>notify</code> event and call a method.</p>

<pre><code class=""language-prettyprint lang-html prettyprinted""><span class=""com"">/* parent.conponent.html */</span><span class=""pln"">
</span><span class=""str"">&lt;div&gt;</span><span class=""pln"">  
  </span><span class=""str"">&lt;h1&gt;</span><span class=""pln"">I</span><span class=""str"">'m a container component&lt;/h1&gt;
  &lt;child-selector (notify)='</span><span class=""pln"">onNotify</span><span class=""pun"">(</span><span class=""pln"">$event</span><span class=""pun"">)&gt;&lt;/</span><span class=""pln"">child</span><span class=""pun"">-</span><span class=""pln"">selector</span><span class=""pun"">&gt;</span><span class=""pln"">
</span><span class=""pun"">&lt;/</span><span class=""pln"">div</span><span class=""pun"">&gt;</span><span class=""pln"">  </span></code></pre>

<p>We have to pass the <code>$event</code> to the handler because that variable holds the event payload.</p>

<p>The only time we can can specify a nested component's property as an event binding target is when that property is decorated with the @Output decorator.</p>

<p>Our final step is to provide the <code>onNotify</code> method to execute when the notify event occurs. Since the event payload is a string, the <code>onNotify</code> function takes in a string. We can perform any desired action in our handler, but for now let's just alert the payload.</p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""lit"">@Component</span><span class=""pun"">({</span><span class=""pln"">
  selector</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""str"">'parent-selector'</span><span class=""pun"">,</span><span class=""pln"">
  </span><span class=""kwd"">template</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""str"">'parent.component.html'</span><span class=""pun"">,</span><span class=""pln"">
  directives</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""pun"">[</span><span class=""typ"">ChildComponent</span><span class=""pun"">]</span><span class=""pln"">
</span><span class=""pun"">})</span><span class=""pln"">
</span><span class=""kwd"">export</span><span class=""pln""> </span><span class=""kwd"">class</span><span class=""pln""> </span><span class=""typ"">ParentComponent</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">  
  onNotify</span><span class=""pun"">(</span><span class=""pln"">message</span><span class=""pun"">:</span><span class=""kwd"">string</span><span class=""pun"">):</span><span class=""kwd"">void</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">
    alert</span><span class=""pun"">(</span><span class=""pln"">message</span><span class=""pun"">);</span><span class=""pln"">
  </span><span class=""pun"">}</span><span class=""pln"">
</span><span class=""pun"">}</span></code></pre>

<p>So now we should see an alert with the text from the nested component:</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/May/3-1463923094577.png"" alt="""" class=""full-img""></p>

<p>Awesome. </p>

<p>To me, it's a great choice of the Angular team to drop the '@', '&amp;' and '=' concept in version 2.0. The new way is far more declarative, simple and explicit. And I like it! Although I'm still a React fan, these improvements in Angular 2 make me want to play around with it some more and maybe even try it out in a real case some day.</p>
        ","Angular decided to drop the '@', '&' and '=' in version 2.0. If you don't know what they do: good for you! To me, the @&= concept was among Angular 1's worst choices. Luckily, in Angular 2 the communication between components is a lot more explicit and easier to understand. In this post I'd like to show you how to pass data to and from a nested component in Angular 2.
Note: The examples in this post use Angular 2 Alpha. There were some changes in the final release, so dome code might work a bit different now. I'll try to keep it up to date.
Building a Nested Component
We can simply add a nested component by using a component as a directive within another component.
Let's first create a basic component that will be nested in another component later on. This components has a title property that we use in its template:
@Component({
  selector: 'child-selector',
  template: 'child.component.html'
})
export class ChildComponent {  
  title = 'I\'m a nested component';
}
The child.component.html is just an HTML file that shows the value of the title property:
/* child.conponent.html */
<h2>{{title}}</h2>  
Now we want to create a container component. It looks almost identical to the nested component, except we have to specify that we want to use the nested component. We do that by adding the ChildComponent to the directives property of the Component decorator. Without doing this, the ChildComponent can not be used.
@Component({
  selector: 'parent-selector',
  template: 'parent.component.html',
  directives: [ChildComponent]
})
export class ParentComponent { }  
The container component uses the nested component by specifying its directive in the template:
/* parent.component.html */
<div>  
  <h1>I'm a container component</h1>
  <child-selector></child-selector>
</div>  
Now, when we run our code we see something like this:
Nothing too fancy yet. Let's pass some data to the nested component!
Passing data to a nested component
If a nested component wants to receive input from its container, it must expose a property to that container. The nested component exposes a property it can use to receive input from its container using the @Input decorator.
We use the Input decorator to decorate any property in the nested component class. This works with every property type, including objects. In our example, we'll pass a string value to the nested component's title property, so we'll mark that property with the @Input decorator:
@Component({
  selector: 'child-selector',
  template: 'child.component.html'
})
export class ChildComponent {  
  @Input() title:string;
}
Now our nested component is ready to receive input from its parent component.
In the container component, we need to define the property we want to pass to the nested component. We call it childTitle:
@Component({
  selector: 'parent-selector',
  template: 'parent.component.html',
  directives: [ChildComponent]
})
export class ParentComponent {  
  childTitle:string = 'This text is passed to child';
}
Now the container component should pass the value of childTitle to the nested component by settings this property with property binding. When using property binding, we enclose the binding target in square brackets. The binding target refers to the title property of the nested component. We set the binding source to the data that the container wants to pass to the nested component, which is childTitle.
/* parent.component.html */
<div>  
  <h1>I'm a container component</h1>
  <child-selector [title]='childTitle'></child-selector>
</div>  
The only time we can specify a nested component's property as a property binding target, is when that property is decorated with the @Input decorator, like we did earlier.
When we run our code, we should see the passed value in the nested component's H2 element:
In this example we only exposed one input property, but of course you can expose multiple input properties as needed.
Passing data from a Nested Component
In the previous example I showed how the container can pass data to the nested component by binding to a nested component's property, that is declared with the @Input decorator.
If the nested component wants to send information back to its container, it can raise an event. The nested component exposes an event it can use to pass output to its container using the @Output decorator.
Like with the @Input decorator, we can use the @Output decorator to decorate any property of the nested components class. However, the property type must be an event. The only way a nested component can pass data back to its container, is with an event. The data to pass is called the event payload. In Angular, an event is defined with an EventEmitter object.
So let's start by creating a new instance of an Event Emitter and decorate the property with the @Output decorator.
@output() notify: EventEmitter<string> = new EventEmitter<string>();
If you're not familiar with generics, this syntax may look a bit odd to you. Generics allow us to identify a specific type that the object instance will work with.
The generic argument, string, identifies of the event payload. So now we can only pass string values to the container. If we would want to pass a integer for example, we'd write something like this:
@output() notify: EventEmitter<number> = new EventEmitter<number>();
Although not recommended, you can let it accept any type. You can use TypeScript's any type. We'll stick with a string for now. Please note that JavaScript does not support generics by itself; it's a TypeScript feature. I've personally only played with generics in C# a bit, and I think the concept is fairly new to the Javascript community in general.
Now we've got an event emitter in place, let's use the notify event property and call its emit method to raise the notify event and pass in our payload as an argument.
this.notify.emit('payload');  
We'll need a user interaction that will raise the event, so I'll add a link that will do so.
/* child.component.html */
<h2>Hi, I'm a nested component</h2>  
<span (click)='onClick()'>Click me please!</span>  
Now we'll add the click event handler to the component, and raise the notify event:
@Component({
  selector: 'child-selector',
  template: 'child.component.html'
})
export class ChildComponent {  
  @output() notify: EventEmitter<string> = new EventEmitter<string>();

  onClick() {
    this.notify.emit('Click from nested component');
  }
}
So every time a user clicks on the 'Click me please' link, the nested component will dispatch an event to its parent component.
The parent component receives that event and its payload. We use event binding to bind to this notify event and call a method.
/* parent.conponent.html */
<div>  
  <h1>I'm a container component</h1>
  <child-selector (notify)='onNotify($event)></child-selector>
</div>  
We have to pass the $event to the handler because that variable holds the event payload.
The only time we can can specify a nested component's property as an event binding target is when that property is decorated with the @Output decorator.
Our final step is to provide the onNotify method to execute when the notify event occurs. Since the event payload is a string, the onNotify function takes in a string. We can perform any desired action in our handler, but for now let's just alert the payload.
@Component({
  selector: 'parent-selector',
  template: 'parent.component.html',
  directives: [ChildComponent]
})
export class ParentComponent {  
  onNotify(message:string):void {
    alert(message);
  }
}
So now we should see an alert with the text from the nested component:
Awesome.
To me, it's a great choice of the Angular team to drop the '@', '&' and '=' concept in version 2.0. The new way is far more declarative, simple and explicit. And I like it! Although I'm still a React fan, these improvements in Angular 2 make me want to play around with it some more and maybe even try it out in a real case some day.","[Code, angularjs]"
40,How and why to test your product's performance on a slow internet connection,/how-to-test-how-your-products-work-on-a-slow-internet-connection/,"
            <p>Most of the time, we're blessed with a fast 4G, WiFi or wired internet connection, and everything is just fine. But how do our tools perform when the internet connection is not that great? Some time ago, a client sent me a screenshot of one of their creatives looking terribly broken in a live environment. How could this happen? We have a very strict quality control process. How could this have slipped through? After some testing, we've found out the creative breaks when loaded on a (very) slow internet connection. Due to this incident, checking our creatives on slow connection is now part of the quality control process.</p>

<p>Internet does not only get slow in the less developed parts of the world, but also in the western world. The coverage of 4G is far from 100% and public WiFi can get pretty terrible (in public transport for example). I've seen my phone indicate a GPRS connection more than once while traveling by train.</p>

<p>In <a href=""http://www.amazon.com/How-Google-Works-Eric-Schmidt/dp/1455582344"">How Google Works</a>, Eric Schmidt writes that at Google they even have a special day once in a while where Googlers can opt-in to have their internet slowed down for a day, so they can see how their tools perform under these circumstances. This really follows their 'it's all about the user' philosophy, and I like it. It's naive to only test the <em>happy path</em>.</p>

<p>My girlfriend joked that anyone that wants to test their product on a sulky connection, can come to our house because ""she can hardly stream her favourite Netflix show here"". So, if you're not able to drop by my house or we can't have a special day to let the company slow your connection down, how can we fake a slow internet connection otherwise?</p>

<h2 id=""limityourinternetconnectionusingchromedevtools"">Limit your internet connection using Chrome dev tools</h2>

<p>Starting with Chrome 38 you can do this without any separate tool or plugins. The Chrome browser comes with a specific feature set called the Developer Tools. One of the dev tools features is, you could have guessed it, network throttling. And the good news is: it's a breeze! Just press F12 (Windows) or Alt+Command+i (OSX):</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/May/i1-1463933979350.png"" alt="""" class=""full-img""></p>

<p>If you're not familiar with code, don't let this overwhelming view hold you back. I promise: throttling the network does not involve coding ;). Next, click the 'Network' tab. In the top bar of the dev tools window you see the 'Throttling' dropdown. </p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/May/i2-1463934197226.png"" alt="""" class=""full-img""></p>

<p>The dropdown contains some presets you can pick from:</p>

<ul>
<li>Offline</li>
<li>GPRS</li>
<li>Regular 2G</li>
<li>Good 2G</li>
<li>Regular 3G</li>
<li>Good 3G</li>
<li>Regular 4G</li>
<li>DSL</li>
<li>WiFi</li>
</ul>

<p>If you, for example, select 'Regular 2G', and refresh your page, you should see that the page is loading very, very slowly. So there you have it: you faked a slow internet connection. Can users still use (at least the basics of) your product?</p>

<p>By te way, the list also contains the 'Offline' option, which can come in handy if you're working on <a href=""https://css-tricks.com/serviceworker-for-offline/"">an offline version</a> of your product. It's a better option than manually plugging your ethernet cable in and out, or enabling/disabling your WiFi connection. </p>

<h2 id=""customizethethrottling"">Customize the throttling</h2>

<p>If you want to test your site on a very specific bandwidth that's not in the default list, you can add a custom profile to the throttling list. To do so, press F1 in Chrome dev tools to open the Settings window, or select 'Add' in the throttling dropdown. In the 'Throttling' tab, you'll find a button to add your custom profile:</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/May/i3-1463934875110.png"" alt="""" class=""full-img""></p>

<p>Note that an internet connection speed relies on more factors than bandwidth only. You should also take the latency of the network in account. The third column of Network Throttling Profiles is the profile's latency. Network latency is the term used to indicate any kind of delay that happens in data communication over a network. </p>

<p>Oh, and don't forget to disable throttling when you're done testing.</p>

<h2 id=""alternatives"">Alternatives</h2>

<p>Although faking a slow internet connection using Google Chrome's dev tools is convenient, it has its limits. If you're running Windows, <a href=""http://www.telerik.com/fiddler"">Fiddler</a> is a superb tool and has a lot of advanced features. For example, for someone who wants more control it can add latency to each request. Some people prefer using a tool like this to putting latency code in an application as it is a much more realistic simulation. <a href=""http://donchevp.blogspot.nl/2009/11/limiting-your-internet-connection-speed.html"">This article</a> at Pavel Donchev's blog on Software Technologies shows how to create custom simulated speeds using Fiddler.</p>

<p>If you're on Linux, you can give Dummynet or <a href=""http://mark.koli.ch/slowdown-throttle-bandwidth-linux-network-interface"">tc</a> a try. For people on an OS X machine there's <a href=""http://nshipster.com/network-link-conditioner/"">Network Link Conditioner</a>.</p>
        ","Most of the time, we're blessed with a fast 4G, WiFi or wired internet connection, and everything is just fine. But how do our tools perform when the internet connection is not that great? Some time ago, a client sent me a screenshot of one of their creatives looking terribly broken in a live environment. How could this happen? We have a very strict quality control process. How could this have slipped through? After some testing, we've found out the creative breaks when loaded on a (very) slow internet connection. Due to this incident, checking our creatives on slow connection is now part of the quality control process.
Internet does not only get slow in the less developed parts of the world, but also in the western world. The coverage of 4G is far from 100% and public WiFi can get pretty terrible (in public transport for example). I've seen my phone indicate a GPRS connection more than once while traveling by train.
In How Google Works, Eric Schmidt writes that at Google they even have a special day once in a while where Googlers can opt-in to have their internet slowed down for a day, so they can see how their tools perform under these circumstances. This really follows their 'it's all about the user' philosophy, and I like it. It's naive to only test the happy path.
My girlfriend joked that anyone that wants to test their product on a sulky connection, can come to our house because ""she can hardly stream her favourite Netflix show here"". So, if you're not able to drop by my house or we can't have a special day to let the company slow your connection down, how can we fake a slow internet connection otherwise?
Limit your internet connection using Chrome dev tools
Starting with Chrome 38 you can do this without any separate tool or plugins. The Chrome browser comes with a specific feature set called the Developer Tools. One of the dev tools features is, you could have guessed it, network throttling. And the good news is: it's a breeze! Just press F12 (Windows) or Alt+Command+i (OSX):
If you're not familiar with code, don't let this overwhelming view hold you back. I promise: throttling the network does not involve coding ;). Next, click the 'Network' tab. In the top bar of the dev tools window you see the 'Throttling' dropdown.
The dropdown contains some presets you can pick from:
Offline
GPRS
Regular 2G
Good 2G
Regular 3G
Good 3G
Regular 4G
DSL
WiFi
If you, for example, select 'Regular 2G', and refresh your page, you should see that the page is loading very, very slowly. So there you have it: you faked a slow internet connection. Can users still use (at least the basics of) your product?
By te way, the list also contains the 'Offline' option, which can come in handy if you're working on an offline version of your product. It's a better option than manually plugging your ethernet cable in and out, or enabling/disabling your WiFi connection.
Customize the throttling
If you want to test your site on a very specific bandwidth that's not in the default list, you can add a custom profile to the throttling list. To do so, press F1 in Chrome dev tools to open the Settings window, or select 'Add' in the throttling dropdown. In the 'Throttling' tab, you'll find a button to add your custom profile:
Note that an internet connection speed relies on more factors than bandwidth only. You should also take the latency of the network in account. The third column of Network Throttling Profiles is the profile's latency. Network latency is the term used to indicate any kind of delay that happens in data communication over a network.
Oh, and don't forget to disable throttling when you're done testing.
Alternatives
Although faking a slow internet connection using Google Chrome's dev tools is convenient, it has its limits. If you're running Windows, Fiddler is a superb tool and has a lot of advanced features. For example, for someone who wants more control it can add latency to each request. Some people prefer using a tool like this to putting latency code in an application as it is a much more realistic simulation. This article at Pavel Donchev's blog on Software Technologies shows how to create custom simulated speeds using Fiddler.
If you're on Linux, you can give Dummynet or tc a try. For people on an OS X machine there's Network Link Conditioner.",[]
41,Increase the reach of your TV campaign with programmatic video,/increase-the-reach-of-your-tv-campaign-with-programmatic-video/,"
            <p>Television is still the go-to medium for reaching large numbers of people in a short amount of time. However, smart marketers also use the plentiful online and mobile options to increase the reach of their TV campaigns even further. Which is where programmatic video comes in. Programmatic video is video advertising, bought through software, like a demand side platform. Unlike traditional, manual purchasing, programmatic impressions are bought automatically. So instead of buying video ads in bulk, each impression is selected individually. </p>

<p>In the past few years, the Dutch television landscape changed drastically due to the rise of online video. Or, as the Dutch institute for promotion and optimisation of television commercials phrased it in their <a href=""http://spot.nl/wp-content/uploads/2016/03/20160308-SPOT-Jaarrapport-2015-definitief.pdf"">year report 2015 (Dutch)</a>: new online video platforms are creating a new reality, in which TV content remains an important factor. The new possibilities lead to a different use of the TV screen and contribute to the growth of total screen time (via TV, smartphone, tablet or PC). </p>

<p>More and more brands and marketers are using the new possibilities of online video to generate additional reach for their TV campaigns, and parties like Google and Facebook are also placing their bets on online video. Quite literally, because a fast growing number of video impressions is purchased programmatically. Just like on the display market. On top of that, more and more publishers are joining the online auction.</p>

<h2 id=""importantadvantages"">Important advantages</h2>

<p>The new possibilities of online video give advertisers a couple of important advantages. First of all, these extra channels will generate a larger reach, on top of their TV campaigns. Meticulous <a href=""http://www.videologygroup.com/news-press/P60/videology-research-shows-tangible-benefits-of-cross-screen-tv-planning/#.VzLFCvmLSUk"">research</a> by the international video advertising platform Videology shows the significant impact of cross-screen TV planning by several large brands in sectors like Consumer Packaged Goods, Healthy &amp; Beauty, Cars and Technology. </p>

<p>On top of increased reach, online advertising can target specific audiences more accurately. It’s often cheaper as well, and online video ads can reach people who aren’t watching television. So, how can marketers benefit from programmatic online video to optimise their TV campaigns? A few pointers:   </p>

<h4 id=""1decidewhereyouwanttoshowyouradsandwhereyoudontwithablackandwhitelist"">1. Decide where you want to show your ads (and where you don’t) with a black- and whitelist</h4>

<p>Brand managers are used to buying in on specific channels because panel research shows that this is where their target audience is. There are also shows they’d rather avoid. Like insurance companies who prefer not to advertise before or after the news, because of an unwanted connection to negative news items. Which is also why many brand managers fear they might lose control by choosing to use programmatic video at an online auction.   </p>

<p>Quite the opposite is true. Thanks to <a href=""http://programmaticadvertising.org/2014/10/24/are-you-utilizing-a-whitelist-and-a-blacklist-for-your-marketing-campaign/"">black-  and whitelisting</a>, you have a much greater control of the environment of an impression, reducing the risk of unwanted surprises. ‘Online auction’ doesn’t equal ‘low quality’ either. Many premium publishers offer their inventory on online auctions. In the Netherlands, major publishers like RTL XL, KIJK and Telegraaf offer a lot of advertising space through the online auction. </p>

<h4 id=""2discoverthepossibilitiesoftargetingbytimeandlocation"">2. Discover the possibilities of targeting by time and location</h4>

<p>TV is very price sensitive when it comes to time targeting. Primetime is a lot more expensive than advertising during the day. This isn’t the case with online video: an impression at 13.00 can cost as much as an impression at 21.00. Marketers can select prime time impressions, without paying extra. A pizza delivery company might want to advertise between 17.00 and 23.00. For promoting a new film, advertising from Wednesday to Saturday might be your best bet. </p>

<p>By creatively managing the time your campaign is live, you can greatly increase its efficiency. This also counts for the possibility to target videos based on location. A retailer who only has stores in certain areas can limit its ads to those specific places. The options for this are very accurate. Take Facebook for instance, where you can <a href=""https://www.facebook.com/business/help/633474486707199"">advertise within a specific radius</a>.</p>

<h4 id=""3usethedatatooptimiseyourcampaign"">3. Use the data to optimise your campaign</h4>

<p>TV’s options for optimisation are limited. Online video gives you a greater insight in the reaction of consumers to your brand message, both in terms of clicks as viewer behaviour. For instance: at which point to people stop watching? Which video is performing best? And how effective is adding a call to action? These data are extremely valuable because they give marketers the possibility to learn and continuously optimise according. </p>

<p>Based on these data, you can optimise KPI’s (like VTR, CTR, CPA or viewability). In addition to this, you can use the interaction data to learn more about your target audience. You can use this info to <a href=""https://www.facebook.com/business/learn/facebook-page-build-audience"">build audiences</a> on Facebook to make your follow-up communication more effective. </p>

<p>For instance by simply targeting, but also by sending traffic to your own online environment, to further communicate with leads. Each customer contact becomes a possibility to learn more and make your next video more effective. An opportunity no marketer should miss out on!  </p>
        ","Television is still the go-to medium for reaching large numbers of people in a short amount of time. However, smart marketers also use the plentiful online and mobile options to increase the reach of their TV campaigns even further. Which is where programmatic video comes in. Programmatic video is video advertising, bought through software, like a demand side platform. Unlike traditional, manual purchasing, programmatic impressions are bought automatically. So instead of buying video ads in bulk, each impression is selected individually.
In the past few years, the Dutch television landscape changed drastically due to the rise of online video. Or, as the Dutch institute for promotion and optimisation of television commercials phrased it in their year report 2015 (Dutch): new online video platforms are creating a new reality, in which TV content remains an important factor. The new possibilities lead to a different use of the TV screen and contribute to the growth of total screen time (via TV, smartphone, tablet or PC).
More and more brands and marketers are using the new possibilities of online video to generate additional reach for their TV campaigns, and parties like Google and Facebook are also placing their bets on online video. Quite literally, because a fast growing number of video impressions is purchased programmatically. Just like on the display market. On top of that, more and more publishers are joining the online auction.
Important advantages
The new possibilities of online video give advertisers a couple of important advantages. First of all, these extra channels will generate a larger reach, on top of their TV campaigns. Meticulous research by the international video advertising platform Videology shows the significant impact of cross-screen TV planning by several large brands in sectors like Consumer Packaged Goods, Healthy & Beauty, Cars and Technology.
On top of increased reach, online advertising can target specific audiences more accurately. It’s often cheaper as well, and online video ads can reach people who aren’t watching television. So, how can marketers benefit from programmatic online video to optimise their TV campaigns? A few pointers:
1. Decide where you want to show your ads (and where you don’t) with a black- and whitelist
Brand managers are used to buying in on specific channels because panel research shows that this is where their target audience is. There are also shows they’d rather avoid. Like insurance companies who prefer not to advertise before or after the news, because of an unwanted connection to negative news items. Which is also why many brand managers fear they might lose control by choosing to use programmatic video at an online auction.
Quite the opposite is true. Thanks to black- and whitelisting, you have a much greater control of the environment of an impression, reducing the risk of unwanted surprises. ‘Online auction’ doesn’t equal ‘low quality’ either. Many premium publishers offer their inventory on online auctions. In the Netherlands, major publishers like RTL XL, KIJK and Telegraaf offer a lot of advertising space through the online auction.
2. Discover the possibilities of targeting by time and location
TV is very price sensitive when it comes to time targeting. Primetime is a lot more expensive than advertising during the day. This isn’t the case with online video: an impression at 13.00 can cost as much as an impression at 21.00. Marketers can select prime time impressions, without paying extra. A pizza delivery company might want to advertise between 17.00 and 23.00. For promoting a new film, advertising from Wednesday to Saturday might be your best bet.
By creatively managing the time your campaign is live, you can greatly increase its efficiency. This also counts for the possibility to target videos based on location. A retailer who only has stores in certain areas can limit its ads to those specific places. The options for this are very accurate. Take Facebook for instance, where you can advertise within a specific radius.
3. Use the data to optimise your campaign
TV’s options for optimisation are limited. Online video gives you a greater insight in the reaction of consumers to your brand message, both in terms of clicks as viewer behaviour. For instance: at which point to people stop watching? Which video is performing best? And how effective is adding a call to action? These data are extremely valuable because they give marketers the possibility to learn and continuously optimise according.
Based on these data, you can optimise KPI’s (like VTR, CTR, CPA or viewability). In addition to this, you can use the interaction data to learn more about your target audience. You can use this info to build audiences on Facebook to make your follow-up communication more effective.
For instance by simply targeting, but also by sending traffic to your own online environment, to further communicate with leads. Each customer contact becomes a possibility to learn more and make your next video more effective. An opportunity no marketer should miss out on!","[Analytics, Video, TV, programmatic buying]"
42,Overcoming cross-device challenges with Google Analytics' client ID,/having-fun-with-google-analytcis-cross-device/,"
            <p>Google Analytics has a powerful way of <a href=""https://support.google.com/analytics/answer/3234673?hl=en"">measuring cross-device behaviour</a>. Adding a user ID to your data collection gives you access to reports like device overlap, device paths and device attribution. But it has some limits. In this post, I’ll dive into those limits and discuss ways to overcome those limits.</p>

<h2 id=""howgoogleanalyticsmeasurescrossdevice"">How Google Analytics measures Cross Device</h2>

<p>Google allows you to identify users over different devices by <a href=""https://support.google.com/analytics/answer/3123669"">adding a user ID to your data collection</a>. This will give you three types of reports:</p>

<ul>
<li><strong>Device Overlap:</strong> a report that shows you the overlap of the three device categories desktop, tablet and mobile. </li>
<li><strong>Device paths:</strong> what device paths occur the most before a transaction.</li>
<li><strong>Device attribution:</strong> the attribution of the three device categories. How many revenue do users who had that first session on mobile generate on other devices?</li>
</ul>

<p>These are some powerful reports, but they have their limitations.</p>

<h2 id=""wherecrossdeviceingoogleanalyticsfallsshort"">Where Cross Device in Google Analytics falls short</h2>

<p>First of all, it will only add the three reports mentioned above. The source attribution  models aren't affected by the user ID because Google uses the client id for modelling. Secondly, it will <a href=""https://support.google.com/analytics/answer/4574780"">only include sessions where a user logs in</a>. Every session without a log-in is not included in the view. </p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/May/tmt_ga_crossdevice-1463473322077.PNG"" alt=""Google Analytics cross-device Model"" class=""full-img""></p>

<p><em>Google Analytics Cross Device Model. Blue: sessions that are not included in cross-device reports. Green: sessions that are included in cross-device.</em></p>

<p>It’s weird that Google made it work like this, as it tracks returning users within two years based on the auto-generate client id (cid) by default. How hard is it to attribute a single client ID to a user ID once the user logs in, whether it’s the first, second, or fifteenth session of that user? Just like the <a href=""https://support.google.com/analytics/answer/4574780#on-off"">sessions unification feature</a> that you can set optionally, a feature like user unification would be great. There are ways to overcome this, e.g. by  manually storing the user ID in a cookie as soon as the user logs in. This way you’ll be able to use it when a user returns without logging in (and without removing cookies). But again, it’s weird that GA doesn't offer a solution for this problem by default. </p>

<h2 id=""howtotrickgoogleanalyticsintoextrainsights"">How to trick Google Analytics into extra insights</h2>

<p>Not having cross-device source attribution in Google Analytics cross-device views got me thinking: is there a way to add these insights to my GA reports? Luckily there is, and the solution has always been out there. It has to do with <a href=""https://support.google.com/analytics/answer/1665189?hl=en"">Google Analytics’ last non-direct click model</a>. It's the default model in Google Analytics’ standard reports and it works like this:</p>

<ul>
<li>A user lands on a page</li>
<li>Does the user have a source (referral, organic, or UTM tagging)?
<ul><li>Yes: attribute the session to the source;</li>
<li>No: Did the user have a source in the past 6 months?
<ul><li>Yes: attribute the session to that source.</li>
<li>No: attribute the session to direct traffic</li></ul></li></ul></li>
</ul>

<p>The important part is the <strong>‘Did the user have a source in the past 6 months?’</strong> question. Google looks back 6 months based on your client ID. If a client ID appeared before, it will look up the last known source of your previous session. The client ID is the key to the extra insights.</p>

<h2 id=""settingacustomclientidforcrossdevicesourceattribution"">Setting a custom client ID for cross-device source attribution</h2>

<p>The tracking code of Google’s Universal Analytics tracker allows you to <a href=""https://developers.google.com/analytics/devguides/collection/analyticsjs/field-reference#clientId"">set a custom client ID</a>. By setting it to the value of your cross-device identifier (e.g. a  user ID), you’ll trick Google Analytics into using that value for all of its calculations. This will automatically add cross-device attribution models to your reports. Assuming that a user is logged in in all of his sessions, a direct session on a desktop will be attributed to a mobile source (if the user had a mobile session with a source). </p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/May/tmt_ga_custom_crossdevice-1463473329790.PNG"" alt=""Google Analytics Custom Cross Device solution"" class=""full-img""></p>

<p><em>Google Analytics custom cross-device solution. Blue: sessions without custom cross-device modelling. Green: sessions with custom cross-device modelling.</em></p>

<p>The downside of this trick is that setting the client ID to the value of the user ID during the sessions triggers a new session. After the login (session three in the image above), Google Analytics attributes activity to a new client id (the value changed from auto-generated client ID to the manually set user ID). So for every first login, we’ll measure an extra direct session and a new user. </p>

<p><strong><em>A technical note</em></strong></p>

<p><em>Setting a custom client ID has a high impact on your data collection. I strongly advise you to set up a secondary tracker to collect this data. Also, set a <a href=""https://developers.google.com/analytics/devguides/collection/analyticsjs/field-reference#cookieName"">custom cookie name</a> for the secondary tracker to leave the original _ga cookie untouched.</em> </p>

<h2 id=""thedataofthecustomclientiddatacollection"">The data of the custom client ID data collection</h2>

<p>We’ve had this trick running for a couple of months now for one of our clients and it works. Let’s look at a frequently returning traffic source, Google AdWords, or google CPC:</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/May/cross_device_cpc-1463471585664.PNG"" alt=""Google Analytics added AdWords sessions"" class=""full-img"">
<em>Graph with added AdWords sessions by setting a custom client ID.</em></p>

<p>As you can see, <strong>the new way off collecting data attributes 10% to 25% more sessions to AdWords</strong>. There are also sources that show the opposite effect. A good example is the password reset source:</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/May/custom_device_forgot_pass-1463471661233.PNG"" alt=""Google Analytics added password reset sessions"" class=""full-img"">
<em>Graph with added password reset sessions by setting a custom client ID.</em></p>

<p>What’s interesting here is that the custom client ID has a negative impact on sessions attributed to this source.  Why? Maybe it's good to take a minute to think this one through (or just continue for the answer).</p>

<p>Users who reset their password often login right after the reset. This changes their client id and triggers a new session. Without the solution, Google would attribute all direct sessions after the reset to the reset password source. With the client ID overwrite, it loses the password reset source and with it, less sessions will be attributed to that source. </p>

<p>Our final graph shows you the impact on direct traffic. As discussed, every first login triggers a new session from a new user.  Because of this, you would expect a peak in direct traffic. </p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/May/custom_device_direct-1463471709123.PNG"" alt=""Google Analytics added direct sessions"" class=""full-img"">
<em>Graph with added direct sessions by setting a custom client ID.</em></p>

<p>As you can see, direct got way more sessions than before, over 5 times as many. This is something you’ll have to take into account when implementing the client ID trick. </p>

<h2 id=""wheredowegofromhere"">Where do we go from here</h2>

<p>The biggest challenge with this solution is <strong>reducing the number of direct sessions</strong>. There are several tricks for this:</p>

<ul>
<li><strong>Add the user ID as a parameter to all outgoing emails</strong> to existing clients. This way, every session that starts from an email will start with the cross-device client ID.</li>
<li><strong>Store source information in a cookie</strong> and attribute the new sessions of a first login to the stored UTM tagging by code.</li>
</ul>

<p>Though this fix reduces the number of direct sessions, they don’t fix everything. We will still have an increase in sessions and users. </p>

<h2 id=""leavinggoogleanalyticsbehindforacustommodelapproach"">Leaving Google Analytics behind for a custom model approach</h2>

<p>The best solution may be dropping Google Analytics all together, <strong>and start collecting raw data for a custom model</strong>. We currently use <a href=""http://snowplowanalytics.com/"">Snowplow</a> to collect raw event level data for our data science department. Event level data collectors also generate and collect values similar to a client ID and allow you to add custom data such as a user ID. The power of raw event level data is that you’re completely free to model the data to match your needs. This also applies for cross-device analysis.</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/May/tmt_custom_crossdevice-1463473336527.PNG"" alt=""Custom cross-device data based on event level data"" class=""full-img"">
<em>Custom cross-device model based on event level data. Blue: sessions. Green: sessions with user ID. Faded green: sessions with retroactively applied user IDs.</em></p>

<p>If we look at the image above, we see that only sessions 3 and 6 have a user ID available. But with custom modelling, we can create a model that knows that client ID 123 is the same user as the one with user ID 456. <strong>This model will apply the user ID retroactively</strong>. If client ID 123 has had 40 sessions over the past 3 years, and the user logs in for the first time today, the model will connect those 40 sessions to that one user ID. </p>

<h3 id=""conclusion"">Conclusion</h3>

<p>Google Analytics’ cross-device reports are powerful but fall short in some areas. There are ways to get around this, but as long as Google doesn't change its model, the custom solution will never be perfect. So for now, it’s best to use Google Analytics for your general cross-device analysis. As soon as advanced cross-device modelling is needed, I would advise you to look at a custom data solution. </p>
        ","Google Analytics has a powerful way of measuring cross-device behaviour. Adding a user ID to your data collection gives you access to reports like device overlap, device paths and device attribution. But it has some limits. In this post, I’ll dive into those limits and discuss ways to overcome those limits.
How Google Analytics measures Cross Device
Google allows you to identify users over different devices by adding a user ID to your data collection. This will give you three types of reports:
Device Overlap: a report that shows you the overlap of the three device categories desktop, tablet and mobile.
Device paths: what device paths occur the most before a transaction.
Device attribution: the attribution of the three device categories. How many revenue do users who had that first session on mobile generate on other devices?
These are some powerful reports, but they have their limitations.
Where Cross Device in Google Analytics falls short
First of all, it will only add the three reports mentioned above. The source attribution models aren't affected by the user ID because Google uses the client id for modelling. Secondly, it will only include sessions where a user logs in. Every session without a log-in is not included in the view.
Google Analytics Cross Device Model. Blue: sessions that are not included in cross-device reports. Green: sessions that are included in cross-device.
It’s weird that Google made it work like this, as it tracks returning users within two years based on the auto-generate client id (cid) by default. How hard is it to attribute a single client ID to a user ID once the user logs in, whether it’s the first, second, or fifteenth session of that user? Just like the sessions unification feature that you can set optionally, a feature like user unification would be great. There are ways to overcome this, e.g. by manually storing the user ID in a cookie as soon as the user logs in. This way you’ll be able to use it when a user returns without logging in (and without removing cookies). But again, it’s weird that GA doesn't offer a solution for this problem by default.
How to trick Google Analytics into extra insights
Not having cross-device source attribution in Google Analytics cross-device views got me thinking: is there a way to add these insights to my GA reports? Luckily there is, and the solution has always been out there. It has to do with Google Analytics’ last non-direct click model. It's the default model in Google Analytics’ standard reports and it works like this:
A user lands on a page
Does the user have a source (referral, organic, or UTM tagging)?
Yes: attribute the session to the source;
No: Did the user have a source in the past 6 months?
Yes: attribute the session to that source.
No: attribute the session to direct traffic
The important part is the ‘Did the user have a source in the past 6 months?’ question. Google looks back 6 months based on your client ID. If a client ID appeared before, it will look up the last known source of your previous session. The client ID is the key to the extra insights.
Setting a custom client ID for cross-device source attribution
The tracking code of Google’s Universal Analytics tracker allows you to set a custom client ID. By setting it to the value of your cross-device identifier (e.g. a user ID), you’ll trick Google Analytics into using that value for all of its calculations. This will automatically add cross-device attribution models to your reports. Assuming that a user is logged in in all of his sessions, a direct session on a desktop will be attributed to a mobile source (if the user had a mobile session with a source).
Google Analytics custom cross-device solution. Blue: sessions without custom cross-device modelling. Green: sessions with custom cross-device modelling.
The downside of this trick is that setting the client ID to the value of the user ID during the sessions triggers a new session. After the login (session three in the image above), Google Analytics attributes activity to a new client id (the value changed from auto-generated client ID to the manually set user ID). So for every first login, we’ll measure an extra direct session and a new user.
A technical note
Setting a custom client ID has a high impact on your data collection. I strongly advise you to set up a secondary tracker to collect this data. Also, set a custom cookie name for the secondary tracker to leave the original _ga cookie untouched.
The data of the custom client ID data collection
We’ve had this trick running for a couple of months now for one of our clients and it works. Let’s look at a frequently returning traffic source, Google AdWords, or google CPC:
Graph with added AdWords sessions by setting a custom client ID.
As you can see, the new way off collecting data attributes 10% to 25% more sessions to AdWords. There are also sources that show the opposite effect. A good example is the password reset source:
Graph with added password reset sessions by setting a custom client ID.
What’s interesting here is that the custom client ID has a negative impact on sessions attributed to this source. Why? Maybe it's good to take a minute to think this one through (or just continue for the answer).
Users who reset their password often login right after the reset. This changes their client id and triggers a new session. Without the solution, Google would attribute all direct sessions after the reset to the reset password source. With the client ID overwrite, it loses the password reset source and with it, less sessions will be attributed to that source.
Our final graph shows you the impact on direct traffic. As discussed, every first login triggers a new session from a new user. Because of this, you would expect a peak in direct traffic.
Graph with added direct sessions by setting a custom client ID.
As you can see, direct got way more sessions than before, over 5 times as many. This is something you’ll have to take into account when implementing the client ID trick.
Where do we go from here
The biggest challenge with this solution is reducing the number of direct sessions. There are several tricks for this:
Add the user ID as a parameter to all outgoing emails to existing clients. This way, every session that starts from an email will start with the cross-device client ID.
Store source information in a cookie and attribute the new sessions of a first login to the stored UTM tagging by code.
Though this fix reduces the number of direct sessions, they don’t fix everything. We will still have an increase in sessions and users.
Leaving Google Analytics behind for a custom model approach
The best solution may be dropping Google Analytics all together, and start collecting raw data for a custom model. We currently use Snowplow to collect raw event level data for our data science department. Event level data collectors also generate and collect values similar to a client ID and allow you to add custom data such as a user ID. The power of raw event level data is that you’re completely free to model the data to match your needs. This also applies for cross-device analysis.
Custom cross-device model based on event level data. Blue: sessions. Green: sessions with user ID. Faded green: sessions with retroactively applied user IDs.
If we look at the image above, we see that only sessions 3 and 6 have a user ID available. But with custom modelling, we can create a model that knows that client ID 123 is the same user as the one with user ID 456. This model will apply the user ID retroactively. If client ID 123 has had 40 sessions over the past 3 years, and the user logs in for the first time today, the model will connect those 40 sessions to that one user ID.
Conclusion
Google Analytics’ cross-device reports are powerful but fall short in some areas. There are ways to get around this, but as long as Google doesn't change its model, the custom solution will never be perfect. So for now, it’s best to use Google Analytics for your general cross-device analysis. As soon as advanced cross-device modelling is needed, I would advise you to look at a custom data solution.","[Analytics, google analytics, cross device, attribution, modelling]"
43,Making the most of Facebook's video potential,/making-the-most-of-facebooks-video-potential/,"
            <p>New social media like Instagram, WhatsApp, Snapchat and Pinterest are on the rise, making marketeers wonder if Facebook still deserves its spot in their marketing mix. If you are one of them, rest assured that the power of Facebook is still quite unrivalled. Sure, Facebook is no longer the 'cool kid on the block', and many youngsters have moved on to hipper pastures. But the social network still has plenty to offer big brands and small businesses alike. Especially if they know how to effectively use all of Facebook's numerous new video features. </p>

<p>Why is video so essential? First of all, Facebook users are consuming over 100 million hours of video each day, and this number continues to grow. Which is why Facebook predicts that in 2 years, their newsfeed will consist almost entirely of videos. The network is even experimenting with a video-only newsfeed for mobile, anticipating the ever increasing importance of video in social content marketing. </p>

<p>If you want to make the most of these developments (who doesn't!), these are just the tips for you: </p>

<h2 id=""keepitshort"">Keep it short</h2>

<p>Research by Spike (2015) shows that 90% of Facebook's most shared videos averaged a length of 1 minute. Video's from Tasty, Buzzfeed's successful video content format, were no longer than a very snackable 24 seconds. The Microsoft DX video below is a short fragment from the original, provided with a title and call to action. </p>

<div class=""fluid-width-video-wrapper"" style=""padding-top: 56.25%;""><iframe src=""https://www.youtube.com/embed/sgxuPvqoQrg?rel=0"" frameborder=""0"" allowfullscreen="""" id=""fitvid928333""></iframe></div>

<h2 id=""checkhowmanyusersturnonthesound"">Check how many users turn on the sound</h2>

<p>Unless you're 13 or suffering from underdeveloped social skills, you make sure not to disturb others with loud music or video sounds when in a public place. Which is why many Facebook users watch videos without sound. Lucky for you, Facebook added the 'Sound on' vs. 'Sound off' metric to Facebook Insights in February, which is a great way to check if your users are actually listening to your videos. </p>

<h2 id=""addsubtitles"">Add subtitles</h2>

<p>If a large portion of your users turn off sounds, adding subtitles might be more valuable than a voice over. You can do this by uploading .srt files along with your video. The only downside of this method is that .srt files only work on desktop, so a better option is to hardcode your subtitles into your video. </p>

<h2 id=""considerusingfacebooklive"">Consider using Facebook Live</h2>

<p>One of Facebook newest features is Facebook Live. Like Periscope, it offers the possibility to do a live broadcast (hence the name). Metrics are limited at this point, but you can discover the number of (unique) viewers. In <a href=""http://newsroom.fb.com/news/2016/03/news-feed-fyi-taking-into-account-live-video-when-ranking-feed/"">a recent blog post</a>, Facebook announced that Live video will get priority in news feeds, making it even more interesting to explore the possibilities. </p>

<h2 id=""experimentwithcanvas"">Experiment with Canvas</h2>

<p><a href=""https://canvas.facebook.com"">Facebook Canvas</a> gives you the opportunity to build a unique experience, combining text, images, carousels, product feeds and of course, videos and animations. One of the biggest features of Canvas is that it will load videos almost 10 times faster than other mobile apps, decreasing the chances of someone losing interest due to long loading times. A great example of the possibilities of Facebook Canvas is this video from the Dutch Hartstichting (heart foundation):  </p>

<div class=""fluid-width-video-wrapper"" style=""padding-top: 56.25%;""><iframe src=""https://www.youtube.com/embed/d1G3_XsdI3I?rel=0"" frameborder=""0"" allowfullscreen="""" id=""fitvid774449""></iframe></div>

<h2 id=""gosquare"">Go square</h2>

<p>This is one of those 'why didn't we think of this before' tips, but here goes: a recent study by Buzzfeed revealed that 3 in 4 of the most shared videos are square. Why? Because people don't like tipping their phone to watch a video in full screen. A big plus of going square is that your videos can be used for Instagram - if you keep 'm shorter than 15 seconds, of course. Take this video by Chio for example: </p>

<div class=""fluid-width-video-wrapper"" style=""padding-top: 56.25%;""><iframe src=""https://www.youtube.com/embed/1nkrPe2pBdQ?rel=0"" frameborder=""0"" allowfullscreen="""" id=""fitvid741634""></iframe></div>  

<p><em>""Full moon? We're mostly looking at the Space Raiders. What about you?""</em></p>

<h2 id=""addacalltoaction"">Add a call to action</h2>

<p>Just like any piece of quality content, a good call to action is half the battle. To add a call to action to your Facebook video, go to 'Edit Video' and click the 'Call to Action' tab. Here you can select a CTA button to add to the end of your video. You can choose from the following: </p>

<ul>
<li>Shop now</li>
<li>Book now</li>
<li>Learn more</li>
<li>Sign up</li>
<li>Download</li>
<li>Watch more</li>
</ul>

<p>Just add UTM tagging to your URL, and the effect of your CTA will show up in Google Analytics. </p>

<p>And last, but definitely not least:</p>

<h2 id=""takeaudiencebuildingtothenextlevel"">Take audience building to the next level</h2>

<p>Facebook recently launched a new Audience option: Engagement Audiences. With this option, you can create a custom audience, based on the amount of time someone spent watching your video. These are the options available: </p>

<ul>
<li>People who viewed at least 3 seconds of your video</li>
<li>People who viewed at least 10 seconds of your video</li>
<li>People who viewed at least 25% of your video</li>
<li>People who viewed at least 50% of your video</li>
<li>People who viewed at least 75% of your video</li>
<li>People who viewed at least 95% of your video</li>
</ul>

<p>You can also specify the time since their last engagement (up to 180 days). The difference with the old Video View Custom Audiences is that you can select several videos, instead of just one. Very useful if you're running a campaign. By dividing your video audience by engagement rate, you take the first step towards providing each custom audience with even more relevant video content. For instance by excluding people who already viewed at least 75% of your latest video from your next ad with the same video. Or by retargeting them towards sales, since they're already highly engaged.  </p>

<p>Besides that, using Engagement Audiences is a great strategy to convert more of your audience into leads, or to increase traffic to your own domain, leading your viewers to additional information about the campaign or the subject of your video. </p>

<p>A Facebook video view might very well be the first contact in the entire buyer's cycle. If you look at it this way, it makes sense to develop a more integrated strategy and a more dynamic approach to content creation. Hopefully, this blog post convinced you that Facebook video is very much worth considering! </p>

<p><strong>Bonus tip:</strong> Time is precious, so you might want to mention the video's duration in the video post. People are more likely to watch the video if they know the length. </p>

<p><mark>This post was previously published in Dutch on <a href=""http://www.emerce.nl/best-practice/onbenutte-potentieel-facebook"">Emerce</a>.</mark></p>
        ","New social media like Instagram, WhatsApp, Snapchat and Pinterest are on the rise, making marketeers wonder if Facebook still deserves its spot in their marketing mix. If you are one of them, rest assured that the power of Facebook is still quite unrivalled. Sure, Facebook is no longer the 'cool kid on the block', and many youngsters have moved on to hipper pastures. But the social network still has plenty to offer big brands and small businesses alike. Especially if they know how to effectively use all of Facebook's numerous new video features.
Why is video so essential? First of all, Facebook users are consuming over 100 million hours of video each day, and this number continues to grow. Which is why Facebook predicts that in 2 years, their newsfeed will consist almost entirely of videos. The network is even experimenting with a video-only newsfeed for mobile, anticipating the ever increasing importance of video in social content marketing.
If you want to make the most of these developments (who doesn't!), these are just the tips for you:
Keep it short
Research by Spike (2015) shows that 90% of Facebook's most shared videos averaged a length of 1 minute. Video's from Tasty, Buzzfeed's successful video content format, were no longer than a very snackable 24 seconds. The Microsoft DX video below is a short fragment from the original, provided with a title and call to action.
Check how many users turn on the sound
Unless you're 13 or suffering from underdeveloped social skills, you make sure not to disturb others with loud music or video sounds when in a public place. Which is why many Facebook users watch videos without sound. Lucky for you, Facebook added the 'Sound on' vs. 'Sound off' metric to Facebook Insights in February, which is a great way to check if your users are actually listening to your videos.
Add subtitles
If a large portion of your users turn off sounds, adding subtitles might be more valuable than a voice over. You can do this by uploading .srt files along with your video. The only downside of this method is that .srt files only work on desktop, so a better option is to hardcode your subtitles into your video.
Consider using Facebook Live
One of Facebook newest features is Facebook Live. Like Periscope, it offers the possibility to do a live broadcast (hence the name). Metrics are limited at this point, but you can discover the number of (unique) viewers. In a recent blog post, Facebook announced that Live video will get priority in news feeds, making it even more interesting to explore the possibilities.
Experiment with Canvas
Facebook Canvas gives you the opportunity to build a unique experience, combining text, images, carousels, product feeds and of course, videos and animations. One of the biggest features of Canvas is that it will load videos almost 10 times faster than other mobile apps, decreasing the chances of someone losing interest due to long loading times. A great example of the possibilities of Facebook Canvas is this video from the Dutch Hartstichting (heart foundation):
Go square
This is one of those 'why didn't we think of this before' tips, but here goes: a recent study by Buzzfeed revealed that 3 in 4 of the most shared videos are square. Why? Because people don't like tipping their phone to watch a video in full screen. A big plus of going square is that your videos can be used for Instagram - if you keep 'm shorter than 15 seconds, of course. Take this video by Chio for example:
""Full moon? We're mostly looking at the Space Raiders. What about you?""
Add a call to action
Just like any piece of quality content, a good call to action is half the battle. To add a call to action to your Facebook video, go to 'Edit Video' and click the 'Call to Action' tab. Here you can select a CTA button to add to the end of your video. You can choose from the following:
Shop now
Book now
Learn more
Sign up
Download
Watch more
Just add UTM tagging to your URL, and the effect of your CTA will show up in Google Analytics.
And last, but definitely not least:
Take audience building to the next level
Facebook recently launched a new Audience option: Engagement Audiences. With this option, you can create a custom audience, based on the amount of time someone spent watching your video. These are the options available:
People who viewed at least 3 seconds of your video
People who viewed at least 10 seconds of your video
People who viewed at least 25% of your video
People who viewed at least 50% of your video
People who viewed at least 75% of your video
People who viewed at least 95% of your video
You can also specify the time since their last engagement (up to 180 days). The difference with the old Video View Custom Audiences is that you can select several videos, instead of just one. Very useful if you're running a campaign. By dividing your video audience by engagement rate, you take the first step towards providing each custom audience with even more relevant video content. For instance by excluding people who already viewed at least 75% of your latest video from your next ad with the same video. Or by retargeting them towards sales, since they're already highly engaged.
Besides that, using Engagement Audiences is a great strategy to convert more of your audience into leads, or to increase traffic to your own domain, leading your viewers to additional information about the campaign or the subject of your video.
A Facebook video view might very well be the first contact in the entire buyer's cycle. If you look at it this way, it makes sense to develop a more integrated strategy and a more dynamic approach to content creation. Hopefully, this blog post convinced you that Facebook video is very much worth considering!
Bonus tip: Time is precious, so you might want to mention the video's duration in the video post. People are more likely to watch the video if they know the length.
This post was previously published in Dutch on Emerce.","[Analytics, Facebook, Video, Social advertising]"
44,How to create a random data view in Google Analytics,/how-to-create-a-random-data-view-in-google-analytics/,"
            <p>If you're a web analyst, and you’re really proud of a analytics solution or report, you might want to share that solution with others. The hard part is that the company, agency or startup you're working for often doesn’t want you to share the data publicly. And if they do, they'll require you to randomise the data. What if randomising your data set was as easy as opening a view in Google Analytics?</p>

<h2 id=""thehassleofgeneratingrandomiseddata"">The hassle of generating randomised data</h2>

<p>When people ask me to randomise a data set, I normally follow these four steps:</p>

<ol>
<li>Find or set up the report in Google Analytics.  </li>
<li>Export the data.  </li>
<li>Randomise the data.  </li>
<li>Display the data in a format totally different from the Google Analytics interface.</li>
</ol>

<p>These are four steps that might discourage people from even thinking about sharing an awesome analytics solution. Luckily, there’s a way to generate this data set without any hassle.</p>

<h2 id=""abetterwayofgeneratingrandomiseddata"">A better way of generating randomised data</h2>

<p>The solution to our problem is easy: <strong>create a random selection of data by excluding a percentage of IP addresses</strong>. At TMT, we’ve tested this with the following filter:</p>

<p><code>^.*((2[4-9])|(3[0-6])|(6[0-8])|(7[5-9])).*$</code></p>

<p>This filter will exclude all IP addresses that contain any number within the following ranges of numbers:</p>

<ul>
<li>24-29</li>
<li>30-36</li>
<li>60-68</li>
<li>75-79</li>
</ul>

<p>At the beginning of this year, we created a new view in our TMT Google Analytics account. After a test run of 16 weeks we found that <strong>this filter has excluded 20.6% to 24.6% of our sessions</strong> (22,9% on average), <strong>and 19.8% to 26.8% of our full reads</strong> (23,2% on average). Here’s a graph that gives you an idea of how the filter changes the session and transaction (full read) data over time:</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/May/tmt_randomiseddata_1-1462214473160.PNG"" alt=""The Marketing Technologist Randomised sessions and transactions"" class=""full-img""></p>

<p><em>Filtered sessions and transactions of The Marketing Technologist per week.</em></p>

<p>The conversion rate is also an interesting number to look at: how does the filter change the performance of our website? </p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/May/tmt_randomiseddata_2-1462214478810.PNG"" alt=""The Marketing Technologist Randomised conversion rate"" class=""full-img""></p>

<p><em>Change in conversion rate of The Marketing Technologist per week.</em></p>

<p>As you can see, <strong>the change in conversion rate ranges from -8.82% to +10.93%</strong>, the average change was +1.56%. </p>

<p>The average conversion rate for the standard profile was 35,6% over the 16 weeks, while the randomised profile shows an average of 36,2%. Though the number is different, it roughly shows the same performance as both numbers round off to 36%. </p>

<h2 id=""isthisthesolutiontosharingdatapublicly"">Is this the solution to sharing data publicly?</h2>

<p>Looking at the results for TMT, roughly the same share of sessions and transactions are filtered out on average (it only differs by 0,3% over 16 weeks). This makes the conversion rate is roughly the same on a higher level.</p>

<p>The good thing is that when you trend your data per week, or pick a small date range to analyse, it won’t show your actual performance. And if you don’t share your randomised IP filter, there’s no way of knowing how big a chunk of data the reports you’re sharing are missing. So the filter certainly works for volumes, but it's less effective for randomising performance.</p>

<p>What do you think about the idea of randomising a Google Analytics profile based on a range of IP addresses? Let me know in the comments. </p>
        ","If you're a web analyst, and you’re really proud of a analytics solution or report, you might want to share that solution with others. The hard part is that the company, agency or startup you're working for often doesn’t want you to share the data publicly. And if they do, they'll require you to randomise the data. What if randomising your data set was as easy as opening a view in Google Analytics?
The hassle of generating randomised data
When people ask me to randomise a data set, I normally follow these four steps:
Find or set up the report in Google Analytics.
Export the data.
Randomise the data.
Display the data in a format totally different from the Google Analytics interface.
These are four steps that might discourage people from even thinking about sharing an awesome analytics solution. Luckily, there’s a way to generate this data set without any hassle.
A better way of generating randomised data
The solution to our problem is easy: create a random selection of data by excluding a percentage of IP addresses. At TMT, we’ve tested this with the following filter:
^.*((2[4-9])|(3[0-6])|(6[0-8])|(7[5-9])).*$
This filter will exclude all IP addresses that contain any number within the following ranges of numbers:
24-29
30-36
60-68
75-79
At the beginning of this year, we created a new view in our TMT Google Analytics account. After a test run of 16 weeks we found that this filter has excluded 20.6% to 24.6% of our sessions (22,9% on average), and 19.8% to 26.8% of our full reads (23,2% on average). Here’s a graph that gives you an idea of how the filter changes the session and transaction (full read) data over time:
Filtered sessions and transactions of The Marketing Technologist per week.
The conversion rate is also an interesting number to look at: how does the filter change the performance of our website?
Change in conversion rate of The Marketing Technologist per week.
As you can see, the change in conversion rate ranges from -8.82% to +10.93%, the average change was +1.56%.
The average conversion rate for the standard profile was 35,6% over the 16 weeks, while the randomised profile shows an average of 36,2%. Though the number is different, it roughly shows the same performance as both numbers round off to 36%.
Is this the solution to sharing data publicly?
Looking at the results for TMT, roughly the same share of sessions and transactions are filtered out on average (it only differs by 0,3% over 16 weeks). This makes the conversion rate is roughly the same on a higher level.
The good thing is that when you trend your data per week, or pick a small date range to analyse, it won’t show your actual performance. And if you don’t share your randomised IP filter, there’s no way of knowing how big a chunk of data the reports you’re sharing are missing. So the filter certainly works for volumes, but it's less effective for randomising performance.
What do you think about the idea of randomising a Google Analytics profile based on a range of IP addresses? Let me know in the comments.","[Analytics, google analytics, Data, random data]"
45,A GHG Labs concept: The first worldwide web loyalty system,/the-first-world-wide-web-loyalty-system/,"
            <p><p style=""color:#7e4396; font-weight:bold"">This post concerns Greenhouse Group Labs, an innovation program for students, established by Greenhouse Group. Labs is an ideal opportunity to test the latest technologies available, while allowing for talented young individuals to deeply explore them and come up with groundbreaking solutions.</p>AdBlock. The very word sends chills down the spine of the whole digital advertising world. Yet for many consumers, it's the hero they desperately need. To them, obtrusive digital ads have become a constant annoyance during their regular online activities. Although several ads are actually helpful or entertaining, there are just as many that are too long to watch, too loud or simply irrelevant. The worst intruders are pop-up ads, especially when they're almost impossible to get rid of. </p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/May/infographicforfinalusecorrectedwithalaxmarks-1462180996849.png"" alt="""" class=""full-img""></p>

<p>As you can see, long duration, popping up and unwanted sound are the main reasons people dislike ads. But curiously enough, no one mentions privacy invasion. This will be important later on! </p>

<p>AdBlock allows for an ad-free online experience. But there are some downsides, even for the user. AdBlock often creates empty spaces on the website where ads are intended to be. This can disrupt the whole outlook of the website. In addition, a lack of ads also means that consumers get less information on new products, exclusive deals or relevant services. Information that can help them in their customer journey. Also, it prevents them from being entertained by ads that are actually well thought-out and fun. (And yes, usually only the creators of these ads think this is a shame, but still)</p>

<p>All advertisers and publishers fear a rise in AdBlock-users, yet many stay idle. The cold hard truth is they either change or slowly become obsolete. In 2015, 21.8 billion dollars in ad revenue was lost because of AdBlock. And it's only getting worse. So is there really no way to have digital ads that are beneficial to both advertiser and user? Of course, there is. The real question is: are digital advertisers willing to innovate to achieve this holy grail? We certainly are!</p>

<p>To turn this thing around, we need to work on three things. First, we have to encourage consumers to disable their ad blocker. Secondly, we need to make sure the ads they see are actually relevant and interesting. Finally, we need to deliver the ads in a way that isn't obtrusive. Easier said than done. Nevertheless, this change has to happen for advertising to stay afloat.</p>

<p>At <a href=""https://jobs.greenhousegroup.com/labs"">GHG Labs</a>, we're working on a concept that might be able to accomplish all of the things mentioned above. In the past few weeks, we’ve been developing a prototype of the world's first web-based loyalty system. So, how does it work? Research indicates that people don't mind sharing personal details, as long as their privacy is protected and they get something good in return. </p>

<p>In our loyalty system, users provide us with their personal information, so that we can show them ads based on their situation and interests. In return, the user receives credits for every ad they view. They can spend these credits on products or discounts. A win-win situation: first of all, advertisers get better insights into the preferences of their audience, and the audience gets more relevant ads. On top of that, the ads will most likely have a higher effectiveness because they better suit the needs and interests of their audience.   </p>

<p>Curious to learn more? In the next couple of weeks, we'll keep you posted on our progress. Stay tuned for the next part!</p>
        ","This post concerns Greenhouse Group Labs, an innovation program for students, established by Greenhouse Group. Labs is an ideal opportunity to test the latest technologies available, while allowing for talented young individuals to deeply explore them and come up with groundbreaking solutions.
AdBlock. The very word sends chills down the spine of the whole digital advertising world. Yet for many consumers, it's the hero they desperately need. To them, obtrusive digital ads have become a constant annoyance during their regular online activities. Although several ads are actually helpful or entertaining, there are just as many that are too long to watch, too loud or simply irrelevant. The worst intruders are pop-up ads, especially when they're almost impossible to get rid of.
As you can see, long duration, popping up and unwanted sound are the main reasons people dislike ads. But curiously enough, no one mentions privacy invasion. This will be important later on!
AdBlock allows for an ad-free online experience. But there are some downsides, even for the user. AdBlock often creates empty spaces on the website where ads are intended to be. This can disrupt the whole outlook of the website. In addition, a lack of ads also means that consumers get less information on new products, exclusive deals or relevant services. Information that can help them in their customer journey. Also, it prevents them from being entertained by ads that are actually well thought-out and fun. (And yes, usually only the creators of these ads think this is a shame, but still)
All advertisers and publishers fear a rise in AdBlock-users, yet many stay idle. The cold hard truth is they either change or slowly become obsolete. In 2015, 21.8 billion dollars in ad revenue was lost because of AdBlock. And it's only getting worse. So is there really no way to have digital ads that are beneficial to both advertiser and user? Of course, there is. The real question is: are digital advertisers willing to innovate to achieve this holy grail? We certainly are!
To turn this thing around, we need to work on three things. First, we have to encourage consumers to disable their ad blocker. Secondly, we need to make sure the ads they see are actually relevant and interesting. Finally, we need to deliver the ads in a way that isn't obtrusive. Easier said than done. Nevertheless, this change has to happen for advertising to stay afloat.
At GHG Labs, we're working on a concept that might be able to accomplish all of the things mentioned above. In the past few weeks, we’ve been developing a prototype of the world's first web-based loyalty system. So, how does it work? Research indicates that people don't mind sharing personal details, as long as their privacy is protected and they get something good in return.
In our loyalty system, users provide us with their personal information, so that we can show them ads based on their situation and interests. In return, the user receives credits for every ad they view. They can spend these credits on products or discounts. A win-win situation: first of all, advertisers get better insights into the preferences of their audience, and the audience gets more relevant ads. On top of that, the ads will most likely have a higher effectiveness because they better suit the needs and interests of their audience.
Curious to learn more? In the next couple of weeks, we'll keep you posted on our progress. Stay tuned for the next part!","[Labs, Analytics, Adblocker]"
46,Multichannel problem (partly) solved with postal codes,/multichannel-problem-partly-solved-with-postal-codes/,"
            <p>Postal codes play an important role in today's e-commerce. They are used to get online orders delivered to your home, food boxes sent to the right address and to get postmen to their destination as fast as possible. So, it's obvious that the postal code is essential in the logistic process, but what more can we do with it? </p>

<p>Is it possible to use the postal code for other, even smarter purposes?</p>

<p>One of our clients, a furniture retailer, has almost one hundred physical stores across the country and runs a very successful web shop. As with a lot of retail companies, their omnichannel approach is a present-day topic for them. Whether the customer is shopping online from a desktop or mobile device, or offline in a physical store, they deserve a seamless experience. How can we serve the customer across all points of sale, and how can we apply our online marketing activities to support this multichannel strategy? And, more importantly (at least for us), how can we measure the offline effects of our online efforts?</p>

<h2 id=""project1234ab"">Project 1234 AB</h2>

<p>Enter 'Project 1234 AB' (1234 AB is the format for postal codes in The Netherlands): an experiment in using postal codes to solve the multichannel problem of not knowing what your online customers are doing offline and vice versa. So, how do we get those postal codes? On the website of our client, there are three places where the customer is asked to leave their postal code:</p>

<ul>
<li>The product detail page: by entering the postal code, the user can check the stock availability of a product in the nearest store.</li>
<li>The Store Locator: enter your postal code to find the nearest store.</li>
<li>With every order, to make sure the products are be delivered to the correct address.</li>
</ul>

<p>By registering the postal codes on one of the three mentioned moments in Google Analytics, we can match them to offline sales. Because the retailer also asks offline shoppers for their postal codes with every store purchase.</p>

<p>Of course, the postal code itself is not unique to a single customer: you'd need the house number too. Unfortunately, the house number is not collected on the website. Based on our analysis, it turns out that the total of sales in most postal codes is tiny. Thus, only having the zip code (without the house number) is sufficient to get good insights.</p>

<p>It’s important to realise the postal code match doesn’t give us an inclusive overview in multichannel behavior as a whole. There are users that won’t fill in their postal code, simply because they already know where the closest store is. Also, if a user checks his postal code at home and decides to make a purchase at a store near his office, it’s impossible to match the search to the specific purchase. Then there's always a percentage of offline customers not willing to give their postal code.</p>

<h2 id=""insights"">Insights</h2>

<p>Despite these limitations, matching the postal codes has lead to the following insights:</p>

<ol>
<li><p><strong>Research online purchase offline (ROPO)</strong> <br>
We all know there's a connection between visits to a website and purchases in a physical store. Matching postal codes made it possible to proof the correlation for our client. Despite the mentioned limitations, the ROPO (or multi-channel) effect was far bigger than expected. A significant part of the turnover by offline purchases could be traced back to a customer's orientation on the website. </p></li>
<li><p><strong>Revaluation of the media mix</strong> <br>
Now, with the matched postal code data in place, we can measure our online marketing campaign's success by the number of sales in the webshop, but also by the sales in the offline stores. Because we can now optimise on the total turnover, we see a shift in budgets and a change in the online media mix. Media that hadn't been cost-effective before, have been analysed again by allocating the total turnover to different sources. This has lead to a revaluation of the online media mix.</p></li>
<li><p><strong>Retargeting strategy</strong> <br>
The postal code data gave us another interesting insight. By matching the date that the postal codes were collected on the website to the time of purchase in the psychical store, we were able to see how much time generally passed between these two moments. This, of course, is of exceptional value to your retargeting strategy. It gives you leads on how long you should retarget and what message is most effective in this effort.</p></li>
</ol>

<h2 id=""multichannelimpulse"">Multichannel impulse</h2>

<p>This first analyses on postal code level was a simple experiment, but the insights were outstanding. Reason enough to start automating the process and structurally analysing the data. 'Project 1234 AB' has given multichannel thinking a big impulse. At first, the postal code was only needed to get the ordered products to the customer. Nowadays, the four numbers and two letters are important data that are needed to better understand and service the customers, through every channel.</p>

<p><em>Cover image: <a href=""https://www.flickr.com/photos/at-photos/2371616700/"">Alastair Thompson</a>, licence: <a href=""http://creativecommons.org/licenses/by-nc-nd/4.0"">CC BY-NC-ND</a></em></p>
        ","Postal codes play an important role in today's e-commerce. They are used to get online orders delivered to your home, food boxes sent to the right address and to get postmen to their destination as fast as possible. So, it's obvious that the postal code is essential in the logistic process, but what more can we do with it?
Is it possible to use the postal code for other, even smarter purposes?
One of our clients, a furniture retailer, has almost one hundred physical stores across the country and runs a very successful web shop. As with a lot of retail companies, their omnichannel approach is a present-day topic for them. Whether the customer is shopping online from a desktop or mobile device, or offline in a physical store, they deserve a seamless experience. How can we serve the customer across all points of sale, and how can we apply our online marketing activities to support this multichannel strategy? And, more importantly (at least for us), how can we measure the offline effects of our online efforts?
Project 1234 AB
Enter 'Project 1234 AB' (1234 AB is the format for postal codes in The Netherlands): an experiment in using postal codes to solve the multichannel problem of not knowing what your online customers are doing offline and vice versa. So, how do we get those postal codes? On the website of our client, there are three places where the customer is asked to leave their postal code:
The product detail page: by entering the postal code, the user can check the stock availability of a product in the nearest store.
The Store Locator: enter your postal code to find the nearest store.
With every order, to make sure the products are be delivered to the correct address.
By registering the postal codes on one of the three mentioned moments in Google Analytics, we can match them to offline sales. Because the retailer also asks offline shoppers for their postal codes with every store purchase.
Of course, the postal code itself is not unique to a single customer: you'd need the house number too. Unfortunately, the house number is not collected on the website. Based on our analysis, it turns out that the total of sales in most postal codes is tiny. Thus, only having the zip code (without the house number) is sufficient to get good insights.
It’s important to realise the postal code match doesn’t give us an inclusive overview in multichannel behavior as a whole. There are users that won’t fill in their postal code, simply because they already know where the closest store is. Also, if a user checks his postal code at home and decides to make a purchase at a store near his office, it’s impossible to match the search to the specific purchase. Then there's always a percentage of offline customers not willing to give their postal code.
Insights
Despite these limitations, matching the postal codes has lead to the following insights:
Research online purchase offline (ROPO)
We all know there's a connection between visits to a website and purchases in a physical store. Matching postal codes made it possible to proof the correlation for our client. Despite the mentioned limitations, the ROPO (or multi-channel) effect was far bigger than expected. A significant part of the turnover by offline purchases could be traced back to a customer's orientation on the website.
Revaluation of the media mix
Now, with the matched postal code data in place, we can measure our online marketing campaign's success by the number of sales in the webshop, but also by the sales in the offline stores. Because we can now optimise on the total turnover, we see a shift in budgets and a change in the online media mix. Media that hadn't been cost-effective before, have been analysed again by allocating the total turnover to different sources. This has lead to a revaluation of the online media mix.
Retargeting strategy
The postal code data gave us another interesting insight. By matching the date that the postal codes were collected on the website to the time of purchase in the psychical store, we were able to see how much time generally passed between these two moments. This, of course, is of exceptional value to your retargeting strategy. It gives you leads on how long you should retarget and what message is most effective in this effort.
Multichannel impulse
This first analyses on postal code level was a simple experiment, but the insights were outstanding. Reason enough to start automating the process and structurally analysing the data. 'Project 1234 AB' has given multichannel thinking a big impulse. At first, the postal code was only needed to get the ordered products to the customer. Nowadays, the four numbers and two letters are important data that are needed to better understand and service the customers, through every channel.
Cover image: Alastair Thompson, licence: CC BY-NC-ND","[Analytics, Multichannel, Offline]"
47,Upload your local Spark script to an AWS EMR cluster using a simple Python script,/upload-your-local-spark-script-to-an-aws-emr-cluster-using-a-simply-python-script/,"
            <p><strong><a href=""http://spark.apache.org/"">Apache Spark</a></strong> is definitely one of the hottest topics in the Data Science community at the moment. Last month when we visited <a href=""http://pydata.org/amsterdam2016/"">PyData Amsterdam 2016</a> we witnessed a great example of Spark's immense popularity. The speakers at PyData talking about Spark had the largest crowds after all.</p>

<p>Sometimes we see that these popular topics are slowly transforming in <em>buzzwords</em> that are abused for generating publicity, e.g. words as <em>data scientist</em> and <em>deep learning</em> but also <em>Hadoop</em> and <em>DMP</em>. I don't hope that Spark will suffer the same fate as it is definitely a powerful tool for data scientists. In the field of distributed computing Spark provides much more flexibility than MapReduce. Additionally, Spark uses memory more efficiently and therefore writes less data to disk than MapReduce, making Spark on average around 10 to 100 times faster. </p>

<p>In this article we introduce a method to upload our local Spark applications to an <strong>Amazon Web Services (AWS)</strong> cluster in a programmatic manner using a simple Python script. The benefit of doing this programmatically compared to interactively is that it is easier to schedule a Python script to run daily. Additionally, it also saves us time. Time we can spend better by drinking more coffee and thinking of new ideas!</p>

<h2 id=""thechallenge"">The challenge</h2>

<p>For one of our Data Science applications we recently decided to create a new part of the data pipeline with <code>PySpark</code> (Spark in Python). For now, I am not going to elaborate on how to build your own Spark applications as there are already <a href=""https://districtdatalabs.silvrback.com/getting-started-with-spark-in-python"">plenty</a> of <a href=""http://www.tutorialspoint.com/apache_spark/apache_spark_core_programming.htm"">tutorials</a> on <a href=""https://www.dataquest.io/mission/123/introduction-to-spark/"">how to do so</a> on the <a href=""https://www.youtube.com/watch?v=dQw4w9WgXcQ"">world wide web</a>. <br>
As usual we started by creating the Spark application using only a subset of the full dataset. This subset is usually small enough to test the Spark application locally on our laptops. Then, after creating a locally working Spark application, we scale the application up using an AWS <strong>Elastic Map Reduce (EMR)</strong> cluster to process the full dataset. <br>
However, this is where we ran into some inconvenient issues. The original MapReduce data pipeline was also built in Python using the <a href=""https://pythonhosted.org/mrjob/""><code>MRjob</code></a> module. <code>MRjob</code> takes away the trouble of uploading your local code to an AWS cluster by using its built-in functions. However, <code>MRJob</code> does not support Spark applications (yet?) and therefore we have to get our own hands dirty this time...</p>

<h2 id=""theinteractivemethodusingtheawscli"">The interactive method using the AWS CLI</h2>

<p>Using the <a href=""https://aws.amazon.com/cli/""><code>awscli</code></a> module we can quickly spin up an AWS EMR cluster with Spark pre-installed using the commandline.</p>

<pre><code class=""language-prettyprint lang-sh prettyprinted""><span class=""pln"">aws emr create</span><span class=""pun"">-</span><span class=""pln"">cluster \  
</span><span class=""pun"">--</span><span class=""pln"">name </span><span class=""str"">""Spark Example""</span><span class=""pln""> \
</span><span class=""pun"">--</span><span class=""pln"">release</span><span class=""pun"">-</span><span class=""pln"">label emr</span><span class=""pun"">-</span><span class=""lit"">4.4</span><span class=""pun"">.</span><span class=""lit"">0</span><span class=""pln""> \
</span><span class=""pun"">--</span><span class=""pln"">applications </span><span class=""typ"">Name</span><span class=""pun"">=</span><span class=""typ"">Hadoop</span><span class=""pln""> </span><span class=""typ"">Name</span><span class=""pun"">=</span><span class=""typ"">Spark</span><span class=""pln""> 
</span><span class=""pun"">--</span><span class=""pln"">ec2</span><span class=""pun"">-</span><span class=""pln"">attributes </span><span class=""typ"">KeyName</span><span class=""pun"">=</span><span class=""pln"">keypair\
</span><span class=""pun"">--</span><span class=""pln"">instance</span><span class=""pun"">-</span><span class=""pln"">groups </span><span class=""typ"">Name</span><span class=""pun"">=</span><span class=""typ"">EmrMaster</span><span class=""pun"">,</span><span class=""typ"">InstanceGroupType</span><span class=""pun"">=</span><span class=""pln"">MASTER</span><span class=""pun"">,</span><span class=""typ"">InstanceCount</span><span class=""pun"">=</span><span class=""lit"">1</span><span class=""pun"">,</span><span class=""typ"">InstanceType</span><span class=""pun"">=</span><span class=""pln"">m3</span><span class=""pun"">.</span><span class=""pln"">xlarge</span><span class=""pun"">,</span><span class=""typ"">BidPrice</span><span class=""pun"">=</span><span class=""lit"">0.05</span><span class=""pln""> \
</span><span class=""typ"">Name</span><span class=""pun"">=</span><span class=""typ"">EmrCore</span><span class=""pun"">,</span><span class=""typ"">InstanceGroupType</span><span class=""pun"">=</span><span class=""pln"">CORE</span><span class=""pun"">,</span><span class=""typ"">InstanceCount</span><span class=""pun"">=</span><span class=""lit"">2</span><span class=""pun"">,</span><span class=""typ"">InstanceType</span><span class=""pun"">=</span><span class=""pln"">m3</span><span class=""pun"">.</span><span class=""pln"">xlarge</span><span class=""pun"">,</span><span class=""typ"">BidPrice</span><span class=""pun"">=</span><span class=""lit"">0.05</span><span class=""pln""> \  
</span><span class=""pun"">--</span><span class=""kwd"">use</span><span class=""pun"">-</span><span class=""kwd"">default</span><span class=""pun"">-</span><span class=""pln"">roles</span></code></pre>

<p>We need to place our code onto the cluster because we do not want the run the SparkContext on our local computer due to increased latency and availability. Therefore, we SSH into the cluster. On the cluster we create a Python file, e.g. <code>run.py</code>, and copy/paste the code for the Spark application.  </p>

<pre><code class=""language-prettyprint lang-sh prettyprinted""><span class=""pln"">aws emr ssh </span><span class=""pun"">--</span><span class=""pln"">cluster</span><span class=""pun"">-</span><span class=""pln"">id j</span><span class=""pun"">-</span><span class=""pln"">XXXX </span><span class=""pun"">--</span><span class=""pln"">key</span><span class=""pun"">-</span><span class=""pln"">pair</span><span class=""pun"">-</span><span class=""pln"">file keypair</span><span class=""pun"">.</span><span class=""pln"">pem  
sudo nano run</span><span class=""pun"">.</span><span class=""pln"">py  
</span><span class=""pun"">--</span><span class=""pln""> copy</span><span class=""pun"">/</span><span class=""pln"">paste </span><span class=""kwd"">local</span><span class=""pln""> code to cluster</span></code></pre>

<p>We <code>logout</code> of the cluster and add a new step to the EMR cluster to start our Spark application via <code>spark-submit</code>.</p>

<pre><code class=""language-prettyprint lang-sh prettyprinted""><span class=""pln"">aws emr add</span><span class=""pun"">-</span><span class=""pln"">steps \  
</span><span class=""pun"">--</span><span class=""pln"">cluster</span><span class=""pun"">-</span><span class=""pln"">id j</span><span class=""pun"">-</span><span class=""pln"">XXXXX \
</span><span class=""pun"">--</span><span class=""pln"">steps </span><span class=""typ"">Type</span><span class=""pun"">=</span><span class=""pln"">CUSTOM_JAR</span><span class=""pun"">,</span><span class=""typ"">Name</span><span class=""pun"">=</span><span class=""str"">""Spark Program""</span><span class=""pun"">,</span><span class=""typ"">Jar</span><span class=""pun"">=</span><span class=""str"">""command-runner.jar""</span><span class=""pun"">,</span><span class=""typ"">ActionOnFailure</span><span class=""pun"">=</span><span class=""pln"">CONTINUE</span><span class=""pun"">,</span><span class=""typ"">Args</span><span class=""pun"">=[</span><span class=""str"">""spark-submit""</span><span class=""pun"">,</span><span class=""pln"">home</span><span class=""pun"">/</span><span class=""pln"">hadoop</span><span class=""pun"">/</span><span class=""pln"">run</span><span class=""pun"">.</span><span class=""pln"">py</span><span class=""pun"">]</span></code></pre>

<p>Note that Amazons EMR clusters have access to S3 buckets (if the IAM roles are configured properly though). Therefore, we do not need to add other steps to copy our data back and forth between S3 and the cluster. We can just specify the proper S3 bucket in our Spark application by using for example  </p>

<pre><code class=""language-prettyprint lang-python prettyprinted""><span class=""pln"">data </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""pun"">(</span><span class=""str"">""s3://input_bucket/*""</span><span class=""pun"">)</span><span class=""pln"">  </span></code></pre>

<p>or  </p>

<pre><code class=""language-prettyprint lang-python prettyprinted""><span class=""pln"">data </span><span class=""pun"">=</span><span class=""pln""> saveAsTextFile</span><span class=""pun"">(</span><span class=""str"">""s3://output_bucket/""</span><span class=""pun"">)</span><span class=""pln"">  </span></code></pre>

<p>Unfortunately, this S3 connection only works within our Spark application. We cannot run a Spark Python script hosted on S3 by <code>spark-submit s3://bucket/spark_code.py</code>...</p>

<h2 id=""thisisstilltoomuchwork"">This is still too much work...</h2>

<p>Using the AWS command-line interface and the above commands we can interactively move our local Spark application to an AWS cluster. However, this interactive method is not easy to schedule daily as it requires some manual steps. <strong>Especially SSHing into the cluster and copy-pasting our local code to the cluster itself is tricky</strong>. It also does not fit well in our current Python data pipeline. Therefore, we prefer a more programmatic method in Python. This Python script can then be easily scheduled to run daily/weekly/monthly.</p>

<h2 id=""thefinalsolution"">The final solution</h2>

<p>Therefore we developed a simple Python script to execute all the necessary steps. The biggest challenge was <strong>how to 'copy/paste' our local code onto the cluster without using SSH?</strong> The solution for this problem turned out to be relatively easy. That is, we compress our local Spark script in a single file, upload this file to a temporary S3 bucket and add a Bootstrap action to the cluster that downloads and decompresses this file. </p>

<p>Hence, the final solution consists of the following steps executed in Python using <a href=""https://boto3.readthedocs.org/en/latest/""><code>Boto3</code></a> (an AWS SDK for Python):</p>

<ul>
<li>Define a S3 bucket to store our files temporarily and check if it exists</li>
</ul>

<pre><code class=""language-prettyprint lang-python prettyprinted""><span class=""kwd"">def</span><span class=""pln""> temp_bucket_exists</span><span class=""pun"">(</span><span class=""kwd"">self</span><span class=""pun"">,</span><span class=""pln""> s3</span><span class=""pun"">):</span><span class=""pln"">  
    </span><span class=""kwd"">try</span><span class=""pun"">:</span><span class=""pln"">
        s3</span><span class=""pun"">.</span><span class=""pln"">meta</span><span class=""pun"">.</span><span class=""pln"">client</span><span class=""pun"">.</span><span class=""pln"">head_bucket</span><span class=""pun"">(</span><span class=""typ"">Bucket</span><span class=""pun"">=</span><span class=""kwd"">self</span><span class=""pun"">.</span><span class=""pln"">s3_bucket_temp_files</span><span class=""pun"">)</span><span class=""pln"">
    </span><span class=""kwd"">except</span><span class=""pln""> botocore</span><span class=""pun"">.</span><span class=""pln"">exceptions</span><span class=""pun"">.</span><span class=""typ"">ClientError</span><span class=""pln""> </span><span class=""kwd"">as</span><span class=""pln""> e</span><span class=""pun"">:</span><span class=""pln"">
        </span><span class=""com""># If a client error is thrown, then check that it was a 404 error.</span><span class=""pln"">
        </span><span class=""com""># If it was a 404 error, then the bucket does not exist.</span><span class=""pln"">
        error_code </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""kwd"">int</span><span class=""pun"">(</span><span class=""pln"">e</span><span class=""pun"">.</span><span class=""pln"">response</span><span class=""pun"">[</span><span class=""str"">'Error'</span><span class=""pun"">][</span><span class=""str"">'Code'</span><span class=""pun"">])</span><span class=""pln"">
        </span><span class=""kwd"">if</span><span class=""pln""> error_code </span><span class=""pun"">==</span><span class=""pln""> </span><span class=""lit"">404</span><span class=""pun"">:</span><span class=""pln"">
            terminate</span><span class=""pun"">(</span><span class=""str"">""Bucket for temporary files does not exist""</span><span class=""pun"">)</span><span class=""pln"">
        terminate</span><span class=""pun"">(</span><span class=""str"">""Error while connecting to Bucket""</span><span class=""pun"">)</span><span class=""pln"">
    </span><span class=""kwd"">return</span><span class=""pln""> </span><span class=""kwd"">true</span></code></pre>

<ul>
<li>Compress the Python files of the Spark application to a <code>.tar</code> file.</li>
</ul>

<pre><code class=""language-prettyprint lang-python prettyprinted""><span class=""kwd"">def</span><span class=""pln""> tar_python_script</span><span class=""pun"">(</span><span class=""kwd"">self</span><span class=""pun"">):</span><span class=""pln"">  
    </span><span class=""com""># Create tar.gz file</span><span class=""pln"">
    t_file </span><span class=""pun"">=</span><span class=""pln""> tarfile</span><span class=""pun"">.</span><span class=""pln"">open</span><span class=""pun"">(</span><span class=""str"">""files/script.tar.gz""</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""str"">'w:gz'</span><span class=""pun"">)</span><span class=""pln"">
    </span><span class=""com""># Add Spark script path to tar.gz file</span><span class=""pln"">
    files </span><span class=""pun"">=</span><span class=""pln""> os</span><span class=""pun"">.</span><span class=""pln"">listdir</span><span class=""pun"">(</span><span class=""kwd"">self</span><span class=""pun"">.</span><span class=""pln"">path_script</span><span class=""pun"">)</span><span class=""pln"">
    </span><span class=""kwd"">for</span><span class=""pln""> f </span><span class=""kwd"">in</span><span class=""pln""> files</span><span class=""pun"">:</span><span class=""pln"">
        t_file</span><span class=""pun"">.</span><span class=""pln"">add</span><span class=""pun"">(</span><span class=""kwd"">self</span><span class=""pun"">.</span><span class=""pln"">path_script </span><span class=""pun"">+</span><span class=""pln""> f</span><span class=""pun"">,</span><span class=""pln""> arcname</span><span class=""pun"">=</span><span class=""pln"">f</span><span class=""pun"">)</span><span class=""pln"">
    t_file</span><span class=""pun"">.</span><span class=""pln"">close</span><span class=""pun"">()</span></code></pre>

<ul>
<li>Upload the <code>tar</code> file to the S3 bucket for temporary files.</li>
</ul>

<pre><code class=""language-prettyprint lang-python prettyprinted""><span class=""kwd"">def</span><span class=""pln""> upload_temp_files</span><span class=""pun"">(</span><span class=""kwd"">self</span><span class=""pun"">,</span><span class=""pln""> s3</span><span class=""pun"">):</span><span class=""pln"">  
    </span><span class=""com""># Shell file: setup (download S3 files to local machine)</span><span class=""pln"">
    s3</span><span class=""pun"">.</span><span class=""typ"">Object</span><span class=""pun"">(</span><span class=""kwd"">self</span><span class=""pun"">.</span><span class=""pln"">s3_bucket_temp_files</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""kwd"">self</span><span class=""pun"">.</span><span class=""pln"">job_name </span><span class=""pun"">+</span><span class=""pln""> </span><span class=""str"">'/setup.sh'</span><span class=""pun"">).</span><span class=""pln"">put</span><span class=""pun"">(</span><span class=""pln"">
       </span><span class=""typ"">Body</span><span class=""pun"">=</span><span class=""pln"">open</span><span class=""pun"">(</span><span class=""str"">'files/setup.sh'</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""str"">'rb'</span><span class=""pun"">),</span><span class=""pln""> </span><span class=""typ"">ContentType</span><span class=""pun"">=</span><span class=""str"">'text/x-sh'</span><span class=""pln"">
    </span><span class=""pun"">)</span><span class=""pln"">
    </span><span class=""com""># Shell file: Terminate idle cluster</span><span class=""pln"">
    s3</span><span class=""pun"">.</span><span class=""typ"">Object</span><span class=""pun"">(</span><span class=""kwd"">self</span><span class=""pun"">.</span><span class=""pln"">s3_bucket_temp_files</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""kwd"">self</span><span class=""pun"">.</span><span class=""pln"">job_name </span><span class=""pun"">+</span><span class=""pln""> </span><span class=""str"">'/terminate_idle_cluster.sh'</span><span class=""pun"">).</span><span class=""pln"">put</span><span class=""pun"">(</span><span class=""pln"">
        </span><span class=""typ"">Body</span><span class=""pun"">=</span><span class=""pln"">open</span><span class=""pun"">(</span><span class=""str"">'files/terminate_idle_cluster.sh'</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""str"">'rb'</span><span class=""pun"">),</span><span class=""pln""> </span><span class=""typ"">ContentType</span><span class=""pun"">=</span><span class=""str"">'text/x-sh'</span><span class=""pln"">
    </span><span class=""pun"">)</span><span class=""pln"">
    </span><span class=""com""># Compressed Python script files (tar.gz)</span><span class=""pln"">
    s3</span><span class=""pun"">.</span><span class=""typ"">Object</span><span class=""pun"">(</span><span class=""kwd"">self</span><span class=""pun"">.</span><span class=""pln"">s3_bucket_temp_files</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""kwd"">self</span><span class=""pun"">.</span><span class=""pln"">job_name </span><span class=""pun"">+</span><span class=""pln""> </span><span class=""str"">'/script.tar.gz'</span><span class=""pun"">).</span><span class=""pln"">put</span><span class=""pun"">(</span><span class=""pln"">
        </span><span class=""typ"">Body</span><span class=""pun"">=</span><span class=""pln"">open</span><span class=""pun"">(</span><span class=""str"">'files/script.tar.gz'</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""str"">'rb'</span><span class=""pun"">),</span><span class=""pln""> </span><span class=""typ"">ContentType</span><span class=""pun"">=</span><span class=""str"">'application/x-tar'</span><span class=""pln"">
    </span><span class=""pun"">)</span><span class=""pln"">        </span></code></pre>

<ul>
<li>Spin up an AWS EMR cluster with Hadoop and Spark as application plus two bootstrap actions. One bootstrap action is a shell script which downloads the <code>tar</code> file from our temporary files S3 bucket and decompresses the <code>tar</code> file on the remote cluster. The other bootstrap action ensures that the cluster is terminated after an hour of inactivity to prevent high unexpected AWS charges.</li>
</ul>

<p><strong>setup.sh</strong></p>

<pre><code class=""language-prettyprint lang-sh prettyprinted""><span class=""com"">#!/bin/bash</span><span class=""pln"">
</span><span class=""com""># Parse arguments</span><span class=""pln"">
s3_bucket_script</span><span class=""pun"">=</span><span class=""str"">""$1/script.tar.gz""</span><span class=""pln"">  
</span><span class=""com""># Download compressed script tar file from S3</span><span class=""pln"">
aws s3 cp $s3_bucket_script</span><span class=""pun"">/</span><span class=""pln"">home</span><span class=""pun"">/</span><span class=""pln"">hadoop</span><span class=""pun"">/</span><span class=""pln"">script</span><span class=""pun"">.</span><span class=""pln"">tar</span><span class=""pun"">.</span><span class=""pln"">gz  
</span><span class=""com""># Untar file</span><span class=""pln"">
tar zxvf </span><span class=""str"">""/home/hadoop/script.tar.gz""</span><span class=""pln""> </span><span class=""pun"">-</span><span class=""pln"">C </span><span class=""pun"">/</span><span class=""pln"">home</span><span class=""pun"">/</span><span class=""pln"">hadoop</span><span class=""pun"">/</span><span class=""pln"">  
</span><span class=""com""># Install requirements for additional Python modules (uncomment if needed)</span><span class=""pln"">
</span><span class=""com""># sudo python2.7 -m pip install pandas</span></code></pre>

<p><strong>.py</strong></p>

<pre><code class=""language-prettyprint lang-python prettyprinted""><span class=""kwd"">def</span><span class=""pln""> start_spark_cluster</span><span class=""pun"">(</span><span class=""kwd"">self</span><span class=""pun"">,</span><span class=""pln""> c</span><span class=""pun"">):</span><span class=""pln"">  
    response </span><span class=""pun"">=</span><span class=""pln""> c</span><span class=""pun"">.</span><span class=""pln"">run_job_flow</span><span class=""pun"">(</span><span class=""pln"">
        </span><span class=""typ"">Name</span><span class=""pun"">=</span><span class=""kwd"">self</span><span class=""pun"">.</span><span class=""pln"">job_name</span><span class=""pun"">,</span><span class=""pln"">
        </span><span class=""typ"">ReleaseLabel</span><span class=""pun"">=</span><span class=""str"">""emr-4.4.0""</span><span class=""pun"">,</span><span class=""pln"">
        </span><span class=""typ"">Instances</span><span class=""pun"">={</span><span class=""pln"">
            </span><span class=""str"">'InstanceGroups'</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""pun"">[</span><span class=""pln"">
                </span><span class=""pun"">{</span><span class=""str"">'Name'</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""str"">'EmrMaster'</span><span class=""pun"">,</span><span class=""pln"">
                 </span><span class=""str"">'Market'</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""str"">'SPOT'</span><span class=""pun"">,</span><span class=""pln"">
                 </span><span class=""str"">'InstanceRole'</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""str"">'MASTER'</span><span class=""pun"">,</span><span class=""pln"">
                 </span><span class=""str"">'BidPrice'</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""str"">'0.05'</span><span class=""pun"">,</span><span class=""pln"">
                 </span><span class=""str"">'InstanceType'</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""str"">'m3.xlarge'</span><span class=""pun"">,</span><span class=""pln"">
                 </span><span class=""str"">'InstanceCount'</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""lit"">1</span><span class=""pun"">},</span><span class=""pln"">
                </span><span class=""pun"">{</span><span class=""str"">'Name'</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""str"">'EmrCore'</span><span class=""pun"">,</span><span class=""pln"">
                 </span><span class=""str"">'Market'</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""str"">'SPOT'</span><span class=""pun"">,</span><span class=""pln"">
                 </span><span class=""str"">'InstanceRole'</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""str"">'CORE'</span><span class=""pun"">,</span><span class=""pln"">
                 </span><span class=""str"">'BidPrice'</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""str"">'0.05'</span><span class=""pun"">,</span><span class=""pln"">
                 </span><span class=""str"">'InstanceType'</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""str"">'m3.xlarge'</span><span class=""pun"">,</span><span class=""pln"">
                 </span><span class=""str"">'InstanceCount'</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""lit"">2</span><span class=""pun"">}</span><span class=""pln"">
            </span><span class=""pun"">],</span><span class=""pln"">
            </span><span class=""str"">'Ec2KeyName'</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""kwd"">self</span><span class=""pun"">.</span><span class=""pln"">ec2_key_name</span><span class=""pun"">,</span><span class=""pln"">
            </span><span class=""str"">'KeepJobFlowAliveWhenNoSteps'</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""kwd"">False</span><span class=""pln"">
        </span><span class=""pun"">},</span><span class=""pln"">
        </span><span class=""typ"">Applications</span><span class=""pun"">=[{</span><span class=""str"">'Name'</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""str"">'Hadoop'</span><span class=""pun"">},</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""str"">'Name'</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""str"">'Spark'</span><span class=""pun"">}],</span><span class=""pln"">
        </span><span class=""typ"">JobFlowRole</span><span class=""pun"">=</span><span class=""str"">'EMR_EC2_DefaultRole'</span><span class=""pun"">,</span><span class=""pln"">
        </span><span class=""typ"">ServiceRole</span><span class=""pun"">=</span><span class=""str"">'EMR_DefaultRole'</span><span class=""pun"">,</span><span class=""pln"">
        </span><span class=""typ"">VisibleToAllUsers</span><span class=""pun"">=</span><span class=""kwd"">True</span><span class=""pun"">,</span><span class=""pln"">
        </span><span class=""typ"">BootstrapActions</span><span class=""pun"">=[</span><span class=""pln"">
            </span><span class=""pun"">{</span><span class=""str"">'Name'</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""str"">'setup'</span><span class=""pun"">,</span><span class=""pln"">
             </span><span class=""str"">'ScriptBootstrapAction'</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">
                 </span><span class=""str"">'Path'</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""str"">'s3n://{}/{}/setup.sh'</span><span class=""pun"">.</span><span class=""pln"">format</span><span class=""pun"">(</span><span class=""kwd"">self</span><span class=""pun"">.</span><span class=""pln"">s3_bucket_temp_files</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""kwd"">self</span><span class=""pun"">.</span><span class=""pln"">job_name</span><span class=""pun"">),</span><span class=""pln"">
                 </span><span class=""str"">'Args'</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""pun"">[</span><span class=""str"">'s3://{}/{}'</span><span class=""pun"">.</span><span class=""pln"">format</span><span class=""pun"">(</span><span class=""kwd"">self</span><span class=""pun"">.</span><span class=""pln"">s3_bucket_temp_files</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""kwd"">self</span><span class=""pun"">.</span><span class=""pln"">job_name</span><span class=""pun"">)]}},</span><span class=""pln"">
            </span><span class=""pun"">{</span><span class=""str"">'Name'</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""str"">'idle timeout'</span><span class=""pun"">,</span><span class=""pln"">
             </span><span class=""str"">'ScriptBootstrapAction'</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">
                 </span><span class=""str"">'Path'</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""str"">'s3n://{}/{}/terminate_idle_cluster.sh'</span><span class=""pun"">.</span><span class=""pln"">format</span><span class=""pun"">(</span><span class=""kwd"">self</span><span class=""pun"">.</span><span class=""pln"">s3_bucket_temp_files</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""kwd"">self</span><span class=""pun"">.</span><span class=""pln"">job_name</span><span class=""pun"">),</span><span class=""pln"">
                 </span><span class=""str"">'Args'</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""pun"">[</span><span class=""str"">'3600'</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""str"">'300'</span><span class=""pun"">]</span><span class=""pln"">
                    </span><span class=""pun"">}</span><span class=""pln"">
                </span><span class=""pun"">},</span><span class=""pln"">
            </span><span class=""pun"">],</span><span class=""pln"">
        </span><span class=""pun"">)</span><span class=""pln"">        </span></code></pre>

<ul>
<li>Add a step to the EMR cluster to run the Spark application using <code>spark-submit</code>.</li>
</ul>

<pre><code class=""language-prettyprint lang-python prettyprinted""><span class=""kwd"">def</span><span class=""pln""> step_spark_submit</span><span class=""pun"">(</span><span class=""kwd"">self</span><span class=""pun"">,</span><span class=""pln""> c</span><span class=""pun"">,</span><span class=""pln""> arguments</span><span class=""pun"">):</span><span class=""pln"">  
    response </span><span class=""pun"">=</span><span class=""pln""> c</span><span class=""pun"">.</span><span class=""pln"">add_job_flow_steps</span><span class=""pun"">(</span><span class=""pln"">
        </span><span class=""typ"">JobFlowId</span><span class=""pun"">=</span><span class=""kwd"">self</span><span class=""pun"">.</span><span class=""pln"">job_flow_id</span><span class=""pun"">,</span><span class=""pln"">
        </span><span class=""typ"">Steps</span><span class=""pun"">=[{</span><span class=""pln"">
            </span><span class=""str"">'Name'</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""str"">'Spark Application'</span><span class=""pun"">,</span><span class=""pln"">
            </span><span class=""str"">'ActionOnFailure'</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""str"">'CANCEL_AND_WAIT'</span><span class=""pun"">,</span><span class=""pln"">
            </span><span class=""str"">'HadoopJarStep'</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">
               </span><span class=""str"">'Jar'</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""str"">'command-runner.jar'</span><span class=""pun"">,</span><span class=""pln"">
               </span><span class=""str"">'Args'</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""pun"">[</span><span class=""str"">""spark-submit""</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""str"">""/home/hadoop/run.py""</span><span class=""pun"">,</span><span class=""pln""> arguments</span><span class=""pun"">]</span><span class=""pln"">
            </span><span class=""pun"">}</span><span class=""pln"">
        </span><span class=""pun"">}]</span><span class=""pln"">
    </span><span class=""pun"">)</span></code></pre>

<ul>
<li>Describe status of cluster until all steps are finished and cluster is terminated.</li>
</ul>

<pre><code class=""language-prettyprint lang-python prettyprinted""><span class=""kwd"">def</span><span class=""pln""> describe_status_until_terminated</span><span class=""pun"">(</span><span class=""kwd"">self</span><span class=""pun"">,</span><span class=""pln""> c</span><span class=""pun"">):</span><span class=""pln"">  
    stop </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""kwd"">False</span><span class=""pln"">
    </span><span class=""kwd"">while</span><span class=""pln""> stop </span><span class=""kwd"">is</span><span class=""pln""> </span><span class=""kwd"">False</span><span class=""pun"">:</span><span class=""pln"">
        description </span><span class=""pun"">=</span><span class=""pln""> c</span><span class=""pun"">.</span><span class=""pln"">describe_cluster</span><span class=""pun"">(</span><span class=""typ"">ClusterId</span><span class=""pun"">=</span><span class=""kwd"">self</span><span class=""pun"">.</span><span class=""pln"">job_flow_id</span><span class=""pun"">)</span><span class=""pln"">
        state </span><span class=""pun"">=</span><span class=""pln""> description</span><span class=""pun"">[</span><span class=""str"">'Cluster'</span><span class=""pun"">][</span><span class=""str"">'Status'</span><span class=""pun"">][</span><span class=""str"">'State'</span><span class=""pun"">]</span><span class=""pln"">
        </span><span class=""kwd"">if</span><span class=""pln""> state </span><span class=""pun"">==</span><span class=""pln""> </span><span class=""str"">'TERMINATED'</span><span class=""pln""> </span><span class=""kwd"">or</span><span class=""pln""> state </span><span class=""pun"">==</span><span class=""pln""> </span><span class=""str"">'TERMINATED_WITH_ERRORS'</span><span class=""pun"">:</span><span class=""pln"">
            stop </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""kwd"">True</span><span class=""pln"">
        </span><span class=""kwd"">print</span><span class=""pun"">(</span><span class=""pln"">state</span><span class=""pun"">)</span><span class=""pln"">
        time</span><span class=""pun"">.</span><span class=""pln"">sleep</span><span class=""pun"">(</span><span class=""lit"">30</span><span class=""pun"">)</span></code></pre>

<ul>
<li>Remove the temporary files from the S3 bucket when the cluster is terminated.</li>
</ul>

<pre><code class=""language-prettyprint lang-python prettyprinted""><span class=""kwd"">def</span><span class=""pln""> remove_temp_files</span><span class=""pun"">(</span><span class=""kwd"">self</span><span class=""pun"">,</span><span class=""pln""> s3</span><span class=""pun"">):</span><span class=""pln"">  
    bucket </span><span class=""pun"">=</span><span class=""pln""> s3</span><span class=""pun"">.</span><span class=""typ"">Bucket</span><span class=""pun"">(</span><span class=""kwd"">self</span><span class=""pun"">.</span><span class=""pln"">s3_bucket_temp_files</span><span class=""pun"">)</span><span class=""pln"">
    </span><span class=""kwd"">for</span><span class=""pln""> key </span><span class=""kwd"">in</span><span class=""pln""> bucket</span><span class=""pun"">.</span><span class=""pln"">objects</span><span class=""pun"">.</span><span class=""pln"">all</span><span class=""pun"">():</span><span class=""pln"">
        </span><span class=""kwd"">if</span><span class=""pln""> key</span><span class=""pun"">.</span><span class=""pln"">key</span><span class=""pun"">.</span><span class=""pln"">startswith</span><span class=""pun"">(</span><span class=""kwd"">self</span><span class=""pun"">.</span><span class=""pln"">job_name</span><span class=""pun"">)</span><span class=""pln""> </span><span class=""kwd"">is</span><span class=""pln""> </span><span class=""kwd"">True</span><span class=""pun"">:</span><span class=""pln"">
            key</span><span class=""pun"">.</span><span class=""kwd"">delete</span><span class=""pun"">()</span></code></pre>

<ul>
<li>Grab a beer and start analyzing the output data of your Spark application.</li>
</ul>

<h2 id=""finalnotes"">Final notes</h2>

<p>An example code of the full Python code can be found on <a href=""https://github.com/thomhopmans/themarketingtechnologist"">GitHub</a>. Note that my expertise is not building high-performance data pipelines and that the above code therefore probably could be improved in several ways. My interest comes from quickly (read: lazily) deploying and scaling up our models to provide new and better insights for our clients. If you have any tips to make this easier, please leave them in the comments. :)</p>

<p><strong>Tip:</strong> Note that in this example we defined the cluster to terminate after all steps are completed. However, when developing a Spark application we often want the cluster to wait for more steps. This however introduces the risk of high unexpected AWS charges if we forget to terminate the cluster. Therefore, we always add <a href=""https://github.com/Yelp/mrjob/blob/master/mrjob/bootstrap/terminate_idle_cluster.sh""><code>terminate_idle_cluster.sh</code></a> as a bootstrap action when starting the cluster. This small script is developed by <code>MRjob</code> and terminates a cluster after a specified period of inactivity, better to be safe than sorry!</p>
        ","Apache Spark is definitely one of the hottest topics in the Data Science community at the moment. Last month when we visited PyData Amsterdam 2016 we witnessed a great example of Spark's immense popularity. The speakers at PyData talking about Spark had the largest crowds after all.
Sometimes we see that these popular topics are slowly transforming in buzzwords that are abused for generating publicity, e.g. words as data scientist and deep learning but also Hadoop and DMP. I don't hope that Spark will suffer the same fate as it is definitely a powerful tool for data scientists. In the field of distributed computing Spark provides much more flexibility than MapReduce. Additionally, Spark uses memory more efficiently and therefore writes less data to disk than MapReduce, making Spark on average around 10 to 100 times faster.
In this article we introduce a method to upload our local Spark applications to an Amazon Web Services (AWS) cluster in a programmatic manner using a simple Python script. The benefit of doing this programmatically compared to interactively is that it is easier to schedule a Python script to run daily. Additionally, it also saves us time. Time we can spend better by drinking more coffee and thinking of new ideas!
The challenge
For one of our Data Science applications we recently decided to create a new part of the data pipeline with PySpark (Spark in Python). For now, I am not going to elaborate on how to build your own Spark applications as there are already plenty of tutorials on how to do so on the world wide web.
As usual we started by creating the Spark application using only a subset of the full dataset. This subset is usually small enough to test the Spark application locally on our laptops. Then, after creating a locally working Spark application, we scale the application up using an AWS Elastic Map Reduce (EMR) cluster to process the full dataset.
However, this is where we ran into some inconvenient issues. The original MapReduce data pipeline was also built in Python using the MRjob module. MRjob takes away the trouble of uploading your local code to an AWS cluster by using its built-in functions. However, MRJob does not support Spark applications (yet?) and therefore we have to get our own hands dirty this time...
The interactive method using the AWS CLI
Using the awscli module we can quickly spin up an AWS EMR cluster with Spark pre-installed using the commandline.
aws emr create-cluster \  
--name ""Spark Example"" \
--release-label emr-4.4.0 \
--applications Name=Hadoop Name=Spark 
--ec2-attributes KeyName=keypair\
--instance-groups Name=EmrMaster,InstanceGroupType=MASTER,InstanceCount=1,InstanceType=m3.xlarge,BidPrice=0.05 \
Name=EmrCore,InstanceGroupType=CORE,InstanceCount=2,InstanceType=m3.xlarge,BidPrice=0.05 \  
--use-default-roles
We need to place our code onto the cluster because we do not want the run the SparkContext on our local computer due to increased latency and availability. Therefore, we SSH into the cluster. On the cluster we create a Python file, e.g. run.py, and copy/paste the code for the Spark application.
aws emr ssh --cluster-id j-XXXX --key-pair-file keypair.pem  
sudo nano run.py  
-- copy/paste local code to cluster
We logout of the cluster and add a new step to the EMR cluster to start our Spark application via spark-submit.
aws emr add-steps \  
--cluster-id j-XXXXX \
--steps Type=CUSTOM_JAR,Name=""Spark Program"",Jar=""command-runner.jar"",ActionOnFailure=CONTINUE,Args=[""spark-submit"",home/hadoop/run.py]
Note that Amazons EMR clusters have access to S3 buckets (if the IAM roles are configured properly though). Therefore, we do not need to add other steps to copy our data back and forth between S3 and the cluster. We can just specify the proper S3 bucket in our Spark application by using for example
data = (""s3://input_bucket/*"")  
or
data = saveAsTextFile(""s3://output_bucket/"")  
Unfortunately, this S3 connection only works within our Spark application. We cannot run a Spark Python script hosted on S3 by spark-submit s3://bucket/spark_code.py...
This is still too much work...
Using the AWS command-line interface and the above commands we can interactively move our local Spark application to an AWS cluster. However, this interactive method is not easy to schedule daily as it requires some manual steps. Especially SSHing into the cluster and copy-pasting our local code to the cluster itself is tricky. It also does not fit well in our current Python data pipeline. Therefore, we prefer a more programmatic method in Python. This Python script can then be easily scheduled to run daily/weekly/monthly.
The final solution
Therefore we developed a simple Python script to execute all the necessary steps. The biggest challenge was how to 'copy/paste' our local code onto the cluster without using SSH? The solution for this problem turned out to be relatively easy. That is, we compress our local Spark script in a single file, upload this file to a temporary S3 bucket and add a Bootstrap action to the cluster that downloads and decompresses this file.
Hence, the final solution consists of the following steps executed in Python using Boto3 (an AWS SDK for Python):
Define a S3 bucket to store our files temporarily and check if it exists
def temp_bucket_exists(self, s3):  
    try:
        s3.meta.client.head_bucket(Bucket=self.s3_bucket_temp_files)
    except botocore.exceptions.ClientError as e:
        # If a client error is thrown, then check that it was a 404 error.
        # If it was a 404 error, then the bucket does not exist.
        error_code = int(e.response['Error']['Code'])
        if error_code == 404:
            terminate(""Bucket for temporary files does not exist"")
        terminate(""Error while connecting to Bucket"")
    return true
Compress the Python files of the Spark application to a .tar file.
def tar_python_script(self):  
    # Create tar.gz file
    t_file = tarfile.open(""files/script.tar.gz"", 'w:gz')
    # Add Spark script path to tar.gz file
    files = os.listdir(self.path_script)
    for f in files:
        t_file.add(self.path_script + f, arcname=f)
    t_file.close()
Upload the tar file to the S3 bucket for temporary files.
def upload_temp_files(self, s3):  
    # Shell file: setup (download S3 files to local machine)
    s3.Object(self.s3_bucket_temp_files, self.job_name + '/setup.sh').put(
       Body=open('files/setup.sh', 'rb'), ContentType='text/x-sh'
    )
    # Shell file: Terminate idle cluster
    s3.Object(self.s3_bucket_temp_files, self.job_name + '/terminate_idle_cluster.sh').put(
        Body=open('files/terminate_idle_cluster.sh', 'rb'), ContentType='text/x-sh'
    )
    # Compressed Python script files (tar.gz)
    s3.Object(self.s3_bucket_temp_files, self.job_name + '/script.tar.gz').put(
        Body=open('files/script.tar.gz', 'rb'), ContentType='application/x-tar'
    )        
Spin up an AWS EMR cluster with Hadoop and Spark as application plus two bootstrap actions. One bootstrap action is a shell script which downloads the tar file from our temporary files S3 bucket and decompresses the tar file on the remote cluster. The other bootstrap action ensures that the cluster is terminated after an hour of inactivity to prevent high unexpected AWS charges.
setup.sh
#!/bin/bash
# Parse arguments
s3_bucket_script=""$1/script.tar.gz""  
# Download compressed script tar file from S3
aws s3 cp $s3_bucket_script/home/hadoop/script.tar.gz  
# Untar file
tar zxvf ""/home/hadoop/script.tar.gz"" -C /home/hadoop/  
# Install requirements for additional Python modules (uncomment if needed)
# sudo python2.7 -m pip install pandas
.py
def start_spark_cluster(self, c):  
    response = c.run_job_flow(
        Name=self.job_name,
        ReleaseLabel=""emr-4.4.0"",
        Instances={
            'InstanceGroups': [
                {'Name': 'EmrMaster',
                 'Market': 'SPOT',
                 'InstanceRole': 'MASTER',
                 'BidPrice': '0.05',
                 'InstanceType': 'm3.xlarge',
                 'InstanceCount': 1},
                {'Name': 'EmrCore',
                 'Market': 'SPOT',
                 'InstanceRole': 'CORE',
                 'BidPrice': '0.05',
                 'InstanceType': 'm3.xlarge',
                 'InstanceCount': 2}
            ],
            'Ec2KeyName': self.ec2_key_name,
            'KeepJobFlowAliveWhenNoSteps': False
        },
        Applications=[{'Name': 'Hadoop'}, {'Name': 'Spark'}],
        JobFlowRole='EMR_EC2_DefaultRole',
        ServiceRole='EMR_DefaultRole',
        VisibleToAllUsers=True,
        BootstrapActions=[
            {'Name': 'setup',
             'ScriptBootstrapAction': {
                 'Path': 's3n://{}/{}/setup.sh'.format(self.s3_bucket_temp_files, self.job_name),
                 'Args': ['s3://{}/{}'.format(self.s3_bucket_temp_files, self.job_name)]}},
            {'Name': 'idle timeout',
             'ScriptBootstrapAction': {
                 'Path': 's3n://{}/{}/terminate_idle_cluster.sh'.format(self.s3_bucket_temp_files, self.job_name),
                 'Args': ['3600', '300']
                    }
                },
            ],
        )        
Add a step to the EMR cluster to run the Spark application using spark-submit.
def step_spark_submit(self, c, arguments):  
    response = c.add_job_flow_steps(
        JobFlowId=self.job_flow_id,
        Steps=[{
            'Name': 'Spark Application',
            'ActionOnFailure': 'CANCEL_AND_WAIT',
            'HadoopJarStep': {
               'Jar': 'command-runner.jar',
               'Args': [""spark-submit"", ""/home/hadoop/run.py"", arguments]
            }
        }]
    )
Describe status of cluster until all steps are finished and cluster is terminated.
def describe_status_until_terminated(self, c):  
    stop = False
    while stop is False:
        description = c.describe_cluster(ClusterId=self.job_flow_id)
        state = description['Cluster']['Status']['State']
        if state == 'TERMINATED' or state == 'TERMINATED_WITH_ERRORS':
            stop = True
        print(state)
        time.sleep(30)
Remove the temporary files from the S3 bucket when the cluster is terminated.
def remove_temp_files(self, s3):  
    bucket = s3.Bucket(self.s3_bucket_temp_files)
    for key in bucket.objects.all():
        if key.key.startswith(self.job_name) is True:
            key.delete()
Grab a beer and start analyzing the output data of your Spark application.
Final notes
An example code of the full Python code can be found on GitHub. Note that my expertise is not building high-performance data pipelines and that the above code therefore probably could be improved in several ways. My interest comes from quickly (read: lazily) deploying and scaling up our models to provide new and better insights for our clients. If you have any tips to make this easier, please leave them in the comments. :)
Tip: Note that in this example we defined the cluster to terminate after all steps are completed. However, when developing a Spark application we often want the cluster to wait for more steps. This however introduces the risk of high unexpected AWS charges if we forget to terminate the cluster. Therefore, we always add terminate_idle_cluster.sh as a bootstrap action when starting the cluster. This small script is developed by MRjob and terminates a cluster after a specified period of inactivity, better to be safe than sorry!","[Data Science, python, Code, Spark, AWS]"
48,Recap: Google Analytics User Conference - GAUC 2016,/google-analytics-user-conference-2016-gauc/,"
            <p>Last week, I attended the Conference day of the seventh official Google Analytics User Conference in Amsterdam in search of new Google Analytics tips and tricks. More information about the Program of GAUC 2016 can be found here: <a href=""http://www.gauc.nl/programma-conference-day/"">http://www.gauc.nl/programma-conference-day/</a>. </p>

<p>So, what are the key takeaways of the master classes this year?</p>

<h2 id=""1fromofflinetoonline"">1: From offline to online</h2>

<p>As many of our clients are active both offline and online, I attended the Master Class ""Remove the split between online and offline"".</p>

<p>The first difference discussed was the difference between offline and online marketers. As an example, offline marketers trust on Dutch television ratings. These ratings are based on just 1250 boxes that monitor what you are watching and represent the total of 7.7 million Dutch households. Offline targets are mostly goals based on top-of-mind awareness and indirect sales. In contrast to the goals of offline marketers, the targets for online marketers are often based on conversions,  ROI and cost per order. However, this largely depends on offline presence and brand. Besides this, online marketers suffer from <a href=""https://www.themarketingtechnologist.co/our-first-insights-in-the-ad-blocking-consumer/"">adblockers</a> but still have most of the data. </p>

<p>Research on the customer journey shows the shift from offline to online is now clearly visible. Knowing what consumers search for online can be very useful information for offline sales. On the other hand, online marketers can learn a lot about why consumers buy products offline.</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Apr/offline_online-1461051227989.png"" alt="""" class=""full-img""></p>

<p>But how can you integrate online with offline data? In my opinion, this is a question that has not yet been answered. According to this master class there are a number of ways to accomplish this integration:</p>

<ul>
<li><p><strong>Attribution and analysis</strong></p>

<ul><li>more effort of media budget</li>
<li>more insights in campaigns</li>
<li>more insights in customers</li></ul></li>
<li><p><strong>Correlation</strong></p>

<ul><li>display &amp; search</li>
<li>tv &amp; search</li>
<li>tv &amp; online visits</li></ul></li>
<li><p><strong>Measurement protocol</strong></p>

<ul><li>offline transactions</li>
<li>callcenters calls</li>
<li>refunds &amp; confirmations</li></ul></li>
<li><p><strong>Additional</strong></p>

<ul><li>cvs-uploads</li>
<li>leads to sales</li>
<li>beacons</li>
<li>digitization</li></ul></li>
</ul>

<p>For users of Google Analytics, the Measurement Protocol can become an important tool to bridge between online and offline. With the <a href=""https://developers.google.com/analytics/devguides/collection/protocol/v1/"">Measurement Protocol</a> you can collect data from any device with an internet connection. This enables the possibility to send data about offline visits into Google Analytics. An important point here is the customer identifier. A Customer identifier, e.g. an ID attached to a loyalty card, can be used to link online and offline events of a customer.</p>

<p>By adding a customer identifier to your online visit and offline transaction data, you will be able to match these online activities and offline transactions. How many companies use the measurement protocol to integrate online and offline data is not yet known. In addition to the possibility of sending your offline data to GA, the question might rise whether you want to save all your customer data in Google Analytics or in your own database.</p>

<h2 id=""2googleanalyticskpistargets"">2: Google Analytics, KPI's &amp; Targets</h2>

<p>After the session about integrating online and offline, it was time for a session on KPIs and targets. During this session, the importance of determining the right KPIs and targets were discussed. In order to clarify how to determine the right KPIs and targets, OrangeValley uses a KPI framework as shown below.</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Apr/7__Afbeelding_KPI_framework_uitgediept_1-1461052592972.png"" alt="""">
<em>Source: OrangeValley's GAUC talk, 2016</em></p>

<p>At the top of the framework the main KPI is shown. This KPI is related to your business objective. These are objectives like sales or generating leads. Next, there is a split between quantity and quality. The quantity is about the numbers that contribute to your business objectives, such as the number of orders, visits, sessions or impressions. The qualitative values also contribute to your business objectives, such as the average order value, conversion rate or the click-through rate (CTR). </p>

<p>The two main points of the master class were that you have to define your KPIs and Targets before setting up Google Analytics and that you always have to take more then one metric in account. For example, maybe visits have dropped, but the conversion rate was higher (and with it, orders kept coming in). This model helps companies to visualize what their KPIs are and which measures can be used for that KPI. </p>

<h2 id=""3googleanalyticspremium"">3: Google Analytics Premium</h2>

<p>The last masterclass I attended discussed something that you probably heard of last couple of weeks: Google Analytics <del>Premium</del> Suite 360. With this new offer of a complete package that includes analytics, A/B testing and data visualization, Google seems to follow the competitor Adobe. Google Analytics 360 Suite consists of a total of six products, three of them are truly new.</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Apr/360-1461053039106.png"" alt="""" class=""full-img"">
*The new Google Analytics 360 Suite.
*</p>

<h3 id=""whatsnew"">What's new?</h3>

<p><strong>Google Audience Center 360.</strong></p>

<p>Audience Center, which was called <em>DoubleClick Audience Center</em> before is their Data Management Platform (DMP). This allows marketers to get more insights on who their visitors and customers are and find similar people across multiple channels, devices, and campaigns. So far we know that this platform will have a native integration with AdWords and DoubleClick, but also the possibility to integrate it with other demand-side platforms (DSP).</p>

<p><strong>Google Optimize 360.</strong></p>

<p>Google Optimize can be used for A/B testing and personalization purposes. In addition to Google Website Optimizer and Google Experiments, this is Google's third attempt in the A/B testing landscape. Like Optimizely and Visual Website Optimizer, Google Optimize will also contain a 'What-You-See-Is-What-You-Get-Editor' to create a new variant from the current web page. Adapting the website will mainly be based on the Tag Manager 360 technology. The big advantage for Google's own A/B testing tool will be the very easy integration with other tools such as Google Analytics, Audience Center and AdWords.</p>

<p><strong>Google Data Studio 360.</strong></p>

<p>Like the previous two features, the Google Data Studio 360 is new. This product will be the platform where data can be analyzed and visualized. The Data Studio integrates data from all Google Analytics Suite 360 products and other external data sources. The product is based on the Google Docs technology and allows users to collaborate in real-time and share data to create interactive reports and dashboards. The good part is that integration with BigQuery is possible, the bad part is that other (non-Google) data sources will probably not be supported when Data Studio 360 launches.</p>

<h3 id=""whatskindofnew"">What's kind of new?</h3>

<p><strong>Google Tag Manager 360.</strong></p>

<p>As a Data Technologist, I use the free version of Google Tag Manager quite a lot. So what's new in the 360 version? In the beginning, nothing, except the Service Level Agreement that comes with the 360 edition. Google's Advocate Daniel Waisberg mentioned that the Google Tag Manager 360 will most likely have different features than the free version of Google Tag Manager, but what these new features are is not clear yet. </p>

<p><strong>Google Attribution 360.</strong></p>

<p>Google Attribution, formerly known as Adometry, used to be a standalone attribution tool that Google has integrated in Google Analytics Premium. After the name change, this product will become part of the Google Analytics 360 Suite and will continue to provide insights on the actual value of marketing investments across multiple channels, devices, systems and offline data points (e.g. When TV spots are broadcast). </p>

<p><strong>Google Analytics 360.</strong></p>

<p>Formerly known as Premium, this will continue under the new name: Analytics 360. Google Analytics 360 distinguishes itself from the free version with special features:</p>

<ul>
<li>a lower sample rate.</li>
<li>the integration with DoubleClick.</li>
<li>a data-driven attribution model.</li>
</ul>

<p>Most of our clients use the standard version of Google Analytics which has enough features to work with. The main reason for switching to 360 would be sampling. </p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Apr/table-1461055240828.png"" alt="""" class=""full-img"">
<em>Overview of analytics features: GA 360 vs. GA Standard</em></p>

<h3 id=""shouldyoumaketheswitchto360"">Should you make the switch to 360?</h3>

<p>So, is it time to change to the Google Analytics 360 Suite? It's good to know that all the products in the 360 Suite are paid, and the price of using the various products is not entirely clear. Google Analytics Certified Partners will soon be notified of updates so that they can inform their customers about it. While the name of the Suite contains Google Analytics, it is not mandatory to buy Google Analytics 360 in order to use the other 360 products. During the coming weeks and months, Google will release the first beta's of Audience Center, Optimize and Data Studio. As a fan of data visualizations, I am looking forward to work with the Data Studio 360.</p>

<p>All things considered, the Google Analytics 360 suite looks very promising. It is too early to conclude that you should or shouldn't buy the whole suite, as the beta versions of the new products are not released yet and the prices are not known. The products Google Analytics 360 and Google Tag Manager 360 won't offer any new features upon launch, so the question remains, why switch now? I am not sure if Google Analytics 360 suite will become a success if they only support Google products / data sources. Nevertheless, I am very excited to start testing the beta versions of the Suite in the coming months.</p>
        ","Last week, I attended the Conference day of the seventh official Google Analytics User Conference in Amsterdam in search of new Google Analytics tips and tricks. More information about the Program of GAUC 2016 can be found here: http://www.gauc.nl/programma-conference-day/.
So, what are the key takeaways of the master classes this year?
1: From offline to online
As many of our clients are active both offline and online, I attended the Master Class ""Remove the split between online and offline"".
The first difference discussed was the difference between offline and online marketers. As an example, offline marketers trust on Dutch television ratings. These ratings are based on just 1250 boxes that monitor what you are watching and represent the total of 7.7 million Dutch households. Offline targets are mostly goals based on top-of-mind awareness and indirect sales. In contrast to the goals of offline marketers, the targets for online marketers are often based on conversions, ROI and cost per order. However, this largely depends on offline presence and brand. Besides this, online marketers suffer from adblockers but still have most of the data.
Research on the customer journey shows the shift from offline to online is now clearly visible. Knowing what consumers search for online can be very useful information for offline sales. On the other hand, online marketers can learn a lot about why consumers buy products offline.
But how can you integrate online with offline data? In my opinion, this is a question that has not yet been answered. According to this master class there are a number of ways to accomplish this integration:
Attribution and analysis
more effort of media budget
more insights in campaigns
more insights in customers
Correlation
display & search
tv & search
tv & online visits
Measurement protocol
offline transactions
callcenters calls
refunds & confirmations
Additional
cvs-uploads
leads to sales
beacons
digitization
For users of Google Analytics, the Measurement Protocol can become an important tool to bridge between online and offline. With the Measurement Protocol you can collect data from any device with an internet connection. This enables the possibility to send data about offline visits into Google Analytics. An important point here is the customer identifier. A Customer identifier, e.g. an ID attached to a loyalty card, can be used to link online and offline events of a customer.
By adding a customer identifier to your online visit and offline transaction data, you will be able to match these online activities and offline transactions. How many companies use the measurement protocol to integrate online and offline data is not yet known. In addition to the possibility of sending your offline data to GA, the question might rise whether you want to save all your customer data in Google Analytics or in your own database.
2: Google Analytics, KPI's & Targets
After the session about integrating online and offline, it was time for a session on KPIs and targets. During this session, the importance of determining the right KPIs and targets were discussed. In order to clarify how to determine the right KPIs and targets, OrangeValley uses a KPI framework as shown below.
Source: OrangeValley's GAUC talk, 2016
At the top of the framework the main KPI is shown. This KPI is related to your business objective. These are objectives like sales or generating leads. Next, there is a split between quantity and quality. The quantity is about the numbers that contribute to your business objectives, such as the number of orders, visits, sessions or impressions. The qualitative values also contribute to your business objectives, such as the average order value, conversion rate or the click-through rate (CTR).
The two main points of the master class were that you have to define your KPIs and Targets before setting up Google Analytics and that you always have to take more then one metric in account. For example, maybe visits have dropped, but the conversion rate was higher (and with it, orders kept coming in). This model helps companies to visualize what their KPIs are and which measures can be used for that KPI.
3: Google Analytics Premium
The last masterclass I attended discussed something that you probably heard of last couple of weeks: Google Analytics Premium Suite 360. With this new offer of a complete package that includes analytics, A/B testing and data visualization, Google seems to follow the competitor Adobe. Google Analytics 360 Suite consists of a total of six products, three of them are truly new.
*The new Google Analytics 360 Suite. *
What's new?
Google Audience Center 360.
Audience Center, which was called DoubleClick Audience Center before is their Data Management Platform (DMP). This allows marketers to get more insights on who their visitors and customers are and find similar people across multiple channels, devices, and campaigns. So far we know that this platform will have a native integration with AdWords and DoubleClick, but also the possibility to integrate it with other demand-side platforms (DSP).
Google Optimize 360.
Google Optimize can be used for A/B testing and personalization purposes. In addition to Google Website Optimizer and Google Experiments, this is Google's third attempt in the A/B testing landscape. Like Optimizely and Visual Website Optimizer, Google Optimize will also contain a 'What-You-See-Is-What-You-Get-Editor' to create a new variant from the current web page. Adapting the website will mainly be based on the Tag Manager 360 technology. The big advantage for Google's own A/B testing tool will be the very easy integration with other tools such as Google Analytics, Audience Center and AdWords.
Google Data Studio 360.
Like the previous two features, the Google Data Studio 360 is new. This product will be the platform where data can be analyzed and visualized. The Data Studio integrates data from all Google Analytics Suite 360 products and other external data sources. The product is based on the Google Docs technology and allows users to collaborate in real-time and share data to create interactive reports and dashboards. The good part is that integration with BigQuery is possible, the bad part is that other (non-Google) data sources will probably not be supported when Data Studio 360 launches.
What's kind of new?
Google Tag Manager 360.
As a Data Technologist, I use the free version of Google Tag Manager quite a lot. So what's new in the 360 version? In the beginning, nothing, except the Service Level Agreement that comes with the 360 edition. Google's Advocate Daniel Waisberg mentioned that the Google Tag Manager 360 will most likely have different features than the free version of Google Tag Manager, but what these new features are is not clear yet.
Google Attribution 360.
Google Attribution, formerly known as Adometry, used to be a standalone attribution tool that Google has integrated in Google Analytics Premium. After the name change, this product will become part of the Google Analytics 360 Suite and will continue to provide insights on the actual value of marketing investments across multiple channels, devices, systems and offline data points (e.g. When TV spots are broadcast).
Google Analytics 360.
Formerly known as Premium, this will continue under the new name: Analytics 360. Google Analytics 360 distinguishes itself from the free version with special features:
a lower sample rate.
the integration with DoubleClick.
a data-driven attribution model.
Most of our clients use the standard version of Google Analytics which has enough features to work with. The main reason for switching to 360 would be sampling.
Overview of analytics features: GA 360 vs. GA Standard
Should you make the switch to 360?
So, is it time to change to the Google Analytics 360 Suite? It's good to know that all the products in the 360 Suite are paid, and the price of using the various products is not entirely clear. Google Analytics Certified Partners will soon be notified of updates so that they can inform their customers about it. While the name of the Suite contains Google Analytics, it is not mandatory to buy Google Analytics 360 in order to use the other 360 products. During the coming weeks and months, Google will release the first beta's of Audience Center, Optimize and Data Studio. As a fan of data visualizations, I am looking forward to work with the Data Studio 360.
All things considered, the Google Analytics 360 suite looks very promising. It is too early to conclude that you should or shouldn't buy the whole suite, as the beta versions of the new products are not released yet and the prices are not known. The products Google Analytics 360 and Google Tag Manager 360 won't offer any new features upon launch, so the question remains, why switch now? I am not sure if Google Analytics 360 suite will become a success if they only support Google products / data sources. Nevertheless, I am very excited to start testing the beta versions of the Suite in the coming months.","[Google Analytics User Conference GAUC 2016, Analytics, GA 360, Google Analytics 360, Google Analytics Premium, GAUC 2016]"
49,How close is Facebook to becoming The Circle?,/how-close-is-facebook-to-completing-the-circle/,"
            <p>Sometimes the future is coming at you at a faster rate than you expect it to come. This post is about exactly that feeling. I never thought the ideas I read in <a href=""http://www.amazon.com/The-Circle-Dave-Eggers/dp/0345807294"">The Circle</a> would become a thing in the actual world as quickly as they did. Let me start with a synopsis of the book. </p>

<h2 id=""thecirclestory"">The Circle Story</h2>

<p>The story is about Mae, a young woman starting a career at a tech company (The Circle) comparable to Google or Facebook. Their goal is to make 100% of internet traffic go through them. This way, they’ll have a complete view of the internet and are able to better serve both their users and advertisers. </p>

<p><strong>Spoiler alert: from this point onwards, this post contains details about the story and plot of The Circle by Dave Eggers. If you haven’t read the book, and still want to read it without spoilers, don’t proceed.</strong></p>

<p>As you follow Mae's progress through The Circle, new products get introduced. One of these products is a live streaming camera. The camera streams its recordings directly to The Circle. People using the Circle can log in and see what every camera is streaming at any time. Not long after, the idea of ‘going transparent’ is introduced. </p>

<p>To become transparent, one would have to wear one of these streaming cameras around their neck, and with it you will start streaming every part of your life. The first people to do this are politicians, people who have great value in being perceived as  transparent. But not long after, the main character also goes transparent. The philosophy of The Circle is that by going transparent, you’ll become a better person because you potentially have millions, or even billions, of people who can watch your every step. This will make it less likely for you to make bad decisions. Essentially turning people into 'better' versions of themselves. The book main character chooses in favour of this transparent movement. But I don’t like the idea.</p>

<p>Last summer, as I finished The Circle during my holiday, the story made me think differently about my day-to-day activities during my work: data collection and analysis.  I had not looked at Google and Facebook in this way before. I'm a frequent user of both platforms and their tools, but seeing the potential power of a tech giant in this way, well, it frightened me. Though at that time, I thought it wouldn't happen that quickly and without anyone saying anything about it. </p>

<p>And then, Facebook held the 2016 <a href=""https://www.fbf8.com/"">F8 conference</a>.</p>

<h2 id=""thefacebookpromiseoflivestreamingforeveryone"">The Facebook promise of live streaming for everyone</h2>

<p>At  April 12 2016, during Facebook’s annual F8 developer conference, Mark announced the availability of <a href=""http://www.theverge.com/2016/4/12/11415244/facebook-live-video-dji-drone-stream-f8-conference-2016/in/11179381"">live streaming for everyone</a>.</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Apr/fb_live_api-1461063491919.PNG"" alt=""Facebook Live product page"" class=""full-img""></p>

<p><em>The Facebook Live product page. Visit <a href=""https://live.fb.com"">https://live.fb.com</a> for details.</em></p>

<p>This instantly revived my memories of The Circle. Live streaming for everyone: it seems fun for now, but what if it actually goes the way of the The Circle. In the book, the first example for a use case of the camera is about a surfer: what if he could see the waves at the coast, before actually going there? This way he knows if he will be able to surf when he gets there. A great example. But as mentioned above, it soon switches to the 'transparent idea'. </p>

<p>There will be cameras everywhere, streaming everything. Even if you’re not logged into The Circle (Facebook), you could still be tracked by them. You won’t have a choice. Though I still thought the utopian view of the book to be miles away.</p>

<p>This past Monday, I switched my mind. It happened when a colleague showed me <a href=""https://www.facebook.com/livemap"">Facebook Livemap</a> (you need to set Facebook to English to enable this feature):</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Apr/fb_livemap-1461057267571.PNG"" alt=""Facebook Livemap for current video streams"" class=""full-img""></p>

<p><em>Facebook Livemap shows all active public live streams that are currently being broadcast around the world.</em></p>

<p>To me, this is the potential step one, or maybe  0.1, to the transparent future described in The Circle. People will first start streaming what they like, but I think it won’t be long before the first ever streaming cameras will be put into place. And not long after, there’ll be no way to get around them. </p>

<h2 id=""facebookbecomingfullcircle"">Facebook becoming full circle</h2>

<p>Our big blue friend has produced several tools over the past few years to get closer to a holistic view of the internet:</p>

<ul>
<li>They track other pages with their like buttons and <a href=""https://developers.facebook.com/docs/marketing-api/facebook-pixel/v2.6"">other tracking techniques</a>.</li>
<li>Facebook will show you how many people were near your store, or in your store, with the help of both <a href=""https://www.facebook.com/business/a/facebook-bluetooth-beacons"">beacons</a> and <a href=""https://www.facebook.com/business/news/local-awareness-updates"">location services</a>.</li>
<li>M, Facebooks AI, <a href=""http://www.theverge.com/2015/10/26/9605526/facebook-m-hands-on-personal-assistant-ai"">is their promised personal assistant</a> that will arrange as many things for us as possible.</li>
<li><a href=""https://instantarticles.fb.com/"">Instant articles</a> lets publishers create content for Facebook, without users ever leaving the site. </li>
<li><a href=""https://developers.facebook.com/blog/post/2016/04/12/bots-for-messenger/"">Chat bots</a> (also announced at this year’s F8) promise to deliver all the info you need through their messenger system.</li>
</ul>

<p>Add the discussed live streaming to this list, and we have our candidate for a real world equivalent of The Circle. Keep in mind that this is what you opt-in to when you’re using the social media platform. </p>

<h2 id=""whataboutnonusers"">What about non-users?</h2>

<p>Every once in a while, stories pop up about <a href=""http://www.ibtimes.com/facebook-privacy-lawsuit-alleges-social-network-created-shadow-profiles-non-users-1895840"">Facebook building visitor profiles of users who don’t have a Facebook account</a>. In these cases, judges normally require Facebook to delete the profiles. But this isn’t something they do by default apparently. So when your stance in this is ""I  don’t use Facebook, so this won’t impact me"", think again.</p>

<h2 id=""conclusion"">Conclusion</h2>

<p>In the end, this example shows just how fast a fictional future may become an actual reality. The book was first published in October 2013, I read it in the summer of 2015, and the first steps to the omni-streaming transparent utopian world of The Circle are already here in early 2016. Though somehow, I always imagined it would be Google to make these steps, not Facebook. </p>

<p>Where this will lead to, I don’t know. But I'm happy that I'm aware of the development.</p>
        ","Sometimes the future is coming at you at a faster rate than you expect it to come. This post is about exactly that feeling. I never thought the ideas I read in The Circle would become a thing in the actual world as quickly as they did. Let me start with a synopsis of the book.
The Circle Story
The story is about Mae, a young woman starting a career at a tech company (The Circle) comparable to Google or Facebook. Their goal is to make 100% of internet traffic go through them. This way, they’ll have a complete view of the internet and are able to better serve both their users and advertisers.
Spoiler alert: from this point onwards, this post contains details about the story and plot of The Circle by Dave Eggers. If you haven’t read the book, and still want to read it without spoilers, don’t proceed.
As you follow Mae's progress through The Circle, new products get introduced. One of these products is a live streaming camera. The camera streams its recordings directly to The Circle. People using the Circle can log in and see what every camera is streaming at any time. Not long after, the idea of ‘going transparent’ is introduced.
To become transparent, one would have to wear one of these streaming cameras around their neck, and with it you will start streaming every part of your life. The first people to do this are politicians, people who have great value in being perceived as transparent. But not long after, the main character also goes transparent. The philosophy of The Circle is that by going transparent, you’ll become a better person because you potentially have millions, or even billions, of people who can watch your every step. This will make it less likely for you to make bad decisions. Essentially turning people into 'better' versions of themselves. The book main character chooses in favour of this transparent movement. But I don’t like the idea.
Last summer, as I finished The Circle during my holiday, the story made me think differently about my day-to-day activities during my work: data collection and analysis. I had not looked at Google and Facebook in this way before. I'm a frequent user of both platforms and their tools, but seeing the potential power of a tech giant in this way, well, it frightened me. Though at that time, I thought it wouldn't happen that quickly and without anyone saying anything about it.
And then, Facebook held the 2016 F8 conference.
The Facebook promise of live streaming for everyone
At April 12 2016, during Facebook’s annual F8 developer conference, Mark announced the availability of live streaming for everyone.
The Facebook Live product page. Visit https://live.fb.com for details.
This instantly revived my memories of The Circle. Live streaming for everyone: it seems fun for now, but what if it actually goes the way of the The Circle. In the book, the first example for a use case of the camera is about a surfer: what if he could see the waves at the coast, before actually going there? This way he knows if he will be able to surf when he gets there. A great example. But as mentioned above, it soon switches to the 'transparent idea'.
There will be cameras everywhere, streaming everything. Even if you’re not logged into The Circle (Facebook), you could still be tracked by them. You won’t have a choice. Though I still thought the utopian view of the book to be miles away.
This past Monday, I switched my mind. It happened when a colleague showed me Facebook Livemap (you need to set Facebook to English to enable this feature):
Facebook Livemap shows all active public live streams that are currently being broadcast around the world.
To me, this is the potential step one, or maybe 0.1, to the transparent future described in The Circle. People will first start streaming what they like, but I think it won’t be long before the first ever streaming cameras will be put into place. And not long after, there’ll be no way to get around them.
Facebook becoming full circle
Our big blue friend has produced several tools over the past few years to get closer to a holistic view of the internet:
They track other pages with their like buttons and other tracking techniques.
Facebook will show you how many people were near your store, or in your store, with the help of both beacons and location services.
M, Facebooks AI, is their promised personal assistant that will arrange as many things for us as possible.
Instant articles lets publishers create content for Facebook, without users ever leaving the site.
Chat bots (also announced at this year’s F8) promise to deliver all the info you need through their messenger system.
Add the discussed live streaming to this list, and we have our candidate for a real world equivalent of The Circle. Keep in mind that this is what you opt-in to when you’re using the social media platform.
What about non-users?
Every once in a while, stories pop up about Facebook building visitor profiles of users who don’t have a Facebook account. In these cases, judges normally require Facebook to delete the profiles. But this isn’t something they do by default apparently. So when your stance in this is ""I don’t use Facebook, so this won’t impact me"", think again.
Conclusion
In the end, this example shows just how fast a fictional future may become an actual reality. The book was first published in October 2013, I read it in the summer of 2015, and the first steps to the omni-streaming transparent utopian world of The Circle are already here in early 2016. Though somehow, I always imagined it would be Google to make these steps, not Facebook.
Where this will lead to, I don’t know. But I'm happy that I'm aware of the development.","[Facebook, f8, the circle, video streaming, live video, future, Analytics]"
50,Virtual reality: connecting Unity to the CAVE,/virtual-reality-connecting-unity-to-the-cave/,"
            <p><p style=""color:#7e4396; font-weight:bold"">This post concerns Greenhouse Group Labs, an innovation program for students, established by Greenhouse Group. Labs is an ideal opportunity to test the latest technologies available, while allowing for talented young individuals to deeply explore them and come up with groundbreaking solutions.</p>Imagine being given the opportunity to experience a life-like virtual environment together with your peers just by using technology. Imagine seeing things you have not pictured possible, like visiting an alien planet, exploring the deepest parts of the ocean or even using it for teaching young children. </p>

<p>As the technology for virtual and augmented reality is increasing and maturing over time, many new technologies are on the rise. One of them is the CAVE. In our latest project we were given the opportunity to use the CAVE and explore the possibilities with it for our project.</p>

<h3 id=""whatisthecave"">What is the CAVE?</h3>

<p>The name itself is quite amusing as it is a recursive acronym which stands for CAVE Automatic Virtual Environment. It basically is made up of a cube-shaped room in which the walls act as the screens to project scenes onto, creating a virtual reality environment.</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Apr/cave-1461066372782.png"" alt="""" class=""full-img""></p>

<p>One or more people can stand anywhere within the CAVE and are able to see what everyone else in the room sees. The experience is so realistic, it is scary. The CAVE was developed for the purpose of overcoming some of the limitations which HMD (head-mounted devices) such as the Oculus, Samsung Gear VR have. A HMD is limited to provide the experience to only one person at a time, whereas a CAVE is at least 9m2 big, enabling for more than one person to experience.</p>

<p>As mentioned before the CAVE is a cube-shaped room where each wall of the cube is used as a screen. In most CAVEs the front, left and right walls are used to project onto, but sometimes the back and floor are also used. For each screen there are one or more designated projectors.</p>

<p>One important thing to understand is that all camera views are running in synchronization with each other, it all works as one. If something were to occur or change in one camera-view, the other camera-views are aware of this change and act to it accordingly. Say if a car moves from the front screen to the right, the right screen is aware of this occurrence and acts accordingly by displaying the car moving into the right camera-view. </p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Apr/ss__2016_04_19_at_01_43_09_-1461066411463.jpg"" alt="""">
In the image above it can be seen that the front camera-view and right camera-view are in synchronization. The wall and the floor are extended between the two views as you can see.</p>

<h3 id=""ourapproach"">Our approach</h3>

<p>There are multiple programs to show a virtual world in a CAVE, however we chose to use Unity. Normally one might think that Unity runs one game instance, and displays four different camera-views from that one instance. Well, due to how the game engine is built, this would be too difficult of a modification to core functionality.</p>

<p>Instead, Unity came up with a pretty clever solution. Instead of running one instance, they thought they would run four synchronized instances. To make this work however, they had to make some adaptations, to keep the four instances synchronized.</p>

<ul>
<li>They automatically send the random seeds and input signals from the master instance to the slaves. This means that for example, character movement or random enemies spawning will be the same on all views. This already synchronizes most things within a Unity scene. In most cases, though, you will have one view displaying a view that is harder to render than the others, which would mean that this instance will have a lower frame-rate and will fall behind compared to the others. </li>
<li>To solve this, Unity developers made the master wait for all the slaves to have their frame ready, and then every instance would display it simultaneously. This means though, that the slowest frame-rate is the leading one, which can have bad consequences when running graphically challenging Unity programs.</li>
</ul>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Apr/3-1461046707214.jpg"" alt="""" class=""full-img""></p>

<ul>
<li>Also, we had to set up the CAVE configuration in the editor, which is as much as assigning a number to each camera direction and pass this number in the launch arguments of the slave instances running on the corresponding workstations.</li>
</ul>

<p>By simply setting up the Cluster Rendering part of Unity, we have already succeeded in displaying our Unity project on the walls of the CAVE. The difficult thing, however, is that this Cluster Rendering is still in closed beta, and we had some struggles being allowed to participate in the beta. Cluster Rendering being in beta has, however, not had any negative consequences on the stability of the feature.</p>
        ","This post concerns Greenhouse Group Labs, an innovation program for students, established by Greenhouse Group. Labs is an ideal opportunity to test the latest technologies available, while allowing for talented young individuals to deeply explore them and come up with groundbreaking solutions.
Imagine being given the opportunity to experience a life-like virtual environment together with your peers just by using technology. Imagine seeing things you have not pictured possible, like visiting an alien planet, exploring the deepest parts of the ocean or even using it for teaching young children.
As the technology for virtual and augmented reality is increasing and maturing over time, many new technologies are on the rise. One of them is the CAVE. In our latest project we were given the opportunity to use the CAVE and explore the possibilities with it for our project.
What is the CAVE?
The name itself is quite amusing as it is a recursive acronym which stands for CAVE Automatic Virtual Environment. It basically is made up of a cube-shaped room in which the walls act as the screens to project scenes onto, creating a virtual reality environment.
One or more people can stand anywhere within the CAVE and are able to see what everyone else in the room sees. The experience is so realistic, it is scary. The CAVE was developed for the purpose of overcoming some of the limitations which HMD (head-mounted devices) such as the Oculus, Samsung Gear VR have. A HMD is limited to provide the experience to only one person at a time, whereas a CAVE is at least 9m2 big, enabling for more than one person to experience.
As mentioned before the CAVE is a cube-shaped room where each wall of the cube is used as a screen. In most CAVEs the front, left and right walls are used to project onto, but sometimes the back and floor are also used. For each screen there are one or more designated projectors.
One important thing to understand is that all camera views are running in synchronization with each other, it all works as one. If something were to occur or change in one camera-view, the other camera-views are aware of this change and act to it accordingly. Say if a car moves from the front screen to the right, the right screen is aware of this occurrence and acts accordingly by displaying the car moving into the right camera-view.
In the image above it can be seen that the front camera-view and right camera-view are in synchronization. The wall and the floor are extended between the two views as you can see.
Our approach
There are multiple programs to show a virtual world in a CAVE, however we chose to use Unity. Normally one might think that Unity runs one game instance, and displays four different camera-views from that one instance. Well, due to how the game engine is built, this would be too difficult of a modification to core functionality.
Instead, Unity came up with a pretty clever solution. Instead of running one instance, they thought they would run four synchronized instances. To make this work however, they had to make some adaptations, to keep the four instances synchronized.
They automatically send the random seeds and input signals from the master instance to the slaves. This means that for example, character movement or random enemies spawning will be the same on all views. This already synchronizes most things within a Unity scene. In most cases, though, you will have one view displaying a view that is harder to render than the others, which would mean that this instance will have a lower frame-rate and will fall behind compared to the others.
To solve this, Unity developers made the master wait for all the slaves to have their frame ready, and then every instance would display it simultaneously. This means though, that the slowest frame-rate is the leading one, which can have bad consequences when running graphically challenging Unity programs.
Also, we had to set up the CAVE configuration in the editor, which is as much as assigning a number to each camera direction and pass this number in the launch arguments of the slave instances running on the corresponding workstations.
By simply setting up the Cluster Rendering part of Unity, we have already succeeded in displaying our Unity project on the walls of the CAVE. The difficult thing, however, is that this Cluster Rendering is still in closed beta, and we had some struggles being allowed to participate in the beta. Cluster Rendering being in beta has, however, not had any negative consequences on the stability of the feature.","[Labs, Code, VR, Virtual Reality, Cave]"
51,Other tag managers on the block: Qubit Opentag,/other-tag-managers-on-the-block-qubit-opentag/,"
            <p>At Greenhouse Group, we’ve been working with tag manager for over 3.5 years now. Over the last part of that period, a lot of articles popped up about Google Tag Manager (GTM), Google’s solution to tag management. It’s a great option, but there are some other vendors out there that also offer great tag management systems (TMS). A TMS that we have used and still use a lot is <a href=""http://www.qubit.com/opentag"">Qubit Opentag</a>, which was recently <a href=""http://www.qubit.com/opentag-v3"">updated to version 3</a>. So I thought now would be a good time to discuss this TMS and how it compares to GTM. </p>

<h2 id=""howqubitbecameourtagmanagerofchoice"">How Qubit became our tag manager of choice</h2>

<p>First, let’s rewind back to late 2012. We were just starting to look at tag management options for our clients. Qubit popped up as our number one choice because of three reasons:</p>

<ul>
<li>It has a built-in cookie consent window.</li>
<li>It supports tag dependencies.</li>
<li>It is <a href=""https://opentag.qubitproducts.com/tagsdk/docs/template.html#!/api"">open source</a>.</li>
</ul>

<p>The first benefit is clear, as cookie consent windows were about to become mandatory for all websites in the Netherlands (where most of our clients are located). </p>

<p>The second one is extremely powerful. The option for dependencies allows you to easily fire tags in order. Let’s look at a basic Google Analytics setup as an example:</p>

<ul>
<li><strong>GA core tag</strong>: first, we load our GA core tag on all pages. It only loads the library and configures the tracker.</li>
<li><strong>GA page view tag</strong>: this tag also loads on all pages, but is dependent on the GA core tag. Because of that, it will only fire after the core has fired.</li>
</ul>

<p>This feature is very powerful because you break your tags down into several components.  If there’s a new setting we want to apply to each analytics hit we send (page view, event and ecommerce). We simply add it to the core tag, and it’ll be applied to all dependent tags. </p>

<p>The third benefit (open source) allowed us to look under the hood and see how the TMS works. </p>

<p>Back then, GTM didn't have as many features as Qubit Opentag, and therefore it wasn't a serious option for us. </p>

<h2 id=""whatsnewwithqubitopentagversion3"">What’s new with Qubit Opentag version 3</h2>

<p>Over the course of 2014 and 2015, GTM catched up to Qubit Opentag, and in some areas even surpassed it. Luckily Qubit released version 3 of their TMS in February 2016. With it, Qubit upped their TMS game. These were the four most important updates for us:</p>

<h3 id=""1tagspecificversioning"">1: Tag specific versioning</h3>

<p>One thing we really missed in Qubit, especially over the later part of the past 3.5 years, was versioning. With version 3, they've added versioning to their TMS, but not as other TMS vendors do it. Where you normally have a version for each release, Qubit has a version for each tag. So if you release a big update, and only one tag has an issue, you can simple roll back to an old version of that one specific tag. </p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Apr/geek_versioning-1460713985437.PNG"" alt=""Qubit Opentag v3 Versions per Tag"" class=""full-img""></p>

<p><em>Qubit Change History. It allows you to roll back to an older version of a specific tag with the icon on the right.</em></p>

<h3 id=""2keeptrackofchangeswithtabs"">2: Keep track of changes with tabs</h3>

<p>The second great improvement is the use of tabs. When you’re working on several changes, including tags, load rules, and/or variables, it can be hard to keep track of what you've been working on. Qubit Opentag opens a separate tab within their interface for every change your working on. This gives you an instant overview and allows you to quickly see what has changed.</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Apr/Qubit_active_tabs-1460714142961.PNG"" alt=""Qubit Opentag v3 active tabs""></p>

<p><em>Qubit shows you an overview of your recent changes by opening a tab for each changed tag, rule or variable.</em></p>

<h3 id=""3thethreestatesoftags"">3: The three states of tags</h3>

<p>Qubit allows you to set a tag to three possible states:</p>

<ul>
<li><strong>Active</strong>: when published, the tag will be active for everyone.</li>
<li><strong>Inactive</strong>: when published, the tag will only be active if you run it manually.</li>
<li><strong>Archived</strong>: when published, the tag will not be active.</li>
</ul>

<p>The inactive state is a great way to test new tags. As you can see, Qubit doesn’t allow you to delete tags, you’re only able to archive tags. The benefit of archiving over deletion, is that you’ll be able to unarchive old tags instantly. Say your client doesn’t want to use a heatmap tool anymore. You archive it. After six months, they change their mind and see great value in that same tool. All you have to is unarchive the tag, and you’re done. </p>

<h3 id=""4managingtagsrulesandvariables"">4: Managing tags, rules and variables</h3>

<p>Compared to GTM, Qubit had one key point to catch up to: centralized management of tags, rules and variables. Before version 3, you had to set up a rule per tag. Not optimal, but manageable (because we could work with tag dependencies). Luckily, all has changed, and tags, rules and variables are ‘first class citizens’ now.</p>

<h3 id=""otherbenefits"">Other benefits</h3>

<p>Besides the big updates mentioned above, Qubit Opentag has some other great features in their TMS:</p>

<ul>
<li>A/B test deliver tags.</li>
<li>Pure JS tags (no <code>&lt;script&gt;&lt;/script&gt;</code> tag required).</li>
<li>Copy tags from one container to another.</li>
</ul>

<p>Now you know what's great about Qubit, it's time to see how it compares to the well known alternative by Google. </p>

<h2 id=""qubitopentagcomparedtogoogletagmanager"">Qubit Opentag compared to Google Tag Manager</h2>

<p>When comparing tag managers, I look at four things:</p>

<ul>
<li>1: features</li>
<li>2: data layer</li>
<li>3: support</li>
<li>4: pricing</li>
</ul>

<h3 id=""1features"">1: Features</h3>

<p>Both GTM and Qubit support roughly the same features. Some features are unique to Qubit and offer a benefit, e.g. tag dependencies, tracking changes with active tabs and tag specific versions. Other features are better in GTM, e.g. variable management. In Qubit, variable management is more quirky as it should be. You set up a variable, and then you’ll need to create a tag specific script parameter to use it. An unnecessary extra step. Though in practice we haven’t really run into this isue, because of the tag dependencies (we only need to put the Google Analytics property ID in the core tag for example).</p>

<h3 id=""2datalayer"">2: Data layer</h3>

<p>Regarding the data layer, my vote goes to Qubit’s Universal Variable setup. I’ve explained the differences of both in detail in <a href=""https://www.themarketingtechnologist.co/the-ceddl-a-data-layer-standard-that-never-was/"">my post about the CEDDL standard</a>. In short, my vote goes to Qubit because the data layer is a) more readable, and b) setup as an easy to use data layer, whereas Google’s is really focused on measuring Google Analaytics via GTM. </p>

<h3 id=""3support"">3: Support</h3>

<p>There’s a key difference here. Google has a large community of users. This will make it easy for the community to answer any GTM related questions. Because of this, you’ll hardly ever contact support. Qubit on the other hand, has great support that’ll normally reply within a day, or within the hour if it’s urgent.</p>

<h3 id=""4pricing"">4: Pricing</h3>

<p>Qubit is free if your volumes are low enough. If you keep to a maximum of 1 million page loads a month, you won’t have any costs. If you top that number, it’ll be $99/mo. After this, it’ll be an extra $99 for each 10 million page loads. GTM is always free. The good thing is that Qubit offers support.</p>

<h2 id=""conclusion"">Conclusion</h2>

<p>Currently, I would pick Qubit over GTM because of several key differences. Because of these differences, it feels more suited for users with a background in development compared to GTM. Therefore, it's a better fit for me personally.   This doesn’t mean that GTM is a bad choice, some of our clients have it running as well, and I also use it for some personal projects.</p>

<p>At Greenhouse Group, we strongly believe in picking the best tool for our clients challenges. Right now, it’s Qubit (though GTM is close), but this might change in the future. To make sure we’re flexible with our solutions, we run tests where we mix different setups: e.g. Qubit Opentag with Google’s dataLayer, or Google Tag Manager with Qubit’s Universal Variable. As long as they both support custom JavaScript tags, we’re able to interchange tag managers if one of them would greatly outperform the other. A comforting thought. </p>

<p>If you’re familiar with another tag manager and like to share your experiences with it, let us know in the comments. </p>
        ","At Greenhouse Group, we’ve been working with tag manager for over 3.5 years now. Over the last part of that period, a lot of articles popped up about Google Tag Manager (GTM), Google’s solution to tag management. It’s a great option, but there are some other vendors out there that also offer great tag management systems (TMS). A TMS that we have used and still use a lot is Qubit Opentag, which was recently updated to version 3. So I thought now would be a good time to discuss this TMS and how it compares to GTM.
How Qubit became our tag manager of choice
First, let’s rewind back to late 2012. We were just starting to look at tag management options for our clients. Qubit popped up as our number one choice because of three reasons:
It has a built-in cookie consent window.
It supports tag dependencies.
It is open source.
The first benefit is clear, as cookie consent windows were about to become mandatory for all websites in the Netherlands (where most of our clients are located).
The second one is extremely powerful. The option for dependencies allows you to easily fire tags in order. Let’s look at a basic Google Analytics setup as an example:
GA core tag: first, we load our GA core tag on all pages. It only loads the library and configures the tracker.
GA page view tag: this tag also loads on all pages, but is dependent on the GA core tag. Because of that, it will only fire after the core has fired.
This feature is very powerful because you break your tags down into several components. If there’s a new setting we want to apply to each analytics hit we send (page view, event and ecommerce). We simply add it to the core tag, and it’ll be applied to all dependent tags.
The third benefit (open source) allowed us to look under the hood and see how the TMS works.
Back then, GTM didn't have as many features as Qubit Opentag, and therefore it wasn't a serious option for us.
What’s new with Qubit Opentag version 3
Over the course of 2014 and 2015, GTM catched up to Qubit Opentag, and in some areas even surpassed it. Luckily Qubit released version 3 of their TMS in February 2016. With it, Qubit upped their TMS game. These were the four most important updates for us:
1: Tag specific versioning
One thing we really missed in Qubit, especially over the later part of the past 3.5 years, was versioning. With version 3, they've added versioning to their TMS, but not as other TMS vendors do it. Where you normally have a version for each release, Qubit has a version for each tag. So if you release a big update, and only one tag has an issue, you can simple roll back to an old version of that one specific tag.
Qubit Change History. It allows you to roll back to an older version of a specific tag with the icon on the right.
2: Keep track of changes with tabs
The second great improvement is the use of tabs. When you’re working on several changes, including tags, load rules, and/or variables, it can be hard to keep track of what you've been working on. Qubit Opentag opens a separate tab within their interface for every change your working on. This gives you an instant overview and allows you to quickly see what has changed.
Qubit shows you an overview of your recent changes by opening a tab for each changed tag, rule or variable.
3: The three states of tags
Qubit allows you to set a tag to three possible states:
Active: when published, the tag will be active for everyone.
Inactive: when published, the tag will only be active if you run it manually.
Archived: when published, the tag will not be active.
The inactive state is a great way to test new tags. As you can see, Qubit doesn’t allow you to delete tags, you’re only able to archive tags. The benefit of archiving over deletion, is that you’ll be able to unarchive old tags instantly. Say your client doesn’t want to use a heatmap tool anymore. You archive it. After six months, they change their mind and see great value in that same tool. All you have to is unarchive the tag, and you’re done.
4: Managing tags, rules and variables
Compared to GTM, Qubit had one key point to catch up to: centralized management of tags, rules and variables. Before version 3, you had to set up a rule per tag. Not optimal, but manageable (because we could work with tag dependencies). Luckily, all has changed, and tags, rules and variables are ‘first class citizens’ now.
Other benefits
Besides the big updates mentioned above, Qubit Opentag has some other great features in their TMS:
A/B test deliver tags.
Pure JS tags (no <script></script> tag required).
Copy tags from one container to another.
Now you know what's great about Qubit, it's time to see how it compares to the well known alternative by Google.
Qubit Opentag compared to Google Tag Manager
When comparing tag managers, I look at four things:
1: features
2: data layer
3: support
4: pricing
1: Features
Both GTM and Qubit support roughly the same features. Some features are unique to Qubit and offer a benefit, e.g. tag dependencies, tracking changes with active tabs and tag specific versions. Other features are better in GTM, e.g. variable management. In Qubit, variable management is more quirky as it should be. You set up a variable, and then you’ll need to create a tag specific script parameter to use it. An unnecessary extra step. Though in practice we haven’t really run into this isue, because of the tag dependencies (we only need to put the Google Analytics property ID in the core tag for example).
2: Data layer
Regarding the data layer, my vote goes to Qubit’s Universal Variable setup. I’ve explained the differences of both in detail in my post about the CEDDL standard. In short, my vote goes to Qubit because the data layer is a) more readable, and b) setup as an easy to use data layer, whereas Google’s is really focused on measuring Google Analaytics via GTM.
3: Support
There’s a key difference here. Google has a large community of users. This will make it easy for the community to answer any GTM related questions. Because of this, you’ll hardly ever contact support. Qubit on the other hand, has great support that’ll normally reply within a day, or within the hour if it’s urgent.
4: Pricing
Qubit is free if your volumes are low enough. If you keep to a maximum of 1 million page loads a month, you won’t have any costs. If you top that number, it’ll be $99/mo. After this, it’ll be an extra $99 for each 10 million page loads. GTM is always free. The good thing is that Qubit offers support.
Conclusion
Currently, I would pick Qubit over GTM because of several key differences. Because of these differences, it feels more suited for users with a background in development compared to GTM. Therefore, it's a better fit for me personally. This doesn’t mean that GTM is a bad choice, some of our clients have it running as well, and I also use it for some personal projects.
At Greenhouse Group, we strongly believe in picking the best tool for our clients challenges. Right now, it’s Qubit (though GTM is close), but this might change in the future. To make sure we’re flexible with our solutions, we run tests where we mix different setups: e.g. Qubit Opentag with Google’s dataLayer, or Google Tag Manager with Qubit’s Universal Variable. As long as they both support custom JavaScript tags, we’re able to interchange tag managers if one of them would greatly outperform the other. A comforting thought.
If you’re familiar with another tag manager and like to share your experiences with it, let us know in the comments.","[tag management, data layer, qubit opentag, qubit, google tag manager, gtm, tms, Analytics]"
52,3D print marketing and best practices,/3d-print-marketing-and-best-practices/,"
            <p><p style=""color:#7e4396; font-weight:bold"">This post concerns Greenhouse Group Labs, an innovation program for students, established by Greenhouse Group. Labs is an ideal opportunity to test the latest technologies available, while allowing for talented young individuals to deeply explore them and come up with groundbreaking solutions.</p>3D printing is the process in which a 3D object is created from a digital file. This is also known as an additive process. In this process, an object is manufactured by laying down successive layers of material until an object is created. Since 3D printing started booming in 2012, it became clear that it would not be long before the technology would hit consumer level. 3D printing has actually been around since the early 1980’s, but it remained a niche market until a key-patent expired in 2009 allowing startups to appear.  </p>

<h3 id=""ontheriseinmarketing"">On the rise, in marketing?</h3>

<p>According to Gartner, consumer 3D printing will hit the plateau of productivity in five to ten years, while enterprise 3D printing will hit the plateau in already two to five years. Meaning, that by the time these technologies hit the plateau, about 30% of the potential market has adopted the technology. </p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Apr/2-1459955587989.png"" alt="""" class=""full-img"">
<em>Figure1: Gartner's 2015 Hype Cycle</em></p>

<p>Businesses are never far behind on innovations such as 3D printing. A number of brands have already been experimenting with this technology. In the past few years brands like Coca Cola, Warner Bros. and LEGO have been taking on 3D printing as a method to gain advantage over their competitors. In 2013, Coca-Cola launched a campaign where people could create their own mini-me on their smartphones and print them at the Coca-Cola factory. This was launched alongside their fun <a href=""https://www.youtube.com/watch?v=V0FzVNKg6Jo"">marketing campaign</a>.</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Apr/cola-1459955570332.png"" alt="""" class=""full-img"">
<em>Figure 2: Coca-Cola Mini-me</em></p>

<p>By the end of 2013, Warner Bros. used 3D printing in the marketing campaign for The Hobbit: Desolation of Smaug. Anyone interested could download the digital blueprint of the Key to Erebor so they could print the key. Especially for the hardcore fans, it added great appeal. </p>

<p>Even though a number of big brands have been using 3D printing in their campaigns, it has yet to take off in its full form. This means that there is still a lot of room for experimentation and pioneers to use it in their advantage, or that it is just a gimmick that will phase out quickly. Since consumer 3D printing is just around the corner, new possibilities appear. Companies could, for example, offer unique downloadable goods and props for consumers to print.</p>

<h3 id=""printingmoney"">Printing money</h3>

<p>Even though fiddling around with 3D printers, creating designs and making objects is cool and all, at heart, I’m still a hardcore business-guy. When I first touched a 3D printer my mind started racing. The possibilities are endless; I can make whatever I want, when I want to. This would enable pioneers and entrepreneurs to set themselves free from constraints that traditional manufacturing has. </p>

<p>I always find myself a bit in the through of disillusionment. There are so many possibilities. 3D printing breaks down barriers. But still, I found myself wondering, struggling and overwhelmed about turning this technology in a valuable business. 3D printing has many pros and cons. Is it a real valuable business asset? Or is 3D printing just an overly hyped gimmick?</p>

<p>I will try to save you the emotional roller-coaster I encountered on my path to finding business in 3D printing. I funneled the ideas, ups and downs to a point where I had a certain understanding of the possibilities the technology offers.</p>

<p><strong>Offer your 3D printer as a service</strong> – This concept is as simple as purchasing something and trying to make a profit by renting it out. The real downside to this concept is the initial investment it requires. Purchasing a 3D printer can cost you anywhere ranging from hundreds to thousands of euros. This is also not the right choice if you are looking to make a quick buck. The time before you get a Return on Investment can be long. Companies such as Shapeways have used this model to generate over 1.7 million revenue over 2014. </p>

<p><strong>Sell your designs</strong> – in contrary to the previous model, this is a way to quickly generate revenue. After creating a design in for example TinkerCAD, you can sell your model on a platform. </p>

<p><strong>Sell customized goods</strong> – 3D printing has so far been used the most to create goods such as toys. With a little creativity you can create your own, customized product which you can sell. Here’s some untapped ideas: why don’t you make a 3D model of a pregnant woman’s echo and print it for the couple? Why can’t people design and print their own earbuds yet? Why can’t people design their own jewelry that they receive the next day? These are all things that 3D printing makes possible. </p>

<p><strong>Spare costs on R&amp;D</strong> – Whenever you are developing a new product, there will come a point where you prototype your concept. 3D printing enables you to rapidly iterate the prototyping phase. You can have a constant hands-on experience with you own product, tweak it and print it again. 3D printing can broaden your horizon, make you think faster and make you innovate better. For example, the aerospace industry uses 3D printing to test new models in a wind tunnel. The architecture industry uses it to make pretty and durable models and the automotive industry uses it to prototype parts quickly. </p>

<p><strong>Make your life fun!</strong> – You can have fun while you save money! The number of useful, money saving object is enormous. For example, I’ve printed my own smartphone cover, a business card holder, microphone stand and custom-designed company keychains we can use for marketing and fun.</p>

<h3 id=""creatingisfungivingisevenmorefun"">Creating is fun, giving is even more fun!</h3>

<p>LABS KIDS was a recently organized event at Greenhouse Group’s location in which kids were taken on a journey in the digital world of tech and innovation. 3D printing was used to teach the kids about the future of technology (and just have fun).</p>

<p>For the event, we decided it would be nice if we could give the kids something cool to take with them. So the 3D printer, once again, showed some good use. The 3D print sweatshop opened and we printed around 50 Erlenmeyer keychains. </p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Apr/2-1459956223825.png"" alt=""""></p>

<h3 id=""printingtips"">Printing tips</h3>

<p>Printing tips are not sacred, you should always be experimenting and looking for ways to improve the quality of your prints. The following tips have helped me greatly and gave me the best results. </p>

<h4 id=""printsettingsfor"">Print settings for:</h4>

<p><strong>PLA</strong> – print nozzle temperature: 210°C, heated bed: 70°C (not necessary), print with fans on 100%</p>

<p><strong>ABS</strong> – print nozzle temperature: 250°C, heated bed: 110°C, fans off, do not print fast</p>

<p><strong>Printing high quality</strong></p>

<p><strong>PLA</strong> - If you want to achieve a high quality print with PLA, the basic guideline suggest printing a 60 micron surface at 50 mm/s with a 15% infill with the fans and retraction enabled.</p>

<p><strong>ABS</strong> - If you wish to achieve a high quality print with ABS, the basic guideline suggest printing at 40 mm/s with a 20% infill.</p>

<h4 id=""generaltips"">General tips</h4>

<p>A big issue many printers face is warping. Warping is especially bad when printing something with ABS filament. Warping means that the object sets itself free from the print bed and it bends (warps). Warping happens when the object cools down too rapidly. </p>

<p>It is best countered by taking the following measures:</p>

<ul>
<li>Use a heated bed (build platform), set it at 70°C for PLA and 110°C for ABS</li>
<li>Print with a raft – a flat lattice work of printed material underneath the object</li>
<li>Calibrate the build plate to make sure the first layer is perfect</li>
<li>Clean the surface – before printing clean the bed, get rid of any dust and other debris</li>
<li>Lower the printing speed</li>
</ul>

<p>Another way to counter warping is by making sure the plastic sticks better to the surface. This can be done by applying another polymer to the printing bed. Many people use Pritt or hairspray, but this makes cleaning the plate difficult. In my experience, using blue 3M masking tape seems to work just about perfect. If the object still warps, a way of raising the temperature around the printing bed is to seal the printer’s front side with cling film. This helps to isolate the platform and keep the warmth inside.</p>

<p>Purchase a digital caliper. These are infinitely useful for 3D printing. You can use them to measure filament, calibrate the printer and adjust your utility-oriented prints so you know it will fit perfectly. It will also be very useful when designing your own models. <br>
Lastly, do not treat your slicer software as a fast drive-through where you quickly pass through on your way to the printer. If the print settings in the slicing software are not correct, the print will not end well, no matter how good the printer is calibrated. Good printing settings can make life a lot easier. </p>
        ","This post concerns Greenhouse Group Labs, an innovation program for students, established by Greenhouse Group. Labs is an ideal opportunity to test the latest technologies available, while allowing for talented young individuals to deeply explore them and come up with groundbreaking solutions.
3D printing is the process in which a 3D object is created from a digital file. This is also known as an additive process. In this process, an object is manufactured by laying down successive layers of material until an object is created. Since 3D printing started booming in 2012, it became clear that it would not be long before the technology would hit consumer level. 3D printing has actually been around since the early 1980’s, but it remained a niche market until a key-patent expired in 2009 allowing startups to appear.
On the rise, in marketing?
According to Gartner, consumer 3D printing will hit the plateau of productivity in five to ten years, while enterprise 3D printing will hit the plateau in already two to five years. Meaning, that by the time these technologies hit the plateau, about 30% of the potential market has adopted the technology.
Figure1: Gartner's 2015 Hype Cycle
Businesses are never far behind on innovations such as 3D printing. A number of brands have already been experimenting with this technology. In the past few years brands like Coca Cola, Warner Bros. and LEGO have been taking on 3D printing as a method to gain advantage over their competitors. In 2013, Coca-Cola launched a campaign where people could create their own mini-me on their smartphones and print them at the Coca-Cola factory. This was launched alongside their fun marketing campaign.
Figure 2: Coca-Cola Mini-me
By the end of 2013, Warner Bros. used 3D printing in the marketing campaign for The Hobbit: Desolation of Smaug. Anyone interested could download the digital blueprint of the Key to Erebor so they could print the key. Especially for the hardcore fans, it added great appeal.
Even though a number of big brands have been using 3D printing in their campaigns, it has yet to take off in its full form. This means that there is still a lot of room for experimentation and pioneers to use it in their advantage, or that it is just a gimmick that will phase out quickly. Since consumer 3D printing is just around the corner, new possibilities appear. Companies could, for example, offer unique downloadable goods and props for consumers to print.
Printing money
Even though fiddling around with 3D printers, creating designs and making objects is cool and all, at heart, I’m still a hardcore business-guy. When I first touched a 3D printer my mind started racing. The possibilities are endless; I can make whatever I want, when I want to. This would enable pioneers and entrepreneurs to set themselves free from constraints that traditional manufacturing has.
I always find myself a bit in the through of disillusionment. There are so many possibilities. 3D printing breaks down barriers. But still, I found myself wondering, struggling and overwhelmed about turning this technology in a valuable business. 3D printing has many pros and cons. Is it a real valuable business asset? Or is 3D printing just an overly hyped gimmick?
I will try to save you the emotional roller-coaster I encountered on my path to finding business in 3D printing. I funneled the ideas, ups and downs to a point where I had a certain understanding of the possibilities the technology offers.
Offer your 3D printer as a service – This concept is as simple as purchasing something and trying to make a profit by renting it out. The real downside to this concept is the initial investment it requires. Purchasing a 3D printer can cost you anywhere ranging from hundreds to thousands of euros. This is also not the right choice if you are looking to make a quick buck. The time before you get a Return on Investment can be long. Companies such as Shapeways have used this model to generate over 1.7 million revenue over 2014.
Sell your designs – in contrary to the previous model, this is a way to quickly generate revenue. After creating a design in for example TinkerCAD, you can sell your model on a platform.
Sell customized goods – 3D printing has so far been used the most to create goods such as toys. With a little creativity you can create your own, customized product which you can sell. Here’s some untapped ideas: why don’t you make a 3D model of a pregnant woman’s echo and print it for the couple? Why can’t people design and print their own earbuds yet? Why can’t people design their own jewelry that they receive the next day? These are all things that 3D printing makes possible.
Spare costs on R&D – Whenever you are developing a new product, there will come a point where you prototype your concept. 3D printing enables you to rapidly iterate the prototyping phase. You can have a constant hands-on experience with you own product, tweak it and print it again. 3D printing can broaden your horizon, make you think faster and make you innovate better. For example, the aerospace industry uses 3D printing to test new models in a wind tunnel. The architecture industry uses it to make pretty and durable models and the automotive industry uses it to prototype parts quickly.
Make your life fun! – You can have fun while you save money! The number of useful, money saving object is enormous. For example, I’ve printed my own smartphone cover, a business card holder, microphone stand and custom-designed company keychains we can use for marketing and fun.
Creating is fun, giving is even more fun!
LABS KIDS was a recently organized event at Greenhouse Group’s location in which kids were taken on a journey in the digital world of tech and innovation. 3D printing was used to teach the kids about the future of technology (and just have fun).
For the event, we decided it would be nice if we could give the kids something cool to take with them. So the 3D printer, once again, showed some good use. The 3D print sweatshop opened and we printed around 50 Erlenmeyer keychains.
Printing tips
Printing tips are not sacred, you should always be experimenting and looking for ways to improve the quality of your prints. The following tips have helped me greatly and gave me the best results.
Print settings for:
PLA – print nozzle temperature: 210°C, heated bed: 70°C (not necessary), print with fans on 100%
ABS – print nozzle temperature: 250°C, heated bed: 110°C, fans off, do not print fast
Printing high quality
PLA - If you want to achieve a high quality print with PLA, the basic guideline suggest printing a 60 micron surface at 50 mm/s with a 15% infill with the fans and retraction enabled.
ABS - If you wish to achieve a high quality print with ABS, the basic guideline suggest printing at 40 mm/s with a 20% infill.
General tips
A big issue many printers face is warping. Warping is especially bad when printing something with ABS filament. Warping means that the object sets itself free from the print bed and it bends (warps). Warping happens when the object cools down too rapidly.
It is best countered by taking the following measures:
Use a heated bed (build platform), set it at 70°C for PLA and 110°C for ABS
Print with a raft – a flat lattice work of printed material underneath the object
Calibrate the build plate to make sure the first layer is perfect
Clean the surface – before printing clean the bed, get rid of any dust and other debris
Lower the printing speed
Another way to counter warping is by making sure the plastic sticks better to the surface. This can be done by applying another polymer to the printing bed. Many people use Pritt or hairspray, but this makes cleaning the plate difficult. In my experience, using blue 3M masking tape seems to work just about perfect. If the object still warps, a way of raising the temperature around the printing bed is to seal the printer’s front side with cling film. This helps to isolate the platform and keep the warmth inside.
Purchase a digital caliper. These are infinitely useful for 3D printing. You can use them to measure filament, calibrate the printer and adjust your utility-oriented prints so you know it will fit perfectly. It will also be very useful when designing your own models.
Lastly, do not treat your slicer software as a fast drive-through where you quickly pass through on your way to the printer. If the print settings in the slicing software are not correct, the print will not end well, no matter how good the printer is calibrated. Good printing settings can make life a lot easier.","[Labs, 3D Printing, marketing]"
53,The differences between Adobe Experience Design and Sketch App,/the-differences-between-adobe-experience-designer-and-sketch-app/,"
            <p><strong>Coming from an Adobe Fireworks background I was at least to say displeased with Adobe pulling the plug on my favourite tool. Me and the rest of the Fireworks community felt abandoned by Adobe. How would they ever make up for this mischief?</strong></p>

<p>I have continued to use Adobe Fireworks because there is no better alternative, except <a href=""https://www.sketchapp.com/"">Sketch</a>. Unfortunately using Sketch at my company is not really an option since not everybody uses a Mac (yet), and we have a Creative Cloud subscription that makes it inefficient to buy yet another license for a tool.</p>

<h2 id=""projectcometisnowadobeexperiencedesign"">Project Comet is now Adobe Experience Design</h2>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Mar/Screen_Shot_2016_03_24_at_17_04_47-1458835521329.png"" alt="""">
Adobe's <a href=""https://twitter.com/mchaize"">Michael Chaize</a> announced project 'Comet' last year at the DDA conference in Eindhoven. I was very excited to hear about a possible competitor for Sketch. So up to until a few weeks ago I waited for this tool I was really looking forward to. When I was expecting a tool named 'Comet' I found out it was released under the name 'Adobe Experience Design'. Here are some notes of my first impression about Adobes new tool.</p>

<h2 id=""thingsilikeaboutadobexd"">Things I like about Adobe XD</h2>

<ul>
<li>Unlike most of Adobe's professional apps, Adobe XD is fast and has a friendly user interface. </li>
</ul>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Apr/xd01-1459841199753.gif"" alt="""" class=""full-img""></p>

<ul>
<li>The Prototyping mode is easy to use en encourages to create interactive versions of your design. Also worked well on my mobile devices.</li>
</ul>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Mar/Screen_Shot_2016_03_23_at_21_14_57-1458836523178.png"" alt="""" class=""full-img""></p>

<ul>
<li>Repeat Grid is awesome. Easily create a list of blog posts or navigation for example. A real time saver.</li>
</ul>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Apr/xd02-1459841479334.gif"" alt="""" class=""full-img""></p>

<ul>
<li>Artboards with custom and device based dimension presets work really nice.</li>
</ul>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Mar/Screen_Shot_2016_03_23_at_21_13_31-1458835442044.png"" alt="""" class=""full-img""></p>

<h2 id=""thedifferencesbetweenadobeexperiencedesignandsketchapp"">The differences between Adobe Experience Design and Sketch App</h2>

<p>If you expect a tool that is similar to Sketch, you will turn out disappointed just like me. Sketch is a full-grown UI design tool, and has great prototyping possibilities as well. After trying Adobe XD for a short while, the absence of essential features came to light.</p>

<p>Fortunately most of these features below are on the roadmap for 2016 (?). But the lack of these features in makes this current version not usable for me at this time.</p>

<ul>
<li>Integration of plugins. This is a big thing in Sketch and hopefully will be in Adobe XD as well</li>
<li>Ability to use styling on multiple elements at once</li>
<li>A 'CSS like' unit system (e.g. for shadows, borders etc.)</li>
<li>Possibility to export CSS styles</li>
<li>Typography features are very basic now</li>
<li>Better exporting features. To PDF for example</li>
<li>Working with (image) masks is not intuitive</li>
</ul>

<h2 id=""communityinvolvement"">Community involvement</h2>

<p>Yes, this is a first (v0.5) release. So the real good stuff still is in the pipeline and hopefully Adobe is involving the community, is open for discussion and will listen to feedback this time. That would be great. </p>

<p>Quoting <a href=""http://blogs.adobe.com/creativecloud/update-on-project-comet-where-we-are-and-whats-to-come/"">this post</a> there are is a lot of sweet stuff coming up in 2016. </p>

<ul>
<li>Working directly with layers in a document</li>
<li>Reusability and sharing of design assets and styles, leveraging Creative Cloud libraries</li>
<li>Specific web design capabilities to aid responsive web design and CSS style creation</li>
<li>Going beyond screen-to-screen transitions, to include support for micro-interactions and animation of elements within an art board</li>
<li>Previewing and making edits in real-time on connected mobile devices</li>
<li>Deeper workflows with both Photoshop and Illustrator, to make it easy to use the advanced capabilities of those tools in conjunction with Project Comet</li>
<li>Making it possible to use real-world data as part of the design</li>
<li>Being able to extend Project Comet via a JavaScript API, to support plug-ins and custom workflows</li>
</ul>

<p>It looks like Adobe is off to claim their share of the 'UI &amp; Prototype tool' market. Hopefully this will pull the UI community away from Photoshop and they will make it up to the old Fireworks community. I will keep you updated on future updates.</p>
        ","Coming from an Adobe Fireworks background I was at least to say displeased with Adobe pulling the plug on my favourite tool. Me and the rest of the Fireworks community felt abandoned by Adobe. How would they ever make up for this mischief?
I have continued to use Adobe Fireworks because there is no better alternative, except Sketch. Unfortunately using Sketch at my company is not really an option since not everybody uses a Mac (yet), and we have a Creative Cloud subscription that makes it inefficient to buy yet another license for a tool.
Project Comet is now Adobe Experience Design
Adobe's Michael Chaize announced project 'Comet' last year at the DDA conference in Eindhoven. I was very excited to hear about a possible competitor for Sketch. So up to until a few weeks ago I waited for this tool I was really looking forward to. When I was expecting a tool named 'Comet' I found out it was released under the name 'Adobe Experience Design'. Here are some notes of my first impression about Adobes new tool.
Things I like about Adobe XD
Unlike most of Adobe's professional apps, Adobe XD is fast and has a friendly user interface.
The Prototyping mode is easy to use en encourages to create interactive versions of your design. Also worked well on my mobile devices.
Repeat Grid is awesome. Easily create a list of blog posts or navigation for example. A real time saver.
Artboards with custom and device based dimension presets work really nice.
The differences between Adobe Experience Design and Sketch App
If you expect a tool that is similar to Sketch, you will turn out disappointed just like me. Sketch is a full-grown UI design tool, and has great prototyping possibilities as well. After trying Adobe XD for a short while, the absence of essential features came to light.
Fortunately most of these features below are on the roadmap for 2016 (?). But the lack of these features in makes this current version not usable for me at this time.
Integration of plugins. This is a big thing in Sketch and hopefully will be in Adobe XD as well
Ability to use styling on multiple elements at once
A 'CSS like' unit system (e.g. for shadows, borders etc.)
Possibility to export CSS styles
Typography features are very basic now
Better exporting features. To PDF for example
Working with (image) masks is not intuitive
Community involvement
Yes, this is a first (v0.5) release. So the real good stuff still is in the pipeline and hopefully Adobe is involving the community, is open for discussion and will listen to feedback this time. That would be great.
Quoting this post there are is a lot of sweet stuff coming up in 2016.
Working directly with layers in a document
Reusability and sharing of design assets and styles, leveraging Creative Cloud libraries
Specific web design capabilities to aid responsive web design and CSS style creation
Going beyond screen-to-screen transitions, to include support for micro-interactions and animation of elements within an art board
Previewing and making edits in real-time on connected mobile devices
Deeper workflows with both Photoshop and Illustrator, to make it easy to use the advanced capabilities of those tools in conjunction with Project Comet
Making it possible to use real-world data as part of the design
Being able to extend Project Comet via a JavaScript API, to support plug-ins and custom workflows
It looks like Adobe is off to claim their share of the 'UI & Prototype tool' market. Hopefully this will pull the UI community away from Photoshop and they will make it up to the old Fireworks community. I will keep you updated on future updates.",[]
54,Improving Optimizely's Google Tag Manager integration,/improving-optimizelys-gtm-integration/,"
            <p>Adding Optimizely data to Google Analytics has a few benefits. First of all, you can verify Optimizely's results in GA, which is always a good thing. By segmenting Optimizely-tested users in analytics, you also open up a world of extra reports, metrics and visualisations, that you won't find in Optimizely. You won't have to create goals for each funnelstep anymore, you can just view the funnel dropoff for your tested users right there in Google Analytics.</p>

<p>After <a href=""https://www.themarketingtechnologist.co/improving-visual-website-optimizers-gtm-integration/"">improving Visual Website Optimizer's GTM integration</a>, I thought the same should be possible for Optimizely. While Optimizely's Google Tag Manager integration <a href=""https://help.optimizely.com/hc/en-us/articles/200039995-Integrating-Optimizely-with-Google-Universal-Analytics"">works pretty well</a>, it's not exactly an easy process to set up. Optimizely requires you to change your Google Analytics tracking code and preferably set a fixed tracker name. If you don't want to (or can't) change your GA tracking code, or you're using GTM's built-in pageview tag, <a href=""https://help.optimizely.com/hc/en-us/articles/203189220?flash_digest=78b96df1befbb743682392ea3441e079e5b90d3f"">you're kind of out of luck</a>. Even if you do get it to work, you're going to have to remember to enable the integration for every test you start. If you're like me, you'll forget that every single time. </p>

<p>So what you really need is a <em>'set it and forget it'</em>-tag, that just does its thing and doesn't get in your way. Luckily, for most use-cases, that's pretty easy, because Optimizely provides us with all the information we need in their javascript object.</p>

<h3 id=""gettingthecorrectoptimizelydata"">Getting the correct Optimizely data</h3>

<p>The first thing you need, is a GTM <em>Custom HTML Tag</em> that gets the relevant data from the Optimizely javascript object: <br>
<img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Mar/Screen_Shot_2016_03_31_at_10_02_11-1459416885888.png"" alt=""alt text"" class=""full-img""></p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""tag"">&lt;script&gt;</span><span class=""pln"">  
</span><span class=""kwd"">function</span><span class=""pln""> optimAnaFire</span><span class=""pun"">(){</span><span class=""pln"">  
  </span><span class=""kwd"">var</span><span class=""pln""> oActId </span><span class=""pun"">=</span><span class=""pln""> optimizely</span><span class=""pun"">.</span><span class=""pln"">activeExperiments</span><span class=""pun"">[</span><span class=""lit"">0</span><span class=""pun"">];</span><span class=""pln"">
  </span><span class=""kwd"">var</span><span class=""pln""> oActVar </span><span class=""pun"">=</span><span class=""pln""> optimizely</span><span class=""pun"">.</span><span class=""pln"">variationNamesMap</span><span class=""pun"">[</span><span class=""pln"">oActId</span><span class=""pun"">];</span><span class=""pln"">
  </span><span class=""kwd"">if</span><span class=""pun"">(</span><span class=""pln"">oActId </span><span class=""pun"">!==</span><span class=""pln""> </span><span class=""kwd"">undefined</span><span class=""pun"">)</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">        
  dataLayer</span><span class=""pun"">.</span><span class=""pln"">push</span><span class=""pun"">({</span><span class=""pln"">
    </span><span class=""str"">'optimizelyTestId'</span><span class=""pun"">:</span><span class=""pln""> oActId</span><span class=""pun"">,</span><span class=""pln"">
    </span><span class=""str"">'optimizelyVarName'</span><span class=""pun"">:</span><span class=""pln""> oActVar</span><span class=""pun"">,</span><span class=""pln"">
    </span><span class=""str"">'event'</span><span class=""pun"">:</span><span class=""str"">'optimizelyData'</span><span class=""pln"">
  </span><span class=""pun"">});</span><span class=""pln"">
  </span><span class=""pun"">}</span><span class=""pln"">
</span><span class=""pun"">}</span><span class=""pln"">
optimAnaFire</span><span class=""pun"">();</span><span class=""pln"">  
</span><span class=""tag"">&lt;/script&gt;</span><span class=""pln"">  </span></code></pre>

<p>Fire this tag on all pages, and if there is an active test on the page, the <code>optimizelyTestId</code> and <code>optimizelyVarName</code> are added to the GTM dataLayer. If you run in to issues getting the data in the dataLayer, the object data might not be ready yet. In that case, create a timed trigger for this tag, firing it after a few hundred miliseconds until you get results.</p>

<h3 id=""gettingtheabtestdatatogoogleanalytics"">Getting the A/B test data to Google Analytics</h3>

<p>Once you have the dataLayer variables for your active test, you need to add everything to Google Tag Manager. You'll need to create 2 variables, so you can use the dataLayer information in other tags, a trigger that fires when the Optimizely data is added to the dataLayer and a tag that actually sends the data to Google Analytics. </p>

<p>Let start with the variables: Create 2 variables, one for <code>optimizelyVarName</code> (shown below) and one for <code>optimizelyTestId</code>, both are Data Layer Variables: <br>
<img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Mar/Screen_Shot_2016_03_31_at_10_18_08-1459416953673.png"" alt=""alt text"" class=""full-img""></p>

<p>Next up, the trigger: When the first <em>Custom HTML Tag</em> adds the relevant data to the dataLayer, it also ads an event called <code>optimizelyData</code>. Create a new trigger that fires when this happens:</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Mar/Screen_Shot_2016_03_31_at_10_22_54-1459416963434.png"" alt=""alt text"" class=""full-img""></p>

<p>Almost there, now all we need to do is send all this information to Google Analytics:</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Mar/Screen_Shot_2016_03_31_at_10_25_54-1459416969924.png"" alt=""alt text"" class=""full-img""></p>

<p>This tag is where the magic happens; essentially, this is a regular GA event. For easy reference, call the category <code>optimizelyEvent</code> and add the testID variable as the event action. Now, when you test if everything works, you should see the events in GA's real-time view, you won't have to wait for the dimensions to be applied.</p>

<p>The most important part is the custom dimension: set the index to the number you set in your GA Admin. The value could be anything, I like to use <code>{{optimizelyTestID}}: {{optimizelyVarName}}</code>, which results in an easy to filter dimension. Remember to fire this tag on the Optimizely event you created earlier. You now have a custom dimension filled with your test ID's and the variation the users saw. Filtering and creating custom reports is instantly much easier, and you can plot lines like the Analytics Guru you are.</p>

<h4 id=""possibleissues"">Possible issues</h4>

<p>If you run in to problems, it might be because this method has some caveats:</p>

<ul>
<li>if you have troubles finding the testID in optimizely, go to a test's results page and check the URL.</li>
<li>The tags should run after the Optimizely object is filled, because Optimizely is loaded in the head of the page, and GTM is loaded at the beginning of the body. If you have timing issues, or advanced tests that use either the Optimizely API or JS triggering you'll have to delay the firing of the first tag. You can either do this by firing the tag on a timer, or changing the tag code to wait for the DOM to be ready. If you're using advanced triggering, I assume you know what to do...</li>
<li>Only the first active test is used in the script. For 99% of you, that should work fine, as you're probably not running multiple tests on the same page (why would you?). If you need multiple tests you will need to create multiple dimensions so data will not get overwritten and you need to create a loop for each <code>optimizely.activeExperiments</code> object.</li>
</ul>
        ","Adding Optimizely data to Google Analytics has a few benefits. First of all, you can verify Optimizely's results in GA, which is always a good thing. By segmenting Optimizely-tested users in analytics, you also open up a world of extra reports, metrics and visualisations, that you won't find in Optimizely. You won't have to create goals for each funnelstep anymore, you can just view the funnel dropoff for your tested users right there in Google Analytics.
After improving Visual Website Optimizer's GTM integration, I thought the same should be possible for Optimizely. While Optimizely's Google Tag Manager integration works pretty well, it's not exactly an easy process to set up. Optimizely requires you to change your Google Analytics tracking code and preferably set a fixed tracker name. If you don't want to (or can't) change your GA tracking code, or you're using GTM's built-in pageview tag, you're kind of out of luck. Even if you do get it to work, you're going to have to remember to enable the integration for every test you start. If you're like me, you'll forget that every single time.
So what you really need is a 'set it and forget it'-tag, that just does its thing and doesn't get in your way. Luckily, for most use-cases, that's pretty easy, because Optimizely provides us with all the information we need in their javascript object.
Getting the correct Optimizely data
The first thing you need, is a GTM Custom HTML Tag that gets the relevant data from the Optimizely javascript object:
<script>  
function optimAnaFire(){  
  var oActId = optimizely.activeExperiments[0];
  var oActVar = optimizely.variationNamesMap[oActId];
  if(oActId !== undefined) {        
  dataLayer.push({
    'optimizelyTestId': oActId,
    'optimizelyVarName': oActVar,
    'event':'optimizelyData'
  });
  }
}
optimAnaFire();  
</script>  
Fire this tag on all pages, and if there is an active test on the page, the optimizelyTestId and optimizelyVarName are added to the GTM dataLayer. If you run in to issues getting the data in the dataLayer, the object data might not be ready yet. In that case, create a timed trigger for this tag, firing it after a few hundred miliseconds until you get results.
Getting the A/B test data to Google Analytics
Once you have the dataLayer variables for your active test, you need to add everything to Google Tag Manager. You'll need to create 2 variables, so you can use the dataLayer information in other tags, a trigger that fires when the Optimizely data is added to the dataLayer and a tag that actually sends the data to Google Analytics.
Let start with the variables: Create 2 variables, one for optimizelyVarName (shown below) and one for optimizelyTestId, both are Data Layer Variables:
Next up, the trigger: When the first Custom HTML Tag adds the relevant data to the dataLayer, it also ads an event called optimizelyData. Create a new trigger that fires when this happens:
Almost there, now all we need to do is send all this information to Google Analytics:
This tag is where the magic happens; essentially, this is a regular GA event. For easy reference, call the category optimizelyEvent and add the testID variable as the event action. Now, when you test if everything works, you should see the events in GA's real-time view, you won't have to wait for the dimensions to be applied.
The most important part is the custom dimension: set the index to the number you set in your GA Admin. The value could be anything, I like to use {{optimizelyTestID}}: {{optimizelyVarName}}, which results in an easy to filter dimension. Remember to fire this tag on the Optimizely event you created earlier. You now have a custom dimension filled with your test ID's and the variation the users saw. Filtering and creating custom reports is instantly much easier, and you can plot lines like the Analytics Guru you are.
Possible issues
If you run in to problems, it might be because this method has some caveats:
if you have troubles finding the testID in optimizely, go to a test's results page and check the URL.
The tags should run after the Optimizely object is filled, because Optimizely is loaded in the head of the page, and GTM is loaded at the beginning of the body. If you have timing issues, or advanced tests that use either the Optimizely API or JS triggering you'll have to delay the firing of the first tag. You can either do this by firing the tag on a timer, or changing the tag code to wait for the DOM to be ready. If you're using advanced triggering, I assume you know what to do...
Only the first active test is used in the script. For 99% of you, that should work fine, as you're probably not running multiple tests on the same page (why would you?). If you need multiple tests you will need to create multiple dimensions so data will not get overwritten and you need to create a loop for each optimizely.activeExperiments object.","[tag management, Analytics, cro]"
55,What I’ve learned by giving a talk about data to 100 people,/what-ive-learned-by-giving-a-talk-about-data-to-100-people/,"
            <p>Last week, I gave my first ever talk for a large audience at the <a href=""http://nextsymposium.nl/"">Next Symposium on Growth Hacking</a> at Tilburg University.  The message of my talk: “data helps you make better decisions for your company, project or startup. Put it at the heart”. It was a very interesting experience, and because of that, I decided to share 6 key insights from that day.</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Mar/next_symposium-1458633928934.jpg"" alt=""Erik Driessen at Growth Hacking Symposium"" class=""full-img"">
<em>The intro of my talk at the Growth Hacking Symposium.</em></p>

<h2 id=""1challengeyourselfbygivingatalk"">1: Challenge yourself by giving a talk</h2>

<p>Let me tell you up front: I was quite nervous for giving my talk. I was hoping it would all go well, as I never did this for a large audience before. For me, the main challenge was to capture a big chunk of my day-to-day  activities into a 25-minute-long presentation. And to do so in an understandable manner.  It’s  a great exercise to see if you can capture the essence of your job into a small package.</p>

<h2 id=""2understandtheaudience"">2: Understand the audience</h2>

<p>It’s important to understand your audience. It’ll determine how you should approach the audience. For me the, audience was a mix of Marketing and Business/IT students. I was in luck because one of the Marketing students was graduating at my company. As he had more info about the audience, I set up a meeting with him to discuss my setup. He had some great suggestions  and because of these, I’ve improved my overall story. </p>

<h2 id=""3learnfromtheotherspeakers"">3: Learn from the other speakers</h2>

<p>The Symposium was about Growth Hacking. And though I know quite a lot about data, and data collection, this term was fairly new to me. One of the great things about talking at a Symposium, is that you get to enjoy the talks of the other speakers as well. Four very interesting talks. My key takeaway: growth hacking is missing one important word: system. So the correct phrase should be growth system hacking. The best way to achieve growth is by creating a growth system, test if it works, and if so, keep improving on it. If it doesn't work, create a new growth system. </p>

<h2 id=""4talksarenetworkboosters"">4: Talks are network boosters</h2>

<p>Whether you like it or not, you’ll have to network. As soon as the talks end, you’ll be talking with other speakers and people from the audience. The subjects varied from </p>

<blockquote>
  <p>What was the form analysis tool you mentioned again? I've been looking for that quite a while.</p>
  
  <p>(it's <a href=""https://www.themarketingtechnologist.co/introducing-formagical-form-analytics-with-dashboarding/"">Formagical</a>)</p>
</blockquote>

<p>to</p>

<blockquote>
  <p>I’d like to come work at Greenhouse Group as soon as I graduate, it seems like a great company to work at. </p>
  
  <p>(it really is). </p>
</blockquote>

<p>It’s great to hear what other people thought of your talk, and it’s one of the best ways to verify if the presentation actually worked. Besides this, the varied questions require you to apply your knowledge to very different situations. Again, this is a great moment to learn and get new insights. </p>

<h2 id=""5thedata"">5: The data</h2>

<p>Of course I’d like to look at the data behind this talk, as my talk was about data. I’ve noticed that LinkedIn is the best place to look for the impact of this talk:</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Mar/linkedin_metrics-1458633091014.PNG"" alt=""LinkedIn profile view increase"">
<em>The 'Who's vied your profile' graph from LinkedIn.</em></p>

<p>As you can see, the exposure to my LinkedIn profile greatly increased. I’ve connected with some interesting new people, both speakers and students. These are new connections that I can directly attribute to the talk. </p>

<h2 id=""6theimportanceofagoodhost"">6: The importance of a good host</h2>

<p>In between the talks, the chairman of the event is very important. It’s the glue that holds the talks together. During the Next Symposium, <a href=""https://www.linkedin.com/in/wout-van-wengerden-81271b23"">Wout van Wengerden</a> was the chairman, and if I remember it well, this was his first chairman experience (just as it was my first talk). </p>

<p>The most important thing: he did great. He had an introductory question for each speaker (we didn’t know what the question would be) that made every intro a bit more personal than just starting the talk. He also made sure to engage the audience whenever he could. He did a great job in keeping the symposium interactive and lively. </p>

<h2 id=""havefun"">Have fun</h2>

<p>In the end, it all comes down to having fun. You should enjoy setting up the talk, giving the talk and discussing the subject afterwards. If you haven’t done this before, give it a try. Personally, I had one of my greatest experiences ever. Now I know I like it, and I’d love to do it more often. </p>
        ","Last week, I gave my first ever talk for a large audience at the Next Symposium on Growth Hacking at Tilburg University. The message of my talk: “data helps you make better decisions for your company, project or startup. Put it at the heart”. It was a very interesting experience, and because of that, I decided to share 6 key insights from that day.
The intro of my talk at the Growth Hacking Symposium.
1: Challenge yourself by giving a talk
Let me tell you up front: I was quite nervous for giving my talk. I was hoping it would all go well, as I never did this for a large audience before. For me, the main challenge was to capture a big chunk of my day-to-day activities into a 25-minute-long presentation. And to do so in an understandable manner. It’s a great exercise to see if you can capture the essence of your job into a small package.
2: Understand the audience
It’s important to understand your audience. It’ll determine how you should approach the audience. For me the, audience was a mix of Marketing and Business/IT students. I was in luck because one of the Marketing students was graduating at my company. As he had more info about the audience, I set up a meeting with him to discuss my setup. He had some great suggestions and because of these, I’ve improved my overall story.
3: Learn from the other speakers
The Symposium was about Growth Hacking. And though I know quite a lot about data, and data collection, this term was fairly new to me. One of the great things about talking at a Symposium, is that you get to enjoy the talks of the other speakers as well. Four very interesting talks. My key takeaway: growth hacking is missing one important word: system. So the correct phrase should be growth system hacking. The best way to achieve growth is by creating a growth system, test if it works, and if so, keep improving on it. If it doesn't work, create a new growth system.
4: Talks are network boosters
Whether you like it or not, you’ll have to network. As soon as the talks end, you’ll be talking with other speakers and people from the audience. The subjects varied from
What was the form analysis tool you mentioned again? I've been looking for that quite a while.
(it's Formagical)
to
I’d like to come work at Greenhouse Group as soon as I graduate, it seems like a great company to work at.
(it really is).
It’s great to hear what other people thought of your talk, and it’s one of the best ways to verify if the presentation actually worked. Besides this, the varied questions require you to apply your knowledge to very different situations. Again, this is a great moment to learn and get new insights.
5: The data
Of course I’d like to look at the data behind this talk, as my talk was about data. I’ve noticed that LinkedIn is the best place to look for the impact of this talk:
The 'Who's vied your profile' graph from LinkedIn.
As you can see, the exposure to my LinkedIn profile greatly increased. I’ve connected with some interesting new people, both speakers and students. These are new connections that I can directly attribute to the talk.
6: The importance of a good host
In between the talks, the chairman of the event is very important. It’s the glue that holds the talks together. During the Next Symposium, Wout van Wengerden was the chairman, and if I remember it well, this was his first chairman experience (just as it was my first talk).
The most important thing: he did great. He had an introductory question for each speaker (we didn’t know what the question would be) that made every intro a bit more personal than just starting the talk. He also made sure to engage the audience whenever he could. He did a great job in keeping the symposium interactive and lively.
Have fun
In the end, it all comes down to having fun. You should enjoy setting up the talk, giving the talk and discussing the subject afterwards. If you haven’t done this before, give it a try. Personally, I had one of my greatest experiences ever. Now I know I like it, and I’d love to do it more often.","[Data, growth, talks]"
56,Introduction to data binding in Angular 2 versus Angular 1,/introduction-to-data-binding-in-angular-2-versus-angular-1/,"
            <p>Data binding, one of the most loved and hated concepts of Angular 1, made its way to Angular 2. There are a couple of ways to bind data in Angular: interpolation, one way binding (unidirectional), two-way binding and event binding. These four types of data binding were already available in Angular 1, but now in Angular 2 the syntax has changed a bit. In this post, I'd like to share with you how the Angular 2 syntax looks compared to Angular 1's.  </p>

<h3 id=""interpolation"">Interpolation</h3>

<p>Interpolation is the easiest and best-known way of data binding. In Angular 1, it looked likes this:</p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""tag"">&lt;h1&gt;</span><span class=""pln"">{{vm.employee.name}}</span><span class=""tag"">&lt;/h1&gt;</span><span class=""pln"">  </span></code></pre>

<p>In Angular 2, we can still use the curly braces, but we can omit <code>vm</code>, because we already have the context. Not really a big difference, but we can save some keystrokes here.  </p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""tag"">&lt;h1&gt;</span><span class=""pln"">{{employee.name}}</span><span class=""tag"">&lt;/h1&gt;</span><span class=""pln"">  </span></code></pre>

<h3 id=""onewaybinding"">One Way Binding</h3>

<p>The way we used to apply one-way binding is <code>ng-bind</code> just like this:  </p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""tag"">&lt;h1</span><span class=""pln""> </span><span class=""atn"">ng-bind</span><span class=""pun"">=</span><span class=""atv"">""employee.name""</span><span class=""tag"">&gt;&lt;/h1&gt;</span><span class=""pln"">  </span></code></pre>

<p>This is exactly what interpolation ends up to look like anyway. In Angular 2 we can use square brackets round the property we want to bind to.  </p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""tag"">&lt;h1</span><span class=""pln""> [</span><span class=""atn"">innerText</span><span class=""pln"">]</span><span class=""pun"">=</span><span class=""atv"">""employee.name""</span><span class=""tag"">&gt;&lt;/h1&gt;</span><span class=""pln"">  </span></code></pre>

<p>This may look a bit strange at first, but it definitely is valid HTML. We actually can take any HTML property, like <code>innerText</code>, wrap it in square braces and you can then bind it to a model. This could for example also work on <code>style</code> properties:</p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""tag"">&lt;span</span><span class=""pln""> [</span><span class=""atn"">style</span><span class=""pln"">.</span><span class=""atn"">backgroundColor</span><span class=""pln"">]</span><span class=""pun"">=</span><span class=""atv"">""employee.favouriteColor""</span><span class=""tag"">&gt;&lt;/span&gt;</span><span class=""pln"">  </span></code></pre>

<h3 id=""twowaybinding"">Two Way Binding</h3>

<p>The most used use case of two-way binding in Angular is using it on an <code>input</code> field or any other form elements. When we type something in the input on one side, the value goes to the controller and then back and forth. In Angular 1 we'd use the <code>ng-model</code> directive on the element and bind it to the value:</p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""tag"">&lt;input</span><span class=""pln""> </span><span class=""atn"">ng-model</span><span class=""pun"">=</span><span class=""atv"">""employee.name""</span><span class=""tag"">/&gt;</span><span class=""pln"">  </span></code></pre>

<p>In Angular 2 we also have a special directive called <code>ngModel</code>. </p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""tag"">&lt;input</span><span class=""pln""> [(</span><span class=""atn"">ngModel</span><span class=""pln"">)]</span><span class=""pun"">=</span><span class=""atv"">""employee.name""</span><span class=""tag"">/&gt;</span><span class=""pln"">  </span></code></pre>

<p>But notice how the syntax is just a bit different. We use the square brackets because it's a property, but we also use the parenthesis. This syntax is called Banana in a Box ([()]). What this means is when you see this syntax, it's two-way binding at work. </p>

<p>Also, in Angular 2 they dropped the convention of splitting words with dashes and use camelcase instead (ng-repeat vs ngFor, ng-if vs ngIf etc). </p>

<h3 id=""eventbinding"">Event Binding</h3>

<p>You can do event binding with any valid HTML event available, like click, focus or blur. In Angular 1 you should need built-in directives to use event binding. For example, for binding to a click, one would use:</p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""tag"">&lt;button</span><span class=""pln""> </span><span class=""atn"">ng-click</span><span class=""pun"">=</span><span class=""atv"">""vm.sendForm()""</span><span class=""tag"">&gt;</span><span class=""pln"">Send</span><span class=""tag"">&lt;/h1&gt;</span><span class=""pln"">  </span></code></pre>

<p>In Angular 2, we can just take the same property that is on the HTML element (<code>click</code> in the previous example) and wrap it in parenthesis. So, with properties we use square braces, but with events we use parenthesis:</p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""tag"">&lt;button</span><span class=""pln""> (</span><span class=""atn"">click</span><span class=""pln"">)</span><span class=""pun"">=</span><span class=""atv"">""sendForm()""</span><span class=""tag"">&gt;</span><span class=""pln"">Send</span><span class=""tag"">&lt;/h1&gt;</span><span class=""pln"">  </span></code></pre>

<p>I want to emphasize that there are no event directives like <code>click</code> in Angular 2, and that these are in fact HTML element events. Angular dropped tons of directives (like ng-click, ng-blur, ng-focus etc) because of this, and I think it's a great choice. </p>
        ","Data binding, one of the most loved and hated concepts of Angular 1, made its way to Angular 2. There are a couple of ways to bind data in Angular: interpolation, one way binding (unidirectional), two-way binding and event binding. These four types of data binding were already available in Angular 1, but now in Angular 2 the syntax has changed a bit. In this post, I'd like to share with you how the Angular 2 syntax looks compared to Angular 1's.
Interpolation
Interpolation is the easiest and best-known way of data binding. In Angular 1, it looked likes this:
<h1>{{vm.employee.name}}</h1>  
In Angular 2, we can still use the curly braces, but we can omit vm, because we already have the context. Not really a big difference, but we can save some keystrokes here.
<h1>{{employee.name}}</h1>  
One Way Binding
The way we used to apply one-way binding is ng-bind just like this:
<h1 ng-bind=""employee.name""></h1>  
This is exactly what interpolation ends up to look like anyway. In Angular 2 we can use square brackets round the property we want to bind to.
<h1 [innerText]=""employee.name""></h1>  
This may look a bit strange at first, but it definitely is valid HTML. We actually can take any HTML property, like innerText, wrap it in square braces and you can then bind it to a model. This could for example also work on style properties:
<span [style.backgroundColor]=""employee.favouriteColor""></span>  
Two Way Binding
The most used use case of two-way binding in Angular is using it on an input field or any other form elements. When we type something in the input on one side, the value goes to the controller and then back and forth. In Angular 1 we'd use the ng-model directive on the element and bind it to the value:
<input ng-model=""employee.name""/>  
In Angular 2 we also have a special directive called ngModel.
<input [(ngModel)]=""employee.name""/>  
But notice how the syntax is just a bit different. We use the square brackets because it's a property, but we also use the parenthesis. This syntax is called Banana in a Box ([()]). What this means is when you see this syntax, it's two-way binding at work.
Also, in Angular 2 they dropped the convention of splitting words with dashes and use camelcase instead (ng-repeat vs ngFor, ng-if vs ngIf etc).
Event Binding
You can do event binding with any valid HTML event available, like click, focus or blur. In Angular 1 you should need built-in directives to use event binding. For example, for binding to a click, one would use:
<button ng-click=""vm.sendForm()"">Send</h1>  
In Angular 2, we can just take the same property that is on the HTML element (click in the previous example) and wrap it in parenthesis. So, with properties we use square braces, but with events we use parenthesis:
<button (click)=""sendForm()"">Send</h1>  
I want to emphasize that there are no event directives like click in Angular 2, and that these are in fact HTML element events. Angular dropped tons of directives (like ng-click, ng-blur, ng-focus etc) because of this, and I think it's a great choice.","[Code, angularjs]"
57,Facebook marketing: 5 insights you can get out of the new Audience Overlap-tool,/facebook-marketing-5-insights-you-can-get-out-of-the-new-audience-overlap-tool-2/,"
            <p>Recently, my heart started beating faster. While I was uploading a new CRM file in Facebook for a customer, I discovered a new feature: Audience Overlap. With this tool you can differentiate target audiences / segments (you name it) and discover their similarities. I didn't have a use case that would benefit from this at first, but I had the feeling this had serious potential. After a few days of trial my enthusiasm hasn't gone down and I've listed my insights in this article.</p>

<h2 id=""audiences"">Audiences</h2>

<p>I can already hear the veteran Facebook marketeer say: ""Gaining insights for your audiences, don't you already have Facebook Audience Insights for that?"". Absolutely, this tool gives you fantastic insights about segments, but it can't answer what percentage of my Facebook likes visited my website in the last 90 days. Audience Overlap answers this question with great panache.</p>

<p>Before I share my insights, I'll quickly guide you through the different Facebook audience types.</p>

<h3 id=""savedaudiences"">Saved Audiences</h3>

<p>There isn't a better tool than Facebook to reach married mothers between the 30 and 40 years old, interested in football, right? When an advertiser wants to aim at a target audience he can save this as ""saved audiences"". A saved audience is a segment that an advertiser can create for the purpose of advertising. When you often want to reach the same target audience in various ads, this functionality is very handy and time efficient. You can create audiences based on location (country, province, city), gender, age, interests (like football), behaviour (like relationship status) and connections to the advertiser's page (like the Facebook likes of brand X).</p>

<h3 id=""customaudiences"">Custom Audiences</h3>

<p>When you upload CRM lists, or segment visitors through the Website Custom Audience (WCA) pixel as an advertiser, Facebook calls this Custom Audience. Besides CRM and website traffic it's also possible to make segments of App usage through an SDK.</p>

<h3 id=""lookalikeaudiences"">Look-a-like Audiences</h3>

<p>Look-a-like audiences are very useful for searching new customers, prospecting. You can let Facebook select the most similar 1 to 10 percent from a custom audience by looking at socio-demos, interests and behaviour.</p>

<p>To gain insights through the Audience Overlap tool, an audience needs to count at least 1000. This is to protect Facebook's users' privacy. You can combine endlessly with Audience Overlap. For example, you can compare saved audiences with look-a-like audiences, look-a-like audiences with custom audiences and custom audiences with saved audiences.</p>

<p>Every comparison output is always a percentage. This percentage shows how much percent of audience A overlaps with audience B, and vice versa. While many combinations only show brand-specific insights, there are many other common examples that everyone can benefit from. I'll share five with you.</p>

<h4 id=""1thevalueofyourfacebooklikes"">1. The value of your Facebook likes</h4>

<p>What's the ROI of social media and community building? Many marketeers and scientists have clamped onto this question in the last couple of years. We're getting closer to the answer with Audience Overlap. Of course we can measure our ads and read the Google Analytics results to see if someone came from Facebook. However, this is often cookie-based and not cross-device. Knowing Facebook has a very high mobile propagation, it's of high interest to see how like behaviour converts to website visits on desktop.</p>

<p>We segment website traffic through the WCA pixel for this customer. These custom audiences are cross-device since Facebook uses profiles instead of cookies. From the following analysis we can conclude that 37 percent of all page likes of this brand visited the website in the last 90 days. 9 percent even did this in the past 7 days. Obviously, you can make this more tangible by taking orders, shopping baskets and/or other on-site segments in mind.</p>

<h4 id=""2aguidefordeterminingretargetingstrategies"">2. A guide for determining retargeting strategies</h4>

<p>Enabling or disabling certain segments in a retargeting campaign could make or break the success. Audience Overlap can support you with this. The following analysis is made for a customer who's product is a subscription, and the customer panel (for invoices, etc.) is segmented through the WCA pixel. In these situations a decision is made to exclude visitors from their panels since they're already members.</p>

<p>This analysis shows that 28 percent of the converted product A visitors also visited the customer panel in the last 30 days. These are existing customers that upgrade their subscription package. Through this insight we didn't exclude the segment. Besides, we notice that 52 percent of the converting product A visitors also visited product B's product page. In this case it could be profitable to show product B in the retargeting campaign of product A.</p>

<h4 id=""3howsuccessfulisyourcrosssellstrategy"">3. How successful is your cross-sell strategy?</h4>

<p>When you enter the market with a new product as an established brand, existing customers often are important prospects for this new product. They're already familiar with the brand, and it's feasible to offer them package or synergy benefits.</p>

<p>These prospects are often reached through phone or email because these credentials are already present. The success of these contact attempts will be measured by sales, and in the case of email campaigns onsite metrics can be used. Through Audience Overlap's CRM files you can see the percentage of matches with your online funnel.</p>

<p>We could conclude 10 percent of the visitors that have put together a certain package, already owns product of this brand. Measuring irrespectively of what source they're coming from is Audience Overlap's great benefit. If it's through search, direct traffic, display or affiliates, Audience Overlap is able to match the visitor to the CRM file and determine the cross-sell's success. Despite not clicking the newsletter or following through after contacting the callcenter, the interest could be generated anyway.</p>

<h4 id=""4howaccurateismyfacebooktargeting"">4. How accurate is my Facebook targeting?</h4>

<p>The Small Business Owners segment (SMBO) might be one of the most used segments within Facebook targeting. I've always been partly skeptic about this segment, which holds an immense amount of 560,000 users. There are rumours that page admins are in this segment by default. That seems rather premature by me.</p>

<p>With Audience Overlap we were able to save the SMBO segment as a 'Saved Audience' and compare it to visitors of the business section of a website of one of our customers. From analysis it appears only 18 percent of visitors within the business section of the website is recognised as SMBO. Considering the size of the segment (560,000 users) I think this is a dreadfully low percentage. Even more so when you consider there's a Facebook campaign running on this segment, and its traffic is already included in the 18 percent!</p>

<h4 id=""5whatsthegreatestaffinityofmycustomerbase"">5. What's the greatest affinity of my customer base?</h4>

<p>Imagine: as a brand you're considering lending a sponsor contract to Soccer Club A, Soccer Club B or Soccer Club C, but you don't know which of these clubs matches your customers best. Audience Overlap can help you with this. Create three saved audiences with their interest set respectively to Soccer Club A, Soccer Club B and Soccer Club C and observe the overlap with the CRM file.</p>

<p>Based on the analysis below we can conclude that Soccer Club B has the most common ground with the existing customers. When the brand wants to avoid disagreements with their customers, Soccer Club B would be the most safe choice. A similar analysis can be used for media choices, partnerships, etc.</p>

<h3 id=""frominsightstoaction"">From insights to action?</h3>

<p>Audience Overlap is a new functionality within Facebook that can give you valuable insights when you establish all basics like CRM integrations and Website Custom Audiences. Not just in your Facebook ads but in e-commerce as a whole.</p>

<p>In this article I shared five of my first insights that became apparent through this tool. Now we have to wait for Facebook to enable access to overlapping parts of segments specifically. That would make our Facebook ads even more relevant!</p>

<p>You can find the The Audience Overlap tool in the Ads Manager next to Audiences under the 'tools' button. Good luck with your first analyses.</p>
        ","Recently, my heart started beating faster. While I was uploading a new CRM file in Facebook for a customer, I discovered a new feature: Audience Overlap. With this tool you can differentiate target audiences / segments (you name it) and discover their similarities. I didn't have a use case that would benefit from this at first, but I had the feeling this had serious potential. After a few days of trial my enthusiasm hasn't gone down and I've listed my insights in this article.
Audiences
I can already hear the veteran Facebook marketeer say: ""Gaining insights for your audiences, don't you already have Facebook Audience Insights for that?"". Absolutely, this tool gives you fantastic insights about segments, but it can't answer what percentage of my Facebook likes visited my website in the last 90 days. Audience Overlap answers this question with great panache.
Before I share my insights, I'll quickly guide you through the different Facebook audience types.
Saved Audiences
There isn't a better tool than Facebook to reach married mothers between the 30 and 40 years old, interested in football, right? When an advertiser wants to aim at a target audience he can save this as ""saved audiences"". A saved audience is a segment that an advertiser can create for the purpose of advertising. When you often want to reach the same target audience in various ads, this functionality is very handy and time efficient. You can create audiences based on location (country, province, city), gender, age, interests (like football), behaviour (like relationship status) and connections to the advertiser's page (like the Facebook likes of brand X).
Custom Audiences
When you upload CRM lists, or segment visitors through the Website Custom Audience (WCA) pixel as an advertiser, Facebook calls this Custom Audience. Besides CRM and website traffic it's also possible to make segments of App usage through an SDK.
Look-a-like Audiences
Look-a-like audiences are very useful for searching new customers, prospecting. You can let Facebook select the most similar 1 to 10 percent from a custom audience by looking at socio-demos, interests and behaviour.
To gain insights through the Audience Overlap tool, an audience needs to count at least 1000. This is to protect Facebook's users' privacy. You can combine endlessly with Audience Overlap. For example, you can compare saved audiences with look-a-like audiences, look-a-like audiences with custom audiences and custom audiences with saved audiences.
Every comparison output is always a percentage. This percentage shows how much percent of audience A overlaps with audience B, and vice versa. While many combinations only show brand-specific insights, there are many other common examples that everyone can benefit from. I'll share five with you.
1. The value of your Facebook likes
What's the ROI of social media and community building? Many marketeers and scientists have clamped onto this question in the last couple of years. We're getting closer to the answer with Audience Overlap. Of course we can measure our ads and read the Google Analytics results to see if someone came from Facebook. However, this is often cookie-based and not cross-device. Knowing Facebook has a very high mobile propagation, it's of high interest to see how like behaviour converts to website visits on desktop.
We segment website traffic through the WCA pixel for this customer. These custom audiences are cross-device since Facebook uses profiles instead of cookies. From the following analysis we can conclude that 37 percent of all page likes of this brand visited the website in the last 90 days. 9 percent even did this in the past 7 days. Obviously, you can make this more tangible by taking orders, shopping baskets and/or other on-site segments in mind.
2. A guide for determining retargeting strategies
Enabling or disabling certain segments in a retargeting campaign could make or break the success. Audience Overlap can support you with this. The following analysis is made for a customer who's product is a subscription, and the customer panel (for invoices, etc.) is segmented through the WCA pixel. In these situations a decision is made to exclude visitors from their panels since they're already members.
This analysis shows that 28 percent of the converted product A visitors also visited the customer panel in the last 30 days. These are existing customers that upgrade their subscription package. Through this insight we didn't exclude the segment. Besides, we notice that 52 percent of the converting product A visitors also visited product B's product page. In this case it could be profitable to show product B in the retargeting campaign of product A.
3. How successful is your cross-sell strategy?
When you enter the market with a new product as an established brand, existing customers often are important prospects for this new product. They're already familiar with the brand, and it's feasible to offer them package or synergy benefits.
These prospects are often reached through phone or email because these credentials are already present. The success of these contact attempts will be measured by sales, and in the case of email campaigns onsite metrics can be used. Through Audience Overlap's CRM files you can see the percentage of matches with your online funnel.
We could conclude 10 percent of the visitors that have put together a certain package, already owns product of this brand. Measuring irrespectively of what source they're coming from is Audience Overlap's great benefit. If it's through search, direct traffic, display or affiliates, Audience Overlap is able to match the visitor to the CRM file and determine the cross-sell's success. Despite not clicking the newsletter or following through after contacting the callcenter, the interest could be generated anyway.
4. How accurate is my Facebook targeting?
The Small Business Owners segment (SMBO) might be one of the most used segments within Facebook targeting. I've always been partly skeptic about this segment, which holds an immense amount of 560,000 users. There are rumours that page admins are in this segment by default. That seems rather premature by me.
With Audience Overlap we were able to save the SMBO segment as a 'Saved Audience' and compare it to visitors of the business section of a website of one of our customers. From analysis it appears only 18 percent of visitors within the business section of the website is recognised as SMBO. Considering the size of the segment (560,000 users) I think this is a dreadfully low percentage. Even more so when you consider there's a Facebook campaign running on this segment, and its traffic is already included in the 18 percent!
5. What's the greatest affinity of my customer base?
Imagine: as a brand you're considering lending a sponsor contract to Soccer Club A, Soccer Club B or Soccer Club C, but you don't know which of these clubs matches your customers best. Audience Overlap can help you with this. Create three saved audiences with their interest set respectively to Soccer Club A, Soccer Club B and Soccer Club C and observe the overlap with the CRM file.
Based on the analysis below we can conclude that Soccer Club B has the most common ground with the existing customers. When the brand wants to avoid disagreements with their customers, Soccer Club B would be the most safe choice. A similar analysis can be used for media choices, partnerships, etc.
From insights to action?
Audience Overlap is a new functionality within Facebook that can give you valuable insights when you establish all basics like CRM integrations and Website Custom Audiences. Not just in your Facebook ads but in e-commerce as a whole.
In this article I shared five of my first insights that became apparent through this tool. Now we have to wait for Facebook to enable access to overlapping parts of segments specifically. That would make our Facebook ads even more relevant!
You can find the The Audience Overlap tool in the Ads Manager next to Audiences under the 'tools' button. Good luck with your first analyses.","[Facebook, Analytics, Social advertising]"
58,Everything you need to know about the new Facebook Video Insights,/everything-you-need-to-know-about-the-new-facebook-video-insights/,"
            <p>They have been announced by Facebook a while ago but have now finally been rolled out to EMEA markets as well: the new Facebook Video Insights. Big deal right? Actually, yes! Why? Because we can <em>finally</em> get more elaborate insights into the performance of our Facebook video content!</p>

<p>Below you can find a screenshot of what the metrics used to look like:</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Mar/old_metrics-1456904121577.png"" alt=""Old metrics"" class=""full-img""></p>

<p>And another one of the new metrics that became available for a couple of our brands to me today:</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Mar/new_metrics-1456904155728.png"" alt=""New metrics"" class=""full-img""></p>

<p>As you can see a lot of new options and metrics have been added to the video insights dashboard compared to the previous situation. Besides the added metrics there's also the option to click on these metrics to get an even more in-depth analysis of the metrics mentioned in the screenshot above. Most make a distinction between organic &amp; paid, but when we click on the 'video views' metric, even more interesting metrics appear!</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Mar/snip_20160301131118-1456904164917.png"" alt=""""></p>

<p>What I'm referring to, obviously, is the 'sound on - sound off' metric. This is crucial information! When I look at this video for example, I can see that more than 92% (!) of my video views happened with the sound off. For this particular video, this isn't a big deal because the content of the video contains a lot of text in the screen, which doesn't necessarily mean that the video has to be watched with the sound on. When posting a commercial or another video with a lot of dialogue however, this makes it evident that effort on adding subtitles to the video would be well spent. </p>

<p>In my experience, video's with subtitles have a longer and more qualitative viewing time. Last year for example, we created a series of video's for one of our clients around New Years that had no sound at all. We didn't receive a single comment about it. Nobody missed the sound in our video's! </p>

<p>Another interesting metric is 'average % completion':</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Mar/completion-1456904204470.png"" alt=""""></p>

<p>You can dive deeper into all the metrics by using options in the drop down menu as you can see on the screenshot above. </p>

<p>Are you as excited about these new metrics as I am? Where do you see the most added value? Please share in the comments :)!</p>

<p>P.s. You can find the official Facebook update <a href=""http://media.fb.com/2016/02/18/updates-to-video-metrics-in-page-insights/"">here</a>.</p>
        ","They have been announced by Facebook a while ago but have now finally been rolled out to EMEA markets as well: the new Facebook Video Insights. Big deal right? Actually, yes! Why? Because we can finally get more elaborate insights into the performance of our Facebook video content!
Below you can find a screenshot of what the metrics used to look like:
And another one of the new metrics that became available for a couple of our brands to me today:
As you can see a lot of new options and metrics have been added to the video insights dashboard compared to the previous situation. Besides the added metrics there's also the option to click on these metrics to get an even more in-depth analysis of the metrics mentioned in the screenshot above. Most make a distinction between organic & paid, but when we click on the 'video views' metric, even more interesting metrics appear!
What I'm referring to, obviously, is the 'sound on - sound off' metric. This is crucial information! When I look at this video for example, I can see that more than 92% (!) of my video views happened with the sound off. For this particular video, this isn't a big deal because the content of the video contains a lot of text in the screen, which doesn't necessarily mean that the video has to be watched with the sound on. When posting a commercial or another video with a lot of dialogue however, this makes it evident that effort on adding subtitles to the video would be well spent.
In my experience, video's with subtitles have a longer and more qualitative viewing time. Last year for example, we created a series of video's for one of our clients around New Years that had no sound at all. We didn't receive a single comment about it. Nobody missed the sound in our video's!
Another interesting metric is 'average % completion':
You can dive deeper into all the metrics by using options in the drop down menu as you can see on the screenshot above.
Are you as excited about these new metrics as I am? Where do you see the most added value? Please share in the comments :)!
P.s. You can find the official Facebook update here.","[Facebook, Analytics, Social advertising]"
59,How to keep SEO rankings when changing domains,/how-to-keep-seo-rankings-when-changing-domains/,"
            <p>When we changed the name, and with it, the domain of our blog Geek to The Marketing Technologist; after a few days a colleague mentioned a drop in organic traffic. Logically one would immediately refer to incorrectly implemented 301 redirects; as this is a very important one, to make sure Google knows where to find your information after the URLs have switched. Apparently, this wasn’t the case, our 301 redirects were correctly added and the content stayed the same. </p>

<p>We then took a look at our Analytics data:</p>

<p>As the graphic below makes clear, the domain switch took place around November 10th. And right after the switch, the organic traffic to The Marketing Technologist started to drop below average, until the beginning of January (that’s the moment, we threw in some magic ).</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Feb/Organic_results-1456317654297.JPG"" alt="""" class=""full-img"">
<em>Organic traffic for both hostnames, before and after changing the domain</em></p>

<p>To make sure this isn’t a general drop in traffic, we also compared the stats for the direct traffic, but as you can see, no big differences are showing up right after the domain switch.</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Feb/Direct_results2-1456317701897.JPG"" alt="""" class=""full-img"">
<em>Direct traffic for both hostnames, before and after changing the domain</em></p>

<h2 id=""thesearchconsoleiswhereyougetgooglesattentionregardingseoissues"">The search console is where you get Google’s attention regarding SEO issues</h2>

<p>I started exploring the search console (former Webmaster Tools), and hoped I wouldn’t find any newly added domains there (and by adding the domain fixing the problem). But our new domain was added to the search console... <br>
Taking a deeper look led me to the painful conclusion that no search data nor organic clicks were collected at all. Which actually meant,  our new domain was live, with lots of awesome content, correctly implemented 301 redirects, but Google didn’t have a clue what we were doing, and that’s when you don’t show up in search results as glamorous as you used to!</p>

<h2 id=""keepyourseorankingswhilechangingdomains"">Keep your SEO-rankings while changing domains</h2>

<p>Let’s have a look at the steps you should take when changing your domain, regarding SEO issues. Keep in mind that the search console is kind of a ‘message center’ through which you can say to Google “Look,  here’s my website, this is my content and here’s what I’ve changed.”</p>

<p><strong>1 Make an export of your indexed pages and create 301 redirects to make sure vistors to the old URLs will be redirected to the new ones</strong></p>

<p>Make a new file in your Google Docs. <br>
enter the following code and replace domain with your domain:</p>

<p><code>=importXml(""https://www.google.com/search?q=site:www.domain.com&amp;num=100&amp;start=1""; ""//cite"")</code></p>

<p>You’ll now see the first 100 results of your indexed pages. To see the next 100, change start=1 to start=100 and add the code to cell 101. <br>
You can now export the file and use it to create redirects. </p>

<p><strong>2 Make sure your old and new domain are added to the Search Console</strong></p>

<p>Add all new versions of your domain to the Search Console (i.e. www.domain.com | <a href=""https://www.domain.com"">https://www.domain.com</a> | <a href=""http://domain.com"">http://domain.com</a> etc).</p>

<p><strong>3 Mark your preferred domain</strong></p>

<p>Mark your preferred domain in site settings:</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Feb/preferreddomain-1456319006962.jpg"" alt="""" class=""full-img""></p>

<p><strong>4 Extremely important : Add the correct sitemap(s)</strong></p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Feb/Sitemaps-1456319128593.jpg"" alt=""""></p>

<p>This is a very important one, as your sitemap is the way search engines ‘read’ your site. Errors here (i.e. lack of sitemap(s), old versions of sitemaps, sitemaps of only one part of your website) can really mess up a lot. A search engine doesn’t understand what’s going on on your website. For most  websites there are plugins that automatically create sitemaps, so it shouldn't be difficult to do.</p>

<p><strong>5 Tell Google your address has changed</strong></p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Feb/changeaddress-1456319308054.jpg"" alt="""" class=""full-img""></p>

<p><strong>6 After everything has changed</strong></p>

<p>Check, check, check:</p>

<ul>
<li>If data is collected (this may take a few days)</li>
<li>Are there any blocked URLs? Make sure your robots.txt hasn’t got too much restrictions. As writing the robots.txt too strictly can cause blocked JavaScript or CSS items that Google needs to ‘describe’ your page.  So a high number of blocked URLs can also cause drops in organic traffic. Regularly check this one!
<img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Feb/blockedurl-1456319569631.jpg"" alt=""""></li>
</ul>

<p>Hurray! To help you check this one, the search console has got a robots.txt tester: </p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Feb/robotstxttester-1456320198603.jpg"" alt="""" class=""full-img""></p>

<ul>
<li>Are there any Crawl errors? Check if there are any crawl errors like 404 page errors.</li>
<li>The number of indexed versus submitted items. Check if the number of indexed pages and images is close to the number of pages and images you submitted regarding your sitemap.
<img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Feb/indexed_submittedv2-1456320645416.jpg"" alt=""""></li>
</ul>

<p>The search console is an interesting tool which provides lots more stuff to manage and adjust SEO related items, besides the problem solving ones as described above. More about that later on!</p>
        ","When we changed the name, and with it, the domain of our blog Geek to The Marketing Technologist; after a few days a colleague mentioned a drop in organic traffic. Logically one would immediately refer to incorrectly implemented 301 redirects; as this is a very important one, to make sure Google knows where to find your information after the URLs have switched. Apparently, this wasn’t the case, our 301 redirects were correctly added and the content stayed the same.
We then took a look at our Analytics data:
As the graphic below makes clear, the domain switch took place around November 10th. And right after the switch, the organic traffic to The Marketing Technologist started to drop below average, until the beginning of January (that’s the moment, we threw in some magic ).
Organic traffic for both hostnames, before and after changing the domain
To make sure this isn’t a general drop in traffic, we also compared the stats for the direct traffic, but as you can see, no big differences are showing up right after the domain switch.
Direct traffic for both hostnames, before and after changing the domain
The search console is where you get Google’s attention regarding SEO issues
I started exploring the search console (former Webmaster Tools), and hoped I wouldn’t find any newly added domains there (and by adding the domain fixing the problem). But our new domain was added to the search console...
Taking a deeper look led me to the painful conclusion that no search data nor organic clicks were collected at all. Which actually meant, our new domain was live, with lots of awesome content, correctly implemented 301 redirects, but Google didn’t have a clue what we were doing, and that’s when you don’t show up in search results as glamorous as you used to!
Keep your SEO-rankings while changing domains
Let’s have a look at the steps you should take when changing your domain, regarding SEO issues. Keep in mind that the search console is kind of a ‘message center’ through which you can say to Google “Look, here’s my website, this is my content and here’s what I’ve changed.”
1 Make an export of your indexed pages and create 301 redirects to make sure vistors to the old URLs will be redirected to the new ones
Make a new file in your Google Docs.
enter the following code and replace domain with your domain:
=importXml(""https://www.google.com/search?q=site:www.domain.com&num=100&start=1""; ""//cite"")
You’ll now see the first 100 results of your indexed pages. To see the next 100, change start=1 to start=100 and add the code to cell 101.
You can now export the file and use it to create redirects.
2 Make sure your old and new domain are added to the Search Console
Add all new versions of your domain to the Search Console (i.e. www.domain.com | https://www.domain.com | http://domain.com etc).
3 Mark your preferred domain
Mark your preferred domain in site settings:
4 Extremely important : Add the correct sitemap(s)
This is a very important one, as your sitemap is the way search engines ‘read’ your site. Errors here (i.e. lack of sitemap(s), old versions of sitemaps, sitemaps of only one part of your website) can really mess up a lot. A search engine doesn’t understand what’s going on on your website. For most websites there are plugins that automatically create sitemaps, so it shouldn't be difficult to do.
5 Tell Google your address has changed
6 After everything has changed
Check, check, check:
If data is collected (this may take a few days)
Are there any blocked URLs? Make sure your robots.txt hasn’t got too much restrictions. As writing the robots.txt too strictly can cause blocked JavaScript or CSS items that Google needs to ‘describe’ your page. So a high number of blocked URLs can also cause drops in organic traffic. Regularly check this one!
Hurray! To help you check this one, the search console has got a robots.txt tester:
Are there any Crawl errors? Check if there are any crawl errors like 404 page errors.
The number of indexed versus submitted items. Check if the number of indexed pages and images is close to the number of pages and images you submitted regarding your sitemap.
The search console is an interesting tool which provides lots more stuff to manage and adjust SEO related items, besides the problem solving ones as described above. More about that later on!","[Analytics, SEO, Search Console, Organic Search]"
60,Passing query string parameters to a creative in Appnexus,/passing-query-string-parameters-to-creatives-in-appnexus/,"
            <p>After years of experience in performance marketing, we can state that it's often true that the deeper a user lands in a funnel, the higher the chance they will convert. When building a banner ad, the banner should ideally be prefilled with as much information possible. This is something we should always aim for. </p>

<p>At Blue Mango, we have a couple of clients (like Ditzo Car Insurance and Carglass window repair) that requests a visitors license plate in the first step of the funnel. We've built a dozen of banners where the visitor gets to fill in their license plate in a banner ad. This way, they land in step 2 of the funnel after clicking. This is a great boost for the conversion rate.</p>

<p>A different version of this banner is a banner with a 'pre-filled' license plate. When a visitor looks at a second-hand car on selected websites, the banner automatically displays its license plate, lowering the conversion threshold even further.   </p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Feb/car-1456060459864.jpg"" alt="""" title="""" class=""full-img""><em>Banner ad with pre-filled license plate (translated from Dutch)</em></p>

<h3 id=""passdatatoacreative"">Pass data to a creative</h3>

<p>The easiest way to pass information to a banner ad is through the <a href=""https://en.wikipedia.org/wiki/Query_string"">query string</a>. Until recently, we hosted these banners on Amazon S3, so we had complete control over how we handled the values passed to the query string. </p>

<p>We would simply create an iFrame tag with the source pointed to our banner on S3, and just add the <code>licensePlate</code> query string parameter. In our banner, we would read the value of the licensePlate parameter and display it in the creative. The iFrame looks something like this:</p>

<pre><code class=""language-prettyprint lang-html prettyprinted""><span class=""tag"">&lt;iframe</span><span class=""pln""> </span><span class=""atn"">width</span><span class=""pun"">=</span><span class=""atv"">""275""</span><span class=""pln""> </span><span class=""atn"">height</span><span class=""pun"">=</span><span class=""atv"">""135""</span><span class=""pln""> </span><span class=""atn"">src</span><span class=""pun"">=</span><span class=""atv"">""http://ouraccount.s3.amazonaws.com/client/banner.html?licensePlate={{LICENSE_PLATE}}""</span><span class=""tag"">&gt;&lt;/iframe&gt;</span><span class=""pln"">  </span></code></pre>

<p>Easy enough. However, recently we moved all our license plate creatives from S3 to Appnexus. When you create an iFrame tag for your creative in Appnexus, you lose complete control over the query string. The <code>licensePlate</code> parameter will simply be denied and is inaccessible in the creative. </p>

<h3 id=""creativemacrosforquerystringvariables"">Creative macros for query string variables</h3>

<p>Luckily, Appnexus has creative macros for URL parameters. A creative macro is a text placeholder that Appnexus replaces with a useful piece of impression level information when a creative is served.</p>

<p>The most popular creative macros are <code>${CLICK_URL}</code> and <code>${CLICK_URL_ENC}</code>. But there are <a href=""https://wiki.appnexus.com/display/console/Creative+Macros"">a lot more</a>. The one we need is <code>${PT1}</code>. This macro can be populated with arbitrary custom data that you send in using the placement tag query string parameters pt1. Exactly what we're looking for. Appnexus also supports pt2 to pt9, but in our example with the license plate, we just need one parameter, so we pick <code>${PT1}</code>. </p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""kwd"">var</span><span class=""pln""> licensePlate </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""str"">'${PT1}'</span><span class=""pun"">;</span><span class=""pln"">  </span></code></pre>

<p>Next, we fill the license plate element in the creative with the value of licensePlate. Below is a simple example of how you might use the macro text inside your JavaScript creative's code to populate the license plate input field:</p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""pln"">document</span><span class=""pun"">.</span><span class=""pln"">querySelector</span><span class=""pun"">(</span><span class=""str"">'#licensePlate'</span><span class=""pun"">).</span><span class=""pln"">value </span><span class=""pun"">=</span><span class=""pln""> licensePlate</span><span class=""pun"">;</span><span class=""pln"">  </span></code></pre>

<h3 id=""createacreativeandiframetag"">Create a creative and iFrame tag</h3>

<p>Now all we need to do is add a new creative in Appnexus and create an iFrame tag. We normally use a 3rd party JavaScript creative tag, but you can use any type of 3rd party creative.</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Feb/Screen_Shot_2016_02_21_at_13_53_14-1456059261747.png"" alt="""" class=""full-img""></p>

<p>This feature will only work if you uncheck 'Serve in iFrame', as shown below. This is because Appnexus does not pass the query string parameters when generating iFrames.</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Feb/Screen_Shot_2016_02_21_at_13_55_12-1456059327922.png"" alt=""""></p>

<p>Another thing to keep in mind is that Appnexus does not pass the query string parameters when previewing your creative. You really have to set up a campaign and a placement tag for the creative. </p>

<p>It's out of this article's scope to explain how to create a placement tag, so you should ask your campaign manager to create an iFrame placement for your creative. The placement tag should look something like this:</p>

<pre><code class=""language-prettyprint lang-html prettyprinted""><span class=""com"">&lt;!-- BEGIN IFRAME TAG - Name of your campaign &lt; - DO NOT MODIFY --&gt;</span><span class=""pln"">  
</span><span class=""tag"">&lt;IFRAME</span><span class=""pln""> </span><span class=""atn"">SRC</span><span class=""pun"">=</span><span class=""atv"">""http://ib.adnxs.com/tt?id=xxxxxx&amp;cb=[CACHEBUSTER]&amp;pubclick=[INSERT_CLICK_TAG]""</span><span class=""pln""> </span><span class=""atn"">FRAMEBORDER</span><span class=""pun"">=</span><span class=""atv"">""0""</span><span class=""pln""> </span><span class=""atn"">SCROLLING</span><span class=""pun"">=</span><span class=""atv"">""no""</span><span class=""pln""> </span><span class=""atn"">MARGINHEIGHT</span><span class=""pun"">=</span><span class=""atv"">""0""</span><span class=""pln""> </span><span class=""atn"">MARGINWIDTH</span><span class=""pun"">=</span><span class=""atv"">""0""</span><span class=""pln""> </span><span class=""atn"">TOPMARGIN</span><span class=""pun"">=</span><span class=""atv"">""0""</span><span class=""pln""> </span><span class=""atn"">LEFTMARGIN</span><span class=""pun"">=</span><span class=""atv"">""0""</span><span class=""pln""> </span><span class=""atn"">ALLOWTRANSPARENCY</span><span class=""pun"">=</span><span class=""atv"">""true""</span><span class=""pln""> </span><span class=""atn"">WIDTH</span><span class=""pun"">=</span><span class=""atv"">""300""</span><span class=""pln""> </span><span class=""atn"">HEIGHT</span><span class=""pun"">=</span><span class=""atv"">""250""</span><span class=""tag"">&gt;&lt;/IFRAME&gt;</span><span class=""pln"">  
</span><span class=""com"">&lt;!-- END TAG --&gt;</span><span class=""pln"">  </span></code></pre>

<h3 id=""addthequerystringvariable"">Add the query string variable</h3>

<p>Now that we have an iFrame tag, we can test our creative. The only thing you have to do is add the <code>PT1</code> parameter to the <code>src</code> of the iFrame tag:</p>

<pre><code class=""language-prettyprint lang-html prettyprinted""><span class=""com"">&lt;!-- BEGIN IFRAME TAG - Name of your campaign &lt; - DO NOT MODIFY --&gt;</span><span class=""pln"">  
</span><span class=""tag"">&lt;IFRAME</span><span class=""pln""> </span><span class=""atn"">SRC</span><span class=""pun"">=</span><span class=""atv"">""http://ib.adnxs.com/tt?id=xxxxxx&amp;cb=[CACHEBUSTER]&amp;pubclick=[INSERT_CLICK_TAG]&amp;PT1={{LICENSE_PLATE}}""</span><span class=""pln""> </span><span class=""atn"">FRAMEBORDER</span><span class=""pun"">=</span><span class=""atv"">""0""</span><span class=""pln""> </span><span class=""atn"">SCROLLING</span><span class=""pun"">=</span><span class=""atv"">""no""</span><span class=""pln""> </span><span class=""atn"">MARGINHEIGHT</span><span class=""pun"">=</span><span class=""atv"">""0""</span><span class=""pln""> </span><span class=""atn"">MARGINWIDTH</span><span class=""pun"">=</span><span class=""atv"">""0""</span><span class=""pln""> </span><span class=""atn"">TOPMARGIN</span><span class=""pun"">=</span><span class=""atv"">""0""</span><span class=""pln""> </span><span class=""atn"">LEFTMARGIN</span><span class=""pun"">=</span><span class=""atv"">""0""</span><span class=""pln""> </span><span class=""atn"">ALLOWTRANSPARENCY</span><span class=""pun"">=</span><span class=""atv"">""true""</span><span class=""pln""> </span><span class=""atn"">WIDTH</span><span class=""pun"">=</span><span class=""atv"">""275""</span><span class=""pln""> </span><span class=""atn"">HEIGHT</span><span class=""pun"">=</span><span class=""atv"">""135""</span><span class=""tag"">&gt;&lt;/IFRAME&gt;</span><span class=""pln"">  
</span><span class=""com"">&lt;!-- END TAG --&gt;</span><span class=""pln"">  </span></code></pre>

<p>You can now replace <code>{{LICENSE_PLATE}}</code> with whatever license plate. We place our creatives on multiple car seller websites, and every site has a different way to fill the license plate parameter, so we just adjust the placeholder to their implementation.  </p>
        ","After years of experience in performance marketing, we can state that it's often true that the deeper a user lands in a funnel, the higher the chance they will convert. When building a banner ad, the banner should ideally be prefilled with as much information possible. This is something we should always aim for.
At Blue Mango, we have a couple of clients (like Ditzo Car Insurance and Carglass window repair) that requests a visitors license plate in the first step of the funnel. We've built a dozen of banners where the visitor gets to fill in their license plate in a banner ad. This way, they land in step 2 of the funnel after clicking. This is a great boost for the conversion rate.
A different version of this banner is a banner with a 'pre-filled' license plate. When a visitor looks at a second-hand car on selected websites, the banner automatically displays its license plate, lowering the conversion threshold even further.
Banner ad with pre-filled license plate (translated from Dutch)
Pass data to a creative
The easiest way to pass information to a banner ad is through the query string. Until recently, we hosted these banners on Amazon S3, so we had complete control over how we handled the values passed to the query string.
We would simply create an iFrame tag with the source pointed to our banner on S3, and just add the licensePlate query string parameter. In our banner, we would read the value of the licensePlate parameter and display it in the creative. The iFrame looks something like this:
<iframe width=""275"" height=""135"" src=""http://ouraccount.s3.amazonaws.com/client/banner.html?licensePlate={{LICENSE_PLATE}}""></iframe>  
Easy enough. However, recently we moved all our license plate creatives from S3 to Appnexus. When you create an iFrame tag for your creative in Appnexus, you lose complete control over the query string. The licensePlate parameter will simply be denied and is inaccessible in the creative.
Creative macros for query string variables
Luckily, Appnexus has creative macros for URL parameters. A creative macro is a text placeholder that Appnexus replaces with a useful piece of impression level information when a creative is served.
The most popular creative macros are ${CLICK_URL} and ${CLICK_URL_ENC}. But there are a lot more. The one we need is ${PT1}. This macro can be populated with arbitrary custom data that you send in using the placement tag query string parameters pt1. Exactly what we're looking for. Appnexus also supports pt2 to pt9, but in our example with the license plate, we just need one parameter, so we pick ${PT1}.
var licensePlate = '${PT1}';  
Next, we fill the license plate element in the creative with the value of licensePlate. Below is a simple example of how you might use the macro text inside your JavaScript creative's code to populate the license plate input field:
document.querySelector('#licensePlate').value = licensePlate;  
Create a creative and iFrame tag
Now all we need to do is add a new creative in Appnexus and create an iFrame tag. We normally use a 3rd party JavaScript creative tag, but you can use any type of 3rd party creative.
This feature will only work if you uncheck 'Serve in iFrame', as shown below. This is because Appnexus does not pass the query string parameters when generating iFrames.
Another thing to keep in mind is that Appnexus does not pass the query string parameters when previewing your creative. You really have to set up a campaign and a placement tag for the creative.
It's out of this article's scope to explain how to create a placement tag, so you should ask your campaign manager to create an iFrame placement for your creative. The placement tag should look something like this:
<!-- BEGIN IFRAME TAG - Name of your campaign < - DO NOT MODIFY -->  
<IFRAME SRC=""http://ib.adnxs.com/tt?id=xxxxxx&cb=[CACHEBUSTER]&pubclick=[INSERT_CLICK_TAG]"" FRAMEBORDER=""0"" SCROLLING=""no"" MARGINHEIGHT=""0"" MARGINWIDTH=""0"" TOPMARGIN=""0"" LEFTMARGIN=""0"" ALLOWTRANSPARENCY=""true"" WIDTH=""300"" HEIGHT=""250""></IFRAME>  
<!-- END TAG -->  
Add the query string variable
Now that we have an iFrame tag, we can test our creative. The only thing you have to do is add the PT1 parameter to the src of the iFrame tag:
<!-- BEGIN IFRAME TAG - Name of your campaign < - DO NOT MODIFY -->  
<IFRAME SRC=""http://ib.adnxs.com/tt?id=xxxxxx&cb=[CACHEBUSTER]&pubclick=[INSERT_CLICK_TAG]&PT1={{LICENSE_PLATE}}"" FRAMEBORDER=""0"" SCROLLING=""no"" MARGINHEIGHT=""0"" MARGINWIDTH=""0"" TOPMARGIN=""0"" LEFTMARGIN=""0"" ALLOWTRANSPARENCY=""true"" WIDTH=""275"" HEIGHT=""135""></IFRAME>  
<!-- END TAG -->  
You can now replace {{LICENSE_PLATE}} with whatever license plate. We place our creatives on multiple car seller websites, and every site has a different way to fill the license plate parameter, so we just adjust the placeholder to their implementation.","[Code, appnexus]"
61,Reduce the amount of sampled data in your Google Analytics Reporting API requests,/reduce-the-amount-of-sampled-data-in-your-google-analytics-reporting-api-requests/,"
            <p>Have you ever wondered: is there a way to reduce the amount of sampled data in my Google Analytis Reporting API requests? Well there is. Let me explain how.</p>

<h2 id=""theproblem"">The problem</h2>

<p>Let's look at the problem first. It consists of two parts: </p>

<ul>
<li>1: Sampled data</li>
<li>2: API restrictions</li>
</ul>

<p><strong>1: Sampled data</strong></p>

<p>Sampled data occurs when the amount of sessions that your data is based on is larger than 500.000 (25M for premium). If your interested in the details, you can find them <a href=""https://support.google.com/analytics/answer/1042498?hl=en"">here</a>.</p>

<p><strong>2: API restrictions</strong></p>

<p>The API has two main restrictions:</p>

<ul>
<li><a href=""https://developers.google.com/analytics/devguides/reporting/core/v3/reference#maxResults""><strong>Result limits</strong></a> - the API allows you to collect 10.000 rows in one call. You can use the <a href=""https://developers.google.com/analytics/devguides/reporting/core/v3/reference#startIndex"">start index parameter</a> to return more data when you hit that limit. </li>
<li><a href=""https://developers.google.com/analytics/devguides/reporting/core/v3/limits-quotas#discovery""><strong>General Quota</strong></a> - Google allows you to call the API 50,000 times a day per project, and up to 10 queries per second per IP. </li>
</ul>

<p>When you import larger periods of data, you'll be more likely to run into sampled data. Luckily, there's an easy fix.</p>

<h2 id=""thesolution"">The Solution</h2>

<p>When you import a larger period of data, you normally ask the API to get data from the start date, to the end date:</p>

<pre><code>start-date =&gt; '2016-01-01'  
end-date =&gt; '2016-01-31'  
</code></pre>

<p>The solution is easy: chop up your dates. Based on the start and end date, you can easily get all the dates in between. So looking at the example above, this will result in the following list of dates:</p>

<ul>
<li>2016-01-01</li>
<li>2016-01-02</li>
<li>...</li>
<li>2016-01-30</li>
<li>2016-01-31</li>
</ul>

<p>Now after this, you can loop through this list of dates, and get the results for each date separately:</p>

<p><strong>Request date 1</strong></p>

<pre><code>start-date =&gt; '2016-01-01'  
end-date =&gt; '2016-01-01'  
</code></pre>

<p><strong>Request date 2</strong></p>

<pre><code>start-date =&gt; '2016-01-02'  
end-date =&gt; '2016-01-02'  
</code></pre>

<p>...</p>

<p><strong>Request date 30</strong></p>

<pre><code>start-date =&gt; '2016-01-30'  
end-date =&gt; '2016-01-30'  
</code></pre>

<p><strong>Request date 31</strong></p>

<pre><code>start-date =&gt; '2016-01-31'  
end-date =&gt; '2016-01-31'  
</code></pre>

<p>With this approach, you'll collect data day by day and stitch it into one data set. This greatly reduces the chance of hitting that limit (you'll only hit it when a single day hits the sampled data limit). </p>

<h2 id=""dontworryabouttherequestlimit"">Don't worry about the request limit</h2>

<p>If you're worried about the reporting limit, you shouldn't be. With 50.000 calls per project per day, you'll be able to get a little over 136 years worth of data without hitting the limit. </p>
        ","Have you ever wondered: is there a way to reduce the amount of sampled data in my Google Analytis Reporting API requests? Well there is. Let me explain how.
The problem
Let's look at the problem first. It consists of two parts:
1: Sampled data
2: API restrictions
1: Sampled data
Sampled data occurs when the amount of sessions that your data is based on is larger than 500.000 (25M for premium). If your interested in the details, you can find them here.
2: API restrictions
The API has two main restrictions:
Result limits - the API allows you to collect 10.000 rows in one call. You can use the start index parameter to return more data when you hit that limit.
General Quota - Google allows you to call the API 50,000 times a day per project, and up to 10 queries per second per IP.
When you import larger periods of data, you'll be more likely to run into sampled data. Luckily, there's an easy fix.
The Solution
When you import a larger period of data, you normally ask the API to get data from the start date, to the end date:
start-date => '2016-01-01'  
end-date => '2016-01-31'  
The solution is easy: chop up your dates. Based on the start and end date, you can easily get all the dates in between. So looking at the example above, this will result in the following list of dates:
2016-01-01
2016-01-02
...
2016-01-30
2016-01-31
Now after this, you can loop through this list of dates, and get the results for each date separately:
Request date 1
start-date => '2016-01-01'  
end-date => '2016-01-01'  
Request date 2
start-date => '2016-01-02'  
end-date => '2016-01-02'  
...
Request date 30
start-date => '2016-01-30'  
end-date => '2016-01-30'  
Request date 31
start-date => '2016-01-31'  
end-date => '2016-01-31'  
With this approach, you'll collect data day by day and stitch it into one data set. This greatly reduces the chance of hitting that limit (you'll only hit it when a single day hits the sampled data limit).
Don't worry about the request limit
If you're worried about the reporting limit, you shouldn't be. With 50.000 calls per project per day, you'll be able to get a little over 136 years worth of data without hitting the limit.",[]
62,A recommendation system for blogs: Content-based similarity (part 2),/a-recommendation-system-for-blogs-content-based-similarity-part-2/,"
            <p>In this second post in a series of posts about a content recommendation system for The Marketing Technologist (TMT) website we are going to elaborate on the concept of content-based recommendation systems. In <a href=""https://www.themarketingtechnologist.co/building-a-recommendation-engine-for-geek-setting-up-the-prerequisites-13/"">the first post</a> we described the benefits of recommendation systems and we roughly divided them in two different types of recommenders: content-based and collaborative filtering. The first post also described the prerequisites in order to set-up both types of recommenders. If you haven’t read this first post yet, it is recommended to do this first before you continue. In this article we take our first steps in content-based recommendation systems by describing a quantified approach to express the similarity of articles. </p>

<p>The final code of this article can be found on my <a href=""https://github.com/thomhopmans/themarketingtechnologist"">Github</a>.</p>

<h3 id=""theconceptbehindcontentbasedrecommendation"">The concept behind content-based recommendation</h3>

<p>The goal is to provide our readers with recommendations for other TMT articles. We assume that a reader who has fully read an article liked that article and wants to read more <strong>similar</strong> articles. Therefore we want to build a content-based recommender that is going to recommend new similar articles based on the users historical reading behaviour. <br>
To achieve accurate and useful recommendations we want to use a mathematical and quantified approach to find the best possible recommendations. Otherwise we are going to lose the interest of our reader and he will leave TMT. Therefore we want to find articles that are similar to each other and thus lie <strong>“close to each other”</strong>. That is, for which the <strong>“distance”</strong> between the articles is small, where a smaller distance implies a higher similarity. </p>

<p><img src=""http://i68.tinypic.com/23w1jia.png"" alt=""Visualization of TMT articles in a 2-dimensional space"" class=""full-img"">
<em>Example of visualizing TMT articles in a 2-dimensional space</em></p>

<p>So, how does this distance concept work? Assume that we can plot any TMT article in a two-dimensional space. The figure above provides an example of 76 TMT articles plotted in a 2-dimensional space. Furthermore we assume that the closer two points lie to each other the more similar they are. Therefore, if a user is reading an article, other articles that lie close to this point in the 2D space can be seen as a good recommendation as they are similar. How close points lie to each other can be calculated using the Euclidean distance formula. In a 2-dimensional space this distance formula simply comes down to the Pythagorean theorem. Note that this distance formula also works for higher dimensions, for example in a 100-dimensional space (although we cannot visualize this when we have more than 3 dimensions).</p>

<p><img src=""http://i65.tinypic.com/2zjgw1x.png"" alt=""Euclidean distance formula"">
<em>Euclidean distance formula</em></p>

<p>A different distance formula to measure similarity of two points is <a href="""">cosine similarity</a>. The cosine similarity function uses the difference in the direction that two articles go, i.e. the difference in angle between two article directions. Imagine that an article can be assigned a direction to which it tends. For example, in a 2-dimensional case one article goes North and the other article goes West. The difference in directions is then -90 degrees. This difference in angle is normalized to the interval [-1, 1], where 1 implies the same direction and thus perfect similarity and -1 the complete opposite direction and thus no similarity.</p>

<p><img src=""http://dataconomy.com/wp-content/uploads/2015/04/Five-most-popular-similarity-measures-implementation-in-python-4-620x475.png"" alt=""Cosine similarity"">
<em>(Cosine similarity; Image from <a href=""http://dataconomy.com/implementing-the-five-most-popular-similarity-measures-in-python/"">Dataconomy.com</a>)</em> </p>

<p>We use the cosine similarity metric for measuring the similarity of TMT articles as the direction of articles is more important than the exact distance between them. Additionally, we tried both metrics (Euclidean distance and cosine similarity) and the cosine similarity metric simply performs better. :)</p>

<h4 id=""twonewchallenges"">Two new challenges</h4>

<p>Above we described the concept of similarity between articles using a quantified approach. However, now two new challenges arise:</p>

<ul>
<li>How can we plot each post in a 2-dimensional space?</li>
<li>How do we plot these posts such that the distance between the points gives an indication about the similarity of the articles? </li>
</ul>

<p>An article can be plotted in a 2-dimensional space by assigning it coordinates, i.e. an x and y coordinate. This means we first need to translate our articles to a numeric format and then reduce it to two values, i.e. the x and y coordinate. Therefore, we are first going to elaborate on a scientific approach to quantify the text in the TMT articles by applying <strong>feature extraction</strong>. <br>
Note that the feature extraction method we discuss below is specifically designed for dealing with text in TMT articles. You can imagine that if you're building a content-based recommender for telephones you probably need a different method to translate the properties (content) of telephones to a numerical format.</p>

<h3 id=""convertingtmtarticlestoanumericformat"">Converting TMT articles to a numeric format</h3>

<p>The TMT articles, consisting of large phrases of words and punctuation, need to be translated to numerical vectors without losing the content of the article in the process. Preferably we also want vectors of a fixed size. Why do we want a fixed size? Recall the 2-dimensional example above. It would be strange to compare a point in a 2 dimensional space to a point in a 100-dimensional space right? Additionally, if we want to say something about the similarity between articles we also need to express them in a similar manner. <br>
To obtain vectors of a fixed size we are going to create <code>features</code>, e.g. measurable article properties. In text analysis a feature often refers to words, phrases, numbers or symbols. All articles can then be measured and expressed in terms of the same set of features, resulting in fixed-size numerical vectors for all articles. The whole process of converting text to numerical vectors is called feature extraction and is often done in three steps: <em>tokenization</em>, <em>counting</em> and <em>weighting</em>. </p>

<p><img src=""http://i63.tinypic.com/303bklx.png"" alt=""Example of vectorizing text"">
<em>Example of vectorizing text</em></p>

<p><strong>Step 1: tokenization</strong></p>

<p>The first step in obtaining numerical features is tokenization. In text analysis, tokenization is described as obtaining meaningful basic units from large samples of text. For example, in physics speed can be expressed in meters per second. In text analysis, large strings of text can be expressed in tokens. These tokens often correspond to words. Therefore, a simple tokenization method to obtain tokens for the sentence <code>I am happy, because the sun shines</code> is by splitting them on whitespaces. This splitting results in seven tokens, i.e. <code>I</code>, <code>am</code>, <code>happy,</code>, <code>because</code>, <code>the</code>, <code>sun</code>, <code>shines</code>. After tokenization it is possible to express the original sentence in terms of these tokens.</p>

<p>This simple tokenization method however provides several new problems:</p>

<ul>
<li><p>For one, this method does not filter out any punctuation and thus the token <code>happy,</code> contains a comma at the end. This implies that the token <code>happy,</code> and <code>happy</code> are two different tokens, although both tokens imply the same word. Therefore, we filter out all types of punctuation, because punctuation is almost never relevant for the meaning of a word in our articles. 
Note that punctuation can be relevant in other situations. For example, when analyzing Twitter messages punctuation can be important as they are often used to create smiley's which express a lot of sentiment. The smiley example emphasizes the fact that every data source needs its own feature extraction method. </p></li>
<li><p>Second, using the simple tokenization method it is possible to obtain the tokens <code>works</code> and <code>working</code>. However, these tokens are just different forms of the same word, i.e. <code>to work</code>. The same argument holds for tokens where one is the plural form of the other. For our content-based recommendation system, we assume that both forms of these words imply the same word. Therefore, the tokens can be reduced to their stem and used as a single token. To do this, a stemming algorithm that reduces every word to its stem is required. Note that such a stemming algorithm is language specific. Luckily there are several freely available packages such as the <code>NLTK</code> library that can do this for us.</p></li>
<li><p>A third problem that typically occurs in text analytics is how to deal with combinations or negations of words. For example, just using the individual tokens <code>Google</code> and <code>Analytics</code> may not always imply that we are talking about the product <code>Google Analytics</code>. Therefore, we also create tokens of two or three consecutive words, called respectively bi-grams and tri-grams. The sentence <code>I like big data</code> then translates to the tokens <code>I</code>, <code>like</code>, <code>big</code>, <code>data</code>, <code>I like</code>, <code>like big</code>, <code>big data</code>, <code>I like big</code> and <code>like big data</code>.</p></li>
</ul>

<p>Note that this tokenization method does not take into account the position and the order of the words. For example, after tokenization it cannot be said at what position in the original sentence or article the token <code>big</code> occurred. Also, the token itself does not mention anything about the words in front or after it. Therefore, we lose some information about the original sentence during tokenization. The art is to capture as much information about the original sentence while retaining a workable set of tokens. <br>
In our tokenization method we lose information about the structure of the sentences. There are other tokenization methods which take the position and order of words into account as well. For example, Part-Of-Speech (POS) tagging also adds additional information such as the word-class of a token, e.g. whether a token occurs as a verb, adjective, noun or direct object. However, we assume that POS tagging does not greatly increase the performance of our recommender because the order of words within sentences is not of great importance for making recommendations.</p>

<p><strong>Step 2: token frequency counts</strong></p>

<p>In the second step, the frequency of each token in each article is counted. These frequencies are used in the next step for assigning weights to tokens in articles. Additionally these counts are used to later on perform a basic feature selection, i.e. to reduce the number of features. Note that a typical property of text analysis is that the majority of the tokens are only used in a couple of articles. Therefore, the frequency of most tokens in an article is zero.</p>

<p><strong>Step 3: token weights</strong></p>

<p>In the last step the tokens and token frequency counts from the previous steps are used to convert all articles to a numerical format. This is done by encoding each article to a numeric vector whose elements represent the tokens from step 1. Moreover, a token weighting procedure is applied using the frequency counts from step 2. After a token is weighted, it is not any more referred to as a token but as a <strong>feature</strong>. Hence, a feature represents a token and the value of a feature for an article is a weight assigned by a weighting method.</p>

<p>There are several possible feature weighting methods:</p>

<ul>
<li><p>The most basic weighting method is <strong>Feature Frequency (FF)</strong>. FF simply uses the frequency of a token in an article as the weight for a token. For example, given the token set {mad, happy}, the sentence <code>I am not mad, but happy, very happy</code> is weighted as the vector  [1  2].</p></li>
<li><p><strong>Feature Presence (FP)</strong> is a similar basic weighting method. In FP the weight of a token is simply given by a binary variable which is 1 if the token occurs in an article and 0 otherwise. The sentence from the previous example would be represented as the vector [1  1] when using FP, because both tokens are present in this sentence. An additional advantage of FP as weighting method is that a binary dataset is obtained, which does not suffer scaling problems. The latter can occur in algorithms when for example calculating complicated values such as eigenvalues or Hessian.</p></li>
<li><p>A more complex feature weighting procedure is the <strong>Term Frequency and Inverse Document Frequency' (TF-IDF)</strong> weighting method. This method uses two scores, i.e. the <code>term frequency</code> score and the <code>inverse document frequency</code> score. The term frequency score is calculated by taking the frequency of a token in an article. The inverse document frequency score is calculated by the logarithm of dividing the total number of articles by the number of articles in which the token occurs. When multiplying these two scores, a value is obtained that is high for features that occur frequently in a small number of articles, and is low for features that occur often in many articles. </p></li>
</ul>

<p>For our content-based recommendation system we are going to use the FP weighting method because it is fast and does not perform worse than the other weighting methods. Additionally, it results in a sparse matrix which has additional computational benefits.</p>

<h3 id=""reducingthedimensionality"">Reducing the dimensionality</h3>

<p>After applying the above feature recommendation method we are left with a list of features with which we can numerically express each TMT article. We could calculate the similarity between each article in this very high dimensional space but we prefer not to. Features that barely express similarity between articles, such as <code>is</code> and <code>a</code>, can be removed from the feature set to significantly reduce the number of features and to improve the quality of the recommendations. Additionally, a smaller dataset improves the speed of the recommendation system.</p>

<p><strong>Document Frequency (DF)</strong> selection is the simplest feature selection method to reduce dimensionality and is a must for many text analysis problems. We first remove the most common English stop words, e.g. the words <code>the</code>, <code>a</code>, <code>is</code>, et cetera, which do not give much information about the similarity between articles. After that, all features with a very high and very low document frequency are removed from the data set as these features are also not likely to help in differentiating articles. </p>

<p>Recall that at the beginning of this article we visualized the articles in a 2-dimensional space. However, after applying DF we are still in a very high dimensional space. Therefore we are going to apply an algorithm that reduces the high dimensional space to the 2-dimensional space in which we can neatly visualize the articles. Moreover, it is much easier to understand how the principle of recommendation systems work in a 2-dimensional space as the distance concept then intuitively works well. The algorithm that we use to bring us back to the 2-dimensional space is <strong>Singular Value Decomposition (SVD)</strong>. A different algorithm one can use is <strong>Principal Component Analysis (PCA)</strong>. We do not extensively explain in this article how these algorithms work. In short though, the essence of both is to find the most meaningful basis with which we can reconstruct the original dataset and capture as much of the original variance as possible. Fortunately, <a href=""http://scikit-learn.org/stable/"">scikit-learn</a> already has a built-in version of SVD and PCA which we can therefore easily use.</p>

<p>There are more methods to reduce the dimensionality such as the Information Gain and Chi Square criterion or Random Mapping but for sake of simplicity we stick to the DF feature selection method and PCA dimensionality reduction method. </p>

<p><img src=""http://i63.tinypic.com/2qsmhci.jpg"" alt=""An example of SVD for dimensionality reduction on the Iris dataset"" class=""full-img"">
<em>An example of SVD for dimensionality reduction on the Iris dataset. SVD is applied to reduce the dimensionality from 3D to 2D without losing much information.</em></p>

<h3 id=""makingrecommendations"">Making recommendations</h3>

<p>After we have applied all of the above we reduced all TMT articles to coordinates in a 2-dimensional space. For any article we can now calculate the distance between the two coordinates. The only thing we still need is a function that given the current article as input returns a fixed number of TMT articles that have the lowest distance to this article! Using the Euclidean distance formula this function is trivial to write.</p>

<p>Let's run some scenarios to test our content-based recommender. Suppose we are a user who just finished reading the article <a href=""https://www.themarketingtechnologist.co/caching-http-requests-in-angularjs/"">Caching $http requests in AngularJS</a>. Our content-based recommender system provides the following TMT article suggestion for follow-up: <a href=""https://www.themarketingtechnologist.co/where-have-my-factories-services-constants-and-values-gone-in-angular-2/"">Angular 2: Where have my factories, services, constants and values gone?</a>. Sounds reasonable, right? The table below provides the results of more scenarios.</p>

<table>  
<tbody><tr>  
<th>Current article</th>  
<th>Recommendation</th>  
</tr>

<tr>  
<td>Caching $http requests in AngularJS</td>  
<td>Angular 2: Where have my factories, services, constants and values gone?  
</td>  
</tr>

<tr>  
<td>  
Track content performance using Google Analytics Enhanced Ecommerce report  
</td>  
<td>  
How article size helps you understand your content performance  
</td>  
</tr>

<tr>  
<td>  
Data collection and strange values in CSV format  
</td>  
<td>  
Calculating ad stocks in a fast and readable way in Python  
</td>  
</tr>

<tr>  
<td>  
How npm 3 solves WebStorm's performance issues  
</td>  
<td>  
Webstorm 10 improves the performance of file indexing  
</td>  
</tr>

</tbody></table>

<h3 id=""finalremarks"">Final remarks</h3>

<p>We would like to conclude with a few remarks about our first steps in content-based recommendation systems:</p>

<ul>
<li>In our final recommendation system we used SVD to reduce the dimensionality to 30 features instead of 2. This was done because too much information about the features was lost when we reduced it to a 2-dimensional space. Therefore, the similarity metric of articles was also less reliable. We only used the 2-dimensional space for visualization purposes.</li>
<li>We also applied feature extraction on the title and tags of the TMT articles. This drastically improved the quality of the recommendations.</li>
<li>The parameters in our recommendation system were chosen intuitively and are not optimized. This is something for a future article!</li>
</ul>

<p>We hope you learned a few things from this article! Moreover, if you liked this article we recommend to continue reading on TMT with <a href=""https://www.themarketingtechnologist.co/optimize-media-spends-using-s-response-curves/"">Optimizing media spends using S-response curves</a>... a shameless attempt to improve my <a href=""https://www.themarketingtechnologist.co/track-content-performance-using-google-analytics-enhanced-ecommerce-report/"">author value on TMT</a>. ;-)  </p>

<section class=""related miss"" style=""background-color: #439654 ; padding: 1em;""><h6 style=""color: #fff;"">  
This was the second article in a series on recommendation engines.  
</h6>  
<p style=""color: #fff; margin-bottom: 0;"">  
The third article will follow soon. Hopefully sooner than this one... :)</p>  
</section>  

<p><br></p>
        ","In this second post in a series of posts about a content recommendation system for The Marketing Technologist (TMT) website we are going to elaborate on the concept of content-based recommendation systems. In the first post we described the benefits of recommendation systems and we roughly divided them in two different types of recommenders: content-based and collaborative filtering. The first post also described the prerequisites in order to set-up both types of recommenders. If you haven’t read this first post yet, it is recommended to do this first before you continue. In this article we take our first steps in content-based recommendation systems by describing a quantified approach to express the similarity of articles.
The final code of this article can be found on my Github.
The concept behind content-based recommendation
The goal is to provide our readers with recommendations for other TMT articles. We assume that a reader who has fully read an article liked that article and wants to read more similar articles. Therefore we want to build a content-based recommender that is going to recommend new similar articles based on the users historical reading behaviour.
To achieve accurate and useful recommendations we want to use a mathematical and quantified approach to find the best possible recommendations. Otherwise we are going to lose the interest of our reader and he will leave TMT. Therefore we want to find articles that are similar to each other and thus lie “close to each other”. That is, for which the “distance” between the articles is small, where a smaller distance implies a higher similarity.
Example of visualizing TMT articles in a 2-dimensional space
So, how does this distance concept work? Assume that we can plot any TMT article in a two-dimensional space. The figure above provides an example of 76 TMT articles plotted in a 2-dimensional space. Furthermore we assume that the closer two points lie to each other the more similar they are. Therefore, if a user is reading an article, other articles that lie close to this point in the 2D space can be seen as a good recommendation as they are similar. How close points lie to each other can be calculated using the Euclidean distance formula. In a 2-dimensional space this distance formula simply comes down to the Pythagorean theorem. Note that this distance formula also works for higher dimensions, for example in a 100-dimensional space (although we cannot visualize this when we have more than 3 dimensions).
Euclidean distance formula
A different distance formula to measure similarity of two points is cosine similarity. The cosine similarity function uses the difference in the direction that two articles go, i.e. the difference in angle between two article directions. Imagine that an article can be assigned a direction to which it tends. For example, in a 2-dimensional case one article goes North and the other article goes West. The difference in directions is then -90 degrees. This difference in angle is normalized to the interval [-1, 1], where 1 implies the same direction and thus perfect similarity and -1 the complete opposite direction and thus no similarity.
(Cosine similarity; Image from Dataconomy.com)
We use the cosine similarity metric for measuring the similarity of TMT articles as the direction of articles is more important than the exact distance between them. Additionally, we tried both metrics (Euclidean distance and cosine similarity) and the cosine similarity metric simply performs better. :)
Two new challenges
Above we described the concept of similarity between articles using a quantified approach. However, now two new challenges arise:
How can we plot each post in a 2-dimensional space?
How do we plot these posts such that the distance between the points gives an indication about the similarity of the articles?
An article can be plotted in a 2-dimensional space by assigning it coordinates, i.e. an x and y coordinate. This means we first need to translate our articles to a numeric format and then reduce it to two values, i.e. the x and y coordinate. Therefore, we are first going to elaborate on a scientific approach to quantify the text in the TMT articles by applying feature extraction.
Note that the feature extraction method we discuss below is specifically designed for dealing with text in TMT articles. You can imagine that if you're building a content-based recommender for telephones you probably need a different method to translate the properties (content) of telephones to a numerical format.
Converting TMT articles to a numeric format
The TMT articles, consisting of large phrases of words and punctuation, need to be translated to numerical vectors without losing the content of the article in the process. Preferably we also want vectors of a fixed size. Why do we want a fixed size? Recall the 2-dimensional example above. It would be strange to compare a point in a 2 dimensional space to a point in a 100-dimensional space right? Additionally, if we want to say something about the similarity between articles we also need to express them in a similar manner.
To obtain vectors of a fixed size we are going to create features, e.g. measurable article properties. In text analysis a feature often refers to words, phrases, numbers or symbols. All articles can then be measured and expressed in terms of the same set of features, resulting in fixed-size numerical vectors for all articles. The whole process of converting text to numerical vectors is called feature extraction and is often done in three steps: tokenization, counting and weighting.
Example of vectorizing text
Step 1: tokenization
The first step in obtaining numerical features is tokenization. In text analysis, tokenization is described as obtaining meaningful basic units from large samples of text. For example, in physics speed can be expressed in meters per second. In text analysis, large strings of text can be expressed in tokens. These tokens often correspond to words. Therefore, a simple tokenization method to obtain tokens for the sentence I am happy, because the sun shines is by splitting them on whitespaces. This splitting results in seven tokens, i.e. I, am, happy,, because, the, sun, shines. After tokenization it is possible to express the original sentence in terms of these tokens.
This simple tokenization method however provides several new problems:
For one, this method does not filter out any punctuation and thus the token happy, contains a comma at the end. This implies that the token happy, and happy are two different tokens, although both tokens imply the same word. Therefore, we filter out all types of punctuation, because punctuation is almost never relevant for the meaning of a word in our articles. Note that punctuation can be relevant in other situations. For example, when analyzing Twitter messages punctuation can be important as they are often used to create smiley's which express a lot of sentiment. The smiley example emphasizes the fact that every data source needs its own feature extraction method.
Second, using the simple tokenization method it is possible to obtain the tokens works and working. However, these tokens are just different forms of the same word, i.e. to work. The same argument holds for tokens where one is the plural form of the other. For our content-based recommendation system, we assume that both forms of these words imply the same word. Therefore, the tokens can be reduced to their stem and used as a single token. To do this, a stemming algorithm that reduces every word to its stem is required. Note that such a stemming algorithm is language specific. Luckily there are several freely available packages such as the NLTK library that can do this for us.
A third problem that typically occurs in text analytics is how to deal with combinations or negations of words. For example, just using the individual tokens Google and Analytics may not always imply that we are talking about the product Google Analytics. Therefore, we also create tokens of two or three consecutive words, called respectively bi-grams and tri-grams. The sentence I like big data then translates to the tokens I, like, big, data, I like, like big, big data, I like big and like big data.
Note that this tokenization method does not take into account the position and the order of the words. For example, after tokenization it cannot be said at what position in the original sentence or article the token big occurred. Also, the token itself does not mention anything about the words in front or after it. Therefore, we lose some information about the original sentence during tokenization. The art is to capture as much information about the original sentence while retaining a workable set of tokens.
In our tokenization method we lose information about the structure of the sentences. There are other tokenization methods which take the position and order of words into account as well. For example, Part-Of-Speech (POS) tagging also adds additional information such as the word-class of a token, e.g. whether a token occurs as a verb, adjective, noun or direct object. However, we assume that POS tagging does not greatly increase the performance of our recommender because the order of words within sentences is not of great importance for making recommendations.
Step 2: token frequency counts
In the second step, the frequency of each token in each article is counted. These frequencies are used in the next step for assigning weights to tokens in articles. Additionally these counts are used to later on perform a basic feature selection, i.e. to reduce the number of features. Note that a typical property of text analysis is that the majority of the tokens are only used in a couple of articles. Therefore, the frequency of most tokens in an article is zero.
Step 3: token weights
In the last step the tokens and token frequency counts from the previous steps are used to convert all articles to a numerical format. This is done by encoding each article to a numeric vector whose elements represent the tokens from step 1. Moreover, a token weighting procedure is applied using the frequency counts from step 2. After a token is weighted, it is not any more referred to as a token but as a feature. Hence, a feature represents a token and the value of a feature for an article is a weight assigned by a weighting method.
There are several possible feature weighting methods:
The most basic weighting method is Feature Frequency (FF). FF simply uses the frequency of a token in an article as the weight for a token. For example, given the token set {mad, happy}, the sentence I am not mad, but happy, very happy is weighted as the vector [1 2].
Feature Presence (FP) is a similar basic weighting method. In FP the weight of a token is simply given by a binary variable which is 1 if the token occurs in an article and 0 otherwise. The sentence from the previous example would be represented as the vector [1 1] when using FP, because both tokens are present in this sentence. An additional advantage of FP as weighting method is that a binary dataset is obtained, which does not suffer scaling problems. The latter can occur in algorithms when for example calculating complicated values such as eigenvalues or Hessian.
A more complex feature weighting procedure is the Term Frequency and Inverse Document Frequency' (TF-IDF) weighting method. This method uses two scores, i.e. the term frequency score and the inverse document frequency score. The term frequency score is calculated by taking the frequency of a token in an article. The inverse document frequency score is calculated by the logarithm of dividing the total number of articles by the number of articles in which the token occurs. When multiplying these two scores, a value is obtained that is high for features that occur frequently in a small number of articles, and is low for features that occur often in many articles.
For our content-based recommendation system we are going to use the FP weighting method because it is fast and does not perform worse than the other weighting methods. Additionally, it results in a sparse matrix which has additional computational benefits.
Reducing the dimensionality
After applying the above feature recommendation method we are left with a list of features with which we can numerically express each TMT article. We could calculate the similarity between each article in this very high dimensional space but we prefer not to. Features that barely express similarity between articles, such as is and a, can be removed from the feature set to significantly reduce the number of features and to improve the quality of the recommendations. Additionally, a smaller dataset improves the speed of the recommendation system.
Document Frequency (DF) selection is the simplest feature selection method to reduce dimensionality and is a must for many text analysis problems. We first remove the most common English stop words, e.g. the words the, a, is, et cetera, which do not give much information about the similarity between articles. After that, all features with a very high and very low document frequency are removed from the data set as these features are also not likely to help in differentiating articles.
Recall that at the beginning of this article we visualized the articles in a 2-dimensional space. However, after applying DF we are still in a very high dimensional space. Therefore we are going to apply an algorithm that reduces the high dimensional space to the 2-dimensional space in which we can neatly visualize the articles. Moreover, it is much easier to understand how the principle of recommendation systems work in a 2-dimensional space as the distance concept then intuitively works well. The algorithm that we use to bring us back to the 2-dimensional space is Singular Value Decomposition (SVD). A different algorithm one can use is Principal Component Analysis (PCA). We do not extensively explain in this article how these algorithms work. In short though, the essence of both is to find the most meaningful basis with which we can reconstruct the original dataset and capture as much of the original variance as possible. Fortunately, scikit-learn already has a built-in version of SVD and PCA which we can therefore easily use.
There are more methods to reduce the dimensionality such as the Information Gain and Chi Square criterion or Random Mapping but for sake of simplicity we stick to the DF feature selection method and PCA dimensionality reduction method.
An example of SVD for dimensionality reduction on the Iris dataset. SVD is applied to reduce the dimensionality from 3D to 2D without losing much information.
Making recommendations
After we have applied all of the above we reduced all TMT articles to coordinates in a 2-dimensional space. For any article we can now calculate the distance between the two coordinates. The only thing we still need is a function that given the current article as input returns a fixed number of TMT articles that have the lowest distance to this article! Using the Euclidean distance formula this function is trivial to write.
Let's run some scenarios to test our content-based recommender. Suppose we are a user who just finished reading the article Caching $http requests in AngularJS. Our content-based recommender system provides the following TMT article suggestion for follow-up: Angular 2: Where have my factories, services, constants and values gone?. Sounds reasonable, right? The table below provides the results of more scenarios.
Current article Recommendation
Caching $http requests in AngularJS Angular 2: Where have my factories, services, constants and values gone?
Track content performance using Google Analytics Enhanced Ecommerce report How article size helps you understand your content performance
Data collection and strange values in CSV format Calculating ad stocks in a fast and readable way in Python
How npm 3 solves WebStorm's performance issues Webstorm 10 improves the performance of file indexing
Final remarks
We would like to conclude with a few remarks about our first steps in content-based recommendation systems:
In our final recommendation system we used SVD to reduce the dimensionality to 30 features instead of 2. This was done because too much information about the features was lost when we reduced it to a 2-dimensional space. Therefore, the similarity metric of articles was also less reliable. We only used the 2-dimensional space for visualization purposes.
We also applied feature extraction on the title and tags of the TMT articles. This drastically improved the quality of the recommendations.
The parameters in our recommendation system were chosen intuitively and are not optimized. This is something for a future article!
We hope you learned a few things from this article! Moreover, if you liked this article we recommend to continue reading on TMT with Optimizing media spends using S-response curves... a shameless attempt to improve my author value on TMT. ;-)
This was the second article in a series on recommendation engines.
The third article will follow soon. Hopefully sooner than this one... :)","[Data Science, Recommenders, python]"
63,Send users to your site after clicking on an animated GIF on Facebook,/redirect-users-to-your-site-after-clicking-on-a-gif-on-facebook/,"
            <p>In 2015, Facebook begun supporting animated GIF images. An animated GIF is a graphic image that moves in a loop. Animated GIFs are commonly used by websites like Imgur, Twitter, Reddit and BuzzFeed. Businesses can use the power of animated GIFs to get people's attention. Posting one on Facebook is easy: just post the URL of the GIF into the status update box. </p>

<p>There's one downside: GIFs aren't very suitable for generating traffic to your website. At least, not if a user clicks or taps the GIF. The GIF automatically plays when a user scrolls by, and when someone clicks the little button in the bottom right corner of the GIF, the GIF opens in a new window. On mobile, a tap on the GIF also results in a new window playing the GIF.</p>

<p>In this post, I'll show you how you can use a small Node.js web application to solve this shortcoming. Although we're using Node.js in our code examples, the same concept can be easily implemented using other server side languages like C# or PHP. </p>

<p>So, what do we want to accomplish? We want to share an animated GIF on Facebook. When someone sees the GIF in Facebook's feed, we want to display the GIF. But when the GIF is clicked, we want it to lead to our website, not to a new window with the GIF. </p>

<p>For our example, we'll use this GIF:</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Feb/facebook-1454877185530.gif"" alt="""" class=""full-img""></p>

<h3 id=""controlthegif"">Control the GIF</h3>

<p>As long as we use the original URL of the GIF, the GIF will always to be shown as it is. Instead, we want the GIF to become a redirect to our website, and thus we need more control over the GIF. To do this, we will use some code to <em>proxy</em> the GIF request. </p>

<p>To proxy a request means that instead of showing an image directly, the image is loaded into your own application. That way Facebook sees the image served by your server instead of the original server. </p>

<p>I won't go into the specifics of writing a proxy with Node.js, but you can read about it all over the internet. With the code below, we create a small application that starts an Express web server to proxy the image using the <code>imageUrl</code> query string parameter. </p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""kwd"">var</span><span class=""pln""> express </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""kwd"">require</span><span class=""pun"">(</span><span class=""str"">'express'</span><span class=""pun"">);</span><span class=""pln"">  
</span><span class=""kwd"">var</span><span class=""pln""> app </span><span class=""pun"">=</span><span class=""pln""> express</span><span class=""pun"">();</span><span class=""pln"">  
</span><span class=""kwd"">var</span><span class=""pln""> url </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""kwd"">require</span><span class=""pun"">(</span><span class=""str"">'url'</span><span class=""pun"">);</span><span class=""pln"">  
</span><span class=""kwd"">var</span><span class=""pln""> http </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""kwd"">require</span><span class=""pun"">(</span><span class=""str"">'http'</span><span class=""pun"">);</span><span class=""pln"">

</span><span class=""com"">// Start the Express server</span><span class=""pln"">
app</span><span class=""pun"">.</span><span class=""pln"">listen</span><span class=""pun"">(</span><span class=""pln"">process</span><span class=""pun"">.</span><span class=""pln"">env</span><span class=""pun"">.</span><span class=""pln"">PORT </span><span class=""pun"">||</span><span class=""pln""> </span><span class=""lit"">3000</span><span class=""pun"">)</span><span class=""pln"">

</span><span class=""com"">// When someone hits facebook.gif</span><span class=""pln"">
app</span><span class=""pun"">.</span><span class=""kwd"">get</span><span class=""pun"">(</span><span class=""str"">'/facebook.gif'</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""kwd"">function</span><span class=""pun"">(</span><span class=""pln"">request_from_client</span><span class=""pun"">,</span><span class=""pln""> response_to_client</span><span class=""pun"">){</span><span class=""pln"">  
    </span><span class=""com"">// Get real GIF URL from the query string (?imageUrl=...)</span><span class=""pln"">
    </span><span class=""kwd"">var</span><span class=""pln""> imageUrl </span><span class=""pun"">=</span><span class=""pln""> request_from_client</span><span class=""pun"">.</span><span class=""pln"">query</span><span class=""pun"">.</span><span class=""pln"">imageUrl</span><span class=""pun"">;</span><span class=""pln"">
    </span><span class=""kwd"">var</span><span class=""pln""> image_host_name </span><span class=""pun"">=</span><span class=""pln""> url</span><span class=""pun"">.</span><span class=""pln"">parse</span><span class=""pun"">(</span><span class=""pln"">imageUrl</span><span class=""pun"">).</span><span class=""pln"">hostname
    </span><span class=""kwd"">var</span><span class=""pln""> http_client </span><span class=""pun"">=</span><span class=""pln""> http</span><span class=""pun"">.</span><span class=""pln"">createClient</span><span class=""pun"">(</span><span class=""lit"">80</span><span class=""pun"">,</span><span class=""pln""> image_host_name</span><span class=""pun"">);</span><span class=""pln"">
    </span><span class=""kwd"">var</span><span class=""pln""> image_get_request </span><span class=""pun"">=</span><span class=""pln""> http_client</span><span class=""pun"">.</span><span class=""pln"">request</span><span class=""pun"">(</span><span class=""str"">'GET'</span><span class=""pun"">,</span><span class=""pln""> imageUrl</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""str"">""host""</span><span class=""pun"">:</span><span class=""pln""> image_host_name</span><span class=""pun"">});</span><span class=""pln"">
    image_get_request</span><span class=""pun"">.</span><span class=""pln"">addListener</span><span class=""pun"">(</span><span class=""str"">'response'</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""kwd"">function</span><span class=""pun"">(</span><span class=""pln"">proxy_response</span><span class=""pun"">){</span><span class=""pln"">
        response_to_client</span><span class=""pun"">.</span><span class=""pln"">writeHead</span><span class=""pun"">(</span><span class=""pln"">proxy_response</span><span class=""pun"">.</span><span class=""pln"">statusCode</span><span class=""pun"">,</span><span class=""pln""> proxy_response</span><span class=""pun"">.</span><span class=""pln"">headers</span><span class=""pun"">)</span><span class=""pln"">
        proxy_response</span><span class=""pun"">.</span><span class=""pln"">setEncoding</span><span class=""pun"">(</span><span class=""str"">'binary'</span><span class=""pun"">);</span><span class=""pln"">
        proxy_response</span><span class=""pun"">.</span><span class=""pln"">addListener</span><span class=""pun"">(</span><span class=""str"">'data'</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""kwd"">function</span><span class=""pun"">(</span><span class=""pln"">chunk</span><span class=""pun"">){</span><span class=""pln"">
            response_to_client</span><span class=""pun"">.</span><span class=""pln"">write</span><span class=""pun"">(</span><span class=""pln"">chunk</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""str"">""binary""</span><span class=""pun"">);</span><span class=""pln"">
        </span><span class=""pun"">});</span><span class=""pln"">
        proxy_response</span><span class=""pun"">.</span><span class=""pln"">addListener</span><span class=""pun"">(</span><span class=""str"">'end'</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""kwd"">function</span><span class=""pun"">(){</span><span class=""pln"">
            response_to_client</span><span class=""pun"">.</span><span class=""kwd"">end</span><span class=""pun"">();</span><span class=""pln"">
        </span><span class=""pun"">});</span><span class=""pln"">
    </span><span class=""pun"">});</span><span class=""pln"">
    image_get_request</span><span class=""pun"">.</span><span class=""kwd"">end</span><span class=""pun"">();</span><span class=""pln"">
</span><span class=""pun"">});</span></code></pre>

<p>I've got this code running in a Heroku app. You can test it for yourself replacing the <code>imageUrl</code> value in the query string: <a href=""http://smart-facebook-image.herokuapp.com/facebook.gif?imageUrl=https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Feb/facebook-1454877185530.gif"">http://smart-facebook-image.herokuapp.com/facebook.gif?imageUrl=http://www.yoururl.com/originalGif.gif</a>. This should just show the GIF. </p>

<h3 id=""redirecttoyoursiteoutsidefacebook"">Redirect to your site outside Facebook</h3>

<p>The proxy just returns the original image, and that's exactly what we want when our image is displayed in the Facebook Feed. But if we click it, we want to open our site. So we need to find out whether the image is shown in the Feed or in a new window. Luckily, <a href=""http://stackoverflow.com/questions/8626812/how-to-recognize-facebook-user-agent"">it's pretty easy</a> to recognize the Facebook user agent. The user agent string contains <code>visionutils</code> or <code>facebookexternalhit</code> when a URL is called from within Facebook. </p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""kwd"">const</span><span class=""pln""> agent </span><span class=""pun"">=</span><span class=""pln""> req</span><span class=""pun"">.</span><span class=""pln"">headers</span><span class=""pun"">[</span><span class=""str"">'user-agent'</span><span class=""pun"">].</span><span class=""pln"">toLowerCase</span><span class=""pun"">();</span><span class=""pln"">  
</span><span class=""kwd"">const</span><span class=""pln""> isFacebook </span><span class=""pun"">=</span><span class=""pln""> agent</span><span class=""pun"">.</span><span class=""pln"">indexOf</span><span class=""pun"">(</span><span class=""str"">'visionutils'</span><span class=""pun"">)</span><span class=""pln""> </span><span class=""pun"">&gt;</span><span class=""pln""> </span><span class=""pun"">-</span><span class=""lit"">1</span><span class=""pln""> </span><span class=""pun"">||</span><span class=""pln""> agent</span><span class=""pun"">.</span><span class=""pln"">indexOf</span><span class=""pun"">(</span><span class=""str"">'facebookexternalhit'</span><span class=""pun"">)</span><span class=""pln""> </span><span class=""pun"">&gt;</span><span class=""pln""> </span><span class=""pun"">-</span><span class=""lit"">1</span><span class=""pun"">;</span><span class=""pln"">  </span></code></pre>

<p><code>isFacebook</code> is <code>true</code> when a link is opened in Facebook (a GIF in a feed) and <code>false</code> when it's outside Facebook (a click on the GIF). Now all we have to do is check for this value, and if it's true, redirect the user to our site.</p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""com"">// Redirect URL</span><span class=""pln"">
</span><span class=""kwd"">const</span><span class=""pln""> redirectUrl </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""str"">'http://www.yoursite.com'</span><span class=""pun"">;</span><span class=""pln"">

</span><span class=""kwd"">if</span><span class=""pun"">(!</span><span class=""pln"">isFacebook</span><span class=""pun"">)</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">  
    </span><span class=""com"">// Redirect to the redirect URL</span><span class=""pln"">
    res</span><span class=""pun"">.</span><span class=""pln"">writeHead</span><span class=""pun"">(</span><span class=""lit"">302</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">
        </span><span class=""str"">'Location'</span><span class=""pun"">:</span><span class=""pln""> redirectUrl</span><span class=""pun"">,</span><span class=""pln"">
    </span><span class=""pun"">});</span><span class=""pln"">
    </span><span class=""com"">// Close the connection</span><span class=""pln"">
    </span><span class=""kwd"">return</span><span class=""pln""> res</span><span class=""pun"">.</span><span class=""kwd"">end</span><span class=""pun"">();</span><span class=""pln"">
</span><span class=""pun"">}</span></code></pre>

<p>The complete code is available on [GitHub].(<a href=""https://gist.github.com/codeorelse/604ff9de4e7e5c25f111"">https://gist.github.com/codeorelse/604ff9de4e7e5c25f111</a>). </p>

<p>You can test it for yourself replacing the <code>redirectUrl</code> and <code>imageUrl</code> value in the query string, and post the URL into the status update box: <a href=""http://smart-facebook-image.herokuapp.com/facebook.gif?imageUrl=https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Feb/facebook-1454877185530.gif&amp;redirectUrl=http://www.yoururl.com"">http://smart-facebook-image.herokuapp.com/facebook.gif?imageUrl=http://www.yoururl.com/originalGif.gif&amp;redirectUrl=http://www.yoururl.com</a>. Make sure you <a href=""http://meyerweb.com/eric/tools/dencoder/"">decode</a> the URLs before using them in the querystring. You should see the GIF appear like this:</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Feb/Screen_Shot_2016_02_07_at_21_31_40-1454877541174.png"" alt="""" class=""full-img""></p>

<p>If you want to make sure your proxy returns the correct GIF, you can use Facebook's <a href=""https://developers.facebook.com/tools/debug/og/object/"">Open Graph Object Debugger</a>.</p>

<p>I noticed that the code above doesn't play well with <a href=""http://graphicdesign.stackexchange.com/questions/2844/difference-between-normal-and-interlaced-saving-a-gif"">interlaced GIFs</a>, so that's something to keep in mind when creating and exporting your GIF animation. Facebook caches your GIFs, so you don't have to prepare your server for a big number of requests. On desktop, Facebook actually transforms your animated GIF to an MP4 video that can be played in the Feed. </p>

<p>Of course, using this technique only makes sense if you use animated GIFs on Facebook to get people to your site. Right now, we are testing this technique with some of our clients. As soon as we've got some insights on the performance of these GIFs, we'll share them on The Marketing Technologist. </p>
        ","In 2015, Facebook begun supporting animated GIF images. An animated GIF is a graphic image that moves in a loop. Animated GIFs are commonly used by websites like Imgur, Twitter, Reddit and BuzzFeed. Businesses can use the power of animated GIFs to get people's attention. Posting one on Facebook is easy: just post the URL of the GIF into the status update box.
There's one downside: GIFs aren't very suitable for generating traffic to your website. At least, not if a user clicks or taps the GIF. The GIF automatically plays when a user scrolls by, and when someone clicks the little button in the bottom right corner of the GIF, the GIF opens in a new window. On mobile, a tap on the GIF also results in a new window playing the GIF.
In this post, I'll show you how you can use a small Node.js web application to solve this shortcoming. Although we're using Node.js in our code examples, the same concept can be easily implemented using other server side languages like C# or PHP.
So, what do we want to accomplish? We want to share an animated GIF on Facebook. When someone sees the GIF in Facebook's feed, we want to display the GIF. But when the GIF is clicked, we want it to lead to our website, not to a new window with the GIF.
For our example, we'll use this GIF:
Control the GIF
As long as we use the original URL of the GIF, the GIF will always to be shown as it is. Instead, we want the GIF to become a redirect to our website, and thus we need more control over the GIF. To do this, we will use some code to proxy the GIF request.
To proxy a request means that instead of showing an image directly, the image is loaded into your own application. That way Facebook sees the image served by your server instead of the original server.
I won't go into the specifics of writing a proxy with Node.js, but you can read about it all over the internet. With the code below, we create a small application that starts an Express web server to proxy the image using the imageUrl query string parameter.
var express = require('express');  
var app = express();  
var url = require('url');  
var http = require('http');

// Start the Express server
app.listen(process.env.PORT || 3000)

// When someone hits facebook.gif
app.get('/facebook.gif', function(request_from_client, response_to_client){  
    // Get real GIF URL from the query string (?imageUrl=...)
    var imageUrl = request_from_client.query.imageUrl;
    var image_host_name = url.parse(imageUrl).hostname
    var http_client = http.createClient(80, image_host_name);
    var image_get_request = http_client.request('GET', imageUrl, {""host"": image_host_name});
    image_get_request.addListener('response', function(proxy_response){
        response_to_client.writeHead(proxy_response.statusCode, proxy_response.headers)
        proxy_response.setEncoding('binary');
        proxy_response.addListener('data', function(chunk){
            response_to_client.write(chunk, ""binary"");
        });
        proxy_response.addListener('end', function(){
            response_to_client.end();
        });
    });
    image_get_request.end();
});
I've got this code running in a Heroku app. You can test it for yourself replacing the imageUrl value in the query string: http://smart-facebook-image.herokuapp.com/facebook.gif?imageUrl=http://www.yoururl.com/originalGif.gif. This should just show the GIF.
Redirect to your site outside Facebook
The proxy just returns the original image, and that's exactly what we want when our image is displayed in the Facebook Feed. But if we click it, we want to open our site. So we need to find out whether the image is shown in the Feed or in a new window. Luckily, it's pretty easy to recognize the Facebook user agent. The user agent string contains visionutils or facebookexternalhit when a URL is called from within Facebook.
const agent = req.headers['user-agent'].toLowerCase();  
const isFacebook = agent.indexOf('visionutils') > -1 || agent.indexOf('facebookexternalhit') > -1;  
isFacebook is true when a link is opened in Facebook (a GIF in a feed) and false when it's outside Facebook (a click on the GIF). Now all we have to do is check for this value, and if it's true, redirect the user to our site.
// Redirect URL
const redirectUrl = 'http://www.yoursite.com';

if(!isFacebook) {  
    // Redirect to the redirect URL
    res.writeHead(302, {
        'Location': redirectUrl,
    });
    // Close the connection
    return res.end();
}
The complete code is available on [GitHub].(https://gist.github.com/codeorelse/604ff9de4e7e5c25f111).
You can test it for yourself replacing the redirectUrl and imageUrl value in the query string, and post the URL into the status update box: http://smart-facebook-image.herokuapp.com/facebook.gif?imageUrl=http://www.yoururl.com/originalGif.gif&redirectUrl=http://www.yoururl.com. Make sure you decode the URLs before using them in the querystring. You should see the GIF appear like this:
If you want to make sure your proxy returns the correct GIF, you can use Facebook's Open Graph Object Debugger.
I noticed that the code above doesn't play well with interlaced GIFs, so that's something to keep in mind when creating and exporting your GIF animation. Facebook caches your GIFs, so you don't have to prepare your server for a big number of requests. On desktop, Facebook actually transforms your animated GIF to an MP4 video that can be played in the Feed.
Of course, using this technique only makes sense if you use animated GIFs on Facebook to get people to your site. Right now, we are testing this technique with some of our clients. As soon as we've got some insights on the performance of these GIFs, we'll share them on The Marketing Technologist.","[Code, Facebook, Social advertising]"
64,Instagram: get more out of your carousel ads with this simple trick [case],/instagram-get-more-out-of-your-carousel-ads-with-this-simple-tric-case/,"
            <p>October 1st last year was a day for celebration for Facebook. After trial runs in a couple of countries, Instagram finally became a medium for advertisements. In the Netherlands, Hellman's (a Unilever brand) was the first one to use it. Another Instagram advertising pioneer was Heineken. Although there was some negativity about the user experience of Instagram ads in the beginning, we can finally concentrate on the quality, creativity and performance of the ads.</p>

<p>In this article, I'll point out how a simple, visual trick helped me to double the interaction with an Instagram ad, while decreasing the CPO (cost per order) with 82 percent. </p>

<h3 id=""theproblem"">The problem</h3>

<p>During the launch event on the 30th September in Amsterdam, Facebook recommended not to copy Facebook ads to Instagram, and above all to be creative. To accomplish this, Facebook offers four advertisement types for Instagram: link, video, app install and carousel ads. </p>

<h4 id=""carouselads"">Carousel ads</h4>

<p>Especially carousel ads have evolved tremendously lately. By the end of 2014, Facebook initially introduced this format as multi-product ads. The advertiser can put up to five visuals with corresponding texts and headings in this ad, instead of just a single one. Soon people were using this format not only to show multiple products, but to display a single product from multiple angles. </p>

<p>At first glance, this ad format is perfect to give your creativity free reign, and by doing so, to follow Facebook's advice for a successful Instagram campaign. But there's one small problem: while Facebook users always see one and a half of these so-called 'cards', because these cards have a clear outline, the visual cues that the cards are scrollable are far less obvious on Instagram.</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Feb/Carousal_Ad_Insta_Default-1454604961030.png"" alt=""Carousel ad Facebook"">
<em>Example of a carousel ad on Instagram</em></p>

<h3 id=""thesolution"">The solution</h3>

<p>Apart from the three dots at the bottom and the almost invisible arrows on the sides of the visual, it's the advertiser's job to seduce the user to scroll further to the right. For brands, a second, third or fourth visual (four is the maximum amount of cards in an Instagram carousel ad) is a great opportunity to show their audience multiple messages. Whether these cards show extra USPs, or are intended to inspire the user: the more engaged the user is, the higher the chance of ad and brand recognition. </p>

<p>At <a href=""http://www.bluemangointeractive.com"">Blue Mango Interactive</a> we faced a problem. Our client, the Dutch Staatsloterij (national lotery), wanted to show their target audience that they can win the jackpot and a trip to New York. We decided to replicate the white border between the cards, just like on Facebook. By doing so, we managed to stimulate the desired interaction. At first glance, this solution looks very simple, but that doesn't make it less effective. Below you see the 'old' Instagram on the left, and the new ad with the added white stroke on the right.</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Feb/Carousal_Ad_Insta_Default-1454615748230.png"" alt=""""></p>

<h3 id=""theresults"">The results</h3>

<p>What metrics does one use to review the scrolling behaviour in an ad? Ideally, Facebook would make this data available per card. The percentage of users that scrolls and/or scrolls to the last card is interesting data that could be used to optimize this ad format. Because this data is not (yet) available, we have to manage with the info that Facebook has to offer: clicks on a card and conversion tracking. We assume that if there are more clicks on later cards, these cards have been viewed more and had more user interaction. </p>

<h4 id=""increasedpurchaseintention"">Increased purchase intention</h4>

<p>What are the results of this creative solution? Adding the white stroke in the carousel ad only increased the CTR (click through rate) with 3%. However, the percentage of clicks on cards two and three increased with 104%. We also saw a clear effect in sales. The conversion ratio to transaction was seven times higher, and the cost per transaction dropped with 82%! These figures indicate that the clicks on the ad with the white stroke were far more valuable, compared to the clicks on the default ad. We can carefully conclude that the extra USPs that were displayed in card two (extra prizes) and card three (the jackpot), have increased the purchase intention. </p>

<h3 id=""whatstobeexpected"">What's to be expected?</h3>

<p>According to the most recent figures, 722.000 Dutch people are using Instagram on a daily basis. Internationally it's the fastest growing social media channel ever, it has a bigger user base than Twitter, and it reached the 400 million users milestone faster than its big brother Facebook. </p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Feb/Graph__1_-1454618612656.png"" alt="""">
<em>Growth of Instagram compared to Facebook and Twitter. Via <a href=""http://www.highcarts.com"">highcarts.com</a>.</em></p>

<p>Advertising on Instagram, however, has only just begun. It's to be expected that Facebook will improve the ad formats in the months and years to come. Facebook will analyse the behaviour of the Instagram users and offers the advertisers ad formats that match the brand. Until then, we will wait for the first advertisement formats that are especially developed for the Instagram and match the medium's properties. We will make due! After all, it's the advertiser's job to come up with smart and creative solutions to achieve the desired results.</p>

<p>In this article I showed a simple way to solve the problem of the low visibility of the scroll options in carousel ads. If you've already started working with Instagram ads, you might have faced a similar problem. I'm very curious how you solved it. Please let me know by leaving a comment below! </p>
        ","October 1st last year was a day for celebration for Facebook. After trial runs in a couple of countries, Instagram finally became a medium for advertisements. In the Netherlands, Hellman's (a Unilever brand) was the first one to use it. Another Instagram advertising pioneer was Heineken. Although there was some negativity about the user experience of Instagram ads in the beginning, we can finally concentrate on the quality, creativity and performance of the ads.
In this article, I'll point out how a simple, visual trick helped me to double the interaction with an Instagram ad, while decreasing the CPO (cost per order) with 82 percent.
The problem
During the launch event on the 30th September in Amsterdam, Facebook recommended not to copy Facebook ads to Instagram, and above all to be creative. To accomplish this, Facebook offers four advertisement types for Instagram: link, video, app install and carousel ads.
Carousel ads
Especially carousel ads have evolved tremendously lately. By the end of 2014, Facebook initially introduced this format as multi-product ads. The advertiser can put up to five visuals with corresponding texts and headings in this ad, instead of just a single one. Soon people were using this format not only to show multiple products, but to display a single product from multiple angles.
At first glance, this ad format is perfect to give your creativity free reign, and by doing so, to follow Facebook's advice for a successful Instagram campaign. But there's one small problem: while Facebook users always see one and a half of these so-called 'cards', because these cards have a clear outline, the visual cues that the cards are scrollable are far less obvious on Instagram.
Example of a carousel ad on Instagram
The solution
Apart from the three dots at the bottom and the almost invisible arrows on the sides of the visual, it's the advertiser's job to seduce the user to scroll further to the right. For brands, a second, third or fourth visual (four is the maximum amount of cards in an Instagram carousel ad) is a great opportunity to show their audience multiple messages. Whether these cards show extra USPs, or are intended to inspire the user: the more engaged the user is, the higher the chance of ad and brand recognition.
At Blue Mango Interactive we faced a problem. Our client, the Dutch Staatsloterij (national lotery), wanted to show their target audience that they can win the jackpot and a trip to New York. We decided to replicate the white border between the cards, just like on Facebook. By doing so, we managed to stimulate the desired interaction. At first glance, this solution looks very simple, but that doesn't make it less effective. Below you see the 'old' Instagram on the left, and the new ad with the added white stroke on the right.
The results
What metrics does one use to review the scrolling behaviour in an ad? Ideally, Facebook would make this data available per card. The percentage of users that scrolls and/or scrolls to the last card is interesting data that could be used to optimize this ad format. Because this data is not (yet) available, we have to manage with the info that Facebook has to offer: clicks on a card and conversion tracking. We assume that if there are more clicks on later cards, these cards have been viewed more and had more user interaction.
Increased purchase intention
What are the results of this creative solution? Adding the white stroke in the carousel ad only increased the CTR (click through rate) with 3%. However, the percentage of clicks on cards two and three increased with 104%. We also saw a clear effect in sales. The conversion ratio to transaction was seven times higher, and the cost per transaction dropped with 82%! These figures indicate that the clicks on the ad with the white stroke were far more valuable, compared to the clicks on the default ad. We can carefully conclude that the extra USPs that were displayed in card two (extra prizes) and card three (the jackpot), have increased the purchase intention.
What's to be expected?
According to the most recent figures, 722.000 Dutch people are using Instagram on a daily basis. Internationally it's the fastest growing social media channel ever, it has a bigger user base than Twitter, and it reached the 400 million users milestone faster than its big brother Facebook.
Growth of Instagram compared to Facebook and Twitter. Via highcarts.com.
Advertising on Instagram, however, has only just begun. It's to be expected that Facebook will improve the ad formats in the months and years to come. Facebook will analyse the behaviour of the Instagram users and offers the advertisers ad formats that match the brand. Until then, we will wait for the first advertisement formats that are especially developed for the Instagram and match the medium's properties. We will make due! After all, it's the advertiser's job to come up with smart and creative solutions to achieve the desired results.
In this article I showed a simple way to solve the problem of the low visibility of the scroll options in carousel ads. If you've already started working with Instagram ads, you might have faced a similar problem. I'm very curious how you solved it. Please let me know by leaving a comment below!","[Facebook, Online Advertising, Analytics]"
65,What I've learned by creating a bad watchface for Pebble,/what-ive-learned-by-creating-a-bad-watchface-for-pebble/,"
            <p>At the end of last year, I decided to develop my first watchface for the Pebble Time smartwatch.  I had developed a basic watchface based on a tutorial before, but this time I wanted to do a bit more. </p>

<p>I decided to create a watchface that both shows the time (of course) and counts down to January 1 2016. A watchface for new year’s eve. Pebble watchfaces require you to write your code in C, which was new to me. And when I had an issues with setting the date to countdown to, I decided to use a temporary static timestamp so I could continue with the design of the watchface. I forgot about this change though, and published the watchface on December 31, 18:00. Here’s what happened next.</p>

<h2 id=""theearlyhours"">The early hours</h2>

<p>I sent out a tweet about the watchface, including a link to the pebble store:</p>

<twitterwidget class=""twitter-tweet twitter-tweet-rendered"" id=""twitter-widget-0"" data-tweet-id=""682606781667885058"" style=""position: static; visibility: visible; display: block; transform: rotate(0deg); max-width: 100%; width: 500px; min-width: 220px; margin-top: 10px; margin-bottom: 10px;""></twitterwidget>

<p>Quickly after that, I got both reactions from people on twitter, and questions from the pebble store:</p>

<p><strong>Reactions on Twitter</strong></p>

<twitterwidget class=""twitter-tweet twitter-tweet-rendered"" id=""twitter-widget-1"" data-tweet-id=""682698953788264449"" style=""position: static; visibility: visible; display: block; transform: rotate(0deg); max-width: 100%; width: 500px; min-width: 220px; margin-top: 10px; margin-bottom: 10px;""></twitterwidget>  

<script async="""" src=""//platform.twitter.com/widgets.js"" charset=""utf-8""></script>

<twitterwidget class=""twitter-tweet twitter-tweet-rendered"" id=""twitter-widget-2"" data-tweet-id=""682674094173753344"" style=""position: static; visibility: visible; display: block; transform: rotate(0deg); max-width: 100%; width: 500px; min-width: 220px; margin-top: 10px; margin-bottom: 10px;""></twitterwidget>

<script async="""" src=""//platform.twitter.com/widgets.js"" charset=""utf-8""></script>  

<twitterwidget class=""twitter-tweet twitter-tweet-rendered"" id=""twitter-widget-3"" data-tweet-id=""682675108419260417"" style=""position: static; visibility: visible; display: block; transform: rotate(0deg); max-width: 100%; width: 500px; min-width: 220px; margin-top: 10px; margin-bottom: 10px;""></twitterwidget>

<p><strong>Reactions from the pebble store</strong></p>

<blockquote>I have installed your count down to new years watchface, and the time is right (24hr) but is counting down 7 hours to fast.  I don't see a time zone setting.  At this rate it will finish at 1600 hrs.  Is there something I am doing wrong?  Thanks</blockquote>

<blockquote>Hi  
I was trying the watch face that you created (thanks for that by the way) only issue I have it appears to not pick up the local time zone (in my case EST) it appears to be UTC time as it says right now there's less than 3 hours until 2016. Either way thanks so much and Happy New year :-)</blockquote>

<blockquote>It seems like your NYE countdown watch face is set for a particular time zone. It's 15:46 here and the countdown is showing 2 hours 14 minutes until New Years.</blockquote>

<p>I was receiving more reactions than I expected, which was nice. Sadly, it were not the reactions I wanted. Though it was great to see people sending me pictures of my watchface on their watch, the watchface was clearly counting down incorrectly. People from around the world told me it was counting down to 16:00, 18:00 and other times. At that moment it hit me: “oh no, I forgot to remove the static timestamp”. </p>

<h2 id=""thereactionstotheissue"">The reactions to the issue</h2>

<p>I quickly set up a basic reply to send to users. And luckily, the pebble user base was nice to me. I pointed out the issue, and promised a fix for next year. The reactions were all positive:</p>

<twitterwidget class=""twitter-tweet twitter-tweet-rendered"" id=""twitter-widget-4"" data-tweet-id=""683140324319834112"" style=""position: static; visibility: visible; display: block; transform: rotate(0deg); max-width: 100%; width: 500px; min-width: 220px; margin-top: 10px; margin-bottom: 10px;""></twitterwidget>

<twitterwidget class=""twitter-tweet twitter-tweet-rendered"" id=""twitter-widget-5"" data-tweet-id=""682687064136138752"" style=""position: static; visibility: visible; display: block; transform: rotate(0deg); max-width: 100%; width: 500px; min-width: 220px; margin-top: 10px; margin-bottom: 10px;""></twitterwidget>

<twitterwidget class=""twitter-tweet twitter-tweet-rendered"" id=""twitter-widget-6"" data-tweet-id=""682700142453981184"" style=""position: static; visibility: visible; display: block; transform: rotate(0deg); max-width: 100%; width: 500px; min-width: 220px; margin-top: 10px; margin-bottom: 10px;""></twitterwidget>

<h2 id=""theaftermath"">The aftermath</h2>

<p>While talking to the user base, some of them were even more than just nice, they were actively suggesting new features:</p>

<twitterwidget class=""twitter-tweet twitter-tweet-rendered"" id=""twitter-widget-7"" data-tweet-id=""682738464660582401"" style=""position: static; visibility: visible; display: block; transform: rotate(0deg); max-width: 100%; width: 500px; min-width: 220px; margin-top: 10px; margin-bottom: 10px;""></twitterwidget>

<twitterwidget class=""twitter-tweet twitter-tweet-rendered"" id=""twitter-widget-8"" data-tweet-id=""688397875835965440"" style=""position: static; visibility: visible; display: block; transform: rotate(0deg); max-width: 100%; width: 500px; min-width: 220px; margin-top: 10px; margin-bottom: 10px;""></twitterwidget>

<p>So the user base gave me a direction for the next two updates:</p>

<ul>
<li>Set a custom date to countdown to</li>
<li>Set custom colours</li>
</ul>

<p>After the fix for the main issue, the static timestamp, I decided to add these new features. I’ve recently released <a href=""https://apps.getpebble.com/applications/569a3e8ec94dd6b29c000031"">v1.2 of the watchface</a> which has both the option for a custom countdown date and support for custom colors. </p>

<h2 id=""whatabadproducttaughtme"">What a bad product taught me</h2>

<p>By now, the first tweet of my countdown watchface has become my best tweet ever:</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Feb/tweetactivity-1454489696785.PNG"" alt=""Tweet activity"" class=""full-img""></p>

<p><em>Stats from my first tweet about the watchface. Not bad considering I have an account with about 100 followers.</em></p>

<p>These statistics got me thinking: </p>

<ul>
<li>Did people reply on it the because it didn’t work for them? </li>
<li>What would the stats have looked like if the watchface did work for everyone?</li>
</ul>

<p>I’ll never know. But if I have to choose between a good and bad product, I’ll always pick a good product. I do know that if I accidentally release a bad piece of code, or update, it might just be a good chance to see how active the community is and engage with them.</p>

<script async="""" src=""//platform.twitter.com/widgets.js"" charset=""utf-8""></script>
        ","At the end of last year, I decided to develop my first watchface for the Pebble Time smartwatch. I had developed a basic watchface based on a tutorial before, but this time I wanted to do a bit more.
I decided to create a watchface that both shows the time (of course) and counts down to January 1 2016. A watchface for new year’s eve. Pebble watchfaces require you to write your code in C, which was new to me. And when I had an issues with setting the date to countdown to, I decided to use a temporary static timestamp so I could continue with the design of the watchface. I forgot about this change though, and published the watchface on December 31, 18:00. Here’s what happened next.
The early hours
I sent out a tweet about the watchface, including a link to the pebble store:
Quickly after that, I got both reactions from people on twitter, and questions from the pebble store:
Reactions on Twitter
Reactions from the pebble store
I have installed your count down to new years watchface, and the time is right (24hr) but is counting down 7 hours to fast. I don't see a time zone setting. At this rate it will finish at 1600 hrs. Is there something I am doing wrong? Thanks
Hi I was trying the watch face that you created (thanks for that by the way) only issue I have it appears to not pick up the local time zone (in my case EST) it appears to be UTC time as it says right now there's less than 3 hours until 2016. Either way thanks so much and Happy New year :-)
It seems like your NYE countdown watch face is set for a particular time zone. It's 15:46 here and the countdown is showing 2 hours 14 minutes until New Years.
I was receiving more reactions than I expected, which was nice. Sadly, it were not the reactions I wanted. Though it was great to see people sending me pictures of my watchface on their watch, the watchface was clearly counting down incorrectly. People from around the world told me it was counting down to 16:00, 18:00 and other times. At that moment it hit me: “oh no, I forgot to remove the static timestamp”.
The reactions to the issue
I quickly set up a basic reply to send to users. And luckily, the pebble user base was nice to me. I pointed out the issue, and promised a fix for next year. The reactions were all positive:
The aftermath
While talking to the user base, some of them were even more than just nice, they were actively suggesting new features:
So the user base gave me a direction for the next two updates:
Set a custom date to countdown to
Set custom colours
After the fix for the main issue, the static timestamp, I decided to add these new features. I’ve recently released v1.2 of the watchface which has both the option for a custom countdown date and support for custom colors.
What a bad product taught me
By now, the first tweet of my countdown watchface has become my best tweet ever:
Stats from my first tweet about the watchface. Not bad considering I have an account with about 100 followers.
These statistics got me thinking:
Did people reply on it the because it didn’t work for them?
What would the stats have looked like if the watchface did work for everyone?
I’ll never know. But if I have to choose between a good and bad product, I’ll always pick a good product. I do know that if I accidentally release a bad piece of code, or update, it might just be a good chance to see how active the community is and engage with them.","[usersbase, engagement, smartwatch, product development]"
66,Introducing Robr rate: a metric to spot easy to sell products,/introducing-robr-rate-a-metric-to-spot-easy-to-sell-products/,"
            <p>When you’re running or managing a webshop, it can be hard to identify which products are hardest or easiest to sell. Reports like Google Analytics’ Enhanced Ecommerce show you buy-to-detail and cart-to-detail rates, that give you an indication of how many people see a product, add it to the cart and buy it. But it’s harder to tell how many products got left behind between the cart and the purchase.</p>

<h2 id=""bringintherobrmetrics"">Bring in the Robr metrics</h2>

<p>The Robr metrics are here to help. It started out as a spin on ROAS (Return On Ad Spend) that focused on the potential basket revenue, or Return On Basket Revenue (ROBR or Robr). The goal of  a Robr metric is to easily identify how many products the checkout process ‘robs’ from your cart.</p>

<h2 id=""settinguprobrmetrics"">Setting up Robr metrics</h2>

<p>The Robr metric can be used in every data set, as long as it contains product cart quantity and product purchase quantity. If you’re using Google Analytics Enhanced Ecommerce, you’re in luck. There are two metrics you can easily set up in Google Analytics using <a href=""https://support.google.com/analytics/answer/6121409?hl=en"">calculated metrics [beta]</a>:</p>

<h4 id=""1robrquantity"">1: Robr quantity</h4>

<p>Metric one will show you how many products dropped out between your cart and your purchase page. Use this metric to identify the products with the highest number of dropouts. The formula is <code>quantity of products in cart – quantity products in purchases</code>.</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2015/Dec/geek_robr_quantity-1451468375648.PNG"" alt=""Google Analytics Robr Quantity Calculated Metric"" class=""full-img""></p>

<p><em>Robr quantity setup in Google Analytics.</em></p>

<h4 id=""2robrrate"">2: Robr rate</h4>

<p>Metric two will show you how big the percentage of the Robr quantity is compared to the quantity in the cart. The formula is: <code>(quantity products in cart – quantity products in purchases)/ quantity products in cart</code>. </p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2015/Dec/geek_robr_rate-1451468405133.PNG"" alt=""Google Analytics Robr rate Calculated Metric"" class=""full-img""></p>

<p><em>Robr rate setup in Google Analytics. Note that you can’t use a calculated metric within a second calculated metric. If it would be possible, (quantity products in cart – quantity products in purchases) would be metric 1: Robr quantity.</em></p>

<p>Keep in mind that these metrics only focus on quantity. It’s possible to add revenue into the mix, showing you the lost revenue between the basket and the purchase page. The calculation would be <code>product revenue / (quantity products in purchases/quantity products in cart)</code>.</p>

<h2 id=""usingrobrmetricsforanalysis"">Using Robr metrics for analysis</h2>

<p>The great thing about working at an online marketing agency, is that I can instantly discuss a metric idea with potential users such as marketers or our A/B testing team. Two  examples that I got out these discussions:</p>

<h4 id=""1doupsellproductshaveanegativeimpactonproductsinyourbasket"">1: Do upsell products have a negative impact on products in your basket?</h4>

<p>When analysing upsell options in your basket, it may be hard to identify the effect. For example: if you sell books, and you want to upsell a second (cheaper) book in the checkout as a complementary product. You can look at the effect on both products, but to get the effect on cart dropouts of a product, you’ll need to do some manual analysis. With Robr rate, you can instantly see if the rate has changed for either product. In other words: does your upsell have negative impact on the products the user already has in their basket?</p>

<h4 id=""2whatproductsareagoodfitforproductrecommendation"">2: What products are a good fit for product recommendation?</h4>

<p>If your shop has a suggested products part, you’ll want to suggest products that are both relevant and easy buys. The Robr rate will tell you if a product is likely to be bought as soon as it’s in the cart. This also works for content sites (such as our own). For a related posts part, you’ll want a mix of articles that are relevant, but also easy to read. </p>

<h2 id=""anexamplefromoursite"">An example from our site</h2>

<p>To give you an example, here’s an overview of this December’s article size data from our site: </p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Jan/geek_robr_rate-1452096837742.PNG"" alt=""Google Analytics Robr rate sample report"" class=""full-img""></p>

<p><em>Google Analytics Custom Report from The Marketing Technologist. Dimension: Size. Metrics: Robr rate, Quantity added to cart, Robr quantity, Quantity.</em></p>

<p>This report shows us that the articles have an average Robr rate of 65,44%. It’s surprising to see that large articles have the lowest Robr rate. So our large articles are most likely to be fully read.  You might expect that this would be the small ones, as there is less content to read. This is clear indication that the larger our articles get, the better the content seems to work. </p>

<h2 id=""startusingrobrmetrics"">Start using Robr metrics</h2>

<p>This post gives some examples of the Robr rate for:</p>

<ul>
<li>easier analysis on upsell impact; and</li>
<li>powerful input for recommendations systems for both shops and blogs. </li>
</ul>

<p>Though the examples are focused on Google Analytics, the metrics are useful in each data set that contains the right data (cart quantity and purchase quantity). </p>
        ","When you’re running or managing a webshop, it can be hard to identify which products are hardest or easiest to sell. Reports like Google Analytics’ Enhanced Ecommerce show you buy-to-detail and cart-to-detail rates, that give you an indication of how many people see a product, add it to the cart and buy it. But it’s harder to tell how many products got left behind between the cart and the purchase.
Bring in the Robr metrics
The Robr metrics are here to help. It started out as a spin on ROAS (Return On Ad Spend) that focused on the potential basket revenue, or Return On Basket Revenue (ROBR or Robr). The goal of a Robr metric is to easily identify how many products the checkout process ‘robs’ from your cart.
Setting up Robr metrics
The Robr metric can be used in every data set, as long as it contains product cart quantity and product purchase quantity. If you’re using Google Analytics Enhanced Ecommerce, you’re in luck. There are two metrics you can easily set up in Google Analytics using calculated metrics [beta]:
1: Robr quantity
Metric one will show you how many products dropped out between your cart and your purchase page. Use this metric to identify the products with the highest number of dropouts. The formula is quantity of products in cart – quantity products in purchases.
Robr quantity setup in Google Analytics.
2: Robr rate
Metric two will show you how big the percentage of the Robr quantity is compared to the quantity in the cart. The formula is: (quantity products in cart – quantity products in purchases)/ quantity products in cart.
Robr rate setup in Google Analytics. Note that you can’t use a calculated metric within a second calculated metric. If it would be possible, (quantity products in cart – quantity products in purchases) would be metric 1: Robr quantity.
Keep in mind that these metrics only focus on quantity. It’s possible to add revenue into the mix, showing you the lost revenue between the basket and the purchase page. The calculation would be product revenue / (quantity products in purchases/quantity products in cart).
Using Robr metrics for analysis
The great thing about working at an online marketing agency, is that I can instantly discuss a metric idea with potential users such as marketers or our A/B testing team. Two examples that I got out these discussions:
1: Do upsell products have a negative impact on products in your basket?
When analysing upsell options in your basket, it may be hard to identify the effect. For example: if you sell books, and you want to upsell a second (cheaper) book in the checkout as a complementary product. You can look at the effect on both products, but to get the effect on cart dropouts of a product, you’ll need to do some manual analysis. With Robr rate, you can instantly see if the rate has changed for either product. In other words: does your upsell have negative impact on the products the user already has in their basket?
2: What products are a good fit for product recommendation?
If your shop has a suggested products part, you’ll want to suggest products that are both relevant and easy buys. The Robr rate will tell you if a product is likely to be bought as soon as it’s in the cart. This also works for content sites (such as our own). For a related posts part, you’ll want a mix of articles that are relevant, but also easy to read.
An example from our site
To give you an example, here’s an overview of this December’s article size data from our site:
Google Analytics Custom Report from The Marketing Technologist. Dimension: Size. Metrics: Robr rate, Quantity added to cart, Robr quantity, Quantity.
This report shows us that the articles have an average Robr rate of 65,44%. It’s surprising to see that large articles have the lowest Robr rate. So our large articles are most likely to be fully read. You might expect that this would be the small ones, as there is less content to read. This is clear indication that the larger our articles get, the better the content seems to work.
Start using Robr metrics
This post gives some examples of the Robr rate for:
easier analysis on upsell impact; and
powerful input for recommendations systems for both shops and blogs.
Though the examples are focused on Google Analytics, the metrics are useful in each data set that contains the right data (cart quantity and purchase quantity).","[Analytics, google analytics, metrics, ecommerce]"
67,Create a simple toggle button using reactive programming,/create-a-simple-toggle-button-with-rxjs-using-scan-and-startwith/,"
            <p>I'm pretty familiar with asynchronous programming with callbacks and, more recently, Promises, but I've only started playing around with <a href=""https://en.wikipedia.org/wiki/Reactive_programming"">reactive programming</a> a few weeks ago. More and more people seem to be talking about it, and it looks like the concept will be <a href=""https://github.com/zenparsing/es-observable"">available in ES7</a> in some form as well. </p>

<p>Reactive programming is programming with asynchronous data streams. You are able to create data streams of anything (clicks, hovers, network requests, intervals, etc). Literally everything. At first glance, this concept looked too complex and abstract for the stuff I usually write.</p>

<p>Things got a lot clearer after reading <a href=""https://gist.github.com/staltz/868e7e9bc2a7b8c1f754"">this excellent introduction to reactive programming</a>. If you are new to reactive programming, I'd recommend reading it. When I'm learning a new concept or framework, I try to use it in an actual project as soon as possible. I also learn by writing about it, hence this post ;). </p>

<p>The first use-case I came across was the good old toggle button. You probably know it: initially, an element is hidden, but after you've clicked a button, it becomes visible. When you click the button again, the element disappears. Pretty easy, right? So I decided to write it using streams. </p>

<p>I've read some good stories about <a href=""https://github.com/Reactive-Extensions/RxJS"">RxJS</a>, so I used that library. Another good option is <a href=""https://baconjs.github.io/"">Bacon.js</a>. The principles are the same, so for this toggle button example, it doesn't really matter which one you use. </p>

<h3 id=""togglingusingdomeventstheoldway"">Toggling using DOM events, the 'old way'</h3>

<p>I started writing the code the way I was used to. Just register an event handler on the button, and switch the visibility of the panel in the callback. This code should look pretty familiar and is easy to understand:</p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""kwd"">const</span><span class=""pln""> toggleButton </span><span class=""pun"">=</span><span class=""pln""> document</span><span class=""pun"">.</span><span class=""pln"">querySelector</span><span class=""pun"">(</span><span class=""str"">'.toggleButton'</span><span class=""pun"">);</span><span class=""pln"">  
</span><span class=""kwd"">const</span><span class=""pln""> toggleablePanel </span><span class=""pun"">=</span><span class=""pln""> document</span><span class=""pun"">.</span><span class=""pln"">querySelector</span><span class=""pun"">(</span><span class=""str"">'.panel'</span><span class=""pun"">);</span><span class=""pln"">  
</span><span class=""kwd"">const</span><span class=""pln""> buttonInitialState </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""kwd"">false</span><span class=""pun"">;</span><span class=""pln"">

</span><span class=""kwd"">const</span><span class=""pln""> togglePanel </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""pun"">()</span><span class=""pln""> </span><span class=""pun"">=&gt;</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">  
  </span><span class=""kwd"">const</span><span class=""pln""> panelIsInvisible </span><span class=""pun"">=</span><span class=""pln""> toggleablePanel</span><span class=""pun"">.</span><span class=""pln"">style</span><span class=""pun"">.</span><span class=""pln"">display </span><span class=""pun"">===</span><span class=""pln""> </span><span class=""str"">'none'</span><span class=""pun"">;</span><span class=""pln"">
  toggleablePanel</span><span class=""pun"">.</span><span class=""pln"">style</span><span class=""pun"">.</span><span class=""pln"">display </span><span class=""pun"">=</span><span class=""pln""> panelIsInvisible </span><span class=""pun"">?</span><span class=""pln""> </span><span class=""str"">'block'</span><span class=""pln""> </span><span class=""pun"">:</span><span class=""pln""> </span><span class=""str"">'none'</span><span class=""pun"">;</span><span class=""pln"">
</span><span class=""pun"">};</span><span class=""pln"">

</span><span class=""kwd"">if</span><span class=""pun"">(!</span><span class=""pln"">buttonInitialState</span><span class=""pun"">)</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">  
  toggleablePanel</span><span class=""pun"">.</span><span class=""pln"">style</span><span class=""pun"">.</span><span class=""pln"">display </span><span class=""pun"">=</span><span class=""pln""> buttonInitialState </span><span class=""pun"">?</span><span class=""pln""> </span><span class=""str"">'block'</span><span class=""pln""> </span><span class=""pun"">:</span><span class=""pln""> </span><span class=""str"">'none'</span><span class=""pun"">;</span><span class=""pln"">
</span><span class=""pun"">}</span><span class=""pln"">

toggleButton</span><span class=""pun"">.</span><span class=""pln"">addEventListener</span><span class=""pun"">(</span><span class=""str"">'click'</span><span class=""pun"">,</span><span class=""pln"">togglePanel</span><span class=""pun"">);</span><span class=""pln"">  </span></code></pre>

<p>You see this in action at <a href=""https://jsbin.com/wewesuzaga/edit?js,output"">https://jsbin.com/wewesuzaga</a>. There is nothing wrong with it, and doing it this way is totally acceptable. But seeing our good old friend the <code>addEventListener</code> in action wasn't the reason you got here, right? So, let's...</p>

<h3 id=""bringinthestreams"">.. bring in the streams</h3>

<p>Our variable declaration at the top of the snippet can remain the same. We start by creating a stream of clicks, using <a href=""https://github.com/Reactive-Extensions/RxJS/blob/master/doc/api/core/operators/fromevent.md""><code>Rx.Observable.fromEvent</code></a>. Remember the mantra of reactive programming: everything is a stream!</p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""com"">// Create a stream of clicks on the Toggle button</span><span class=""pln"">
</span><span class=""kwd"">const</span><span class=""pln""> clicks </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""typ"">Rx</span><span class=""pun"">.</span><span class=""typ"">Observable</span><span class=""pun"">.</span><span class=""pln"">fromEvent</span><span class=""pun"">(</span><span class=""pln"">toggleButton</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""str"">'click'</span><span class=""pun"">);</span><span class=""pln"">  </span></code></pre>

<p>This code creates a stream of clicks on the toggleButton, but it doesn't do anything else yet. We need to <code>subscribe</code> to this stream so we can perform an action with a click.  </p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""pln"">clicks</span><span class=""pun"">.</span><span class=""pln"">subscribe </span><span class=""pun"">((</span><span class=""pln"">e</span><span class=""pun"">)</span><span class=""pln""> </span><span class=""pun"">=&gt;</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">  
  </span><span class=""com"">// Handle click</span><span class=""pln"">
</span><span class=""pun"">});</span></code></pre>

<p>The code we've written behaves the same as it did with <code>addEventListener</code>. Now we could simply toggle the visibility in the subscribe callback, but that wouldn't be ideal. After all, we want to separate the decision making from the DOM methods. I want to create a specific stream that tells the subscribers whether the toggle button is off or on. In order to do this, I'll apply a map to the clicks stream and assign it to a 'new' stream, called <code>toggle</code>. Every click event object will be mapped by the <code>determinePanelVisibility</code> method, and it will return the transformed value. </p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""kwd"">const</span><span class=""pln""> determinePanelVisibility </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""pun"">()</span><span class=""pln""> </span><span class=""pun"">=&gt;</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">  
  </span><span class=""kwd"">return</span><span class=""pln""> toggleablePanel</span><span class=""pun"">.</span><span class=""pln"">style</span><span class=""pun"">.</span><span class=""pln"">display </span><span class=""pun"">===</span><span class=""pln""> </span><span class=""str"">'none'</span><span class=""pln"">
</span><span class=""pun"">}</span><span class=""pln"">

</span><span class=""kwd"">const</span><span class=""pln""> toggle </span><span class=""pun"">=</span><span class=""pln""> clicks</span><span class=""pun"">.</span><span class=""pln"">map</span><span class=""pun"">(</span><span class=""pln"">determinePanelVisibility</span><span class=""pun"">);</span><span class=""pln"">  </span></code></pre>

<p>So every click event object will be transformed to a boolean, being true of false, depending on the visibility of the panel. So if we subscribe to the toggle stream, we have this boolean as the first argument, instead of a click event object.</p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""pln"">toggle</span><span class=""pun"">.</span><span class=""pln"">subscribe </span><span class=""pun"">((</span><span class=""pln"">show</span><span class=""pun"">)</span><span class=""pln""> </span><span class=""pun"">=&gt;</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">  
  </span><span class=""com"">// argument `show` is a boolean</span><span class=""pln"">
  toggleablePanel</span><span class=""pun"">.</span><span class=""pln"">style</span><span class=""pun"">.</span><span class=""pln"">display </span><span class=""pun"">=</span><span class=""pln""> show </span><span class=""pun"">?</span><span class=""pln""> </span><span class=""str"">'block'</span><span class=""pln""> </span><span class=""pun"">:</span><span class=""pln""> </span><span class=""str"">'none'</span><span class=""pun"">;</span><span class=""pln"">
</span><span class=""pun"">});</span></code></pre>

<p>When we run this code, we see that the toggle button <a href=""https://jsbin.com/zabuhawovo/edit?html,js,output"">works</a> fine. The only thing we still need to take care of is initially hiding the panel. Of course, we could add the if-statement outside our stream, just like we did in the initial code. But we already have this code in our stream, so why write it again? What we need to do is fake a click on the button, so we can start with the value of <code>buttonInitialState</code>. </p>

<p>This is when the <a href=""https://github.com/Reactive-Extensions/RxJS/blob/master/doc/api/core/operators/startwith.md""><code>startWith</code></a> operator enters the stage. This method does exactly what the name suggests. We can fake a stream dispatch with a specific value, in our case the value of <code>buttonInitialState</code>.</p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""kwd"">const</span><span class=""pln""> toggle </span><span class=""pun"">=</span><span class=""pln""> clicks  
                </span><span class=""pun"">.</span><span class=""pln"">map</span><span class=""pun"">(</span><span class=""pln"">determinePanelVisibility</span><span class=""pun"">)</span><span class=""pln"">
                </span><span class=""pun"">.</span><span class=""pln"">startWith</span><span class=""pun"">(</span><span class=""pln"">buttonInitialState</span><span class=""pun"">);</span></code></pre>

<p>Now the subscribers are called immediately and the panel is initially hidden. <a href=""https://jsbin.com/vazafezefo/edit?html,js,output"">See it in action here</a>. Awesome.  In our scenario, the <code>determinePanelVisibility</code> method doesn't rely on the passed values, so we could also put the <code>startWith</code> method before the <code>map</code>. In most cases, you have to be careful where you place the <code>startWith</code>. For example, if our <code>determinePanelVisibility</code> method would use a property of the passed event's target for making its decision to return true or false, our code would break when we put the startWith before the map. Because <code>buttonInitialState</code> is a boolean and, obviously, <code>determinePanelVisibility</code> wouldn't know what to do with it. </p>

<h3 id=""betterstreamsremovedominteractionfromourstream"">Better streams: remove DOM interaction from our stream</h3>

<p>Although the toggle button worked as intended, I wasn't happy with the result. In the <code>determinePanelVisibility</code>, we still use the DOM to determine the state. I searched the RxJS docs, but couldn't find a method that would work for me. So I dropped a question at <a href=""http://stackoverflow.com/questions/34533158/create-a-toggle-button-with-rxjs"">StackOverflow</a>. A guy named <em>user3743222</em> mentioned the <a href=""http://reactivex.io/documentation/operators/scan.html""><code>Scan</code></a> method, and gave me a small snippet to work with. The Scan operator...</p>

<blockquote>
  <p>... applies a function to the first item emitted by the source Observable and then emits the result of that function as its own first emission. - <a href=""http://reactivex.io/"">http://reactivex.io/</a> </p>
</blockquote>

<p>Exactly what we need! The <code>Scan</code> operator's first argument is an accumulator function to be invoked on each element, and the second argument is the initial accumulator value (<code>buttonInitialState</code> in our case). Please note that this order is switched when using RxJS 2.X. </p>

<p>In our accumulator function, we only want to invert the passed boolean value, because that's how a toggle button works under the hood, right? Hence, we can write a really easy accumulator function. We're only interested in the first parameter, which is the accumulated value.</p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""kwd"">const</span><span class=""pln""> toggleState </span><span class=""pun"">=</span><span class=""pln""> currentState </span><span class=""pun"">=&gt;</span><span class=""pln""> </span><span class=""pun"">!</span><span class=""pln"">currentState</span><span class=""pun"">;</span><span class=""pln"">  </span></code></pre>

<p>We can now replace the <code>map</code> with our new <code>scan</code>. Now the code is far more reusable as the stream does no longer rely on a specific DOM element's visibility. Here's the complete example:  </p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""kwd"">const</span><span class=""pln""> toggleButton </span><span class=""pun"">=</span><span class=""pln""> document</span><span class=""pun"">.</span><span class=""pln"">querySelector</span><span class=""pun"">(</span><span class=""str"">'.toggleButton'</span><span class=""pun"">);</span><span class=""pln"">  
</span><span class=""kwd"">const</span><span class=""pln""> toggleablePanel </span><span class=""pun"">=</span><span class=""pln""> document</span><span class=""pun"">.</span><span class=""pln"">querySelector</span><span class=""pun"">(</span><span class=""str"">'.panel'</span><span class=""pun"">);</span><span class=""pln"">  
</span><span class=""kwd"">const</span><span class=""pln""> buttonInitialState </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""kwd"">true</span><span class=""pun"">;</span><span class=""pln"">  
</span><span class=""kwd"">const</span><span class=""pln""> clicks </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""typ"">Rx</span><span class=""pun"">.</span><span class=""typ"">Observable</span><span class=""pun"">.</span><span class=""pln"">fromEvent</span><span class=""pun"">(</span><span class=""pln"">toggleButton</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""str"">'click'</span><span class=""pun"">);</span><span class=""pln"">  
</span><span class=""kwd"">const</span><span class=""pln""> toggleState </span><span class=""pun"">=</span><span class=""pln""> currentState </span><span class=""pun"">=&gt;</span><span class=""pln""> </span><span class=""pun"">!</span><span class=""pln"">currentState</span><span class=""pun"">;</span><span class=""pln"">

</span><span class=""kwd"">const</span><span class=""pln""> toggle </span><span class=""pun"">=</span><span class=""pln""> clicks  
                </span><span class=""pun"">.</span><span class=""pln"">scan</span><span class=""pun"">(</span><span class=""pln"">toggleState</span><span class=""pun"">,</span><span class=""pln""> buttonInitialState</span><span class=""pun"">)</span><span class=""pln"">
                </span><span class=""pun"">.</span><span class=""pln"">startWith</span><span class=""pun"">(</span><span class=""pln"">buttonInitialState</span><span class=""pun"">);</span><span class=""pln"">

toggle</span><span class=""pun"">.</span><span class=""pln"">subscribe </span><span class=""pun"">((</span><span class=""pln"">show</span><span class=""pun"">)</span><span class=""pln""> </span><span class=""pun"">=&gt;</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">  
  toggleablePanel</span><span class=""pun"">.</span><span class=""pln"">style</span><span class=""pun"">.</span><span class=""pln"">display </span><span class=""pun"">=</span><span class=""pln""> show </span><span class=""pun"">?</span><span class=""pln""> </span><span class=""str"">'block'</span><span class=""pln""> </span><span class=""pun"">:</span><span class=""pln""> </span><span class=""str"">'none'</span><span class=""pun"">;</span><span class=""pln"">
</span><span class=""pun"">});</span></code></pre>

<p>See <a href=""https://jsbin.com/bovewobaxu/edit?js,output"">https://jsbin.com/bovewobaxu/</a> for a working demo. </p>

<p>When you have access to a functional library like <a href=""http://ramdajs.com/"">Ramda</a>, you can even omit the <code>toggleState</code> function, and replace it with something like <a href=""http://ramdajs.com/docs/#not"">`R.not'</a>, making things even more clean. </p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""kwd"">const</span><span class=""pln""> toggle </span><span class=""pun"">=</span><span class=""pln""> clicks  
                </span><span class=""pun"">.</span><span class=""pln"">scan</span><span class=""pun"">(</span><span class=""pln"">R</span><span class=""pun"">.</span><span class=""kwd"">not</span><span class=""pun"">,</span><span class=""pln""> buttonInitialState</span><span class=""pun"">)</span><span class=""pln"">
                </span><span class=""pun"">.</span><span class=""pln"">startWith</span><span class=""pun"">(</span><span class=""pln"">buttonInitialState</span><span class=""pun"">);</span></code></pre>

<p>See demo <a href=""https://jsbin.com/cedacajeka/edit?html,js,output"">here</a>.</p>

<h3 id=""finalthoughts"">Final thoughts</h3>

<p>I'm pretty new to RxJS and reactive programming in general, so I can imagine a seasoned programmer could improve the implementation of the toggle button. I'm very curious, so please drop your thoughts in the comments. </p>

<p>Although I'm starting to wrap my head around the concept of reactive programming and I am pretty confident with functional programming in general, I still feel it can add a lot of (false) complexity to simple programs. Also, it really asks for a very functional mindset that, at least in my environment, isn't (yet) in the heads of average JavaScript developer. </p>

<p>Functional and reactive programming should improve readability of our programs, and I believe it will eventually, as soon as more and more people will start using it in their software. It will change the way we write our code immensely. Adding it so the <a href=""https://github.com/zenparsing/es-observable"">ES7 specification</a> is a great step in the right direction.</p>
        ","I'm pretty familiar with asynchronous programming with callbacks and, more recently, Promises, but I've only started playing around with reactive programming a few weeks ago. More and more people seem to be talking about it, and it looks like the concept will be available in ES7 in some form as well.
Reactive programming is programming with asynchronous data streams. You are able to create data streams of anything (clicks, hovers, network requests, intervals, etc). Literally everything. At first glance, this concept looked too complex and abstract for the stuff I usually write.
Things got a lot clearer after reading this excellent introduction to reactive programming. If you are new to reactive programming, I'd recommend reading it. When I'm learning a new concept or framework, I try to use it in an actual project as soon as possible. I also learn by writing about it, hence this post ;).
The first use-case I came across was the good old toggle button. You probably know it: initially, an element is hidden, but after you've clicked a button, it becomes visible. When you click the button again, the element disappears. Pretty easy, right? So I decided to write it using streams.
I've read some good stories about RxJS, so I used that library. Another good option is Bacon.js. The principles are the same, so for this toggle button example, it doesn't really matter which one you use.
Toggling using DOM events, the 'old way'
I started writing the code the way I was used to. Just register an event handler on the button, and switch the visibility of the panel in the callback. This code should look pretty familiar and is easy to understand:
const toggleButton = document.querySelector('.toggleButton');  
const toggleablePanel = document.querySelector('.panel');  
const buttonInitialState = false;

const togglePanel = () => {  
  const panelIsInvisible = toggleablePanel.style.display === 'none';
  toggleablePanel.style.display = panelIsInvisible ? 'block' : 'none';
};

if(!buttonInitialState) {  
  toggleablePanel.style.display = buttonInitialState ? 'block' : 'none';
}

toggleButton.addEventListener('click',togglePanel);  
You see this in action at https://jsbin.com/wewesuzaga. There is nothing wrong with it, and doing it this way is totally acceptable. But seeing our good old friend the addEventListener in action wasn't the reason you got here, right? So, let's...
.. bring in the streams
Our variable declaration at the top of the snippet can remain the same. We start by creating a stream of clicks, using Rx.Observable.fromEvent. Remember the mantra of reactive programming: everything is a stream!
// Create a stream of clicks on the Toggle button
const clicks = Rx.Observable.fromEvent(toggleButton, 'click');  
This code creates a stream of clicks on the toggleButton, but it doesn't do anything else yet. We need to subscribe to this stream so we can perform an action with a click.
clicks.subscribe ((e) => {  
  // Handle click
});
The code we've written behaves the same as it did with addEventListener. Now we could simply toggle the visibility in the subscribe callback, but that wouldn't be ideal. After all, we want to separate the decision making from the DOM methods. I want to create a specific stream that tells the subscribers whether the toggle button is off or on. In order to do this, I'll apply a map to the clicks stream and assign it to a 'new' stream, called toggle. Every click event object will be mapped by the determinePanelVisibility method, and it will return the transformed value.
const determinePanelVisibility = () => {  
  return toggleablePanel.style.display === 'none'
}

const toggle = clicks.map(determinePanelVisibility);  
So every click event object will be transformed to a boolean, being true of false, depending on the visibility of the panel. So if we subscribe to the toggle stream, we have this boolean as the first argument, instead of a click event object.
toggle.subscribe ((show) => {  
  // argument `show` is a boolean
  toggleablePanel.style.display = show ? 'block' : 'none';
});
When we run this code, we see that the toggle button works fine. The only thing we still need to take care of is initially hiding the panel. Of course, we could add the if-statement outside our stream, just like we did in the initial code. But we already have this code in our stream, so why write it again? What we need to do is fake a click on the button, so we can start with the value of buttonInitialState.
This is when the startWith operator enters the stage. This method does exactly what the name suggests. We can fake a stream dispatch with a specific value, in our case the value of buttonInitialState.
const toggle = clicks  
                .map(determinePanelVisibility)
                .startWith(buttonInitialState);
Now the subscribers are called immediately and the panel is initially hidden. See it in action here. Awesome. In our scenario, the determinePanelVisibility method doesn't rely on the passed values, so we could also put the startWith method before the map. In most cases, you have to be careful where you place the startWith. For example, if our determinePanelVisibility method would use a property of the passed event's target for making its decision to return true or false, our code would break when we put the startWith before the map. Because buttonInitialState is a boolean and, obviously, determinePanelVisibility wouldn't know what to do with it.
Better streams: remove DOM interaction from our stream
Although the toggle button worked as intended, I wasn't happy with the result. In the determinePanelVisibility, we still use the DOM to determine the state. I searched the RxJS docs, but couldn't find a method that would work for me. So I dropped a question at StackOverflow. A guy named user3743222 mentioned the Scan method, and gave me a small snippet to work with. The Scan operator...
... applies a function to the first item emitted by the source Observable and then emits the result of that function as its own first emission. - http://reactivex.io/
Exactly what we need! The Scan operator's first argument is an accumulator function to be invoked on each element, and the second argument is the initial accumulator value (buttonInitialState in our case). Please note that this order is switched when using RxJS 2.X.
In our accumulator function, we only want to invert the passed boolean value, because that's how a toggle button works under the hood, right? Hence, we can write a really easy accumulator function. We're only interested in the first parameter, which is the accumulated value.
const toggleState = currentState => !currentState;  
We can now replace the map with our new scan. Now the code is far more reusable as the stream does no longer rely on a specific DOM element's visibility. Here's the complete example:
const toggleButton = document.querySelector('.toggleButton');  
const toggleablePanel = document.querySelector('.panel');  
const buttonInitialState = true;  
const clicks = Rx.Observable.fromEvent(toggleButton, 'click');  
const toggleState = currentState => !currentState;

const toggle = clicks  
                .scan(toggleState, buttonInitialState)
                .startWith(buttonInitialState);

toggle.subscribe ((show) => {  
  toggleablePanel.style.display = show ? 'block' : 'none';
});
See https://jsbin.com/bovewobaxu/ for a working demo.
When you have access to a functional library like Ramda, you can even omit the toggleState function, and replace it with something like `R.not', making things even more clean.
const toggle = clicks  
                .scan(R.not, buttonInitialState)
                .startWith(buttonInitialState);
See demo here.
Final thoughts
I'm pretty new to RxJS and reactive programming in general, so I can imagine a seasoned programmer could improve the implementation of the toggle button. I'm very curious, so please drop your thoughts in the comments.
Although I'm starting to wrap my head around the concept of reactive programming and I am pretty confident with functional programming in general, I still feel it can add a lot of (false) complexity to simple programs. Also, it really asks for a very functional mindset that, at least in my environment, isn't (yet) in the heads of average JavaScript developer.
Functional and reactive programming should improve readability of our programs, and I believe it will eventually, as soon as more and more people will start using it in their software. It will change the way we write our code immensely. Adding it so the ES7 specification is a great step in the right direction.","[code, reactive programming, rxjs]"
68,Looking back at a fantastic first year: Most read articles in 2015,/looking-back-at-a-fantastic-first-year-most-read-articles-in-2015/,"
            <p>In May 2015, we <a href=""http://www.themarketingtechnologist.co/introducing-geek/"">started</a> this blog as Blue Mango Geek. Our belief? When you're writing a coworker an email longer than one paragraph, you should consider blogging about it. Share it with the rest of the company and the whole world! We decided to write about the things we discover or learned throughout the day working for a digital media agency - stories, both big and small. </p>

<p>After a few months, we decided that we no longer want to be a platform just for the people in our <a href=""http://www.bluemangointeractive.com"">company</a>, but for everyone who is interested in all things marketing technology. So we changed our name to The Marketing Technologist and let people from outside our company write on our platform. </p>

<p>We started only nine months ago, but we've already published over 60 articles. We wrote a lot about web technology and analytics, but also about <a href=""http://www.themarketingtechnologist.co/smart-clothing/"">smart clothing</a>, <a href=""http://www.themarketingtechnologist.co/our-first-insights-in-the-ad-blocking-consumer/"">ad blocking</a>,  <a href=""http://www.themarketingtechnologist.co/getting-started-with-sequential-messaging-the-4-main-stages/"">sequential messaging</a> and <a href=""http://www.themarketingtechnologist.co/how-we-keep-our-mobile-device-lab-small-and-simple/"">a mobile device lab</a>. All our articles are written by data scientists, designers, developers and web analysts, and are read by hundreds of people every day. If you're one of our frequent readers: thank you for your time! If you have any suggestions for future posts, let us know! </p>

<p>We are incredibly proud of what we've achieved so far, and I'm looking forward to a new year filled with awesome posts about marketing technology. If you'd like to participate in any way, please let us know. You can connect with us using <a href=""https://twitter.com/m__technologist"">Twitter</a> or <a href=""https://www.facebook.com/themarketingtechnologist/"">Facebook</a>. </p>

<p>These are the most read articles of The Marketing Technologist in 2015:</p>

<h4 id=""mostreadinanalytics"">Most read in Analytics</h4>

<ol>
<li><a href=""http://www.themarketingtechnologist.co/simplify-iframe-tracking-for-universal-analytics/"">Simplify iFrame tracking for Universal Analytics</a>  </li>
<li><a href=""http://www.themarketingtechnologist.co/bring-on-and-offline-together-with-ibeacons-and-google-analytics/"">Bring on- and offline together with iBeacons and Google Analytics</a>  </li>
<li><a href=""http://www.themarketingtechnologist.co/migrating-to-google-tag-manager-without-changing-hard-coded-_gaq-push-events/"">Migrating to Google Tag Manager without changing hard coded _gaq.push() events</a>  </li>
</ol>

<h4 id=""mostreadaincode"">Most read a in Code</h4>

<ol>
<li><a href=""http://www.themarketingtechnologist.co/caching-http-requests-in-angularjs/"">Caching $http requests in AngularJS</a>  </li>
<li><a href=""http://www.themarketingtechnologist.co/where-have-my-factories-services-constants-and-values-gone-in-angular-2/"">Angular 2: Where have my factories, services, constants and values gone?</a>  </li>
<li><a href=""http://www.themarketingtechnologist.co/5-reasons-why-you-need-a-javascript-style-guide/"">5 reasons why you need a JavaScript style guide</a></li>
</ol>

<h4 id=""mostreadindatascience"">Most read in Data Science</h4>

<ol>
<li><a href=""http://www.themarketingtechnologist.co/optimize-media-spends-using-s-response-curves/"">Optimizing media spends using S-response curves</a>  </li>
<li><a href=""http://www.themarketingtechnologist.co/slashception-with-regexp_extract-in-hive/"">Slashception with regexp_extract in Hive</a>  </li>
<li><a href=""http://www.themarketingtechnologist.co/calculating-ad-stocks-in-a-fast-and-readable-way-in-python/"">Calculating ad stocks in a fast and readable way in Python</a>  </li>
</ol>

<h3 id=""mostreadauthors"">Most read authors</h3>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Jan/logo_white-1451761089901.jpg"" alt="""" class=""full-img""></p>

<ol>
<li><a href=""http://www.themarketingtechnologist.co/author/siebe/"">Siebe Hiemstra</a> - Marketing Engineer  </li>
<li><a href=""http://www.themarketingtechnologist.co/author/erik-driessen/"">Erik Driessen</a> - Web Analyst  </li>
<li><a href=""http://www.themarketingtechnologist.co/author/thom-hopmans/"">Thom Hopmans</a> - Data Scientist</li>
</ol>

<p>Happy new year!</p>
        ","In May 2015, we started this blog as Blue Mango Geek. Our belief? When you're writing a coworker an email longer than one paragraph, you should consider blogging about it. Share it with the rest of the company and the whole world! We decided to write about the things we discover or learned throughout the day working for a digital media agency - stories, both big and small.
After a few months, we decided that we no longer want to be a platform just for the people in our company, but for everyone who is interested in all things marketing technology. So we changed our name to The Marketing Technologist and let people from outside our company write on our platform.
We started only nine months ago, but we've already published over 60 articles. We wrote a lot about web technology and analytics, but also about smart clothing, ad blocking, sequential messaging and a mobile device lab. All our articles are written by data scientists, designers, developers and web analysts, and are read by hundreds of people every day. If you're one of our frequent readers: thank you for your time! If you have any suggestions for future posts, let us know!
We are incredibly proud of what we've achieved so far, and I'm looking forward to a new year filled with awesome posts about marketing technology. If you'd like to participate in any way, please let us know. You can connect with us using Twitter or Facebook.
These are the most read articles of The Marketing Technologist in 2015:
Most read in Analytics
Simplify iFrame tracking for Universal Analytics
Bring on- and offline together with iBeacons and Google Analytics
Migrating to Google Tag Manager without changing hard coded _gaq.push() events
Most read a in Code
Caching $http requests in AngularJS
Angular 2: Where have my factories, services, constants and values gone?
5 reasons why you need a JavaScript style guide
Most read in Data Science
Optimizing media spends using S-response curves
Slashception with regexp_extract in Hive
Calculating ad stocks in a fast and readable way in Python
Most read authors
Siebe Hiemstra - Marketing Engineer
Erik Driessen - Web Analyst
Thom Hopmans - Data Scientist
Happy new year!",[Overview]
69,When creativity hurts data: a visualisation story,/when-creativity-hurts-data-a-visualisation-story/,"
            <p>Yesterday , I saw a graph in the Dutch newspaper 'De Telegraaf' that frustrated me. It shows data about damages caused by fireworks in Dutch cities. Sadly, they decided to visualize in a way that leaves room for interpretation. Three different interpretations to be precise. </p>

<h2 id=""thegraph"">The graph</h2>

<p>Let’s start with the graph:</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2015/Dec/geek_dataviz_telegraaf_crop-1450860123045.jpg"" alt=""the graph""></p>

<p><em>The graph shows a top 5 of Dutch cities based on the damages by fireworks. If you’re wondering “Where’s Amsterdam?”, they don’t collect the data like other cities do.</em></p>

<p>The dataset of the graph is fine, nothing wrong there. But the way they decided to visualise it is bad. First of all, look at the data labels, it’s clear to see that the design of the visualisation was leading here, as the labels change as soon as they don’t fit within the design of the bar. The second problem with this graph, is that it’s possible to interpret the data in three different ways (if I get creative, I can get four more out of it). </p>

<h2 id=""possibleanalyses"">Possible analyses</h2>

<p>As the image above shows, there are three basic ways to look at the data visualisation. </p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2015/Dec/geek_graphs_simple-1450861157946.png"" alt=""the three possible interpretatins"" class=""full-img""></p>

<p>You can compare the firecracker in three ways:</p>

<ul>
<li><strong>1:</strong> Firecracker from the most left point, to the most right point, including the fuse.</li>
<li><strong>2:</strong> Firecracker from the most left point, to the most right point, excluding the fuse.</li>
<li><strong>3:</strong> Firecracker from the centre of the dark red semi-circle to the dark red semi-circle on the right point.</li>
</ul>

<p>In my opinion, a data visualisation should help you understand the data, not tell you three different stories. </p>

<h2 id=""threedifferentstories"">Three different stories</h2>

<p>To show you the problem with the three interpretations, I’ve put the actual data and the data of the three ways of looking at it in a table:</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2015/Dec/geek_graph_data-1450861966251.PNG"" alt=""the simplified data set""></p>

<p>Comparing the data in the visualisation, the comparison of cities can shift anywhere between 1% and 21%, just because the visualisation isn't clear. And as you can see, none of the three graphs' data matches the actual data. That's bad.</p>

<h2 id=""howtodoit"">How to do it</h2>

<p>When visualising, always try to tell the right story with your data. My suggestion would be a less fancy graph, but one that truly tells the story of the data:</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2015/Dec/geek_graphs_sidebyse-1450860761985.png"" alt=""the better graph"" class=""full-img""></p>

<p><em>The actual and simplified graph side by side.</em></p>

<p>The graph isn't as fancy as the original one, but it's accurate. It uses an axis to show the numbers, instead of labelling the bars. The end result is a graph that's easier to read and doesn't leave room for bad interpretation.  </p>

<h2 id=""finalthought"">Final thought</h2>

<p>The example in this post is quite harmless, as it's 'just a fancy graph in a newspaper'. But think about other data analyses, e.g. your monthly sales reports, or startup growth reports. A difference of 21% in data interpretation could badly impact your decision making. </p>

<p>Do yourself a favour: stop making fancy graphs, start making accurate ones. </p>
        ","Yesterday , I saw a graph in the Dutch newspaper 'De Telegraaf' that frustrated me. It shows data about damages caused by fireworks in Dutch cities. Sadly, they decided to visualize in a way that leaves room for interpretation. Three different interpretations to be precise.
The graph
Let’s start with the graph:
The graph shows a top 5 of Dutch cities based on the damages by fireworks. If you’re wondering “Where’s Amsterdam?”, they don’t collect the data like other cities do.
The dataset of the graph is fine, nothing wrong there. But the way they decided to visualise it is bad. First of all, look at the data labels, it’s clear to see that the design of the visualisation was leading here, as the labels change as soon as they don’t fit within the design of the bar. The second problem with this graph, is that it’s possible to interpret the data in three different ways (if I get creative, I can get four more out of it).
Possible analyses
As the image above shows, there are three basic ways to look at the data visualisation.
You can compare the firecracker in three ways:
1: Firecracker from the most left point, to the most right point, including the fuse.
2: Firecracker from the most left point, to the most right point, excluding the fuse.
3: Firecracker from the centre of the dark red semi-circle to the dark red semi-circle on the right point.
In my opinion, a data visualisation should help you understand the data, not tell you three different stories.
Three different stories
To show you the problem with the three interpretations, I’ve put the actual data and the data of the three ways of looking at it in a table:
Comparing the data in the visualisation, the comparison of cities can shift anywhere between 1% and 21%, just because the visualisation isn't clear. And as you can see, none of the three graphs' data matches the actual data. That's bad.
How to do it
When visualising, always try to tell the right story with your data. My suggestion would be a less fancy graph, but one that truly tells the story of the data:
The actual and simplified graph side by side.
The graph isn't as fancy as the original one, but it's accurate. It uses an axis to show the numbers, instead of labelling the bars. The end result is a graph that's easier to read and doesn't leave room for bad interpretation.
Final thought
The example in this post is quite harmless, as it's 'just a fancy graph in a newspaper'. But think about other data analyses, e.g. your monthly sales reports, or startup growth reports. A difference of 21% in data interpretation could badly impact your decision making.
Do yourself a favour: stop making fancy graphs, start making accurate ones.","[Data, Visualisation, dataviz, analysis]"
70,"Takeaways from a talk about LeSS, an evening with Bas Vodde",/takeaways-from-a-talk-about-less-an-evening-with-bas-vodde/,"
            <p>Last week, NLScrum, Agile Consortium and ING presented a Meetup in Amsterdam with Bas Vodde about LeSS : Scrum in multiple scrum teams. <br>
Or, as Bas mentioned on later:  In a productive environment they do “Multiple team Scrum” and not “Multiple Scrum teams”.</p>

<p>These are basically my notes, it might not be a very consistent story.</p>

<p><strong>About Trust</strong></p>

<p>Because of the setup of the presentation, that was about the WHY of LeSS instead of the HOW, I remembered a valuable lesson: <em>If you want people to work together and enable them to do the best job possible, you have to trust them with their taken on responsibilities AND never take those back.</em> <br>
The latter is a 'natural' but counterproductive reaction I have too, once in a while. It's something I learned as a teacher a few years ago: children should learn themselves, I couldn't make them do it. I was just the one facilitating the context. If they didn't learn, they'd fail their test. <br>
Their responsibility and it took them a few low grades to learn it.</p>

<p>It's the same in a team: <em>their</em> work is <em>their</em> responsibility. And failing is an acceptable (even desirable) part of the learning curve of a team.</p>

<p>So this evening the role of a scrummaster became very clear again, but a bit broader defined as I did before:</p>

<blockquote>
  <p>The role of a SM is to make sure the organisation supports &amp;  therefore enables the team to do work in their preferred way. </p>
</blockquote>

<p><strong>About simple systems</strong> </p>

<p>Bas also talked about building a working context that's ""bare minimal"". A bloated system will have a lot of unused or counter productive features, that will slow things down. I strongly believe in a system that has just enough rules to enable the work, but doesn't prescribes the details. LeSS follows this strategy and it's widely recognized, eg in the great video below by  “Morieux” ( TED talk )</p>

<iframe src=""https://embed-ssl.ted.com/talks/yves_morieux_as_work_gets_more_complex_6_rules_to_simplify.html"" width=""640"" height=""360"" frameborder=""0"" scrolling=""no"" webkitallowfullscreen="""" mozallowfullscreen="""" allowfullscreen=""""></iframe>

<h6 id=""justbarenotes"">Just Bare notes</h6>

<p>“Multiple team Scrum” and not “Multiple scrum teams”
"" LeSS === Scrum "" with</p>

<ul>
<li>1 PO on multiple feature teams</li>
<li>1 Product Backlog ( and multiple sprint backlogs) for multiple teams</li>
</ul>

<p><em>Remember this</em>:</p>

<ul>
<li>Don’t do component teams : Feature teams only.</li>
<li>Feature teams don't have to be equal! Of course some teams are more specialised then others. It won't be a problem.</li>
<li>Feature teams are able to talk to clients and have full control of the realisation of the request.</li>
<li>For teams 2-8 : Bring everything from LeSS in at once</li>
<li><p>For larger amount of teams : go easy…</p></li>
<li><p>“ DEV-OPS = Scrum with extended DoD” ( As we do with the BaaS team!)</p></li>
<li>“ An issue? How would you do it in 1 team? Can you do the same with multiple teams?”</li>
<li>“ An issue? How would you do it in private? </li>
<li>Why would you deal with it  in a different way at work?”</li>
<li>“ A PO is about directions, a team is about details”</li>
<li>“ A PO has 2 tasks :  Prioritisation, ( easy to scale) and Clarification ( which doesn't scale and should be relayed to the teams). So in LeSS: the PO is responsible for, but not (primarily) working on clarification.</li>
</ul>

<p>So, it was an enabling evening! Thanks Bas!</p>

<p><em>Published on LinkedIn on 2016-12-20</em></p>
        ","Last week, NLScrum, Agile Consortium and ING presented a Meetup in Amsterdam with Bas Vodde about LeSS : Scrum in multiple scrum teams.
Or, as Bas mentioned on later: In a productive environment they do “Multiple team Scrum” and not “Multiple Scrum teams”.
These are basically my notes, it might not be a very consistent story.
About Trust
Because of the setup of the presentation, that was about the WHY of LeSS instead of the HOW, I remembered a valuable lesson: If you want people to work together and enable them to do the best job possible, you have to trust them with their taken on responsibilities AND never take those back.
The latter is a 'natural' but counterproductive reaction I have too, once in a while. It's something I learned as a teacher a few years ago: children should learn themselves, I couldn't make them do it. I was just the one facilitating the context. If they didn't learn, they'd fail their test.
Their responsibility and it took them a few low grades to learn it.
It's the same in a team: their work is their responsibility. And failing is an acceptable (even desirable) part of the learning curve of a team.
So this evening the role of a scrummaster became very clear again, but a bit broader defined as I did before:
The role of a SM is to make sure the organisation supports & therefore enables the team to do work in their preferred way.
About simple systems
Bas also talked about building a working context that's ""bare minimal"". A bloated system will have a lot of unused or counter productive features, that will slow things down. I strongly believe in a system that has just enough rules to enable the work, but doesn't prescribes the details. LeSS follows this strategy and it's widely recognized, eg in the great video below by “Morieux” ( TED talk )
Just Bare notes
“Multiple team Scrum” and not “Multiple scrum teams” "" LeSS === Scrum "" with
1 PO on multiple feature teams
1 Product Backlog ( and multiple sprint backlogs) for multiple teams
Remember this:
Don’t do component teams : Feature teams only.
Feature teams don't have to be equal! Of course some teams are more specialised then others. It won't be a problem.
Feature teams are able to talk to clients and have full control of the realisation of the request.
For teams 2-8 : Bring everything from LeSS in at once
For larger amount of teams : go easy…
“ DEV-OPS = Scrum with extended DoD” ( As we do with the BaaS team!)
“ An issue? How would you do it in 1 team? Can you do the same with multiple teams?”
“ An issue? How would you do it in private?
Why would you deal with it in a different way at work?”
“ A PO is about directions, a team is about details”
“ A PO has 2 tasks : Prioritisation, ( easy to scale) and Clarification ( which doesn't scale and should be relayed to the teams). So in LeSS: the PO is responsible for, but not (primarily) working on clarification.
So, it was an enabling evening! Thanks Bas!
Published on LinkedIn on 2016-12-20","[scrum, LeSS]"
71,Collecting form abandonment data with Formagical and Google Analytics,/collecting-form-abandonment-data-with-formagical-and-google-analytics/,"
            <p>In April this year, Siebe wrote about his <a href=""http://www.themarketingtechnologist.co/introducing-formagical-form-analytics-with-dashboarding/"">form analysis script Formagical.JS</a>.  I recommend you to read this post to get familiar with the basics of Formagical.</p>

<p>We’ve had the first version of the script running for one of our clients since April. During this period, Siebe and I updated the tool and implementation to make insights easier for the client. This post will discuss both the Formagical update and a best practice for Google Analytics reports based on Formagical.JS.</p>

<h2 id=""whatsnewwithformagical"">What’s new with Formagical</h2>

<p>By default, Formagical tracked all the basics we thought we needed:</p>

<ul>
<li>user opened a page that has Formagical.JS on it</li>
<li>user started using form by interacting with one of its elements</li>
<li>user enters a certain form element (focus)</li>
<li>user leaves a certain form element (unfocus)</li>
<li>user starts typing in a certain element (input and text area only)</li>
<li>users changes the selection of a certain form element (dropdown, radio button and checkbox only)</li>
<li>users pauses typing and continues</li>
<li>user submits form</li>
</ul>

<p>As soon as we put it in the hands of the first client, it was clear that something was missing. Our client found it hard to see the order of input fields as they appear on the form. Good point, as it was impossible to see that at that moment. You ‘d see the input field labels, but no order. Luckily, Siebe added the feature in no time and after that I updated the library in our tag manager. So from that moment onwards, for each input field, we also have an index number available. </p>

<p>There was also a second feature added on their request: a focusOutEmpty event for a focus out on an empty input field. So by testing Formagical in a live environment, two new features were added:</p>

<ul>
<li>element index</li>
<li>focusOutEmpty event</li>
</ul>

<h2 id=""connectingformagicaltogoogleanalytics"">Connecting Formagical to Google Analytics</h2>

<p>Setting up Formagical for tracking in Google Analytics is easy.  All you have to do is send the data to your tracker of choice, in our case: Google Analytics. Implementing Formagical takes three steps:</p>

<ul>
<li>1: Add Formagical.JS to your site. I recommend you to get the latest version from <a href=""https://github.com/codeorelse/formagical.JS"">the GitHub repository</a>. Keep in mind that Formagical requires jQuery. </li>
<li>2: Setup a custom tracker function to send Formagical.JS tracking to your analytics platform.</li>
<li>3: Apply the custom tracker to a form.</li>
</ul>

<p>Let’s look at a code example:</p>

<pre><code class=""language-prettyprint lang-html prettyprinted""><span class=""com"">&lt;!--include Formagical.JS library--&gt;</span><span class=""pln"">  
</span><span class=""tag"">&lt;script</span><span class=""pln""> </span><span class=""atn"">type</span><span class=""pun"">=</span><span class=""atv"">""text/javascript""</span><span class=""pln""> </span><span class=""atn"">src</span><span class=""pun"">=</span><span class=""atv"">""/formagical.js""</span><span class=""tag"">&gt;&lt;/script&gt;</span><span class=""pln"">

</span><span class=""tag"">&lt;script&gt;</span><span class=""pln"">  
  </span><span class=""com"">//if you're not able to upload a JS file on your site, you can also add the Formagical library code here.</span><span class=""pln"">

  </span><span class=""pun"">(</span><span class=""kwd"">function</span><span class=""pun"">(</span><span class=""pln"">formagical</span><span class=""pun"">)</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">
    </span><span class=""com"">//define custom tracker function</span><span class=""pln"">
    formagical</span><span class=""pun"">.</span><span class=""pln"">formagicalGATracker </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""kwd"">function</span><span class=""pun"">(</span><span class=""pln"">element</span><span class=""pun"">,</span><span class=""pln""> typeOfInteraction</span><span class=""pun"">,</span><span class=""pln""> duration</span><span class=""pun"">,</span><span class=""pln""> optional</span><span class=""pun"">,</span><span class=""pln""> elementIndex</span><span class=""pun"">)</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">
      </span><span class=""com"">//set input field duration to integer</span><span class=""pln"">
      </span><span class=""kwd"">var</span><span class=""pln""> timeAsFloat </span><span class=""pun"">=</span><span class=""pln""> parseInt</span><span class=""pun"">(</span><span class=""pln"">duration</span><span class=""pun"">);</span><span class=""pln"">
      </span><span class=""com"">//set input field duration to zero if not available</span><span class=""pln"">
      </span><span class=""kwd"">if</span><span class=""pun"">(</span><span class=""pln"">isNaN</span><span class=""pun"">(</span><span class=""pln"">timeAsFloat</span><span class=""pun"">))</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln""> timeAsFloat </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""lit"">0</span><span class=""pun"">;}</span><span class=""pln"">
      </span><span class=""com"">//set event field label to empty string if not available</span><span class=""pln"">
      </span><span class=""kwd"">if</span><span class=""pun"">(!(</span><span class=""kwd"">typeof</span><span class=""pln""> element </span><span class=""pun"">===</span><span class=""pln""> </span><span class=""str"">'string'</span><span class=""pun"">))</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">element </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""str"">''</span><span class=""pun"">;}</span><span class=""pln"">
      </span><span class=""com"">//set input field prefix</span><span class=""pln"">
      </span><span class=""kwd"">if</span><span class=""pun"">(</span><span class=""pln"">elementIndex </span><span class=""pun"">&lt;</span><span class=""pln""> </span><span class=""lit"">10</span><span class=""pun"">)</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln""> elementIndex </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""str"">'0'</span><span class=""pln""> </span><span class=""pun"">+</span><span class=""pln""> elementIndex</span><span class=""pun"">;</span><span class=""pln""> </span><span class=""pun"">}</span><span class=""pln"">
      </span><span class=""com"">//add input field prefix to name of input field (element)</span><span class=""pln"">
      </span><span class=""kwd"">var</span><span class=""pln""> elementNameWithIndexPrefixed </span><span class=""pun"">=</span><span class=""pln""> elementIndex </span><span class=""pun"">?</span><span class=""pln""> elementIndex </span><span class=""pun"">+</span><span class=""pln""> </span><span class=""str"">'_'</span><span class=""pln""> </span><span class=""pun"">+</span><span class=""pln""> element </span><span class=""pun"">:</span><span class=""pln""> element</span><span class=""pun"">;</span><span class=""pln"">
      </span><span class=""com"">//send the data to GA with an event. </span><span class=""pln"">
      ga</span><span class=""pun"">(</span><span class=""str"">'send'</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""str"">'Formagical'</span><span class=""pun"">,</span><span class=""pln""> typeOfInteraction</span><span class=""pun"">,</span><span class=""pln""> elementNameWithIndexPrefixed</span><span class=""pun"">,</span><span class=""pln""> timeAsFloat</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""str"">'nonInteraction'</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""lit"">1</span><span class=""pun"">});</span><span class=""pln"">
  </span><span class=""pun"">}})(</span><span class=""pln"">window</span><span class=""pun"">.</span><span class=""pln"">formagical </span><span class=""pun"">=</span><span class=""pln""> window</span><span class=""pun"">.</span><span class=""pln"">formagical </span><span class=""pun"">||</span><span class=""pln""> </span><span class=""pun"">{});</span><span class=""pln"">

  </span><span class=""com"">// Add formagical to form on page</span><span class=""pln"">
  $</span><span class=""pun"">(</span><span class=""str"">'form'</span><span class=""pun"">).</span><span class=""pln"">formagical</span><span class=""pun"">({</span><span class=""pln"">track</span><span class=""pun"">:</span><span class=""pln""> window</span><span class=""pun"">.</span><span class=""pln"">formagical</span><span class=""pun"">.</span><span class=""pln"">formagicalGATracker</span><span class=""pun"">});</span><span class=""pln"">
</span><span class=""tag"">&lt;/script&gt;</span><span class=""pln"">  </span></code></pre>

<p><em>Formagical.JS combined with Google’s Universal Analytics event tracker.</em></p>

<p>This example sends the data to Google Analytics in the following structure:</p>

<ul>
<li><strong>Event category:</strong> Formagical</li>
<li><strong>Event action:</strong> Formagical interaction events. Input field/form interactions. Users can either interact with an input field (e.g. focus on input field or focusOut from input field), or with the form in general (e.g. user started using form or form submits). </li>
<li><strong>Event label:</strong> the label of the input field.</li>
<li><strong>Event value:</strong> the time in milliseconds it took to trigger the event. This only applies to events that have a completion time. </li>
</ul>

<p>There are some important things happening here (if you’re not interested in technical details, skip these bullets):</p>

<ul>
<li><strong>Time:</strong> Not all events have a duration. For example, a focus on an input field starts the time counter for that input field. The event doesn’t have a timestamp itself. Because of these events, the time is set to 0 if it’s not available. This way, we can always send the event value with our tracker.</li>
<li><strong>Elements:</strong> not every event has an associated element, for example the first event: Formagical ready for stats. To make sure the GA tracker gets a string, we check if the element string is available, if not, set it to an empty string. </li>
<li><strong>Element label prefix:</strong> to add the index of the input field to the tracker, we prefix the element name with elementIndex. Note that for every index below 10 we prefix that number with a 0. That way it’s easier to sort in your reports (we’ll get to that later). </li>
<li><strong>Adding Formagical to a form:</strong> in this code example, Formagical is applied to a general form. If you have several forms on your page, make sure to specify which one you want to track. </li>
</ul>

<h2 id=""addingthecodetoyourwebpage"">Adding the code to your webpage</h2>

<p>To make life easy for you, here are two code snippets you can directly add to your page of choice. One for standard Google Analytics tracker and one for a GTM tracker.</p>

<ul>
<li><strong><a href=""http://codepen.io/edriessen/pen/XXLxNE.js"">Formagical with GA event</a></strong></li>
<li><strong><a href=""http://codepen.io/edriessen/pen/Rrzerm.js"">Formagical with GTM event</a></strong> </li>
</ul>

<p>Keep in mind that your GTM event setup may be different from the one used in this snippet. Make sure to change the event to match your setup, or add triggers and variables so the events from this example are captured and sent to GA in GTM. </p>

<p>If you have a different analytics tool running in your site, just change the line that sends the data to GA – <code>ga('send', …)</code> – or to GTM – <code>dataLayer.push(...)</code> – to the code that works with your tool.  </p>

<h2 id=""settingupgoogleanalyticsforeasyformanalysis"">Setting up Google Analytics for easy form analysis</h2>

<p>This is where the combination of Formagical and Google Analytics starts to shine. With  the tracker in place, you can find the events in your standard event reports. But with a little extra effort, you can create a (For)magical custom report. </p>

<h4 id=""1usegoogleanalyticsnewcalculatedmetricbeta"">1: Use Google Analytics’ new Calculated Metric beta</h4>

<p>Recently, Google added a new feature to their analytics tool: <a href=""https://support.google.com/analytics/answer/6121409?hl=en"">the calculated metrics beta</a>. This allows you to use calculate metrics based on all the metrics available. You can set up five per profile. (If you’re an active user of this feature already, I recommend you to set up a separate Formagical profile.) Either way, you’ll need to create two calculated metrics:</p>

<p><strong>1: Events per sessions</strong> </p>

<p>With this event, you can see how many times an event occurred per session (total events divided by unique events). Apply this to the Formagical events, and you’ll instantly see how many tries users needed for an input field on average. Set the formatting type to Float, so it shows decimals. </p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2015/Dec/geek_formagical_eventspersession-1450250076702.PNG"" alt=""GA calculated metric events per session""></p>

<p><strong>2: Time per event</strong> </p>

<p>This event shows you how long it took to fill out an input field on average. We use the event value (time in milliseconds), divide this by 1000 to get actual seconds, and then divide it by the total events to get the time in seconds per event. I've also set the format to float and not time (which is also possible). The reason for this is that we mostly deal with seconds here. And for 8,7 seconds, I'd rather see that figure of 8.7 than 00:00:09 (HH:MM:SS). </p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2015/Dec/geek_formagical_timeperevent-1450250886049.PNG"" alt=""GA calculated metric time per event""></p>

<h3 id=""createaformagicalcustomreport"">Create a Formagical custom report</h3>

<p>To make the Formagical data as easy accessible as possible, you’ll need to create a custom report. This report has the following setup:</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2015/Dec/geek_formagical_custmo_report-1450250873683.PNG"" alt=""GA Formagical Custom Report setup"" class=""full-img""></p>

<p>Some details on the setup:</p>

<ul>
<li><strong>Filter</strong>: Event Category exactly matches Formagical. With this filter we’ll only see Formagical events in the report.</li>
<li><strong>Dimension drilldown</strong>:
<ul><li>Event Action. These show you the main actions that Formagical tracks.</li>
<li>Event Label. A possible drilldown on input field related events suchs as focus, focusOut etc. </li></ul></li>
<li><strong>Metrics</strong>:
<ul><li>Total Events: to see how many times an input field event was triggered.</li>
<li>Unique Events: to see if an input field event was triggered in a session.</li>
<li>Events per session: to see how many times an input field event was triggerd per sessions on average.</li>
<li>Time per event: to see how long it took to complete an input field event.</li></ul></li>
</ul>

<h3 id=""thereport"">The report</h3>

<p>Let’s look at how the results may look for a form:</p>

<p><strong>Formagical event actions sample</strong></p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2015/Dec/geek_formagical_results_general-1450251242429.PNG"" alt=""GA Formagical Custom Report results"" class=""full-img""></p>

<p><em>The basic sample results of a form.</em></p>

<p>Looking at the event actions, there are some interesting things to look at:</p>

<ul>
<li><strong>focus, focusOut &amp; focusOutEmpty:</strong> looking at both total and unique events, you'll see there are almost as many focusOut as focus events, indicating that most input fields get filled out. Because of the focusOutEmpty update mentioned earlier in this post, we can tell that 433 of the 3084 focusOut events are empty. So users didn't type anything here. </li>
<li><strong>focusOut:</strong> The time per event metric shows you that an input field takes 13.89 seconds on average to complete. </li>
<li><strong>Form ready for stats &amp; Submitted:</strong> looking at unique events, you can see that 223 out of 320 forms get submitted. Though this number is pretty good, Formagical currently doesn't support validation of the form. So keep that in mind when reporting numbers.</li>
<li><strong>Form ready for stats &amp; User started using form:</strong> comparing these two events, you'll see how many that see the form actually start using it. Comparing the unique events tells you 269 out of 320 users start using the form. </li>
<li><strong>User started using form:</strong> looking at events per session, you instantly see that user start using the form 1.24 times on average. </li>
</ul>

<p><strong>Formagical event labels sample</strong></p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2015/Dec/geek_formagical_results_inputfield-1450253036750.jpg"" alt=""GA Formagical Custom Report results per infputfield"" class=""full-img""></p>

<p><em>Example results for a form asking for personal, address, email and bank account information.</em></p>

<p>The sample report shows the event labels for the focusOut events. If you want to change this, the easiest way to switch the event action is with the drop down beneath the report title:</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2015/Dec/geek_formagical_dropdown-1450253171377.PNG"" alt=""GA Formagical Custom Report event action dropdown""></p>

<p>In the event label report, each column has a purpose. I’ll go through them step by step:</p>

<ul>
<li><strong>Event Label:</strong> For event actions that are associated to  input fields, the event label will show you de details per input field. Sort this to get the order of the input fields.</li>
<li><strong>Total events:</strong> The number of times an event has been triggered in total. Sort to see what input fields users start with the most.</li>
<li><strong>Unique events:</strong> The unique number of times an event has been triggered (one per session). Sort to see what input fields are used the most uniquely (once per session). </li>
<li><strong>Events per session:</strong> The number of times an event occurs per sessions on average. Sort to see what input field takes the most tries to complete.</li>
<li><strong>Time per event:</strong> The amount of time passed to complete an event. Sort to see what in put field takes the longest to complete, and therefore might be the hardest one to fill out. </li>
</ul>

<p>Looking at the sample report, we see that the bank account field has the highest number of total events, the most events per session on average and by far the longest completion atime (almost 35 seconds). This might be the field you want to improve on. Keep in mind that I've only looked at the <strong>focusOut</strong> event in the example. For a thorough analysis you should also look at the other events:</p>

<ul>
<li><strong>focus:</strong> analyse how often users focus (click) on an input field.</li>
<li><strong>started typing:</strong>  analyse how long it takes for a user to start typing in the input field.</li>
<li><strong>pause and continued:</strong> analyse how many times users pause for 0.5 seconds or more while typing in an input field. </li>
<li><strong>focusOutEmpty:</strong> analyse how many times a user exits an input field without entering any data. </li>
</ul>

<p>If you have Formagical implemented on your website, and added the calculated metrics in your GA view, you can add the custom report mentioned above to that view with the following link: <a href=""https://www.google.com/analytics/web/template?uid=v4T5EZu7QAuwWTCLdmIuEQ"">https://www.google.com/analytics/web/template?uid=v4T5EZu7QAuwWTCLdmIuEQ</a>.</p>

<h2 id=""tryit"">Try it</h2>

<p>Now it’s time to for you to try Formagical yourself. It’s a free alternative to tools like Formisimo or Clicktale and it greatly increases your form insights. Use it to find the main bottlenecks of your form and improve on them. And when you have it implemented, don’t forget to tell us what you think. </p>

<p>Enjoy.</p>
        ","In April this year, Siebe wrote about his form analysis script Formagical.JS. I recommend you to read this post to get familiar with the basics of Formagical.
We’ve had the first version of the script running for one of our clients since April. During this period, Siebe and I updated the tool and implementation to make insights easier for the client. This post will discuss both the Formagical update and a best practice for Google Analytics reports based on Formagical.JS.
What’s new with Formagical
By default, Formagical tracked all the basics we thought we needed:
user opened a page that has Formagical.JS on it
user started using form by interacting with one of its elements
user enters a certain form element (focus)
user leaves a certain form element (unfocus)
user starts typing in a certain element (input and text area only)
users changes the selection of a certain form element (dropdown, radio button and checkbox only)
users pauses typing and continues
user submits form
As soon as we put it in the hands of the first client, it was clear that something was missing. Our client found it hard to see the order of input fields as they appear on the form. Good point, as it was impossible to see that at that moment. You ‘d see the input field labels, but no order. Luckily, Siebe added the feature in no time and after that I updated the library in our tag manager. So from that moment onwards, for each input field, we also have an index number available.
There was also a second feature added on their request: a focusOutEmpty event for a focus out on an empty input field. So by testing Formagical in a live environment, two new features were added:
element index
focusOutEmpty event
Connecting Formagical to Google Analytics
Setting up Formagical for tracking in Google Analytics is easy. All you have to do is send the data to your tracker of choice, in our case: Google Analytics. Implementing Formagical takes three steps:
1: Add Formagical.JS to your site. I recommend you to get the latest version from the GitHub repository. Keep in mind that Formagical requires jQuery.
2: Setup a custom tracker function to send Formagical.JS tracking to your analytics platform.
3: Apply the custom tracker to a form.
Let’s look at a code example:
<!--include Formagical.JS library-->  
<script type=""text/javascript"" src=""/formagical.js""></script>

<script>  
  //if you're not able to upload a JS file on your site, you can also add the Formagical library code here.

  (function(formagical) {
    //define custom tracker function
    formagical.formagicalGATracker = function(element, typeOfInteraction, duration, optional, elementIndex) {
      //set input field duration to integer
      var timeAsFloat = parseInt(duration);
      //set input field duration to zero if not available
      if(isNaN(timeAsFloat)) { timeAsFloat = 0;}
      //set event field label to empty string if not available
      if(!(typeof element === 'string')) {element = '';}
      //set input field prefix
      if(elementIndex < 10) { elementIndex = '0' + elementIndex; }
      //add input field prefix to name of input field (element)
      var elementNameWithIndexPrefixed = elementIndex ? elementIndex + '_' + element : element;
      //send the data to GA with an event. 
      ga('send', 'Formagical', typeOfInteraction, elementNameWithIndexPrefixed, timeAsFloat, {'nonInteraction': 1});
  }})(window.formagical = window.formagical || {});

  // Add formagical to form on page
  $('form').formagical({track: window.formagical.formagicalGATracker});
</script>  
Formagical.JS combined with Google’s Universal Analytics event tracker.
This example sends the data to Google Analytics in the following structure:
Event category: Formagical
Event action: Formagical interaction events. Input field/form interactions. Users can either interact with an input field (e.g. focus on input field or focusOut from input field), or with the form in general (e.g. user started using form or form submits).
Event label: the label of the input field.
Event value: the time in milliseconds it took to trigger the event. This only applies to events that have a completion time.
There are some important things happening here (if you’re not interested in technical details, skip these bullets):
Time: Not all events have a duration. For example, a focus on an input field starts the time counter for that input field. The event doesn’t have a timestamp itself. Because of these events, the time is set to 0 if it’s not available. This way, we can always send the event value with our tracker.
Elements: not every event has an associated element, for example the first event: Formagical ready for stats. To make sure the GA tracker gets a string, we check if the element string is available, if not, set it to an empty string.
Element label prefix: to add the index of the input field to the tracker, we prefix the element name with elementIndex. Note that for every index below 10 we prefix that number with a 0. That way it’s easier to sort in your reports (we’ll get to that later).
Adding Formagical to a form: in this code example, Formagical is applied to a general form. If you have several forms on your page, make sure to specify which one you want to track.
Adding the code to your webpage
To make life easy for you, here are two code snippets you can directly add to your page of choice. One for standard Google Analytics tracker and one for a GTM tracker.
Formagical with GA event
Formagical with GTM event
Keep in mind that your GTM event setup may be different from the one used in this snippet. Make sure to change the event to match your setup, or add triggers and variables so the events from this example are captured and sent to GA in GTM.
If you have a different analytics tool running in your site, just change the line that sends the data to GA – ga('send', …) – or to GTM – dataLayer.push(...) – to the code that works with your tool.
Setting up Google Analytics for easy form analysis
This is where the combination of Formagical and Google Analytics starts to shine. With the tracker in place, you can find the events in your standard event reports. But with a little extra effort, you can create a (For)magical custom report.
1: Use Google Analytics’ new Calculated Metric beta
Recently, Google added a new feature to their analytics tool: the calculated metrics beta. This allows you to use calculate metrics based on all the metrics available. You can set up five per profile. (If you’re an active user of this feature already, I recommend you to set up a separate Formagical profile.) Either way, you’ll need to create two calculated metrics:
1: Events per sessions
With this event, you can see how many times an event occurred per session (total events divided by unique events). Apply this to the Formagical events, and you’ll instantly see how many tries users needed for an input field on average. Set the formatting type to Float, so it shows decimals.
2: Time per event
This event shows you how long it took to fill out an input field on average. We use the event value (time in milliseconds), divide this by 1000 to get actual seconds, and then divide it by the total events to get the time in seconds per event. I've also set the format to float and not time (which is also possible). The reason for this is that we mostly deal with seconds here. And for 8,7 seconds, I'd rather see that figure of 8.7 than 00:00:09 (HH:MM:SS).
Create a Formagical custom report
To make the Formagical data as easy accessible as possible, you’ll need to create a custom report. This report has the following setup:
Some details on the setup:
Filter: Event Category exactly matches Formagical. With this filter we’ll only see Formagical events in the report.
Dimension drilldown:
Event Action. These show you the main actions that Formagical tracks.
Event Label. A possible drilldown on input field related events suchs as focus, focusOut etc.
Metrics:
Total Events: to see how many times an input field event was triggered.
Unique Events: to see if an input field event was triggered in a session.
Events per session: to see how many times an input field event was triggerd per sessions on average.
Time per event: to see how long it took to complete an input field event.
The report
Let’s look at how the results may look for a form:
Formagical event actions sample
The basic sample results of a form.
Looking at the event actions, there are some interesting things to look at:
focus, focusOut & focusOutEmpty: looking at both total and unique events, you'll see there are almost as many focusOut as focus events, indicating that most input fields get filled out. Because of the focusOutEmpty update mentioned earlier in this post, we can tell that 433 of the 3084 focusOut events are empty. So users didn't type anything here.
focusOut: The time per event metric shows you that an input field takes 13.89 seconds on average to complete.
Form ready for stats & Submitted: looking at unique events, you can see that 223 out of 320 forms get submitted. Though this number is pretty good, Formagical currently doesn't support validation of the form. So keep that in mind when reporting numbers.
Form ready for stats & User started using form: comparing these two events, you'll see how many that see the form actually start using it. Comparing the unique events tells you 269 out of 320 users start using the form.
User started using form: looking at events per session, you instantly see that user start using the form 1.24 times on average.
Formagical event labels sample
Example results for a form asking for personal, address, email and bank account information.
The sample report shows the event labels for the focusOut events. If you want to change this, the easiest way to switch the event action is with the drop down beneath the report title:
In the event label report, each column has a purpose. I’ll go through them step by step:
Event Label: For event actions that are associated to input fields, the event label will show you de details per input field. Sort this to get the order of the input fields.
Total events: The number of times an event has been triggered in total. Sort to see what input fields users start with the most.
Unique events: The unique number of times an event has been triggered (one per session). Sort to see what input fields are used the most uniquely (once per session).
Events per session: The number of times an event occurs per sessions on average. Sort to see what input field takes the most tries to complete.
Time per event: The amount of time passed to complete an event. Sort to see what in put field takes the longest to complete, and therefore might be the hardest one to fill out.
Looking at the sample report, we see that the bank account field has the highest number of total events, the most events per session on average and by far the longest completion atime (almost 35 seconds). This might be the field you want to improve on. Keep in mind that I've only looked at the focusOut event in the example. For a thorough analysis you should also look at the other events:
focus: analyse how often users focus (click) on an input field.
started typing: analyse how long it takes for a user to start typing in the input field.
pause and continued: analyse how many times users pause for 0.5 seconds or more while typing in an input field.
focusOutEmpty: analyse how many times a user exits an input field without entering any data.
If you have Formagical implemented on your website, and added the calculated metrics in your GA view, you can add the custom report mentioned above to that view with the following link: https://www.google.com/analytics/web/template?uid=v4T5EZu7QAuwWTCLdmIuEQ.
Try it
Now it’s time to for you to try Formagical yourself. It’s a free alternative to tools like Formisimo or Clicktale and it greatly increases your form insights. Use it to find the main bottlenecks of your form and improve on them. And when you have it implemented, don’t forget to tell us what you think.
Enjoy.","[Analytics, google analytics, tag management, google, forms, form abandonment]"
72,Smart clothing makes it feel real,/smart-clothing/,"
            <p><p style=""color:#7e4396; font-weight:bold"">This post concerns Greenhouse Group Labs, an innovation program for students, established by Greenhouse Group. Labs is an ideal opportunity to test the latest technologies available, while allowing for talented young individuals to deeply explore them and come up with groundbreaking solutions.</p>This post is about the Smart Clothing Project of <a href=""http://www.greenhousegroup.com/labs"">Greenhouse Group Labs</a>. Labs is an ideal opportunity for Greenhouse Group to test technology before it has fully matured. The Lab is run by students from tech universities.</p>

<h2 id=""whatissmartclothing"">What is smart clothing?</h2>

<p>Smart clothing is clothing with electronics embedded in it. This can be to transform the clothes, communicate to other hardware or even generate energy. One of the first companies to start with smart clothing was Cutecircuit, launched in 2004. They make dresses with electronics embedded, worn by Katy Perry, Nicole Scherzinger &amp; Irina Shayk. Some big companies are starting with smart clothing. Google &amp; Levis are working on Project Jacquard, which is expected to launch in 2016.</p>

<h3 id=""smartclothingtoenhancetheusersexperience"">Smart clothing to enhance the users experience</h3>

<p>Smart clothing can enhance the users experience while watching movies or playing games. The entertainment business already uses technologies te enhance the customers experience. Examples are IMAX 3D or the newer Dolby cinema used in, you guessed it, cinemas and the Oculus Rift for virtual reality experiences. Also augmented reality is coming.</p>

<p>We're developing a shirt which gives the wearer physical feedback, recreating the characters surrounding. By feeling temperature and using haptics to simulate touch and movement the shirt adds an extra dimension to the wearers experience.</p>

<h2 id=""prototypes"">Prototypes</h2>

<h3 id=""smartsleeveconnectedtoavideoplayer"">Smart sleeve connected to a video player</h3>

<p>We started developing prototypes with arduinos. We experimented with vibrating pads and tried to mimic touch. After some experimenting we developed a sleeve with vibrating pads integrated. We took a video of someone getting a tattoo with some slow motion footage in it. Because of the slow motion footage the needle had different speeds. We wanted to sync our vibrations in the pad with the needle impact. To achieve this we made a file which contained:</p>

<ul>
<li>Start time, the time when the needle hits the skin for the first time.</li>
<li>End time, the time when the needle leaves the skin.</li>
<li>Intensity, the intensity of the needle impact.</li>
<li>Speed, the variations in speed.</li>
</ul>

<p>Put in a txt file it looks like this:  </p>

<pre><code>[0000][0050][0][0]
[0050][3337][4][4]
[3337][8108][3][3]
[8108][13273][0][0]
[13273][13719][4][4]
[13719][14500][0][0]
[14500][15420][4][4]
[15420][19000][0][0]
[18000][31600][4][4]
[31600][31893][0][0]
</code></pre>

<p>We created our own video player. When playing a video it firstly opens the additional file and stores the data. After this the video starts playing, while the video is playing the current time is compared with the start times received from the file. If the times match the player sends the corresponding data to the sleeve. <br>
<img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2015/Dec/2-1449656659087.png"" alt="""" class=""full-img""></p>

<h3 id=""replicatingtouchwiththeuseofvibrationmotors"">Replicating touch with the use of vibration motors</h3>

<p>Vibration has been used as a method for communicating through touch all the way back from 1996. The first use of vibration was to replace the standard ringtone alert. Because of the great success of this implementation we still see this vibration in the smartphones of today. </p>

<p>But the mobile industry was not the only place communicating through vibration succeeded. These days you will see this kind of communication in gaming controllers and personal computers. Sadly these interactions with vibration are still really basic while the possibilities are broad.</p>

<p>Small vibrations in different speeds and intervals could communicate different messages and vibrations that fade in and out in different locations could give the feeling of something moving from place to place. <br>
Combining this with the world of virtual reality, where there has been lots of progress in replicating the visual experience this would give new possibilities to also incorporate a physical experience.</p>

<h3 id=""touchpatternswithvibrations"">Touch patterns with vibrations</h3>

<p>We experimented with different speeds, intensities, intervals and positions of vibrations to see if we could replicate basic types of touches like tapping, pressing and stroking. Tapping was simply accomplished by short vibrations with low intensity. Pressing was accomplished with short vibrations that would start with a low intensity and quickly increase until it reached its highest point.</p>

<pre><code>if(analogValue &lt; maxValue)  
      analogValue+=incrementValue;
</code></pre>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2015/Dec/VibratingPads-1449828780321.jpg"" alt="""" class=""full-img""></p>

<p>Replicating the feeling of stroking was a little more tricky. This was accomplished by using multiple vibration motors. The first motor would start with a low intensity and gradually increase like the pressing pattern. After the first motor reaches its maximum intensity it will gradually decrease again. While the first motor is decreasing the second motor will spring into action and gradually increase at the same rate. While we were only experimenting with these pattern, it felt surprisingly similar.</p>

<pre><code>for(int i = 0; i &lt;= 201; i++) {  
  if(directionRoll != 0) {
    analogWrite(pins[chosenPin],directionRoll-i);
  } else {
    analogWrite(pins[chosenPin],i);
  }

  if(secondStart) {
    analogWrite(pins[chosenPin+1],i);
  }

  if(i &gt;= 200) {
    if(directionRoll != 200) {
      directionRoll = 200;
      secondStart = true;
    }
    else {
      directionRoll = 0; 
      secondStart = false;
    }
    i=0;
  }
  delay(30);  
}
</code></pre>

<h3 id=""receivinginformationfromsmartclothing"">Receiving information from smart clothing</h3>

<p>Vibration motors can be used to give information to the user, but sensors in smart clothing can be used to receive information. Pulse sensors can measure a user’s heart rate for instance. Excitement and arousal are often measured by looking at someone’s heart rate, so this can be very interesting information to receive. You can define which elements in a game or movie excite you. Recommendations for movies won’t be based on your watch history and the genres you like anymore. Instead it will be based on your feelings towards certain elements of certain scenes. </p>

<p>Also, imagine a review for a horror game, based on the user’s heart rate data. It can’t get more honest than seeing how many times the user was scared by elements in the game. Sharing and comparing your data with the rest of the world can give everybody a lot of new insights. </p>

<h2 id=""futureplans"">Future plans</h2>

<h3 id=""smartshirtreplicatingthephysicalexperienceofagame"">Smart shirt replicating the physical experience of a game</h3>

<p>We're currently developing a shirt similar to the sleeve. This will be connected to a demo game which we're developing as well. This shirt will have multiple vibrating pads, heating pads and a breathing sensor integrated.</p>

<p>The character in the demo game has body parts similar to a human. Based on impact on a specific bodypart the shirt will know which vibration motor needs to be triggered. For example: You get shot on the right side of your in-game characters chest, this will trigger the vibration motor on the upper right side of the shirt. The video below shows more information.</p>

<div class=""fluid-width-video-wrapper"" style=""padding-top: 56.25%;""><iframe src=""https://www.youtube.com/embed/KgGI-YrJUlc?vq=hd1080"" frameborder=""0"" allowfullscreen="""" id=""fitvid831192""></iframe></div>

<p>After finishing our prototype we want to get some feedback and reactions from people inside the Greenhouse Group to further improve it and to showcase what we are working on.</p>

<p>We also had the idea to use the Oculus Rift for our later prototypes to give a more complete experience.</p>
        ","This post concerns Greenhouse Group Labs, an innovation program for students, established by Greenhouse Group. Labs is an ideal opportunity to test the latest technologies available, while allowing for talented young individuals to deeply explore them and come up with groundbreaking solutions.
This post is about the Smart Clothing Project of Greenhouse Group Labs. Labs is an ideal opportunity for Greenhouse Group to test technology before it has fully matured. The Lab is run by students from tech universities.
What is smart clothing?
Smart clothing is clothing with electronics embedded in it. This can be to transform the clothes, communicate to other hardware or even generate energy. One of the first companies to start with smart clothing was Cutecircuit, launched in 2004. They make dresses with electronics embedded, worn by Katy Perry, Nicole Scherzinger & Irina Shayk. Some big companies are starting with smart clothing. Google & Levis are working on Project Jacquard, which is expected to launch in 2016.
Smart clothing to enhance the users experience
Smart clothing can enhance the users experience while watching movies or playing games. The entertainment business already uses technologies te enhance the customers experience. Examples are IMAX 3D or the newer Dolby cinema used in, you guessed it, cinemas and the Oculus Rift for virtual reality experiences. Also augmented reality is coming.
We're developing a shirt which gives the wearer physical feedback, recreating the characters surrounding. By feeling temperature and using haptics to simulate touch and movement the shirt adds an extra dimension to the wearers experience.
Prototypes
Smart sleeve connected to a video player
We started developing prototypes with arduinos. We experimented with vibrating pads and tried to mimic touch. After some experimenting we developed a sleeve with vibrating pads integrated. We took a video of someone getting a tattoo with some slow motion footage in it. Because of the slow motion footage the needle had different speeds. We wanted to sync our vibrations in the pad with the needle impact. To achieve this we made a file which contained:
Start time, the time when the needle hits the skin for the first time.
End time, the time when the needle leaves the skin.
Intensity, the intensity of the needle impact.
Speed, the variations in speed.
Put in a txt file it looks like this:
[0000][0050][0][0]
[0050][3337][4][4]
[3337][8108][3][3]
[8108][13273][0][0]
[13273][13719][4][4]
[13719][14500][0][0]
[14500][15420][4][4]
[15420][19000][0][0]
[18000][31600][4][4]
[31600][31893][0][0]
We created our own video player. When playing a video it firstly opens the additional file and stores the data. After this the video starts playing, while the video is playing the current time is compared with the start times received from the file. If the times match the player sends the corresponding data to the sleeve.
Replicating touch with the use of vibration motors
Vibration has been used as a method for communicating through touch all the way back from 1996. The first use of vibration was to replace the standard ringtone alert. Because of the great success of this implementation we still see this vibration in the smartphones of today.
But the mobile industry was not the only place communicating through vibration succeeded. These days you will see this kind of communication in gaming controllers and personal computers. Sadly these interactions with vibration are still really basic while the possibilities are broad.
Small vibrations in different speeds and intervals could communicate different messages and vibrations that fade in and out in different locations could give the feeling of something moving from place to place.
Combining this with the world of virtual reality, where there has been lots of progress in replicating the visual experience this would give new possibilities to also incorporate a physical experience.
Touch patterns with vibrations
We experimented with different speeds, intensities, intervals and positions of vibrations to see if we could replicate basic types of touches like tapping, pressing and stroking. Tapping was simply accomplished by short vibrations with low intensity. Pressing was accomplished with short vibrations that would start with a low intensity and quickly increase until it reached its highest point.
if(analogValue < maxValue)  
      analogValue+=incrementValue;
Replicating the feeling of stroking was a little more tricky. This was accomplished by using multiple vibration motors. The first motor would start with a low intensity and gradually increase like the pressing pattern. After the first motor reaches its maximum intensity it will gradually decrease again. While the first motor is decreasing the second motor will spring into action and gradually increase at the same rate. While we were only experimenting with these pattern, it felt surprisingly similar.
for(int i = 0; i <= 201; i++) {  
  if(directionRoll != 0) {
    analogWrite(pins[chosenPin],directionRoll-i);
  } else {
    analogWrite(pins[chosenPin],i);
  }

  if(secondStart) {
    analogWrite(pins[chosenPin+1],i);
  }

  if(i >= 200) {
    if(directionRoll != 200) {
      directionRoll = 200;
      secondStart = true;
    }
    else {
      directionRoll = 0; 
      secondStart = false;
    }
    i=0;
  }
  delay(30);  
}
Receiving information from smart clothing
Vibration motors can be used to give information to the user, but sensors in smart clothing can be used to receive information. Pulse sensors can measure a user’s heart rate for instance. Excitement and arousal are often measured by looking at someone’s heart rate, so this can be very interesting information to receive. You can define which elements in a game or movie excite you. Recommendations for movies won’t be based on your watch history and the genres you like anymore. Instead it will be based on your feelings towards certain elements of certain scenes.
Also, imagine a review for a horror game, based on the user’s heart rate data. It can’t get more honest than seeing how many times the user was scared by elements in the game. Sharing and comparing your data with the rest of the world can give everybody a lot of new insights.
Future plans
Smart shirt replicating the physical experience of a game
We're currently developing a shirt similar to the sleeve. This will be connected to a demo game which we're developing as well. This shirt will have multiple vibrating pads, heating pads and a breathing sensor integrated.
The character in the demo game has body parts similar to a human. Based on impact on a specific bodypart the shirt will know which vibration motor needs to be triggered. For example: You get shot on the right side of your in-game characters chest, this will trigger the vibration motor on the upper right side of the shirt. The video below shows more information.
After finishing our prototype we want to get some feedback and reactions from people inside the Greenhouse Group to further improve it and to showcase what we are working on.
We also had the idea to use the Oculus Rift for our later prototypes to give a more complete experience.","[Code, Labs, Smart clothing]"
73,Five insights  on display campaigns from Facebook’s Atlas,/five-insights-on-display-campaigns-in-facebooks-atlas/,"
            <p>When Facebook acquired adserver Atlas from Microsoft in 2013, I was a bit surprised. Now I see it was a brilliant move. By connecting to Facebook's userbase, Atlas gives a lot of interesting insights in display campaigns. Although Facebook had to change quite a lot to accomplish this result. Here are the most interesting insights from Facebook Atlas:</p>

<h3 id=""1uniquereachandfrequency"">1. Unique reach and frequency</h3>

<p>Of course: unique reach is not a new metric but until now, this metric was based on cookies. The downside of this approach is that you do not know the number of people you reach, but only the amount of unique devices or browsers. As we see the amount of devices per person grow, this metric is getting less and less reliable. </p>

<p>I measured campaigns where the unique reach was higher than the entire Dutch population. Of course, you can correct these results by using a benchmark of the average device per person, but your report will still be based on an assumption. </p>

<p>Thanks to Atlas, we can now really measure the frequency and reach. Atlas offers 'people based marketing', as they call it. Contrary to the more conventional systems, Atlas doesn't use a Cookie ID for identification, but it connects the same Facebook profile (across multiple cookies). </p>

<p>Think about yourself for a moment. It's likely that you use a laptop, tablet and a phone. All three for different tasks during different parts of the day. Chances are that you are logged into Facebook on all three of these devices. Atlas can tell that the interactions on these devices stem from one person.</p>

<p>This approach isn't 100% accurate either. Multiple people per household can use the same PC or tablet. That's something even Atlas can't see, but their approach gets a lot closer to reality than measuring with cookies. </p>

<p>While it's pretty cool to see the unique reach in Atlas, the interesting part is acting on it. Using Atlas, you can decide on what level you'd like to measure. When an overview of the total reach of your campaign is sufficient, you can use a single tag for your entire campaign. </p>

<p>However, when you'd like more detailed insights, you can add a tag for every website or creative in your media plan. This takes some more time to set up, but the insights are worth the extra effort. By tagging your campaign on a detailed level like this, you get insights in the total reach, but also in the reach per website and the overlap between websites. </p>

<p>Especially the last insight is really interesting for optimisation of your future media efforts. When running an awareness campaign, you can use this insight to eliminate overlap as much as possible, and pick sites that actually add reach. In the figure below, you can see the overlap of three similar sites. It's obvious that the combination of Site 2 and 3 is optimal for a unique reach. Site 1 has a bigger overlap with both other sites, and therefore adds less reach.</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2015/Dec/OverlapReach-1449603311866.PNG"" alt=""Reach overlap"">
<em>Reach overlap</em></p>

<p>Unique reach is closely related to frequency, the average number of interactions a user had with your campaign. Frequency capping is mostly cookie-based within most ad servers. When you look at real people with Atlas, you might notice that one user sees your ad more often then you've intended. When you take this into account, you can prevent annoyance by overexposure, or even reduce waste of campaign budgets. </p>

<h3 id=""2demographics"">2. Demographics</h3>

<p>Not only does Atlas provide insights into how many people you reach and how often you reach them, it also gives you more detailed information on your audience. For instance, you can sort their data on sex and age category. Of course, there are other tools that can provide you with these insights. But the difference is that Atlas is based on real-time Facebook profiles, instead of panel data.  </p>

<p>In a way, Atlas turned Facebook into the world's largest panel. In the Netherlands, there are 9.4 million people with a Facebook profile. This covers a large segment of the internet population, so Atlas can extrapolate this data. </p>

<p>You can get these demographic insights on a campaign level, but also on a website level. This is particularly interesting for brands that focus on a very specific audience. If you want to target young males in the ages between 18 and 35, Atlas will tell you where to find them. This means you no longer have to take your publisher's word for it: you can measure it yourself!</p>

<p>In the image below, you can see the reach and frequency we measured in Atlas after a placement on a large Dutch news platform, sorted by sex and age. As you can see, we reach more women than men (53 versus 47 percent), and 46 percent of the audience is between 35 and 54 years old. </p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2015/Dec/Demografie-1449608328852.PNG"" alt=""Demographics"" class=""full-img""></p>

<h3 id=""3demographicsconversion"">3. Demographics &amp; conversion</h3>

<p>Enough about reach. In the end, all that matters to many e-commerce businesses is money. It's great to see who you're reaching, but who is actually buying your products? Is the target audience really the most interesting group for your product? Atlas gives you insight into the relative performance of a campaign, sorted by age and sex. </p>

<p>This leads to interesting insights. For instance, if the young males you were targeting aren't buying your product, but their older counterparts are, you can adapt your campaign accordingly. You might want to change your targeting, or even your tone of voice and campaign material. </p>

<h3 id=""4thevalueofmobileinapath"">4. The value of mobile in a path</h3>

<p>The past few years, mobile grew from second screen to primary device. This gave mobile an important position in the media mix. While the percentage of mobile purchases continues to grow, marketeers continue to struggle to acknowledge the value of mobile media. </p>

<p>More often than not, there will be a mobile touch point in the customer journey. But if conversion happens on a different device, marketeers don't value the mobile touch point enough. The danger here is that mobile doesn't get the media budgets it deserves, based on its actual contribution. </p>

<p>Luckily, Atlas can help. Atlas looks at people instead of cookies. So you can see who checked your ad on a mobile device before converting on a desktop or laptop. By measuring like this, you'll see a rise in the value of your mobile campaign.</p>

<h3 id=""5theconnectionwithofflinesales"">5. The connection with offline sales</h3>

<p>The most interesting insight Atlas can give you, is the connection between online media efforts and offline sales. Unfortunately, this also leads to the biggest hurdles. </p>

<p>Online, Atlas uses Facebook profiles as a unique identifier. Those profile are not available offline, obviously. Though, a match can be made using a workaround, where every offline customer is asked for their email address (<a href=""http://www.themarketingtechnologist.co/p/bb9a82bf-774e-411f-9776-59dfe693cf9e/"">or one of the other available variables</a>). This email address will eventually be used to match a offline visitor to a Facebook profile.</p>

<p>Of course, not every customer will leave their email address at a store, and not every email is matched to a Facebook profile. But some of them are. And for this segment, Atlas can show which customer got in contact with your online campaign before coming to the store. </p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2015/Dec/koppelingAtlas_OfflineSales-1449608310186.PNG"" alt=""Connection between Atlas and offline sales"" class=""full-img"">
<em>Schematic representation of the connection between Atlas and offline sales</em></p>

<h3 id=""frominsightstotargeting"">From insights to targeting</h3>

<p>For now, Atlas is mainly interesting because of its insights but Facebook promised that in the future, Atlas will be able to help with specific targeting, based on Facebook profiles. Imagine what this could do to your display campaigns! Waste will be greatly reduced if you're able to spend your budget on relevant impressions. </p>

<p>To target women, you don't have stick to websites that are mainly for women. Instead, you can buy 'female impressions' on large news websites or other generic domains. This will easily lead to a 50% waste reduction. A giant leap in efficiency, that'll greatly improve your campaigns. The only downside is that we'll have to wait for it.</p>
        ","When Facebook acquired adserver Atlas from Microsoft in 2013, I was a bit surprised. Now I see it was a brilliant move. By connecting to Facebook's userbase, Atlas gives a lot of interesting insights in display campaigns. Although Facebook had to change quite a lot to accomplish this result. Here are the most interesting insights from Facebook Atlas:
1. Unique reach and frequency
Of course: unique reach is not a new metric but until now, this metric was based on cookies. The downside of this approach is that you do not know the number of people you reach, but only the amount of unique devices or browsers. As we see the amount of devices per person grow, this metric is getting less and less reliable.
I measured campaigns where the unique reach was higher than the entire Dutch population. Of course, you can correct these results by using a benchmark of the average device per person, but your report will still be based on an assumption.
Thanks to Atlas, we can now really measure the frequency and reach. Atlas offers 'people based marketing', as they call it. Contrary to the more conventional systems, Atlas doesn't use a Cookie ID for identification, but it connects the same Facebook profile (across multiple cookies).
Think about yourself for a moment. It's likely that you use a laptop, tablet and a phone. All three for different tasks during different parts of the day. Chances are that you are logged into Facebook on all three of these devices. Atlas can tell that the interactions on these devices stem from one person.
This approach isn't 100% accurate either. Multiple people per household can use the same PC or tablet. That's something even Atlas can't see, but their approach gets a lot closer to reality than measuring with cookies.
While it's pretty cool to see the unique reach in Atlas, the interesting part is acting on it. Using Atlas, you can decide on what level you'd like to measure. When an overview of the total reach of your campaign is sufficient, you can use a single tag for your entire campaign.
However, when you'd like more detailed insights, you can add a tag for every website or creative in your media plan. This takes some more time to set up, but the insights are worth the extra effort. By tagging your campaign on a detailed level like this, you get insights in the total reach, but also in the reach per website and the overlap between websites.
Especially the last insight is really interesting for optimisation of your future media efforts. When running an awareness campaign, you can use this insight to eliminate overlap as much as possible, and pick sites that actually add reach. In the figure below, you can see the overlap of three similar sites. It's obvious that the combination of Site 2 and 3 is optimal for a unique reach. Site 1 has a bigger overlap with both other sites, and therefore adds less reach.
Reach overlap
Unique reach is closely related to frequency, the average number of interactions a user had with your campaign. Frequency capping is mostly cookie-based within most ad servers. When you look at real people with Atlas, you might notice that one user sees your ad more often then you've intended. When you take this into account, you can prevent annoyance by overexposure, or even reduce waste of campaign budgets.
2. Demographics
Not only does Atlas provide insights into how many people you reach and how often you reach them, it also gives you more detailed information on your audience. For instance, you can sort their data on sex and age category. Of course, there are other tools that can provide you with these insights. But the difference is that Atlas is based on real-time Facebook profiles, instead of panel data.
In a way, Atlas turned Facebook into the world's largest panel. In the Netherlands, there are 9.4 million people with a Facebook profile. This covers a large segment of the internet population, so Atlas can extrapolate this data.
You can get these demographic insights on a campaign level, but also on a website level. This is particularly interesting for brands that focus on a very specific audience. If you want to target young males in the ages between 18 and 35, Atlas will tell you where to find them. This means you no longer have to take your publisher's word for it: you can measure it yourself!
In the image below, you can see the reach and frequency we measured in Atlas after a placement on a large Dutch news platform, sorted by sex and age. As you can see, we reach more women than men (53 versus 47 percent), and 46 percent of the audience is between 35 and 54 years old.
3. Demographics & conversion
Enough about reach. In the end, all that matters to many e-commerce businesses is money. It's great to see who you're reaching, but who is actually buying your products? Is the target audience really the most interesting group for your product? Atlas gives you insight into the relative performance of a campaign, sorted by age and sex.
This leads to interesting insights. For instance, if the young males you were targeting aren't buying your product, but their older counterparts are, you can adapt your campaign accordingly. You might want to change your targeting, or even your tone of voice and campaign material.
4. The value of mobile in a path
The past few years, mobile grew from second screen to primary device. This gave mobile an important position in the media mix. While the percentage of mobile purchases continues to grow, marketeers continue to struggle to acknowledge the value of mobile media.
More often than not, there will be a mobile touch point in the customer journey. But if conversion happens on a different device, marketeers don't value the mobile touch point enough. The danger here is that mobile doesn't get the media budgets it deserves, based on its actual contribution.
Luckily, Atlas can help. Atlas looks at people instead of cookies. So you can see who checked your ad on a mobile device before converting on a desktop or laptop. By measuring like this, you'll see a rise in the value of your mobile campaign.
5. The connection with offline sales
The most interesting insight Atlas can give you, is the connection between online media efforts and offline sales. Unfortunately, this also leads to the biggest hurdles.
Online, Atlas uses Facebook profiles as a unique identifier. Those profile are not available offline, obviously. Though, a match can be made using a workaround, where every offline customer is asked for their email address (or one of the other available variables). This email address will eventually be used to match a offline visitor to a Facebook profile.
Of course, not every customer will leave their email address at a store, and not every email is matched to a Facebook profile. But some of them are. And for this segment, Atlas can show which customer got in contact with your online campaign before coming to the store.
Schematic representation of the connection between Atlas and offline sales
From insights to targeting
For now, Atlas is mainly interesting because of its insights but Facebook promised that in the future, Atlas will be able to help with specific targeting, based on Facebook profiles. Imagine what this could do to your display campaigns! Waste will be greatly reduced if you're able to spend your budget on relevant impressions.
To target women, you don't have stick to websites that are mainly for women. Instead, you can buy 'female impressions' on large news websites or other generic domains. This will easily lead to a 50% waste reduction. A giant leap in efficiency, that'll greatly improve your campaigns. The only downside is that we'll have to wait for it.","[Analytics, Facebook, Online Advertising]"
74,The CEDDL: a data layer standard that never was?,/the-ceddl-a-data-layer-standard-that-never-was/,"
            <p>About a month ago, my friend and colleague Siebe introduced me to a document called <a href=""http://www.w3.org/2013/12/ceddl-201312.pdf"">the Customer Experience Digital Data Layer standard</a>, or CEDDL for short. It’s a standard for data layers that describes how onsite data can be structured in a JavaScript object. It was created at the end of 2013 as a uniform way to define a website’s data layer. </p>

<h2 id=""theceddlstructure"">The CEDDL structure</h2>

<p>The CEDDL standard is a not an official standard. This means that you don’t need to use it, but could use it as a guide. Here’s a data layer sample for page variables based on this standard:</p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""pln"">digitalData </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">  
 pageInstanceID</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""str"">""MyHomePage-Production""</span><span class=""pun"">,</span><span class=""pln"">
 page</span><span class=""pun"">:{</span><span class=""pln"">
   pageInfo</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">
     pageID</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""str"">""Home Page""</span><span class=""pun"">,</span><span class=""pln"">
     destinationURL</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""str"">""http://mysite.com/index.html""</span><span class=""pun"">},</span><span class=""pln"">
   category</span><span class=""pun"">:{</span><span class=""pln"">
     primaryCategory</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""str"">""FAQ Pages""</span><span class=""pun"">,</span><span class=""pln"">
    subCategory1</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""str"">""ProductInfo""</span><span class=""pun"">,</span><span class=""pln"">
    pageType</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""str"">""FAQ""</span><span class=""pun"">},</span><span class=""pln"">
  attributes</span><span class=""pun"">:{</span><span class=""pln"">
    country</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""str"">""US""</span><span class=""pun"">,</span><span class=""pln"">
    language</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""str"">""en-US""</span><span class=""pun"">}</span><span class=""pln"">
  </span><span class=""pun"">}</span><span class=""pln"">
</span><span class=""pun"">};</span></code></pre>

<p>As you can see, it’s basically one JavaScript object with nested variables. The same structure is used for other information such as basket or transaction data. It’s interesting to note that a lot of companies were involved when the standard was created. Here’s a list of some of them:</p>

<ul>
<li>Adobe</li>
<li>Best Buy</li>
<li>BrightTag</li>
<li>Criteo</li>
<li>Ensighten</li>
<li>Google</li>
<li>IBM</li>
<li>LunaMetrics</li>
<li>Qubit</li>
<li>Snowplow Analytics</li>
<li>Tealium</li>
</ul>

<p>This made us wonder: do the data layer standards of the tag management vendors in this list match the CEDDL standard? </p>

<h2 id=""putthemoneywhereyourmouthis"">Put the money where your mouth is</h2>

<p>To find out, I reached out to the tag management vendor we directly, or indirectly work with at Blue Mango. The list of vendors we currently work with contains three companies from the list:</p>

<ul>
<li>Qubit – Qubit Opentag</li>
<li>Google - Google Tag Manager</li>
<li>Tealium</li>
</ul>

<p>And one that’s not in the list:</p>

<ul>
<li>Relay 42</li>
</ul>

<p>We’ve noticed that all of the data layer standards are different from the CEDDL standard. So when I  contacted the vendors, I asked:</p>

<blockquote>
  <p>Why does the [tag manager] data layer differ from the CEDDL standard? </p>
</blockquote>

<p>Let’s look at the replies.</p>

<h2 id=""qubit"">Qubit</h2>

<p><strong>Contact:</strong> Harry Hurst - Director, Partnerships at Qubit <br>
<strong>Data layer standard:</strong> <a href=""http://docs.qubitproducts.com/uv/"">Universal Variable</a></p>

<p><strong>The data layer</strong></p>

<p>The Qubit data layer, the Universal Variable, is a nested JavaScript object. Here’s an example of the page variable from their docs:  </p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""pln"">universal_variable </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">  
  </span><span class=""str"">""page""</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">
    </span><span class=""str"">""type""</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""str"">""content""</span><span class=""pun"">,</span><span class=""pln"">
    </span><span class=""str"">""breadcrumb""</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""pun"">[</span><span class=""pln"">
      </span><span class=""str"">""The Fashion Blog""</span><span class=""pun"">,</span><span class=""pln"">
      </span><span class=""str"">""London Fashion Week '13""</span><span class=""pun"">,</span><span class=""pln"">
      </span><span class=""str"">""Behind the scenes at the River Island fashion show""</span><span class=""pln"">
    </span><span class=""pun"">]</span><span class=""pln"">
  </span><span class=""pun"">}</span><span class=""pln"">
</span><span class=""pun"">}</span></code></pre>

<p>The nested structure is easy to read. The page variable contains all page related variables. The page type variable contains information about the type of the page. Therefore, it’s nested inside the page variable. You can use the variable with <code>universal_variable.page.type</code>. This makes the object similar to the CEDDL structure. </p>

<p><strong>Why does the Qubit data layer differ from the CEDDL standard?</strong></p>

<blockquote>
  <p>The W3C data layer is based on a collaboration of lots of online businesses including Qubit.  In fact, Qubit's UV model was submitted as an original template for what this became - we had our model running live with clients several years before the CEDDL standard was agreed and have continued with our naming structure to meet the specific needs of our clients.</p>
  
  <p>We can work with the CEDDL or other any suitable, pre-existing data layer but always push for best practice adoption of values that reflect the desired output of the marketing technology that will use it.  So for example, having a data layer which provides values designed for adtech will probably not meet the needs of a personalisation technology.  The reality is that our understanding of what data clients require moves faster than the standard can adapt so we'll always be slightly ahead of this kind of template.  In each case, we believe that the client should be able to define the values which are important to their business, without limits and then be able to use that full breadth of data.</p>
</blockquote>

<p>Harry mentions that Qubit has been working with a data layer long before the standard was set. Because of this clients had their data layer installed already. Besides that, the original plan was to use the UV as a standard (see <a href=""https://github.com/QubitProducts/UniversalVariable"">their GitHub page</a> updated in January 2013). These were the two main reasons why Qubit decided not to change it. Another good point is that development of data layer requests by clients (or custom changes to the Universal Variable by Qubit) move faster than the adaption of these same requests by a standard. So custom versions of a data layer will always be ahead of the standard.</p>

<h2 id=""googletagmanager"">Google Tag Manager</h2>

<p><strong>Contact:</strong> Lukas Bergstrom  - Product manager for Google Tag Manager <br>
<strong>Data layer standard</strong>: <a href=""https://developers.google.com/tag-manager/devguide?hl=en"">dataLayer</a></p>

<p><strong>The data layer</strong>
Google data layer setup, the dataLayer, is not a single object, but an array containing objects. Let’s look at a page example:  </p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""pln"">dataLayer </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""pun"">[</span><span class=""pln"">  
  </span><span class=""pun"">{</span><span class=""pln"">
    </span><span class=""str"">""page""</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">
      </span><span class=""str"">""type""</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""str"">""content""</span><span class=""pun"">,</span><span class=""pln"">
      </span><span class=""str"">""breadcrumb""</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""pun"">[</span><span class=""pln"">
        </span><span class=""str"">""The Fashion Blog""</span><span class=""pun"">,</span><span class=""pln"">
        </span><span class=""str"">""London Fashion Week '13""</span><span class=""pun"">,</span><span class=""pln"">
        </span><span class=""str"">""Behind the scenes at the River Island fashion show""</span><span class=""pln"">
      </span><span class=""pun"">]</span><span class=""pln"">
  </span><span class=""pun"">}</span><span class=""pln"">
</span><span class=""pun"">]</span></code></pre>

<p>If you’d want to get the pagetype in this structure, it’ll be less easy to do it with vanilla JavaScript. You’ll basically have to loop through your objects to find the page variable:  </p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""kwd"">for</span><span class=""pln""> </span><span class=""pun"">(</span><span class=""kwd"">var</span><span class=""pln""> i </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""lit"">0</span><span class=""pun"">;</span><span class=""pln""> i </span><span class=""pun"">&lt;</span><span class=""pln""> dataLayer</span><span class=""pun"">.</span><span class=""pln"">length</span><span class=""pun"">;</span><span class=""pln""> i</span><span class=""pun"">++)</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">  
  </span><span class=""kwd"">if</span><span class=""pun"">(</span><span class=""kwd"">typeof</span><span class=""pln""> dataLayer</span><span class=""pun"">[</span><span class=""pln"">i</span><span class=""pun"">].</span><span class=""pln"">page </span><span class=""pun"">!==</span><span class=""pln""> </span><span class=""str"">'undefined'</span><span class=""pun"">){</span><span class=""pln"">
    console</span><span class=""pun"">.</span><span class=""pln"">log</span><span class=""pun"">(</span><span class=""pln"">dataLayer</span><span class=""pun"">[</span><span class=""pln"">i</span><span class=""pun"">].</span><span class=""pln"">page</span><span class=""pun"">.</span><span class=""pln"">type</span><span class=""pun"">);</span><span class=""pln"">
  </span><span class=""pun"">}</span><span class=""pln"">
</span><span class=""pun"">}</span></code></pre>

<p>As soon as you’re logged into GTM, and enable the preview mode, things get a lot easier. The preview mode shows you a clear overview of the data layer. Inside GTM you can get the variable more easily by simply setting up a data layer variable (page.type):</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2015/Dec/GTM_data_layer-1449243142754.png"" alt=""GTM data layer variable exmaple""></p>

<p><em>Sample page type variable setup Google Tag Manager</em></p>

<p>Technically, this makes for quite a different data layer setup than the CEDDL standard. An extra reason to find out why Google took this approach. </p>

<p><strong>Why does the GTM data layer differ from the CEDDL standard?</strong></p>

<blockquote>
  <p>The issue is when you have different bits of code on the page that might need to update the CEDDL/data layer. The data layer will avoid any race conditions - different parts of the page can push data and events to its queue of messages and it will just process them in order. The CEDDL, as a static data object, could be changed by one event handler before a prior event handler has been able to send the data it just updated the CEDDL with.</p>
</blockquote>

<p>In other words, with Google data layer setup, you’ll be able to keep updating the data layer by pushing objects into it (messages). All of these messages will be handled separately. Making sure that e.g. ecommerce update one (e.g. a product view) won’t conflict a second update (e.g. a list of suggested product impressions). In one nested object, you could theoretically change the data layers variables while your changing it.</p>

<h2 id=""tealium"">Tealium</h2>

<p><strong>Contact:</strong> Ali Behnam - Co-Founder, Tealium <br>
<strong>Data Layer Standard:</strong> <a href=""http://tealium.com/what-is-a-data-layer/"">custom data layer</a></p>

<p><strong>The data layer</strong>
The Tealium data layer is custom. They don’t have a specific default spec. Looking at their best practices, they describe two structures:</p>

<ul>
<li>Flat: a JavaScript object with one level</li>
<li>Multi-level: a JavaScript object with nested variables (more closely related to the CEDDL)</li>
</ul>

<p>Looking at two examples:</p>

<p><strong>Flat structure example:</strong></p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""kwd"">var</span><span class=""pln""> flat_data </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">  
 page_type </span><span class=""pun"">:</span><span class=""pln""> </span><span class=""str"">""content""</span><span class=""pun"">,</span><span class=""pln"">
 page_breadcrumb </span><span class=""pun"">:</span><span class=""pln""> </span><span class=""pun"">[</span><span class=""pln"">
    </span><span class=""str"">""The Fashion Blog""</span><span class=""pun"">,</span><span class=""pln""> 
    </span><span class=""str"">""London Fashion Week '13""</span><span class=""pun"">,</span><span class=""pln"">
    </span><span class=""str"">""Behind the scenes at the River Island fashion show""</span><span class=""pln"">
  </span><span class=""pun"">]</span><span class=""pln"">
</span><span class=""pun"">}</span></code></pre>

<p><strong>Multi level structure exmample:</strong></p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""kwd"">var</span><span class=""pln""> multi_level_data </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">  
 page </span><span class=""pun"">:</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">
   type </span><span class=""pun"">:</span><span class=""pln""> </span><span class=""str"">""content""</span><span class=""pun"">,</span><span class=""pln"">
   breadcrumb </span><span class=""pun"">:</span><span class=""pln""> </span><span class=""pun"">[</span><span class=""pln"">
    </span><span class=""str"">""The Fashion Blog""</span><span class=""pun"">,</span><span class=""pln""> 
    </span><span class=""str"">""London Fashion Week '13""</span><span class=""pun"">,</span><span class=""pln"">
    </span><span class=""str"">""Behind the scenes at the River Island fashion show""</span><span class=""pln"">
   </span><span class=""pun"">]</span><span class=""pln"">
  </span><span class=""pun"">}</span><span class=""pln""> 
</span><span class=""pun"">}</span></code></pre>

<p><a href=""http://tealium.com/blog/standard/best-practices-implementing-data-layer/"">As Ty Gavin describes on the Teailum blog</a>, the first one is easier to use when coding. You can easily see if a variable is available. In the multi-level structure, you’ll have to check if all the levels are available  (so page, page.attributes, and page.attributes.type). </p>

<p><strong>Why does the Tealium data layer differ from the CEDDL standard?</strong></p>

<blockquote>
  <p>Thanks for reaching out. You're right that none of the TMS vendors or frankly none of the digital marketing vendors (i.e. analytics) have adopted the W3C standard as their own best practice. However, one thing to be optimistic about is that pretty much all vendors are now advocating the use of a single data object for implementation. In our case, we can read the W3C standards just fine, although it's not our best practice for reasons below.</p>
  
  <p>I can only speak for Tealium. Our data layer best practice is to have a single data layer and allow customers to define their data values in that single data object. By default, we name our data object utag_data, but the name can be changed to anything, so as an example the customer can change the name to digitalData (W3C naming) or dataLayer (Google naming) and it will all work. So we treat the name as exactly that: a name.</p>
  
  <p>Another area where you will see differences between us and W3C is in naming of the variables. Our position again is that these are just names. Our best practice is to allow customers to define whatever data they want in as easy a process as possible. Take for example product name, in W3C you'll have to adhere to a specific name and hierarchy, but in our case you call it anything (i.e. product, product_name, productName, sku, or produkt - if the customer is German speaking and prefers German names).</p>
  
  <p>So to summarize, the primary reason for our best practice has been to make life easier for customers. We strongly feel that the TMS product should be able to accommodate customer preference, and not vice-versa.</p>
</blockquote>

<p>What I most like about Ali’s reply is last line: the client should be leading and the data layer should be defined by  their wishes. I also like how his story matches that of the blogpost by Tealium I mentioned: data layers can be flat and simple, or complex and multi leveled. There are scenario’s for either one.</p>

<h2 id=""relay42"">Relay 42</h2>

<p><strong>Contact:</strong> Yorick Breuker - Support Engineer at Relay42 <br>
<strong>Data Layer Standard:</strong> _st (no public docs)</p>

<p><strong>The data layer</strong>
What makes the Relay 42 data layer unique is that they offer both a synchronous and asynchronous data layer capture. You’ll pass the following information when adding a property to the data layer:</p>

<ul>
<li>property label: what property are you pushing?</li>
<li>property value: what value does the property have?</li>
</ul>

<p>These data layer changes can handle all types of data: integers, strings, objects, etc. Here’s are some code examples for possible page variables, both simple, more complex, synchronous and asynchrounous:</p>

<p><strong>Async single properties example:</strong></p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""pln"">_st</span><span class=""pun"">(</span><span class=""str"">'addTagProperty'</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""str"">'page_type'</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""str"">'content'</span><span class=""pun"">);</span><span class=""pln"">  
_st</span><span class=""pun"">(</span><span class=""str"">'addTagProperty'</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""str"">'page_breadcrumb'</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""pun"">[</span><span class=""pln"">  
    </span><span class=""str"">""The Fashion Blog""</span><span class=""pun"">,</span><span class=""pln"">
    </span><span class=""str"">""London Fashion Week '13""</span><span class=""pun"">,</span><span class=""pln"">
    </span><span class=""str"">""Behind the scenes at the River Island fashion show""</span><span class=""pln"">
</span><span class=""pun"">]);</span></code></pre>

<p><strong>Async object  property example:</strong></p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""pln"">_st</span><span class=""pun"">(</span><span class=""str"">'addTagProperties'</span><span class=""pun"">,</span><span class=""pln"">  </span><span class=""pun"">{</span><span class=""pln"">  
  type </span><span class=""pun"">:</span><span class=""pln""> </span><span class=""str"">'content'</span><span class=""pun"">,</span><span class=""pln"">
  breadcrumb </span><span class=""pun"">:</span><span class=""pln""> </span><span class=""pun"">[</span><span class=""pln"">
    </span><span class=""str"">""The Fashion Blog""</span><span class=""pun"">,</span><span class=""pln""> 
    </span><span class=""str"">""London Fashion Week '13""</span><span class=""pun"">,</span><span class=""pln"">
    </span><span class=""str"">""Behind the scenes at the River Island fashion show""</span><span class=""pln"">
  </span><span class=""pun"">]</span><span class=""pln"">
</span><span class=""pun"">});</span></code></pre>

<p><strong>Sync object property example:</strong></p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""pln"">_st</span><span class=""pun"">.</span><span class=""pln"">data</span><span class=""pun"">.</span><span class=""pln"">setProperty</span><span class=""pun"">(</span><span class=""str"">'page'</span><span class=""pun"">,{</span><span class=""pln"">  
  type </span><span class=""pun"">:</span><span class=""pln""> </span><span class=""str"">'content'</span><span class=""pun"">,</span><span class=""pln"">
  breadcrumb </span><span class=""pun"">:</span><span class=""pln""> </span><span class=""pun"">[</span><span class=""pln"">
    </span><span class=""str"">""The Fashion Blog""</span><span class=""pun"">,</span><span class=""pln""> 
    </span><span class=""str"">""London Fashion Week '13""</span><span class=""pun"">,</span><span class=""pln"">
    </span><span class=""str"">""Behind the scenes at the River Island fashion show""</span><span class=""pln"">
  </span><span class=""pun"">]});</span></code></pre>

<p>Technically, the support for both a sync and async data layer additions is unique. Relay sends properties separately to the data layer. This makes it more like Googles dataLayer specification than the CEDDL, where you define a simple JavaScript object. </p>

<p>If you want to access the data directly with JavaScript, you can do so with the following function.</p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""pln"">_st</span><span class=""pun"">.</span><span class=""pln"">data</span><span class=""pun"">.</span><span class=""pln"">getProperty</span><span class=""pun"">(</span><span class=""str"">'naam'</span><span class=""pun"">);</span><span class=""pln"">  </span></code></pre>

<p>Keep in mind that Relay42 has easier ways to view available data with their tag management system. For example the function <code>_st.debug.enable()</code>, this will show you the available parameters on the current page.</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2015/Dec/Relay-1449244109118.png"" alt=""Relay42 data layer varaible example"" class=""full-img""></p>

<p><em>Sample popup from the Relay42 tag manager showing available variables of the current page.</em></p>

<p><strong>Why does the Relay42 data layer differ from the CEDDL standard?</strong></p>

<blockquote>
  <p>To give you a straight answer; The Relay42 philosophy desires more flexibility, and the best practices were already developed before the CEDDL standard was. This doesn't mean you can't use the CEDDL standard for your Relay42 implementation though.Relay42’s approach is similar to Google’s, pushing separate updates to a queue and handling them all independently.  </p>
  
  <p>A datalayer is mainly semantic. It is referring to the place in/on the website where you can find all relevant information. Although we fully support the ideology of a global standard, the way in which this should be done is a trivial discussion. In our case, we have few nice features in place, which wouldn't be possible when using a 'vanilla' javascript object like stated in the CEDDL standard. There are a few things though, we consider important in a datalayer:</p>
  
  <ul>
  <li><p>Being flexible and client centric. Although the Relay42 basescript mostly remains the same, we still host a seperate basescript for each of our clients. The reason for this is that we want to offer our clients the possibility to let us customize it, to make it refer to an alternative custom datalayer for example. </p></li>
  <li><p>Usability: We want our users to be able to use available data without any technical knowledge. To achieve this, we offer all kinds of templates for third party vendors, in which our interface integrated the datalayer, and provides you with a list of available properties for the page you want to place a tag on. You can select these properties for tracking just by clicking them.</p></li>
  <li><p>Correct handling of privacy sensitive information. The examples of adding data to the Relay42 datalayer, can be extended with the 'local' prefix. When doing so, you explicitly deny the property from being available for tag management server logic. This allows you to be privacy compliant in every situation. </p></li>
  </ul>
  
  <p>A technically correct solution for every implementation. As you can see in the examples given, there is a synchronous and an asynchronous way of handling commands. This way our clients can be sure that performance issues won't be a problem.</p>
</blockquote>

<p>What sets Relay42 apart from Google (the most similar when comparing the data layer setup) is that their tag manager automatically show available variables in their tag manager. This way, non-technical people instantly get  a list of available variables on a page they want to place a tag on. Quite convenient. The other tag managers require you to manually add the variables to the tag management system.  </p>

<h2 id=""thelessonlearned"">The lesson learned</h2>

<p>The data layer is something that’s very client dependent. Just as each company has its own structure, so should the data layer. I like the way Ali from Tealium looks to it: </p>

<blockquote>
  <p>The technical specification should never be leading, the client should be.</p>
</blockquote>

<p>Technically, Qubit sticks closest to the standard by clearly opting for a nested data layer in the documentations. Though Tealium is close, allowing for various data layer setups. Besides that, it’s good to know that they all support custom object in one way or another:</p>

<ul>
<li>Tealium: set a custom data layer object.</li>
<li>Qubit and Relay42: write a data layer wrapper.</li>
<li>All: use custom JavaScript tags to read and handle the data layer on your site.</li>
</ul>

<h2 id=""ourviewonthedatalayer"">Our view on the data layer</h2>

<p>At Blue Mango, we've been working with tag managers and data layers since mid-2012. Over the past years, we've learned the value of a well defined data layer and because of this, our briefings have changed. We currently define two parts in our data layer implementation documents:</p>

<h4 id=""1thegeneralpart"">1: The general part</h4>

<p>For the general part, we take a tag managers standard documentation, and apply it to our client’s website. In this part, we remove all data that’s unnecessary. We also try to learn from one tag managers data layer standards, and apply it to others. For example: Qubit’s Universal Variable standard contains more information that Google’s. Two useful variables from Qubit are:</p>

<ul>
<li>User variable: containing information about the user, for example their user ID or a Boolean that tells us if a user has transacted before.</li>
<li>Page variable: containing information about the page, for example the virtual url path, a breadcrumb array and page types. </li>
</ul>

<p>We’ve noticed that some standards are really useful. Page type is one of them. If you want to learn how to apply this standard to GTM, read my article about it <a href=""http://www.themarketingtechnologist.co/start-using-page-types-to-make-your-tag-management-life-easier/"">here</a>.</p>

<h4 id=""2theclientspecificpart"">2: The client specific part</h4>

<p>The second part contains client specific information. This contains information that isn’t generally applicable. Think about the duration of a phone contract, or a the fuel type of car. We put these in a separate chapter as additions to the standard data layer. </p>

<h2 id=""structure"">Structure</h2>

<p>Personally, I like the single object approach because it has a readable structure. This makes the data layer open, allowing developers and web analyst to explore the object easily. Other data layer structures have options to view the more complex data layer more easily, but they always require you to either login or install an extra plugin. In other words: one extra step.</p>

<h2 id=""itsaboutwhatthedatalayercandofortheclient"">It’s about what the data layer can do for the client</h2>

<p>What it comes down to is that you should always put your client first. That way, you’ll optimize the power of the data layer for the client. Keep in mind that theoretically, you can use each data layer with each tag manager. So if you see a great post about best practices for Qubit Opentag, GTM, Tealium or Relay42 and it’s not about the tag manager you’re using, try to look for a way to implement it in your system of choice. </p>

<p><strong>A final note</strong></p>

<p>We use Qubit Opentag’s Universal Variable data layer standard  with Google Tag Manager on The Marketing Technologist. It works like a charm. </p>
        ","About a month ago, my friend and colleague Siebe introduced me to a document called the Customer Experience Digital Data Layer standard, or CEDDL for short. It’s a standard for data layers that describes how onsite data can be structured in a JavaScript object. It was created at the end of 2013 as a uniform way to define a website’s data layer.
The CEDDL structure
The CEDDL standard is a not an official standard. This means that you don’t need to use it, but could use it as a guide. Here’s a data layer sample for page variables based on this standard:
digitalData = {  
 pageInstanceID: ""MyHomePage-Production"",
 page:{
   pageInfo: {
     pageID: ""Home Page"",
     destinationURL: ""http://mysite.com/index.html""},
   category:{
     primaryCategory: ""FAQ Pages"",
    subCategory1: ""ProductInfo"",
    pageType: ""FAQ""},
  attributes:{
    country: ""US"",
    language: ""en-US""}
  }
};
As you can see, it’s basically one JavaScript object with nested variables. The same structure is used for other information such as basket or transaction data. It’s interesting to note that a lot of companies were involved when the standard was created. Here’s a list of some of them:
Adobe
Best Buy
BrightTag
Criteo
Ensighten
Google
IBM
LunaMetrics
Qubit
Snowplow Analytics
Tealium
This made us wonder: do the data layer standards of the tag management vendors in this list match the CEDDL standard?
Put the money where your mouth is
To find out, I reached out to the tag management vendor we directly, or indirectly work with at Blue Mango. The list of vendors we currently work with contains three companies from the list:
Qubit – Qubit Opentag
Google - Google Tag Manager
Tealium
And one that’s not in the list:
Relay 42
We’ve noticed that all of the data layer standards are different from the CEDDL standard. So when I contacted the vendors, I asked:
Why does the [tag manager] data layer differ from the CEDDL standard?
Let’s look at the replies.
Qubit
Contact: Harry Hurst - Director, Partnerships at Qubit
Data layer standard: Universal Variable
The data layer
The Qubit data layer, the Universal Variable, is a nested JavaScript object. Here’s an example of the page variable from their docs:
universal_variable = {  
  ""page"": {
    ""type"": ""content"",
    ""breadcrumb"": [
      ""The Fashion Blog"",
      ""London Fashion Week '13"",
      ""Behind the scenes at the River Island fashion show""
    ]
  }
}
The nested structure is easy to read. The page variable contains all page related variables. The page type variable contains information about the type of the page. Therefore, it’s nested inside the page variable. You can use the variable with universal_variable.page.type. This makes the object similar to the CEDDL structure.
Why does the Qubit data layer differ from the CEDDL standard?
The W3C data layer is based on a collaboration of lots of online businesses including Qubit. In fact, Qubit's UV model was submitted as an original template for what this became - we had our model running live with clients several years before the CEDDL standard was agreed and have continued with our naming structure to meet the specific needs of our clients.
We can work with the CEDDL or other any suitable, pre-existing data layer but always push for best practice adoption of values that reflect the desired output of the marketing technology that will use it. So for example, having a data layer which provides values designed for adtech will probably not meet the needs of a personalisation technology. The reality is that our understanding of what data clients require moves faster than the standard can adapt so we'll always be slightly ahead of this kind of template. In each case, we believe that the client should be able to define the values which are important to their business, without limits and then be able to use that full breadth of data.
Harry mentions that Qubit has been working with a data layer long before the standard was set. Because of this clients had their data layer installed already. Besides that, the original plan was to use the UV as a standard (see their GitHub page updated in January 2013). These were the two main reasons why Qubit decided not to change it. Another good point is that development of data layer requests by clients (or custom changes to the Universal Variable by Qubit) move faster than the adaption of these same requests by a standard. So custom versions of a data layer will always be ahead of the standard.
Google Tag Manager
Contact: Lukas Bergstrom - Product manager for Google Tag Manager
Data layer standard: dataLayer
The data layer Google data layer setup, the dataLayer, is not a single object, but an array containing objects. Let’s look at a page example:
dataLayer = [  
  {
    ""page"": {
      ""type"": ""content"",
      ""breadcrumb"": [
        ""The Fashion Blog"",
        ""London Fashion Week '13"",
        ""Behind the scenes at the River Island fashion show""
      ]
  }
]
If you’d want to get the pagetype in this structure, it’ll be less easy to do it with vanilla JavaScript. You’ll basically have to loop through your objects to find the page variable:
for (var i = 0; i < dataLayer.length; i++) {  
  if(typeof dataLayer[i].page !== 'undefined'){
    console.log(dataLayer[i].page.type);
  }
}
As soon as you’re logged into GTM, and enable the preview mode, things get a lot easier. The preview mode shows you a clear overview of the data layer. Inside GTM you can get the variable more easily by simply setting up a data layer variable (page.type):
Sample page type variable setup Google Tag Manager
Technically, this makes for quite a different data layer setup than the CEDDL standard. An extra reason to find out why Google took this approach.
Why does the GTM data layer differ from the CEDDL standard?
The issue is when you have different bits of code on the page that might need to update the CEDDL/data layer. The data layer will avoid any race conditions - different parts of the page can push data and events to its queue of messages and it will just process them in order. The CEDDL, as a static data object, could be changed by one event handler before a prior event handler has been able to send the data it just updated the CEDDL with.
In other words, with Google data layer setup, you’ll be able to keep updating the data layer by pushing objects into it (messages). All of these messages will be handled separately. Making sure that e.g. ecommerce update one (e.g. a product view) won’t conflict a second update (e.g. a list of suggested product impressions). In one nested object, you could theoretically change the data layers variables while your changing it.
Tealium
Contact: Ali Behnam - Co-Founder, Tealium
Data Layer Standard: custom data layer
The data layer The Tealium data layer is custom. They don’t have a specific default spec. Looking at their best practices, they describe two structures:
Flat: a JavaScript object with one level
Multi-level: a JavaScript object with nested variables (more closely related to the CEDDL)
Looking at two examples:
Flat structure example:
var flat_data = {  
 page_type : ""content"",
 page_breadcrumb : [
    ""The Fashion Blog"", 
    ""London Fashion Week '13"",
    ""Behind the scenes at the River Island fashion show""
  ]
}
Multi level structure exmample:
var multi_level_data = {  
 page : {
   type : ""content"",
   breadcrumb : [
    ""The Fashion Blog"", 
    ""London Fashion Week '13"",
    ""Behind the scenes at the River Island fashion show""
   ]
  } 
}
As Ty Gavin describes on the Teailum blog, the first one is easier to use when coding. You can easily see if a variable is available. In the multi-level structure, you’ll have to check if all the levels are available (so page, page.attributes, and page.attributes.type).
Why does the Tealium data layer differ from the CEDDL standard?
Thanks for reaching out. You're right that none of the TMS vendors or frankly none of the digital marketing vendors (i.e. analytics) have adopted the W3C standard as their own best practice. However, one thing to be optimistic about is that pretty much all vendors are now advocating the use of a single data object for implementation. In our case, we can read the W3C standards just fine, although it's not our best practice for reasons below.
I can only speak for Tealium. Our data layer best practice is to have a single data layer and allow customers to define their data values in that single data object. By default, we name our data object utag_data, but the name can be changed to anything, so as an example the customer can change the name to digitalData (W3C naming) or dataLayer (Google naming) and it will all work. So we treat the name as exactly that: a name.
Another area where you will see differences between us and W3C is in naming of the variables. Our position again is that these are just names. Our best practice is to allow customers to define whatever data they want in as easy a process as possible. Take for example product name, in W3C you'll have to adhere to a specific name and hierarchy, but in our case you call it anything (i.e. product, product_name, productName, sku, or produkt - if the customer is German speaking and prefers German names).
So to summarize, the primary reason for our best practice has been to make life easier for customers. We strongly feel that the TMS product should be able to accommodate customer preference, and not vice-versa.
What I most like about Ali’s reply is last line: the client should be leading and the data layer should be defined by their wishes. I also like how his story matches that of the blogpost by Tealium I mentioned: data layers can be flat and simple, or complex and multi leveled. There are scenario’s for either one.
Relay 42
Contact: Yorick Breuker - Support Engineer at Relay42
Data Layer Standard: _st (no public docs)
The data layer What makes the Relay 42 data layer unique is that they offer both a synchronous and asynchronous data layer capture. You’ll pass the following information when adding a property to the data layer:
property label: what property are you pushing?
property value: what value does the property have?
These data layer changes can handle all types of data: integers, strings, objects, etc. Here’s are some code examples for possible page variables, both simple, more complex, synchronous and asynchrounous:
Async single properties example:
_st('addTagProperty', 'page_type', 'content');  
_st('addTagProperty', 'page_breadcrumb', [  
    ""The Fashion Blog"",
    ""London Fashion Week '13"",
    ""Behind the scenes at the River Island fashion show""
]);
Async object property example:
_st('addTagProperties',  {  
  type : 'content',
  breadcrumb : [
    ""The Fashion Blog"", 
    ""London Fashion Week '13"",
    ""Behind the scenes at the River Island fashion show""
  ]
});
Sync object property example:
_st.data.setProperty('page',{  
  type : 'content',
  breadcrumb : [
    ""The Fashion Blog"", 
    ""London Fashion Week '13"",
    ""Behind the scenes at the River Island fashion show""
  ]});
Technically, the support for both a sync and async data layer additions is unique. Relay sends properties separately to the data layer. This makes it more like Googles dataLayer specification than the CEDDL, where you define a simple JavaScript object.
If you want to access the data directly with JavaScript, you can do so with the following function.
_st.data.getProperty('naam');  
Keep in mind that Relay42 has easier ways to view available data with their tag management system. For example the function _st.debug.enable(), this will show you the available parameters on the current page.
Sample popup from the Relay42 tag manager showing available variables of the current page.
Why does the Relay42 data layer differ from the CEDDL standard?
To give you a straight answer; The Relay42 philosophy desires more flexibility, and the best practices were already developed before the CEDDL standard was. This doesn't mean you can't use the CEDDL standard for your Relay42 implementation though.Relay42’s approach is similar to Google’s, pushing separate updates to a queue and handling them all independently.
A datalayer is mainly semantic. It is referring to the place in/on the website where you can find all relevant information. Although we fully support the ideology of a global standard, the way in which this should be done is a trivial discussion. In our case, we have few nice features in place, which wouldn't be possible when using a 'vanilla' javascript object like stated in the CEDDL standard. There are a few things though, we consider important in a datalayer:
Being flexible and client centric. Although the Relay42 basescript mostly remains the same, we still host a seperate basescript for each of our clients. The reason for this is that we want to offer our clients the possibility to let us customize it, to make it refer to an alternative custom datalayer for example.
Usability: We want our users to be able to use available data without any technical knowledge. To achieve this, we offer all kinds of templates for third party vendors, in which our interface integrated the datalayer, and provides you with a list of available properties for the page you want to place a tag on. You can select these properties for tracking just by clicking them.
Correct handling of privacy sensitive information. The examples of adding data to the Relay42 datalayer, can be extended with the 'local' prefix. When doing so, you explicitly deny the property from being available for tag management server logic. This allows you to be privacy compliant in every situation.
A technically correct solution for every implementation. As you can see in the examples given, there is a synchronous and an asynchronous way of handling commands. This way our clients can be sure that performance issues won't be a problem.
What sets Relay42 apart from Google (the most similar when comparing the data layer setup) is that their tag manager automatically show available variables in their tag manager. This way, non-technical people instantly get a list of available variables on a page they want to place a tag on. Quite convenient. The other tag managers require you to manually add the variables to the tag management system.
The lesson learned
The data layer is something that’s very client dependent. Just as each company has its own structure, so should the data layer. I like the way Ali from Tealium looks to it:
The technical specification should never be leading, the client should be.
Technically, Qubit sticks closest to the standard by clearly opting for a nested data layer in the documentations. Though Tealium is close, allowing for various data layer setups. Besides that, it’s good to know that they all support custom object in one way or another:
Tealium: set a custom data layer object.
Qubit and Relay42: write a data layer wrapper.
All: use custom JavaScript tags to read and handle the data layer on your site.
Our view on the data layer
At Blue Mango, we've been working with tag managers and data layers since mid-2012. Over the past years, we've learned the value of a well defined data layer and because of this, our briefings have changed. We currently define two parts in our data layer implementation documents:
1: The general part
For the general part, we take a tag managers standard documentation, and apply it to our client’s website. In this part, we remove all data that’s unnecessary. We also try to learn from one tag managers data layer standards, and apply it to others. For example: Qubit’s Universal Variable standard contains more information that Google’s. Two useful variables from Qubit are:
User variable: containing information about the user, for example their user ID or a Boolean that tells us if a user has transacted before.
Page variable: containing information about the page, for example the virtual url path, a breadcrumb array and page types.
We’ve noticed that some standards are really useful. Page type is one of them. If you want to learn how to apply this standard to GTM, read my article about it here.
2: The client specific part
The second part contains client specific information. This contains information that isn’t generally applicable. Think about the duration of a phone contract, or a the fuel type of car. We put these in a separate chapter as additions to the standard data layer.
Structure
Personally, I like the single object approach because it has a readable structure. This makes the data layer open, allowing developers and web analyst to explore the object easily. Other data layer structures have options to view the more complex data layer more easily, but they always require you to either login or install an extra plugin. In other words: one extra step.
It’s about what the data layer can do for the client
What it comes down to is that you should always put your client first. That way, you’ll optimize the power of the data layer for the client. Keep in mind that theoretically, you can use each data layer with each tag manager. So if you see a great post about best practices for Qubit Opentag, GTM, Tealium or Relay42 and it’s not about the tag manager you’re using, try to look for a way to implement it in your system of choice.
A final note
We use Qubit Opentag’s Universal Variable data layer standard with Google Tag Manager on The Marketing Technologist. It works like a charm.","[Analytics, tag management, data layer, CEDDL]"
75,Our first insights in the ad-blocking consumer,/our-first-insights-in-the-ad-blocking-consumer/,"
            <p>The use of ad-blockers is growing fast, as consumers are getting tired of irrelevant and annoying advertisements. According to <a href=""http://downloads.pagefair.com/reports/2015_report-the_cost_of_ad_blocking.pdf"">Adobe and PageFair</a>, 181 million online consumers use an ad-blocker and 41,4 billion dollars of income is being missed out by online publishers due to the use of ad-blockers. </p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2015/Dec/Adblock_use-1449498015406.png"" alt="""" class=""full-img""></p>

<p>As these numbers are general, it is interesting to investigate if these numbers are applicable to all websites or dependent on the audience of your website. In order to gain insight into the number of visitors that uses an ad-blocker we have placed a piece of code on this blog and on another website (website X) with a totally different audience. This piece of code tracks if a visitor is using an ad-blocker or not and sends the results to Google Analytics which gives us a more detailed view of the visitors that use an ad-blocker. </p>

<p>As base for the code we used the post of <a href=""https://marthijnhoiting.com/track-the-use-of-adblock-in-google-analytics/"">Martijn Hoiting</a> which explains how to track how many users are using ad-block software trough Google Analytics with some help from Google Tag Manager. The code creates a fake ad at the bottom of the webpage and the javascript code checks if the <code>div</code> is loaded or not. My colleague <a href=""http://www.themarketingtechnologist.co/author/erik-driessen/"">Erik Driessen</a> re-wrote the code and removed the jQuery part. Also he added <code>display: none;</code> to the script in order to make sure the bottomAd div won't be visible on the website.</p>

<p><strong>HTML element to mimic the behaviour of an ad</strong></p>

<pre><code class=""language-prettyprint lang-html prettyprinted""><span class=""tag"">&lt;div</span><span class=""pln""> </span><span class=""atn"">id</span><span class=""pun"">=</span><span class=""atv"">""bottomAd""</span><span class=""pln""> </span><span class=""atn"">style</span><span class=""pun"">=</span><span class=""atv"">""</span><span class=""pln"">font</span><span class=""pun"">-</span><span class=""pln"">size</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""lit"">2px</span><span class=""pun"">;</span><span class=""atv"">""</span><span class=""tag"">&gt;</span><span class=""pln"">&amp;nbsp;</span><span class=""tag"">&lt;/div&gt;</span><span class=""pln"">  </span></code></pre>

<p>This div element is the one that Google AdSense uses to serve ads. If you have AdSense running on your page, don't place this tag. </p>

<p><strong>JavaScript to track ad-blocking</strong></p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""kwd"">var</span><span class=""pln""> getAdElement </span><span class=""pun"">=</span><span class=""pln""> document</span><span class=""pun"">.</span><span class=""pln"">getElementById</span><span class=""pun"">(</span><span class=""str"">'bottomAd'</span><span class=""pun"">);</span><span class=""pln"">  
</span><span class=""kwd"">if</span><span class=""pun"">(</span><span class=""pln"">getAdElement</span><span class=""pun"">.</span><span class=""pln"">clientHeight </span><span class=""pun"">===</span><span class=""pln""> </span><span class=""lit"">0</span><span class=""pun"">){</span><span class=""pln"">  
  </span><span class=""com"">//track ad-blocker enabled</span><span class=""pln"">
</span><span class=""pun"">}</span><span class=""kwd"">else</span><span class=""pun"">{</span><span class=""pln"">   
  </span><span class=""com"">//track ad-blocker disabled</span><span class=""pln"">
</span><span class=""pun"">}</span><span class=""pln"">
getAdElement</span><span class=""pun"">.</span><span class=""pln"">style</span><span class=""pun"">.</span><span class=""pln"">display </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""str"">""none""</span><span class=""pun"">;</span><span class=""pln"">  </span></code></pre>

<p>The JavaScript snippet that detects if an ad-blocker is enabled. </p>

<p>The downside of this code is that it does not work when visitors are also blocking Google Tag Manager or Google Analytics. In order to track these visitors you can also use <a href=""https://github.com/ExpandOnline/AdblockerDetector"">this  script</a> that works with a PHP-file. As you need to place the script hard-coded on every page you want to track, you need access to the webserver which can make it less flexible than implementing our script trough GTM.</p>

<h3 id=""theusageofadblockers"">The usage of ad-blockers</h3>

<p>When looking at the percentage of visitors that uses an ad-blocker, the first difference shows between the two websites. Almost a third of the visitors of The Marketing Technologist (TMT) uses an ad-blocker which is a lot compared to website X where only 5 percent of the visitors uses an ad-blocker. When looking at the usage of ad-blockers in combination with visits from a desktop the percentage increases with around 10 percent at TMT and with 5 percent on website X. The usage of Ad-blockers on mobile devices as tablets and smartphones is negligible.</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2015/Dec/usage2-1449493798207.png"" alt="""" class=""full-img"">
<em>Visitors with ad-blocker from desktop</em></p>

<h3 id=""operatingsystemandbrowser"">Operating System and browser</h3>

<p>As shown in the previous chapter the usage of ad-blockers may vary a lot between different websites. The next question is, do the various operating systems show different usage of ad-blockers? The answer is no. Visitors that use Windows, Mac OS X or Linux practically show the same percentage of ad-blocker users.</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2015/Dec/usage-1449493869514.png"" alt="""" class=""full-img"">
<em>Visitors with ad-blocker per browser</em></p>

<p>The three most used browsers: Chrome, Firefox and Internet Explorer show big differences in ad-blocker usage based on the visitors of TMT. All the visitors using Internet Explorer didn't use an ad-blocker at all. The simplicity of installing plug-ins in Chrome and Firefox result in a high rate of ad-blocker users in the other two browsers: 37% and 42%. </p>

<h3 id=""demographics"">Demographics</h3>

<p>Assuming that the type of audience of your website influences the usage of ad-blockers, it might be interesting to know something about the demographic aspects of ad-blocker users. The first demographic aspect is gender. Ad-blockers are mostly used by male visitors on both websites. At website X the percentage of male ad-blocker users doubles the percentage of female ad-blocker users. </p>

<p>When looking at the age of ad-blocker users it shows that ad-blockers are most popular with the age categories: 18-24 and 25-34 (based on data from TMT). Users older then 34 doesn't seem to be interested in using an ad-blocker or they might don't know how to install them. <br>
<img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2015/Dec/adblock_ins-1449482136617.png"" alt="""" class=""full-img""></p>

<h3 id=""conclusion"">Conclusion</h3>

<p>All things considered, it seems that the percentage of visitors using an ad-blocker is dependent on the audience of your website. The assumption is that more and more consumers will start using an ad-blocker in the future and blocking ads will become the standard. </p>

<hr>

<p><em>It is our intention that in the coming weeks, the script will run on the websites of multiple clients in order to get a better picture of the percentage ad-block users.</em></p>
        ","The use of ad-blockers is growing fast, as consumers are getting tired of irrelevant and annoying advertisements. According to Adobe and PageFair, 181 million online consumers use an ad-blocker and 41,4 billion dollars of income is being missed out by online publishers due to the use of ad-blockers.
As these numbers are general, it is interesting to investigate if these numbers are applicable to all websites or dependent on the audience of your website. In order to gain insight into the number of visitors that uses an ad-blocker we have placed a piece of code on this blog and on another website (website X) with a totally different audience. This piece of code tracks if a visitor is using an ad-blocker or not and sends the results to Google Analytics which gives us a more detailed view of the visitors that use an ad-blocker.
As base for the code we used the post of Martijn Hoiting which explains how to track how many users are using ad-block software trough Google Analytics with some help from Google Tag Manager. The code creates a fake ad at the bottom of the webpage and the javascript code checks if the div is loaded or not. My colleague Erik Driessen re-wrote the code and removed the jQuery part. Also he added display: none; to the script in order to make sure the bottomAd div won't be visible on the website.
HTML element to mimic the behaviour of an ad
<div id=""bottomAd"" style=""font-size: 2px;"">&nbsp;</div>  
This div element is the one that Google AdSense uses to serve ads. If you have AdSense running on your page, don't place this tag.
JavaScript to track ad-blocking
var getAdElement = document.getElementById('bottomAd');  
if(getAdElement.clientHeight === 0){  
  //track ad-blocker enabled
}else{   
  //track ad-blocker disabled
}
getAdElement.style.display = ""none"";  
The JavaScript snippet that detects if an ad-blocker is enabled.
The downside of this code is that it does not work when visitors are also blocking Google Tag Manager or Google Analytics. In order to track these visitors you can also use this script that works with a PHP-file. As you need to place the script hard-coded on every page you want to track, you need access to the webserver which can make it less flexible than implementing our script trough GTM.
The usage of ad-blockers
When looking at the percentage of visitors that uses an ad-blocker, the first difference shows between the two websites. Almost a third of the visitors of The Marketing Technologist (TMT) uses an ad-blocker which is a lot compared to website X where only 5 percent of the visitors uses an ad-blocker. When looking at the usage of ad-blockers in combination with visits from a desktop the percentage increases with around 10 percent at TMT and with 5 percent on website X. The usage of Ad-blockers on mobile devices as tablets and smartphones is negligible.
Visitors with ad-blocker from desktop
Operating System and browser
As shown in the previous chapter the usage of ad-blockers may vary a lot between different websites. The next question is, do the various operating systems show different usage of ad-blockers? The answer is no. Visitors that use Windows, Mac OS X or Linux practically show the same percentage of ad-blocker users.
Visitors with ad-blocker per browser
The three most used browsers: Chrome, Firefox and Internet Explorer show big differences in ad-blocker usage based on the visitors of TMT. All the visitors using Internet Explorer didn't use an ad-blocker at all. The simplicity of installing plug-ins in Chrome and Firefox result in a high rate of ad-blocker users in the other two browsers: 37% and 42%.
Demographics
Assuming that the type of audience of your website influences the usage of ad-blockers, it might be interesting to know something about the demographic aspects of ad-blocker users. The first demographic aspect is gender. Ad-blockers are mostly used by male visitors on both websites. At website X the percentage of male ad-blocker users doubles the percentage of female ad-blocker users.
When looking at the age of ad-blocker users it shows that ad-blockers are most popular with the age categories: 18-24 and 25-34 (based on data from TMT). Users older then 34 doesn't seem to be interested in using an ad-blocker or they might don't know how to install them.
Conclusion
All things considered, it seems that the percentage of visitors using an ad-blocker is dependent on the audience of your website. The assumption is that more and more consumers will start using an ad-blocker in the future and blocking ads will become the standard.
It is our intention that in the coming weeks, the script will run on the websites of multiple clients in order to get a better picture of the percentage ad-block users.","[Analytics, Adblocker]"
76,Creating dynamic videos using JavaScript and After Effects: the basics,/creating-dynamic-videos-using-javascript-and-after-effects-the-basics/,"
            <p>In digital marketing, there is a lot of buzz around dynamic videos. You may have noticed all the start-ups popping up in this scene. Dynamic is nothing new in the world of display advertising. We already see a lot of dynamic ads for retargeting by big parties like <a href=""http://www.criteo.com/"">Criteo</a>, but when it comes to online video, there aren't a lot of dynamic video cases (that we know of). We believe there are a couple of reasons for this, but we'll get into that in another post. In this post, we'll focus on the technical aspects of creating a dynamic video. </p>

<h3 id=""whatsdynamicvideoanyway"">What's dynamic video anyway?</h3>

<p>There are a lot of (buzz) words flying around: interactive video, dynamic video, personalized video etc. And misconceptions, too. For instance: clickable videos, like those with info cards or in-video link-buttons, are interactive, not dynamic. A couple of years ago it was cool (mostly in social) to personalize animations using Flash or HTML5 Canvas. That's not dynamic video either, that's a personalized web animation.   </p>

<p>To me, a video becomes dynamic when a specific video is rendered, pre-rendered or on-the-fly, for a specific context. That context may apply to a big group of people with the same interests, or to a single user. Another alternative term would be <em>personalized video</em>.</p>

<p>I'm aware that these definitions are both pretty technical. If you have a better one, please feel free to share it in the comment section below.  </p>

<p>One of the advantages of creating a dynamic video is that you don't have to spend a lot of money on production when you want to change a specific part of your video. You can theoretically create thousands of variations of one video without touching any video editing software or asking your production party. </p>

<h3 id=""letscreateadynamicvideo"">Let's create a dynamic video</h3>

<p>Enough with the definitions. In this post, I'd like to show how you can add a dynamic section to an existing video and render it using code. What we put in the dynamic section can be configured in a JSON file. In our example, we use a static JSON file, but in a real world example, this file would be generated by a server for a specific situation. For composing and rendering, we'll use Adobe After Effects CC.</p>

<p>In this example, we are using a funny TV commercial by Vodafone. We haven't done any dynamic video cases for Vodafone (yet), so this is just a hypothetical example. Here's the original:</p>

<video width=""640"" height=""380"" controls="""" poster=""http://zandbak.acceptatie.bluemango.nl/geek/dynamic/Screen Shot 2015-11-02 at 20.27.21.png"">  
  <source src=""http://zandbak.acceptatie.bluemango.nl/geek/dynamic/commercial.mp4"" type=""video/mp4"">
</video>

<p>In this post, we'll add a section at the end of the video, showing a nice personalized offer for an iPhone 6. In a real-world retargeting example, we might want to show this video as a preroll to a user that just visited an iPhone 6 page on the Vodafone website. The dynamic video we're going to create will look something like this:</p>

<video width=""640"" height=""380"" controls="""" poster=""http://zandbak.acceptatie.bluemango.nl/geek/dynamic/Screen Shot 2015-11-02 at 20.27.21.png"">  
  <source src=""http://zandbak.acceptatie.bluemango.nl/geek/dynamic/commercial_after.mp4"" type=""video/mp4"">
</video>

<h3 id=""createthedatamodel"">Create the data model</h3>

<p>We want to show a specific proposition at the end of the video. Let's keep it simple for now, and just add the phone image and its name. The code should look this:</p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""com"">// Model.js</span><span class=""pln"">
</span><span class=""kwd"">var</span><span class=""pln""> proposition </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">  
    imageUrl</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""str"">'phone.png'</span><span class=""pun"">,</span><span class=""pln"">
    name</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""str"">'iPhone 6 Space Grey'</span><span class=""pln"">
</span><span class=""pun"">}</span></code></pre>

<p>In our demo, this file is static, but in the real world this data might be generated by some user interface, enabling a marketer to create dynamic videos. Or the data is generated by any trigger, like a scraper or an API call. </p>

<h3 id=""createplaceholdersinaftereffects"">Create placeholders in After Effects</h3>

<p>We need to add some elements that will act as placeholders for our data. As we want to show the name of the phone and its image, let's draw a text field and an image. There is nothing special going on here. To make it look good, let's draw a background as well, and maybe add an animation. For me it looks something like this: <br>
<img src=""http://oi63.tinypic.com/14x165l.jpg"" alt="""" class=""full-img""></p>

<p>My timeline now looks like this: <br>
<img src=""http://oi67.tinypic.com/34i1phv.jpg"" alt="""" class=""full-img""></p>

<h3 id=""filltheplaceholderswiththejsondata"">Fill the placeholders with the JSON data</h3>

<p>Now we've got our static video in place, let's add our JavaScript data model to the placeholders. Let's start with the name of the phone. Select your <code>title</code> layer and extend the text properties. We are looking for the <code>Source Text</code> property. We need to write an <a href=""https://helpx.adobe.com/after-effects/using/expression-basics.html#WS3878526689cb91655866c1103906c6dea-7a26a"">expression</a> to fill this property with our model data. </p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""com"">// Absolute path to our JavaScript model file</span><span class=""pln"">
</span><span class=""kwd"">var</span><span class=""pln""> pathToModel </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""str"">'/path/to/Model.js'</span><span class=""pun"">;</span><span class=""pln"">  
</span><span class=""com"">// Evaluate our model</span><span class=""pln"">
$</span><span class=""pun"">.</span><span class=""pln"">evalFile</span><span class=""pun"">(</span><span class=""pln"">pathToModel</span><span class=""pun"">)</span><span class=""pln"">
</span><span class=""com"">// Now the proposition is accessible, return the name property</span><span class=""pln"">
proposition</span><span class=""pun"">.</span><span class=""pln"">name  </span></code></pre>

<p>After you added this code to the Source Text expression, it looks like this: <br>
<img src=""http://oi63.tinypic.com/vf8rab.jpg"" alt="""" class=""full-img""></p>

<p>For now we just hard code the path to our data model in the After Effects file. Please note that this path cannot be relative.</p>

<h3 id=""renderthevideowiththecommandline"">Render the video with the command-line</h3>

<p>Of course, we could just render the video in the After Effects GUI, and we'd have a nice video based on a data file. But to make it a lot more useful, we want to render the video using code, and without even opening After Effects. So let's start to render the video using the command-line. </p>

<p>Rendering After Effects files without even opening After Effects is possible by using a file called <a href=""https://helpx.adobe.com/after-effects/using/automated-rendering-network-rendering.html"">aerenderer</a>. The executable file aerender.exe is a program with a command-line interface that allows you to automate rendering. The executable file is located in the same folder as the primary After Effects application. If you are working on a web server, you need to make sure you installed After Effects on that machine as well. </p>

<p>The default locations for this file are:</p>

<p>Windows: \Program Files\Adobe\Adobe After Effects CC\Support Files <br>
Mac OS: /Applications/Adobe After Effects CC</p>

<p>This executable accepts a lot of arguments, but for now we're interested in these three:</p>

<ul>
<li><strong>-comp comp<em>name</em></strong><em>: comp</em>name specifies a composition to render</li>
<li><strong>-project -project<em>path</em></strong><em>: project</em>path is a file path or URI specifying a project file to open. If this argument is not used, aerender works with the currently open project. If no project is specified and no project is open, the result is an error.</li>
<li><strong>-output outputpath</strong>: output_path is a file path or URI specifying the destination for the final output file. If this argument is not used, aerender uses the path defined in the project file.</li>
<li><strong>-OMtemplate output<em>module</em>template</strong>: output<em>module</em>template is the name of a template to apply to the output module. In our example, we'll just stick with h264. </li>
</ul>

<p>To render our previously created AE file, we'll enter this command in our terminal:</p>

<pre><code>earenderer -project template.aep -comp final -output video.mov -OMtemplate h264  
</code></pre>

<p>Please note that aerender starts a new instance of After Effects by default, even if one is already running. To use the currently running instance instead, add the <code>–reuse</code> argument.</p>

<h3 id=""usenodejstorenderthevideo"">Use NodeJS to render the video</h3>

<p>If we want to render the video in a web environment, we need some way to call the render command from a web server. Because NodeJS is our server technology of choice, we'll stick with NodeJS. We can call command line instructions in NodeJS using <code>spawn</code> on the <code>child_process</code> module. </p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""com"">// render.js</span><span class=""pln"">
</span><span class=""kwd"">var</span><span class=""pln""> spawn </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""kwd"">require</span><span class=""pun"">(</span><span class=""str"">'child_process'</span><span class=""pun"">).</span><span class=""pln"">spawn</span><span class=""pun"">;</span><span class=""pln"">

</span><span class=""kwd"">var</span><span class=""pln""> ae </span><span class=""pun"">=</span><span class=""pln""> spawn</span><span class=""pun"">(</span><span class=""str"">'/Applications/Adobe After Effects CC 2015/aerender'</span><span class=""pun"">,[</span><span class=""pln"">  
</span><span class=""str"">'-project'</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""str"">'template.aep'</span><span class=""pun"">,</span><span class=""pln"">  
</span><span class=""str"">'-comp'</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""str"">'final'</span><span class=""pun"">,</span><span class=""pln"">  
</span><span class=""str"">'-output'</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""str"">'movie.mov,  
'</span><span class=""pun"">-</span><span class=""typ"">OMtemplate</span><span class=""str"">', '</span><span class=""pln"">h264</span><span class=""str"">'  
]);

ae.stderr.on('</span><span class=""pln"">data</span><span class=""str"">', function (data) {  
// Error occured
console.log('</span><span class=""pln"">stderr</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""str"">' + data);  
});

ae.on('</span><span class=""pln"">close</span><span class=""str"">', function (code) {  
// Video has rendered
});</span></code></pre>

<p>We can now run our index.js with Node:  </p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""pln"">node render</span><span class=""pun"">.</span><span class=""pln"">js  </span></code></pre>

<p>That's it. We've just rendered a dynamic video using After Effects and JavaScript. Nice!</p>

<h3 id=""nextsteps"">Next steps</h3>

<p>If you just want to render a video once in a while, this solution will do the trick. We got everything working for a single video on a local machine, but the real challenge is scalability and infrastructure. If you'd like to render thousands of videos and even do it in a parallel fashion, you'll need some really powerful machines, and start thinking about building a render farm. We'll get to that in a future post. </p>

<p>Eventually, we'd like to serve custom made videos for a single unique visitor on the fly, but so far none of the big advertising platforms support the real-time serving of videos. We're talking to some parties about this, so I expect to see some cool things happen in 2016. </p>

<p>Thanks to my brilliant friend and colleague Niels Wijers, who first came up with this proof of concept. All credits go to him.</p>
        ","In digital marketing, there is a lot of buzz around dynamic videos. You may have noticed all the start-ups popping up in this scene. Dynamic is nothing new in the world of display advertising. We already see a lot of dynamic ads for retargeting by big parties like Criteo, but when it comes to online video, there aren't a lot of dynamic video cases (that we know of). We believe there are a couple of reasons for this, but we'll get into that in another post. In this post, we'll focus on the technical aspects of creating a dynamic video.
What's dynamic video anyway?
There are a lot of (buzz) words flying around: interactive video, dynamic video, personalized video etc. And misconceptions, too. For instance: clickable videos, like those with info cards or in-video link-buttons, are interactive, not dynamic. A couple of years ago it was cool (mostly in social) to personalize animations using Flash or HTML5 Canvas. That's not dynamic video either, that's a personalized web animation.
To me, a video becomes dynamic when a specific video is rendered, pre-rendered or on-the-fly, for a specific context. That context may apply to a big group of people with the same interests, or to a single user. Another alternative term would be personalized video.
I'm aware that these definitions are both pretty technical. If you have a better one, please feel free to share it in the comment section below.
One of the advantages of creating a dynamic video is that you don't have to spend a lot of money on production when you want to change a specific part of your video. You can theoretically create thousands of variations of one video without touching any video editing software or asking your production party.
Let's create a dynamic video
Enough with the definitions. In this post, I'd like to show how you can add a dynamic section to an existing video and render it using code. What we put in the dynamic section can be configured in a JSON file. In our example, we use a static JSON file, but in a real world example, this file would be generated by a server for a specific situation. For composing and rendering, we'll use Adobe After Effects CC.
In this example, we are using a funny TV commercial by Vodafone. We haven't done any dynamic video cases for Vodafone (yet), so this is just a hypothetical example. Here's the original:
In this post, we'll add a section at the end of the video, showing a nice personalized offer for an iPhone 6. In a real-world retargeting example, we might want to show this video as a preroll to a user that just visited an iPhone 6 page on the Vodafone website. The dynamic video we're going to create will look something like this:
Create the data model
We want to show a specific proposition at the end of the video. Let's keep it simple for now, and just add the phone image and its name. The code should look this:
// Model.js
var proposition = {  
    imageUrl: 'phone.png',
    name: 'iPhone 6 Space Grey'
}
In our demo, this file is static, but in the real world this data might be generated by some user interface, enabling a marketer to create dynamic videos. Or the data is generated by any trigger, like a scraper or an API call.
Create placeholders in After Effects
We need to add some elements that will act as placeholders for our data. As we want to show the name of the phone and its image, let's draw a text field and an image. There is nothing special going on here. To make it look good, let's draw a background as well, and maybe add an animation. For me it looks something like this:
My timeline now looks like this:
Fill the placeholders with the JSON data
Now we've got our static video in place, let's add our JavaScript data model to the placeholders. Let's start with the name of the phone. Select your title layer and extend the text properties. We are looking for the Source Text property. We need to write an expression to fill this property with our model data.
// Absolute path to our JavaScript model file
var pathToModel = '/path/to/Model.js';  
// Evaluate our model
$.evalFile(pathToModel)
// Now the proposition is accessible, return the name property
proposition.name  
After you added this code to the Source Text expression, it looks like this:
For now we just hard code the path to our data model in the After Effects file. Please note that this path cannot be relative.
Render the video with the command-line
Of course, we could just render the video in the After Effects GUI, and we'd have a nice video based on a data file. But to make it a lot more useful, we want to render the video using code, and without even opening After Effects. So let's start to render the video using the command-line.
Rendering After Effects files without even opening After Effects is possible by using a file called aerenderer. The executable file aerender.exe is a program with a command-line interface that allows you to automate rendering. The executable file is located in the same folder as the primary After Effects application. If you are working on a web server, you need to make sure you installed After Effects on that machine as well.
The default locations for this file are:
Windows: \Program Files\Adobe\Adobe After Effects CC\Support Files
Mac OS: /Applications/Adobe After Effects CC
This executable accepts a lot of arguments, but for now we're interested in these three:
-comp compname: compname specifies a composition to render
-project -projectpath: projectpath is a file path or URI specifying a project file to open. If this argument is not used, aerender works with the currently open project. If no project is specified and no project is open, the result is an error.
-output outputpath: output_path is a file path or URI specifying the destination for the final output file. If this argument is not used, aerender uses the path defined in the project file.
-OMtemplate outputmoduletemplate: outputmoduletemplate is the name of a template to apply to the output module. In our example, we'll just stick with h264.
To render our previously created AE file, we'll enter this command in our terminal:
earenderer -project template.aep -comp final -output video.mov -OMtemplate h264  
Please note that aerender starts a new instance of After Effects by default, even if one is already running. To use the currently running instance instead, add the –reuse argument.
Use NodeJS to render the video
If we want to render the video in a web environment, we need some way to call the render command from a web server. Because NodeJS is our server technology of choice, we'll stick with NodeJS. We can call command line instructions in NodeJS using spawn on the child_process module.
// render.js
var spawn = require('child_process').spawn;

var ae = spawn('/Applications/Adobe After Effects CC 2015/aerender',[  
'-project', 'template.aep',  
'-comp', 'final',  
'-output', 'movie.mov,  
'-OMtemplate', 'h264'  
]);

ae.stderr.on('data', function (data) {  
// Error occured
console.log('stderr: ' + data);  
});

ae.on('close', function (code) {  
// Video has rendered
});
We can now run our index.js with Node:
node render.js  
That's it. We've just rendered a dynamic video using After Effects and JavaScript. Nice!
Next steps
If you just want to render a video once in a while, this solution will do the trick. We got everything working for a single video on a local machine, but the real challenge is scalability and infrastructure. If you'd like to render thousands of videos and even do it in a parallel fashion, you'll need some really powerful machines, and start thinking about building a render farm. We'll get to that in a future post.
Eventually, we'd like to serve custom made videos for a single unique visitor on the fly, but so far none of the big advertising platforms support the real-time serving of videos. We're talking to some parties about this, so I expect to see some cool things happen in 2016.
Thanks to my brilliant friend and colleague Niels Wijers, who first came up with this proof of concept. All credits go to him.","[Code, Video]"
77,One JavaScript bookmark to make checking tags easier,/one-javascript-bookmark-to-make-checking-tags-easier/,"
            <p>Last Friday, my friend and colleague <a href=""http://www.themarketingtechnologist.co/author/bart-persoons/"">Bart Persoons</a> and I got a request to do a double-check on some pixels. While checking, we thought of a solution that would let less technical people do a double-check by themselves more easily. The result? A JavaScript bookmark.</p>

<h2 id=""whydoweneedthis"">Why do we need this?</h2>

<p>As a web analyst with a technical background, I’m familiar with using developer tools to check if a tag is loaded onto a page. But other less technical people, are less familiar with the checking process. Normally it takes roughly 4 steps:</p>

<ul>
<li>Go to a page</li>
<li>hit F12</li>
<li>hit the network tab</li>
<li>filter on your tag and see if it appears</li>
</ul>

<p>Oh, and don’t forget to refresh the page first</p>

<p>Most platforms have their own testing tools (e.g. <a href=""https://support.google.com/tagassistant/answer/2947093?hl=en"">Google's Tag Assistant</a>), but we thought there might be room for a general simplified one. So, let’s get to it.</p>

<h2 id=""thetagcheckerbookmarkfunctionality"">The Tag Checker bookmark functionality</h2>

<p>The bookmark just has to do one simple thing: check the website for a tag. So all the tag does is check the body of the website for something you tell it to check:</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2015/Nov/geek_tagchecker_crazyegg-1448829045390.PNG"" alt=""Tag checker bookmark crazyegg check""></p>

<p>In this example, I’m just checking for Crazy Egg base URL. You can make this as specific as you want and check the complete URL of the tag. If you hit okay, an alert will tell you if your input is present on the website:</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2015/Nov/geek_tagchecker_bookmark_response-1448829086062.PNG"" alt=""Tag checker bookmark response""></p>

<p>That’s all there’s to it. </p>

<h2 id=""thetagcheckerbookmarksnippet"">The Tag Checker bookmark snippet</h2>

<p>All there's left to do is add the Tag Checker code to a bookmark. Create a new bookmark and use the code below as the page URL: </p>

<pre><code>javascript:(function()%7Bvar%20question%20%3D%20%22Fill%20out%20your%20tag%20URL%3A%22%3Btxt%20%3D%20prompt(question)%3Bresponse%20%3D%20'Not%20present'%3Bif%20(txt%20!%3D%20null)%20%7Btxt%20%3D%20txt.replace(%2F%26%2Fg%2C%20%22%26amp%3B%22)%3Bif(document.body.innerHTML.indexOf(txt)%20!%3D%3D%20-1)%7Bresponse%3D'Present'%3B%7D%7Dtimg%20%3D%20new%20Image()%3Btstring%20%3D%20'https%3A%2F%2Fssl.google-analytics.com%2Fcollect%3Fv%3D1%26tid%3DUA-59068737-3%26z%3D'%2Bnew%20Date().getTime()%2B'%26cid%3D'%2Bdocument.location.hostname%2B'%26t%3Devent%26ec%3DPixelCheck%26ea%3D'%2Bdocument.location.host%2B'%26el%3D'%2Bresponse%2B'%26cd1%3D'%2Btxt%3Bdocument.body.appendChild(timg)%3Btimg.src%20%3D%20tstring%3Balert(response)%3Bvoid(0)%7D)()  
</code></pre>

<p><em>Note that this bookmark code contains tracking with Google Analytics. If you don't want your Tag Checker usage to be tracked, read the <strong>Tracking</strong> part further down in this post.</em></p>

<p>In Google Chrome, adding the bookmark looks like this:</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2015/Nov/geek_tagchecker_bookmark-1448829546130.PNG"" alt=""Tag Checker bookmark insert""></p>

<p><em>Adding the bookmark to Google Chrome. Fill out the JavaScript code in the URL input field.</em></p>

<h2 id=""thetagcheckerbookmarkcode"">The Tag Checker bookmark code</h2>

<p>The code behind the bookmark is pretty basic. Let’s look at the beautified JavaScript code snippet:</p>

<pre><code>var question = ""Fill out your tag URL:"";  
    txt = prompt(question);
    response = 'Not present';
    if (txt != null) {      
      txt = txt.replace(/&amp;/g, ""&amp;amp;"");
      if(document.body.innerHTML.indexOf(txt) !== -1){response='Present';}
    }
    timg = new Image();
    tstring = 'https://ssl.google-analytics.com/collect?v=1&amp;tid=UA-59068737-3&amp;z='+new Date().getTime()+'&amp;cid='+document.location.hostname+'&amp;t=event&amp;ec=PixelCheck&amp;ea='+document.location.host+'&amp;el='+response+'&amp;cd1='+txt;
    document.body.appendChild(timg);
    timg.src = tstring;
    alert(response);
void(0);  
</code></pre>

<p>The code has four basic parts:</p>

<ul>
<li><strong>Prompt</strong>: the bookmark asks you for input: we choose to ask for a tag URL, as the HTML output of a tag may very from system to system.</li>
<li><strong>Check for the tag URL</strong>: we check if the text of the input field has an index in the body of the website. If so, it's 0 or higher and present. If not, it's -1 and not present. If your tag isn't placed directly in the body of the website, the bookmark won't find the tag.</li>
<li><strong>Tracking</strong>: as web analyst, we like to measure everything. So to measure the use of this bookmark, we've added tracking with the <a href=""https://developers.google.com/analytics/devguides/collection/protocol/v1/"">Google Analytics Measurement Protocol</a>. We track the response (Present or Not present), the URL that the bookmark is used on, and the input that you want to check. If you do not want to be tracked, remove the lines from <code>timg = new Image();</code> to <code>timg.src = tstring;</code> and regenerate the bookmark via <a href=""http://mrcoles.com/bookmarklet/"">mrcoles.com/bookmarklet</a>.</li>
<li><strong>Response</strong>: alert the response to the user.</li>
</ul>

<h2 id=""empoweryourcolleaguesandpartners"">Empower your colleagues and partners</h2>

<p>If you’re someone who manages tags, and want to allow your colleagues or partners to easily check their tags without the need to login or enable debugging, share them this bookmark. If you're someone who briefs a lot of tags to other companies or websites, use this to check if the tags are on the page you need them to be. </p>
        ","Last Friday, my friend and colleague Bart Persoons and I got a request to do a double-check on some pixels. While checking, we thought of a solution that would let less technical people do a double-check by themselves more easily. The result? A JavaScript bookmark.
Why do we need this?
As a web analyst with a technical background, I’m familiar with using developer tools to check if a tag is loaded onto a page. But other less technical people, are less familiar with the checking process. Normally it takes roughly 4 steps:
Go to a page
hit F12
hit the network tab
filter on your tag and see if it appears
Oh, and don’t forget to refresh the page first
Most platforms have their own testing tools (e.g. Google's Tag Assistant), but we thought there might be room for a general simplified one. So, let’s get to it.
The Tag Checker bookmark functionality
The bookmark just has to do one simple thing: check the website for a tag. So all the tag does is check the body of the website for something you tell it to check:
In this example, I’m just checking for Crazy Egg base URL. You can make this as specific as you want and check the complete URL of the tag. If you hit okay, an alert will tell you if your input is present on the website:
That’s all there’s to it.
The Tag Checker bookmark snippet
All there's left to do is add the Tag Checker code to a bookmark. Create a new bookmark and use the code below as the page URL:
javascript:(function()%7Bvar%20question%20%3D%20%22Fill%20out%20your%20tag%20URL%3A%22%3Btxt%20%3D%20prompt(question)%3Bresponse%20%3D%20'Not%20present'%3Bif%20(txt%20!%3D%20null)%20%7Btxt%20%3D%20txt.replace(%2F%26%2Fg%2C%20%22%26amp%3B%22)%3Bif(document.body.innerHTML.indexOf(txt)%20!%3D%3D%20-1)%7Bresponse%3D'Present'%3B%7D%7Dtimg%20%3D%20new%20Image()%3Btstring%20%3D%20'https%3A%2F%2Fssl.google-analytics.com%2Fcollect%3Fv%3D1%26tid%3DUA-59068737-3%26z%3D'%2Bnew%20Date().getTime()%2B'%26cid%3D'%2Bdocument.location.hostname%2B'%26t%3Devent%26ec%3DPixelCheck%26ea%3D'%2Bdocument.location.host%2B'%26el%3D'%2Bresponse%2B'%26cd1%3D'%2Btxt%3Bdocument.body.appendChild(timg)%3Btimg.src%20%3D%20tstring%3Balert(response)%3Bvoid(0)%7D)()  
Note that this bookmark code contains tracking with Google Analytics. If you don't want your Tag Checker usage to be tracked, read the Tracking part further down in this post.
In Google Chrome, adding the bookmark looks like this:
Adding the bookmark to Google Chrome. Fill out the JavaScript code in the URL input field.
The Tag Checker bookmark code
The code behind the bookmark is pretty basic. Let’s look at the beautified JavaScript code snippet:
var question = ""Fill out your tag URL:"";  
    txt = prompt(question);
    response = 'Not present';
    if (txt != null) {      
      txt = txt.replace(/&/g, ""&amp;"");
      if(document.body.innerHTML.indexOf(txt) !== -1){response='Present';}
    }
    timg = new Image();
    tstring = 'https://ssl.google-analytics.com/collect?v=1&tid=UA-59068737-3&z='+new Date().getTime()+'&cid='+document.location.hostname+'&t=event&ec=PixelCheck&ea='+document.location.host+'&el='+response+'&cd1='+txt;
    document.body.appendChild(timg);
    timg.src = tstring;
    alert(response);
void(0);  
The code has four basic parts:
Prompt: the bookmark asks you for input: we choose to ask for a tag URL, as the HTML output of a tag may very from system to system.
Check for the tag URL: we check if the text of the input field has an index in the body of the website. If so, it's 0 or higher and present. If not, it's -1 and not present. If your tag isn't placed directly in the body of the website, the bookmark won't find the tag.
Tracking: as web analyst, we like to measure everything. So to measure the use of this bookmark, we've added tracking with the Google Analytics Measurement Protocol. We track the response (Present or Not present), the URL that the bookmark is used on, and the input that you want to check. If you do not want to be tracked, remove the lines from timg = new Image(); to timg.src = tstring; and regenerate the bookmark via mrcoles.com/bookmarklet.
Response: alert the response to the user.
Empower your colleagues and partners
If you’re someone who manages tags, and want to allow your colleagues or partners to easily check their tags without the need to login or enable debugging, share them this bookmark. If you're someone who briefs a lot of tags to other companies or websites, use this to check if the tags are on the page you need them to be.","[Analytics, Tools, bookmark]"
78,Get more out of the Google Analytics app by showing less data,/get-more-out-of-the-google-analytics-app-by-showing-less-data/,"
            <p>Google Analytics has a lot of great reports, but sometimes you’ll only want to see a small subset of your data. For me, that ‘sometimes’ is when I’m using the official Google Analytics app to see how my project is doing.</p>

<h2 id=""creatingacustomviewforoptimaluseofthegoogleanalyticsmobileapp"">Creating a custom view for optimal use of the Google Analytics mobile app</h2>

<p>Usually you’ll create a custom report or dashboard to look at a subset of your data. But on your mobile phone, speed is key. So all I want to see is how my project is doing on a higher level, and get to this data as fast as possible. <br>
By default, the app shows you four widgets:</p>

<ul>
<li><strong>Real-time</strong>: the amount of users currently on your website.</li>
<li><strong>Audience</strong>: the total amount of sessions of your selected time period.</li>
<li><strong>Acquisition</strong>: the traffic sources of your sessions.</li>
<li><strong>Behaviour</strong>: the amount of pageviews.</li>
<li><strong>Conversions</strong>: the goal completions.</li>
</ul>

<p>To get to the data as quickly as possible, you’ll want to see the right data in this view instantly. For a quick view of your data, the collection of widgets is almost perfect. </p>

<p>The real-time part quickly shows me what’s going on. As I know my project inside out, I’ll instantly detect a spike in traffic. The audience part tells me how much traffic I had for the selected date range. Acquisition shows me how this traffic got to my site. The pageview data in the behaviour tab is something I ignore, because the total amount of pageviews don't say anything in my project. And finally, the conversions, is where the app starts to shine if you set up a separate view. </p>

<h2 id=""pickamaingoaltomaketheanalyticsappshine"">Pick a main goal to make the analytics app shine</h2>

<p>A standard Google Analytics view allows you to set up 20 goals. If you set up multiple goals, the conversions widget will show if users completed either one of these goals. You’ll have to tap the widget, dive into the conversions report of the app, and filter the correct goal to see the stats for one specific goal. That’s a step I don’t want to take. </p>

<p>To minimize the steps you’ll have to take, copy your main profile, rename it with something like <em>'GA app view’</em>, and turn off all goals except your main one. Now, if you select that profile with the GA app, the conversions tab on the starting screen will instantly show you how many completions that one goal had. No deep dive required.</p>

<p>For my app, I use it to track successful app uses, the view of a results page. Depending on your business, it can also show you transactions or submitted lead forms. You can always go back to your main view that has all the goals to further look into your data.</p>

<h2 id=""theresult"">The result</h2>

<p>To show you how I personally use the app, here are some screenshots for the ‘GA app view’ profile of my current project (a webapp):</p>

<p><strong>Realtime</strong></p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2015/Nov/geek_app_realtime-1448524364215.PNG"" alt=""GA App Realtime""></p>

<p>This number instantly shows if there are active users on my website right now. Normally, traffic peaks are around 7 or 8 in the morning and at 4 or 5 in the afternoon. I can quickly see how the app is doing at those two moments. The downside to this view is that all visits appear to be 'NEW', but my other data clearly tells me the greater part of app users is 'RETURNING'. So there's a little bug in this view. </p>

<p><strong>Audience</strong></p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2015/Nov/geek_app_sessions-1448524400948.png"" alt=""GA App Audience""></p>

<p>The audience part shows me the total sessions of my selected day period. I normally select ‘Today’ as the default, as I track my website and webapp performance throughout the day. I check this daily, so I know how the numbers compare to other days. As you can see, the numbers aren’t that high yet. </p>

<p><strong>Acquisition</strong></p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2015/Nov/geek_app_acq-1448524443885.PNG"" alt=""GA App Acquisition""></p>

<p>Of course, as an analyst, I also want to know where the sessions came from. Normally the Google Play (I have a converted webapp for Android) and direct (for the default webapp) show up here. On an acquisition day, which are days that have an influential social share, post on my website or blogposts on other sites, I’ll see other sources pop up, e.g. Facebook, LinkedIn or a referring domain. </p>

<p><strong>Behaviour (I ignore this)</strong></p>

<p><strong>Conversions</strong></p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2015/Nov/geek_app_conv-1448524465845.PNG"" alt=""GA App Conversions""></p>

<p>My app shows the time remaining until your next train departure (for the Dutch railway services). So as a main goal, I've set up the results page of those train times. This either appears after opening the app and configuring your settings, or instantly for returning visitors. It allows me to see how many of my session resulted in successful app uses, for both returning and new users. </p>

<p>So for this day, I had 31 sessions (at the time of the screenshot), 23 successful app uses, and 3 new app uses (oh yeah!). So even though the numbers aren't high, the quality of traffic is good: 23 out of 31 sessions were successful app uses. </p>

<h2 id=""finalnote"">Final note</h2>

<p>Of course, this might not be as useful when you have millions of sessions a day. But for your new projects, smaller projects, or for managing separate departments of your site, this is a quick &amp; easy way to glance at the most important data in the Google Analytics app. </p>

<p>The Google Analytics app is available in the <a href=""https://itunes.apple.com/nl/app/google-analytics/id881599038?mt=8"">Apple App Store</a> and <a href=""https://play.google.com/store/apps/details?id=com.google.android.apps.giant&amp;hl=nl"">Google Play</a>.</p>
        ","Google Analytics has a lot of great reports, but sometimes you’ll only want to see a small subset of your data. For me, that ‘sometimes’ is when I’m using the official Google Analytics app to see how my project is doing.
Creating a custom view for optimal use of the Google Analytics mobile app
Usually you’ll create a custom report or dashboard to look at a subset of your data. But on your mobile phone, speed is key. So all I want to see is how my project is doing on a higher level, and get to this data as fast as possible.
By default, the app shows you four widgets:
Real-time: the amount of users currently on your website.
Audience: the total amount of sessions of your selected time period.
Acquisition: the traffic sources of your sessions.
Behaviour: the amount of pageviews.
Conversions: the goal completions.
To get to the data as quickly as possible, you’ll want to see the right data in this view instantly. For a quick view of your data, the collection of widgets is almost perfect.
The real-time part quickly shows me what’s going on. As I know my project inside out, I’ll instantly detect a spike in traffic. The audience part tells me how much traffic I had for the selected date range. Acquisition shows me how this traffic got to my site. The pageview data in the behaviour tab is something I ignore, because the total amount of pageviews don't say anything in my project. And finally, the conversions, is where the app starts to shine if you set up a separate view.
Pick a main goal to make the analytics app shine
A standard Google Analytics view allows you to set up 20 goals. If you set up multiple goals, the conversions widget will show if users completed either one of these goals. You’ll have to tap the widget, dive into the conversions report of the app, and filter the correct goal to see the stats for one specific goal. That’s a step I don’t want to take.
To minimize the steps you’ll have to take, copy your main profile, rename it with something like 'GA app view’, and turn off all goals except your main one. Now, if you select that profile with the GA app, the conversions tab on the starting screen will instantly show you how many completions that one goal had. No deep dive required.
For my app, I use it to track successful app uses, the view of a results page. Depending on your business, it can also show you transactions or submitted lead forms. You can always go back to your main view that has all the goals to further look into your data.
The result
To show you how I personally use the app, here are some screenshots for the ‘GA app view’ profile of my current project (a webapp):
Realtime
This number instantly shows if there are active users on my website right now. Normally, traffic peaks are around 7 or 8 in the morning and at 4 or 5 in the afternoon. I can quickly see how the app is doing at those two moments. The downside to this view is that all visits appear to be 'NEW', but my other data clearly tells me the greater part of app users is 'RETURNING'. So there's a little bug in this view.
Audience
The audience part shows me the total sessions of my selected day period. I normally select ‘Today’ as the default, as I track my website and webapp performance throughout the day. I check this daily, so I know how the numbers compare to other days. As you can see, the numbers aren’t that high yet.
Acquisition
Of course, as an analyst, I also want to know where the sessions came from. Normally the Google Play (I have a converted webapp for Android) and direct (for the default webapp) show up here. On an acquisition day, which are days that have an influential social share, post on my website or blogposts on other sites, I’ll see other sources pop up, e.g. Facebook, LinkedIn or a referring domain.
Behaviour (I ignore this)
Conversions
My app shows the time remaining until your next train departure (for the Dutch railway services). So as a main goal, I've set up the results page of those train times. This either appears after opening the app and configuring your settings, or instantly for returning visitors. It allows me to see how many of my session resulted in successful app uses, for both returning and new users.
So for this day, I had 31 sessions (at the time of the screenshot), 23 successful app uses, and 3 new app uses (oh yeah!). So even though the numbers aren't high, the quality of traffic is good: 23 out of 31 sessions were successful app uses.
Final note
Of course, this might not be as useful when you have millions of sessions a day. But for your new projects, smaller projects, or for managing separate departments of your site, this is a quick & easy way to glance at the most important data in the Google Analytics app.
The Google Analytics app is available in the Apple App Store and Google Play.","[app, growth, google analytics, Analytics]"
79,A moving object in your campaign? Update Adwords accordingly every hour.,/update-your-adwords-geolocation-targeting-every-hour-using-adwords-scripts/,"
            <p>We are running a campaign for one of our clients that is about a fluffy heart with GPS hardware that is travelling through the country. It looks something like this:</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2015/Nov/IMG_3886_JPG-1448485313159.jpeg"" alt=""""></p>

<p>We'll get deeper into this campaign in another post on The Marketing Technologist, but for now it's enough to know that for every kilometre the heart travels our client will donate 10 euro to a good cause.  </p>

<p>We are already using the GPS location, which was made available using an public API, in our display ads (in both our creatives and targeting). But we are also doing some SEA for this campaign, so why not also use the GPS coordinates in Adwords? </p>

<p>We want to achieve two things:</p>

<ul>
<li>Only target people that are in a radius of 20 kilometres around the heart.</li>
<li>Show the distance the heart has already travelled so far, and the current location of the heart. </li>
</ul>

<p>In this post I'll talk about the first goal. Let's get started. </p>

<h3 id=""findthecampaign"">Find the campaign</h3>

<p>Go to the Scripts section in the Adwords Web application (<code>Bulk operations</code> &gt; <code>Scripts</code>). Before we can update the campaign's targeting location, let's find our campaign first.  </p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""com"">// What's the name of the campaign?</span><span class=""pln"">
</span><span class=""kwd"">var</span><span class=""pln""> campaignName </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""str"">'your campaign name'</span><span class=""pun"">;</span><span class=""pln"">

</span><span class=""kwd"">function</span><span class=""pln""> main</span><span class=""pun"">()</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">  
    </span><span class=""com"">// Find our campaign</span><span class=""pln"">
    </span><span class=""kwd"">var</span><span class=""pln""> campaignIterator </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""typ"">AdWordsApp</span><span class=""pun"">.</span><span class=""pln"">campaigns</span><span class=""pun"">()</span><span class=""pln"">
            </span><span class=""pun"">.</span><span class=""pln"">withCondition</span><span class=""pun"">(</span><span class=""typ"">Utilities</span><span class=""pun"">.</span><span class=""pln"">formatString</span><span class=""pun"">(</span><span class=""str"">'CampaignName = ""%s""'</span><span class=""pun"">,</span><span class=""pln""> campaignName</span><span class=""pun"">))</span><span class=""pln"">
            </span><span class=""pun"">.</span><span class=""kwd"">get</span><span class=""pun"">();</span><span class=""pln"">

    </span><span class=""kwd"">while</span><span class=""pln""> </span><span class=""pun"">(</span><span class=""pln"">campaignIterator</span><span class=""pun"">.</span><span class=""pln"">hasNext</span><span class=""pun"">())</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">
        </span><span class=""kwd"">var</span><span class=""pln""> campaign </span><span class=""pun"">=</span><span class=""pln""> campaignIterator</span><span class=""pun"">.</span><span class=""kwd"">next</span><span class=""pun"">();</span><span class=""pln"">
    </span><span class=""pun"">}</span><span class=""pln"">
</span><span class=""pun"">}</span></code></pre>

<p>Now the variable <code>campaign</code> refers to your campaign. In our scenario we have only one campaign that matches our campaign name. </p>

<h3 id=""updatethecampaignstargetinglocation"">Update the campaign's targeting location</h3>

<p>When you want to get the proximate location (remember we want to target with a radius), we need to use <code>targetedProximities()</code> of your campaign's <code>targeting()</code> method. Unfortunately there is no good way to update a location, so we first have to remove the existing ones. To do this we have to get all the locations and call the <code>remove</code> method on them one by one.</p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""com"">// Get our current targeted proximity locations</span><span class=""pln"">
</span><span class=""kwd"">var</span><span class=""pln""> locations </span><span class=""pun"">=</span><span class=""pln""> campaign</span><span class=""pun"">.</span><span class=""pln"">targeting</span><span class=""pun"">().</span><span class=""pln"">targetedProximities</span><span class=""pun"">().</span><span class=""kwd"">get</span><span class=""pun"">();</span><span class=""pln"">

</span><span class=""com"">// Loop through all locations</span><span class=""pln"">
</span><span class=""kwd"">while</span><span class=""pun"">(</span><span class=""pln"">locations</span><span class=""pun"">.</span><span class=""pln"">hasNext</span><span class=""pun"">())</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">  
    </span><span class=""kwd"">var</span><span class=""pln""> location </span><span class=""pun"">=</span><span class=""pln""> locations</span><span class=""pun"">.</span><span class=""kwd"">next</span><span class=""pun"">();</span><span class=""pln"">
    </span><span class=""com"">// Remove the existing proximity location</span><span class=""pln"">
    location</span><span class=""pun"">.</span><span class=""pln"">remove</span><span class=""pun"">();</span><span class=""pln"">
</span><span class=""pun"">}</span></code></pre>

<p>Now your campaign has no more proximity locations. It's time to add our new location. We have to add this location by using its lattitude/longitude coordinates. It's easy to get these coordinates for an address using a service like <a href=""http://www.latlong.net"">http://www.latlong.net</a>. </p>

<p>We can now add the coordinates to the campaign using <code>addProximity</code>. The first two arguments of this function are the lattitude and longitude values. The third one is the radius, followed by the unit of the radius (<code>KILOMETRES</code> or <code>MILES</code>). Make sure your coordinates are both numbers.  </p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""kwd"">var</span><span class=""pln""> radius </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""lit"">10</span><span class=""pun"">;</span><span class=""pln"">  
</span><span class=""kwd"">var</span><span class=""pln""> lat </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""lit"">51.507351</span><span class=""pun"">;</span><span class=""pln"">  
</span><span class=""kwd"">var</span><span class=""pln""> lng </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""pun"">-</span><span class=""lit"">0.127758</span><span class=""pun"">;</span><span class=""pln"">

campaign</span><span class=""pun"">.</span><span class=""pln"">addProximity</span><span class=""pun"">(</span><span class=""pln"">lat</span><span class=""pun"">,</span><span class=""pln""> lng</span><span class=""pun"">,</span><span class=""pln""> radius</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""str"">'KILOMETRES'</span><span class=""pun"">)</span><span class=""pln"">  </span></code></pre>

<p>Save your script and preview it. Your preview panel should look something like this. </p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2015/Nov/Screen_Shot_2015_11_25_at_21_15_23-1448482573106.png"" alt="""" class=""full-img""></p>

<p>We've just changed your campaign's location targeting with code. Nice!</p>

<h3 id=""getlocationdatafromaremotesource"">Get location data from a remote source</h3>

<p>Although it's fun to write these scripts, it's not really useful to hardcode those lat/lng coordinates. In our case we have these data available through an API, so let's see how we can use this data in our script.</p>

<p>I've seen some people use a spread sheet to read an API, and load the spread sheet with the API in Adwords. I'm not going to do that; I want to load the API directly in our code. </p>

<p>Our API returns this JSON data:</p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""pun"">{</span><span class=""pln"">
  lat</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""lit"">51.507351</span><span class=""pun"">,</span><span class=""pln"">
  lng</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""pun"">-</span><span class=""lit"">0.127758</span><span class=""pun"">,</span><span class=""pln"">
  city</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""str"">'London'</span><span class=""pun"">,</span><span class=""pln"">
  distance</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""lit"">22304</span><span class=""pln"">
</span><span class=""pun"">}</span></code></pre>

<p>We can read this API using the <code>fetch</code> method of <code>UrlFetchApp</code>. We only need to pass the URL of our API. </p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""kwd"">var</span><span class=""pln""> url </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""str"">'http://yourapi'</span><span class=""pun"">;</span><span class=""pln"">  
</span><span class=""kwd"">var</span><span class=""pln""> response </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""typ"">UrlFetchApp</span><span class=""pun"">.</span><span class=""pln"">fetch</span><span class=""pun"">(</span><span class=""pln"">url</span><span class=""pun"">);</span><span class=""pln"">  </span></code></pre>

<p>To easily get the values we can parse the response text to JSON.</p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""kwd"">var</span><span class=""pln""> data </span><span class=""pun"">=</span><span class=""pln""> JSON</span><span class=""pun"">.</span><span class=""pln"">parse</span><span class=""pun"">(</span><span class=""pln"">response</span><span class=""pun"">.</span><span class=""pln"">getContentText</span><span class=""pun"">());</span><span class=""pln"">  </span></code></pre>

<p>Now we can read the lat and long like this:  </p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""kwd"">var</span><span class=""pln""> lat </span><span class=""pun"">=</span><span class=""pln""> data</span><span class=""pun"">.</span><span class=""pln"">lat</span><span class=""pun"">;</span><span class=""pln"">  
</span><span class=""kwd"">var</span><span class=""pln""> lng </span><span class=""pun"">=</span><span class=""pln""> data</span><span class=""pun"">.</span><span class=""pln"">lng</span><span class=""pun"">;</span><span class=""pln"">  </span></code></pre>

<p>Awesome. We also have to handle any errors that may occur, like a 404 when the API is not available. We want to check if the <a href=""http://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html"">HTTP response code</a> is not 200 ('Response is OK'), and print an error otherwise. </p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""kwd"">if</span><span class=""pln""> </span><span class=""pun"">(</span><span class=""pln"">response</span><span class=""pun"">.</span><span class=""pln"">getResponseCode</span><span class=""pun"">()</span><span class=""pln""> </span><span class=""pun"">!=</span><span class=""pln""> </span><span class=""lit"">200</span><span class=""pun"">)</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">  
        </span><span class=""kwd"">throw</span><span class=""pln""> </span><span class=""typ"">Utilities</span><span class=""pun"">.</span><span class=""pln"">formatString</span><span class=""pun"">(</span><span class=""pln"">
                </span><span class=""str"">'Error returned by API: %s, Location searched: %s.'</span><span class=""pun"">,</span><span class=""pln"">
                </span><span class=""pun"">,</span><span class=""pln""> location</span><span class=""pun"">);</span><span class=""pln"">
    </span><span class=""pun"">}</span></code></pre>

<h3 id=""thecompletescript"">The complete script</h3>

<p>We've written a script that updates a campaign's targeting location based on data that is loaded from an external data source. Our complete script looks like this:</p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""com"">// What's the name of the campaign?</span><span class=""pln"">
</span><span class=""kwd"">var</span><span class=""pln""> campaignName </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""str"">'your campaign name'</span><span class=""pun"">;</span><span class=""pln"">

</span><span class=""com"">// Url of the API</span><span class=""pln"">
</span><span class=""kwd"">var</span><span class=""pln""> url </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""str"">'http://yourapi'</span><span class=""pln"">

</span><span class=""com"">// The radius in KM around the location you want to use in the campaign targeting</span><span class=""pln"">
</span><span class=""kwd"">var</span><span class=""pln""> radius </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""lit"">20</span><span class=""pun"">;</span><span class=""pln"">

</span><span class=""kwd"">function</span><span class=""pln""> getTheLocationOfTheHeart</span><span class=""pun"">(</span><span class=""pln"">url</span><span class=""pun"">)</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">  
    </span><span class=""kwd"">var</span><span class=""pln""> response </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""typ"">UrlFetchApp</span><span class=""pun"">.</span><span class=""pln"">fetch</span><span class=""pun"">(</span><span class=""pln"">url</span><span class=""pun"">);</span><span class=""pln"">
    </span><span class=""kwd"">if</span><span class=""pln""> </span><span class=""pun"">(</span><span class=""pln"">response</span><span class=""pun"">.</span><span class=""pln"">getResponseCode</span><span class=""pun"">()</span><span class=""pln""> </span><span class=""pun"">!=</span><span class=""pln""> </span><span class=""lit"">200</span><span class=""pun"">)</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">
        </span><span class=""kwd"">throw</span><span class=""pln""> </span><span class=""typ"">Utilities</span><span class=""pun"">.</span><span class=""pln"">formatString</span><span class=""pun"">(</span><span class=""pln"">
                </span><span class=""str"">'Error returned by API: %s, Location searched: %s.'</span><span class=""pun"">,</span><span class=""pln"">
                response</span><span class=""pun"">.</span><span class=""pln"">getContentText</span><span class=""pun"">(),</span><span class=""pln""> location</span><span class=""pun"">);</span><span class=""pln"">
    </span><span class=""pun"">}</span><span class=""pln"">

    </span><span class=""kwd"">return</span><span class=""pln""> JSON</span><span class=""pun"">.</span><span class=""pln"">parse</span><span class=""pun"">(</span><span class=""pln"">response</span><span class=""pun"">.</span><span class=""pln"">getContentText</span><span class=""pun"">());</span><span class=""pln"">
</span><span class=""pun"">}</span><span class=""pln"">

</span><span class=""kwd"">function</span><span class=""pln""> main</span><span class=""pun"">()</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">  
    </span><span class=""com"">// Find our campaign</span><span class=""pln"">
    </span><span class=""kwd"">var</span><span class=""pln""> campaignIterator </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""typ"">AdWordsApp</span><span class=""pun"">.</span><span class=""pln"">campaigns</span><span class=""pun"">()</span><span class=""pln"">
            </span><span class=""pun"">.</span><span class=""pln"">withCondition</span><span class=""pun"">(</span><span class=""typ"">Utilities</span><span class=""pun"">.</span><span class=""pln"">formatString</span><span class=""pun"">(</span><span class=""str"">'CampaignName = ""%s""'</span><span class=""pun"">,</span><span class=""pln""> campaignName</span><span class=""pun"">))</span><span class=""pln"">
            </span><span class=""pun"">.</span><span class=""kwd"">get</span><span class=""pun"">();</span><span class=""pln"">

    </span><span class=""com"">// Get the GPS location of the heart</span><span class=""pln"">
    </span><span class=""kwd"">var</span><span class=""pln""> heartLocation </span><span class=""pun"">=</span><span class=""pln""> getTheLocationOfTheHeart</span><span class=""pun"">(</span><span class=""pln"">url</span><span class=""pun"">);</span><span class=""pln"">

    </span><span class=""kwd"">while</span><span class=""pln""> </span><span class=""pun"">(</span><span class=""pln"">campaignIterator</span><span class=""pun"">.</span><span class=""pln"">hasNext</span><span class=""pun"">())</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">
        </span><span class=""kwd"">var</span><span class=""pln""> campaign </span><span class=""pun"">=</span><span class=""pln""> campaignIterator</span><span class=""pun"">.</span><span class=""kwd"">next</span><span class=""pun"">();</span><span class=""pln"">

        </span><span class=""com"">// Get our current targeted proximities</span><span class=""pln"">
        </span><span class=""kwd"">var</span><span class=""pln""> locations </span><span class=""pun"">=</span><span class=""pln""> campaign</span><span class=""pun"">.</span><span class=""pln"">targeting</span><span class=""pun"">().</span><span class=""pln"">targetedProximities</span><span class=""pun"">().</span><span class=""kwd"">get</span><span class=""pun"">();</span><span class=""pln"">

        </span><span class=""com"">// Remove the existing proximities</span><span class=""pln"">
        </span><span class=""kwd"">while</span><span class=""pun"">(</span><span class=""pln"">locations</span><span class=""pun"">.</span><span class=""pln"">hasNext</span><span class=""pun"">())</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">
            </span><span class=""kwd"">var</span><span class=""pln""> location </span><span class=""pun"">=</span><span class=""pln""> locations</span><span class=""pun"">.</span><span class=""kwd"">next</span><span class=""pun"">();</span><span class=""pln"">
            location</span><span class=""pun"">.</span><span class=""pln"">remove</span><span class=""pun"">();</span><span class=""pln"">
        </span><span class=""pun"">}</span><span class=""pln"">

        </span><span class=""com"">// Add the new location of the heart</span><span class=""pln"">
campaign</span><span class=""pun"">.</span><span class=""pln"">addProximity</span><span class=""pun"">(</span><span class=""typ"">Number</span><span class=""pun"">(</span><span class=""pln"">heartLocation</span><span class=""pun"">.</span><span class=""pln"">lat</span><span class=""pun"">),</span><span class=""pln""> </span><span class=""typ"">Number</span><span class=""pun"">(</span><span class=""pln"">heartLocation</span><span class=""pun"">.</span><span class=""pln"">lng</span><span class=""pun"">),</span><span class=""pln""> radius</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""str"">'KILOMETRES'</span><span class=""pun"">)</span><span class=""pln"">  
    </span><span class=""pun"">}</span><span class=""pln"">
</span><span class=""pun"">}</span></code></pre>

<h3 id=""makeitrunhourly"">Make it run hourly</h3>

<p>We want to update the location every hour. Return to your scripts overview page. You should see something like this:</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2015/Nov/Screen_Shot_2015_11_25_at_22_09_47-1448485906203.png"" alt="""" class=""full-img""></p>

<p>Click 'Create schedule'. In the appeared popout select 'Hourly' and save it. </p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2015/Nov/Screen_Shot_2015_11_25_at_22_10_09-1448485928125.png"" alt="""" class=""full-img""></p>

<p>You script will now run every hour. </p>

<h3 id=""whatsnext"">What's next?</h3>

<p>Next time I'll get into how to display the distance the heart travelled in our ads by the hour. I also have a post coming up about how we've used GPS in our display campaigns, so stay tuned!</p>
        ","We are running a campaign for one of our clients that is about a fluffy heart with GPS hardware that is travelling through the country. It looks something like this:
We'll get deeper into this campaign in another post on The Marketing Technologist, but for now it's enough to know that for every kilometre the heart travels our client will donate 10 euro to a good cause.
We are already using the GPS location, which was made available using an public API, in our display ads (in both our creatives and targeting). But we are also doing some SEA for this campaign, so why not also use the GPS coordinates in Adwords?
We want to achieve two things:
Only target people that are in a radius of 20 kilometres around the heart.
Show the distance the heart has already travelled so far, and the current location of the heart.
In this post I'll talk about the first goal. Let's get started.
Find the campaign
Go to the Scripts section in the Adwords Web application (Bulk operations > Scripts). Before we can update the campaign's targeting location, let's find our campaign first.
// What's the name of the campaign?
var campaignName = 'your campaign name';

function main() {  
    // Find our campaign
    var campaignIterator = AdWordsApp.campaigns()
            .withCondition(Utilities.formatString('CampaignName = ""%s""', campaignName))
            .get();

    while (campaignIterator.hasNext()) {
        var campaign = campaignIterator.next();
    }
}
Now the variable campaign refers to your campaign. In our scenario we have only one campaign that matches our campaign name.
Update the campaign's targeting location
When you want to get the proximate location (remember we want to target with a radius), we need to use targetedProximities() of your campaign's targeting() method. Unfortunately there is no good way to update a location, so we first have to remove the existing ones. To do this we have to get all the locations and call the remove method on them one by one.
// Get our current targeted proximity locations
var locations = campaign.targeting().targetedProximities().get();

// Loop through all locations
while(locations.hasNext()) {  
    var location = locations.next();
    // Remove the existing proximity location
    location.remove();
}
Now your campaign has no more proximity locations. It's time to add our new location. We have to add this location by using its lattitude/longitude coordinates. It's easy to get these coordinates for an address using a service like http://www.latlong.net.
We can now add the coordinates to the campaign using addProximity. The first two arguments of this function are the lattitude and longitude values. The third one is the radius, followed by the unit of the radius (KILOMETRES or MILES). Make sure your coordinates are both numbers.
var radius = 10;  
var lat = 51.507351;  
var lng = -0.127758;

campaign.addProximity(lat, lng, radius, 'KILOMETRES')  
Save your script and preview it. Your preview panel should look something like this.
We've just changed your campaign's location targeting with code. Nice!
Get location data from a remote source
Although it's fun to write these scripts, it's not really useful to hardcode those lat/lng coordinates. In our case we have these data available through an API, so let's see how we can use this data in our script.
I've seen some people use a spread sheet to read an API, and load the spread sheet with the API in Adwords. I'm not going to do that; I want to load the API directly in our code.
Our API returns this JSON data:
{
  lat: 51.507351,
  lng: -0.127758,
  city: 'London',
  distance: 22304
}
We can read this API using the fetch method of UrlFetchApp. We only need to pass the URL of our API.
var url = 'http://yourapi';  
var response = UrlFetchApp.fetch(url);  
To easily get the values we can parse the response text to JSON.
var data = JSON.parse(response.getContentText());  
Now we can read the lat and long like this:
var lat = data.lat;  
var lng = data.lng;  
Awesome. We also have to handle any errors that may occur, like a 404 when the API is not available. We want to check if the HTTP response code is not 200 ('Response is OK'), and print an error otherwise.
if (response.getResponseCode() != 200) {  
        throw Utilities.formatString(
                'Error returned by API: %s, Location searched: %s.',
                , location);
    }
The complete script
We've written a script that updates a campaign's targeting location based on data that is loaded from an external data source. Our complete script looks like this:
// What's the name of the campaign?
var campaignName = 'your campaign name';

// Url of the API
var url = 'http://yourapi'

// The radius in KM around the location you want to use in the campaign targeting
var radius = 20;

function getTheLocationOfTheHeart(url) {  
    var response = UrlFetchApp.fetch(url);
    if (response.getResponseCode() != 200) {
        throw Utilities.formatString(
                'Error returned by API: %s, Location searched: %s.',
                response.getContentText(), location);
    }

    return JSON.parse(response.getContentText());
}

function main() {  
    // Find our campaign
    var campaignIterator = AdWordsApp.campaigns()
            .withCondition(Utilities.formatString('CampaignName = ""%s""', campaignName))
            .get();

    // Get the GPS location of the heart
    var heartLocation = getTheLocationOfTheHeart(url);

    while (campaignIterator.hasNext()) {
        var campaign = campaignIterator.next();

        // Get our current targeted proximities
        var locations = campaign.targeting().targetedProximities().get();

        // Remove the existing proximities
        while(locations.hasNext()) {
            var location = locations.next();
            location.remove();
        }

        // Add the new location of the heart
campaign.addProximity(Number(heartLocation.lat), Number(heartLocation.lng), radius, 'KILOMETRES')  
    }
}
Make it run hourly
We want to update the location every hour. Return to your scripts overview page. You should see something like this:
Click 'Create schedule'. In the appeared popout select 'Hourly' and save it.
You script will now run every hour.
What's next?
Next time I'll get into how to display the distance the heart travelled in our ads by the hour. I also have a post coming up about how we've used GPS in our display campaigns, so stay tuned!","[Code, SEA, Adwords]"
80,A recommendation system for blogs: Setting up the prerequisites (part 1),/building-a-recommendation-engine-for-geek-setting-up-the-prerequisites-13/,"
            <p>The goal of data science is typically described as creating value from Big Data. However, data science should also meet a second goal, that is, avoiding an information overload. One particular type of projects that really meet these two goals are recommendation engines. Online stores such as Amazon but also streaming services such as Netflix suffer from information overload. Customers can easily get lost in their large variety (millions) of products or movies. Recommendation engines help users narrow down the large variety by presenting possible options. Of course, these recommenders can randomly present options to users but this does not really decrease the information overload. Therefore these recommenders apply statistics and science to present ‘better’ solutions which are more likely to meet the expectations of the user. For example, a Netflix user who watched the movie Frozen gets similar children movies from Pixar as a recommendation to watch.</p>

<p><img src=""http://i67.tinypic.com/z9tlx.jpg"" alt=""Example of Netflix's recommendation system"" class=""full-img"">
<em>Example of Netflix's recommendation system</em></p>

<p>In a series of three blog posts we will elaborate on how we can build a recommendation engine for our readers on The Marketing Technologist (TMT). TMT currently has over fifty blog posts covering varying topics from Data Science to coding in ReactJS. Browsing through all the blog posts is time consuming, especially as the number of posts is still increasing. Also chances are readers are only interested in a select few blog posts that lie in their area of interest. If a recommendation engine is able to select those articles an user is interested in then this can definitely be classified as creating value from data and preventing information overload.</p>

<h4 id=""twotypesofrecommendationsystems"">Two types of recommendation systems</h4>

<p>Roughly speaking we can divide recommendation engines in two different types: <strong>collaborative filtering</strong> and <strong>content-based</strong>. As Wikipedia states “collaborative filtering is the process of filtering for information or patterns using techniques involving collaboration among multiple agents, viewpoints, data sources”. In our TMT case, this implies finding patterns among multiple readers. If several readers are interested in a particular set of articles it is very likely that a reader who starts reading one of these articles is also interested in the other articles from this set. Therefore, based upon the reading behavior of other users, suggestions are made to similar users. <br>
Content-based recommendation engines are different as they base their recommendations on the properties of the product. In our case the products are TMT blog posts and the properties are the words within these posts. If a user is reading an article containing the words <em>'Google Analytics'</em> and <em>'Tag Manager'</em>, chances are that this user also likes reading other articles containing these words. Therefore, a content-based recommendation engine will recommend articles containing these words. <br>
Note that since the recent change from Geek to The Marketing Technologist a very simple content-based filtering approach is integrated in TMT. That is, below each article five other related articles are shown to the user as suggestion for continued reading. The suggested articles are the five most recently published articles that contain any of the tags of the article the user is currently reading. In this simple example the tags of the articles can be seen as the properties of the product.</p>

<p><img src=""http://i65.tinypic.com/2ebah6c.png"" alt=""The principle behind collaborative and content-based filtering"" class=""full-img"">
<em>The principle behind collaborative and content-based filtering (Icons made by Freepik from www.flaticon.com)</em></p>

<p>Both systems have their pros and their cons. Content-based recommendation systems are limited in their possibilities as the recommended articles will be close to the article on which a set of recommendations is based. For example, a post about a specific feature in <em>'Google Analytics'</em> will give recommendations based upon similar words in other articles. However, an article about a specific feature in <em>Snowplow</em>, which is a similar analytics tool, is less likely to be recommended. Chances are though that users are interested in both posts as they both cover the theme analytics. Therefore, content-based recommendation systems are not good at finding hidden patterns. <br>
Collaborative filtering outperforms content-based recommendation systems for discovering hidden patterns. Collaborative filtering looks at the reading behaviour of users and not specificly at the content of these articles. So if users reading blog posts about data science are also reading posts about conversion rate optimization (CRO), even when the content of the CRO articles is very different, collaborative filtering will recommend data science readers also CRO articles. The big con of collaborative filtering is that it needs a lot of historical user reading behaviour data in order to find these patterns. Content-based recommendation can be done with none to few historical data and are therefore easier to implement.</p>

<h4 id=""prerequisitesforcollaborativefiltering"">Prerequisites for collaborative filtering</h4>

<p>In the next two blog posts we are going to implement content-based and collorative filtering and analyse the results. However, in order to be able to do so we first need to set-up some prerequisites. For collaborative filtering this implies implementing a method with which we can measure the articles a user has read. Our colleague <a href=""http://www.themarketingtechnologist.co/author/erik-driessen/"">Erik Driessen</a> has implemented a method to track user reading behaviour in Google Analytics by using the client ID. Simo Ahava explains in his blog post <a href=""http://www.simoahava.com/analytics/improve-data-collection-with-four-custom-dimensions/#2"">Storing client ID in Google Analytics</a> how this can be done in an effective manner. Additionaly, because Erik has implemented <a href=""http://www.themarketingtechnologist.co/track-content-performance-using-google-analytics-enhanced-ecommerce-report/"">Enhanced Ecommerce for content</a>, we can track whether a user has fully read an article. Finally, in Google Analytics a custom report can be created which shows the client id and the posts the users has read. </p>

<p>Note that for now no cross-device solution is implemented yet. Therefore, if a user continues reading articles on a different device or removes his cookies, this behaviour cannot be connected to his earlier reading behaviour.</p>

<p><img src=""http://i68.tinypic.com/xg392.png"" alt=""Example of a custom report in Google Analytics which shows reading behaviour on a user level"" class=""full-img"">
<em>Example of a custom report in Google Analytics which shows reading behaviour on a user level</em></p>

<h4 id=""prerequisitesforcontentbased"">Prerequisites for content-based</h4>

<p>For content-based recommendations we are obviously going to need the content of all TMT articles. There are multiple methods to do so. One of these would be to extract the text of the articles directly from the database. However, as we are Geeks, it is more fun to create a Python script that automatically retrieves the articles and corresponding metrics such as author and category. Therefore we created a Python script that scrapes the articles from the TMT website in two steps. The code for step 1 and step 2 can be found <a href=""https://github.com/thomhopmans"">here</a>.</p>

<h6 id=""step1createalistofalltmtarticles"">Step 1: Create a list of all TMT articles.</h6>

<p>In Python the source of webpages can be loaded with the library <code>urllib2</code>. Using the command <code>urllib2.urlopen(""http://www.themarketingtechnologist.co"")</code> we can therefore load the source of the frontpage of our very own TMT blog. This frontpage always shows the ten most recent posts. Using the <code>BeautifulSoup</code> library we can then easily search through the DOM and extract all <code>article</code> elements with <code>class=""post""</code> and store them in a Pandas dataframe. Additionally, within each of these elements we can search for author name and tags by searching for the corresponding elements in the DOM. <br>
Because only the ten most recent blog posts are shown on the frontpage we also need to check whether there is an <code>Older posts &gt;</code> button on the bottom of the page for more posts. Again, this can be done by searching for the proper DOM element, i.e. an <code>a</code> element with <code>class=""older-posts""</code>. From the older posts link the URL to the next page can be extracted by using the <code>get</code> function to extract the value from the <code>href</code> attribute. We repeat the above process for each of the pages. In the end, we have a dataframe with the names, tags and author of all articles + a link to the article content.</p>

<h6 id=""step2retrievingthecontentofeacharticle"">Step 2: Retrieving the content of each article.</h6>

<p>In step 1 we stored a direct link to each article so we can download the full content of each article. There is only one particular problem, i.e. the content of each blog post is loaded via JavaScript. Therefore, if we use <code>urllib2</code> to load the static source of the article we don't get the content of the article. <br>
In order to execute JavaScript to load the content of the articles we actually need to render the posts by opening it in a web browser. Luckily this can be done using the popular <code>Selenium</code> library. Using a few lines of Python code in combination with Selenium a Firefox browser can be opened, directed to the proper URL and render the page. The DOM can then be searched for the information we want, e.g. the content of a blog post. <br>
Note that because all JavaScript is executed using Selenium this also implies that Google Analytics is executed. Therefore, it is wise to take measures to prevent data pollution. For example, by adding your IP address to the list of filters in GA or by installing the <a href=""https://chrome.google.com/webstore/detail/google-analytics-opt-out/fllaojicojecljbmefodhfapmkghcbnh"">Google Analytics Opt-out Addon</a> by Google. Also note that we do not actually need to visually render each page in a Firefox webbrowser. You can also use a headless driver such as <a href=""http://phantomjs.org/"">PhantomJS</a> which renders the page in the background without the visual overhead. </p>

<p>That's it for now with respect to setting up the prerequisites. For the next months we collect reading behaviour on a user level for our collaborative filtering model. Therefore, in the next blog post we start by creating a content-based recommendation system and analyse its results.</p>

<section class=""related miss"" style=""background-color: #439654 ; padding: 1em;""><h6 style=""color: #fff;"">  
This was the first article in a series on recommendation engines.  
</h6>  
<p style=""color: #fff; margin-bottom: 0;"">  
Continue reading with the second article in the series:<br>  
<a style=""color:#FFF"" href=""https://www.themarketingtechnologist.co/a-recommendation-system-for-blogs-content-based-similarity-part-2/"">A recommendation system for blogs: Content-based similarity (part 2)</a>.</p>  
</section>  

<p><br></p>
        ","The goal of data science is typically described as creating value from Big Data. However, data science should also meet a second goal, that is, avoiding an information overload. One particular type of projects that really meet these two goals are recommendation engines. Online stores such as Amazon but also streaming services such as Netflix suffer from information overload. Customers can easily get lost in their large variety (millions) of products or movies. Recommendation engines help users narrow down the large variety by presenting possible options. Of course, these recommenders can randomly present options to users but this does not really decrease the information overload. Therefore these recommenders apply statistics and science to present ‘better’ solutions which are more likely to meet the expectations of the user. For example, a Netflix user who watched the movie Frozen gets similar children movies from Pixar as a recommendation to watch.
Example of Netflix's recommendation system
In a series of three blog posts we will elaborate on how we can build a recommendation engine for our readers on The Marketing Technologist (TMT). TMT currently has over fifty blog posts covering varying topics from Data Science to coding in ReactJS. Browsing through all the blog posts is time consuming, especially as the number of posts is still increasing. Also chances are readers are only interested in a select few blog posts that lie in their area of interest. If a recommendation engine is able to select those articles an user is interested in then this can definitely be classified as creating value from data and preventing information overload.
Two types of recommendation systems
Roughly speaking we can divide recommendation engines in two different types: collaborative filtering and content-based. As Wikipedia states “collaborative filtering is the process of filtering for information or patterns using techniques involving collaboration among multiple agents, viewpoints, data sources”. In our TMT case, this implies finding patterns among multiple readers. If several readers are interested in a particular set of articles it is very likely that a reader who starts reading one of these articles is also interested in the other articles from this set. Therefore, based upon the reading behavior of other users, suggestions are made to similar users.
Content-based recommendation engines are different as they base their recommendations on the properties of the product. In our case the products are TMT blog posts and the properties are the words within these posts. If a user is reading an article containing the words 'Google Analytics' and 'Tag Manager', chances are that this user also likes reading other articles containing these words. Therefore, a content-based recommendation engine will recommend articles containing these words.
Note that since the recent change from Geek to The Marketing Technologist a very simple content-based filtering approach is integrated in TMT. That is, below each article five other related articles are shown to the user as suggestion for continued reading. The suggested articles are the five most recently published articles that contain any of the tags of the article the user is currently reading. In this simple example the tags of the articles can be seen as the properties of the product.
The principle behind collaborative and content-based filtering (Icons made by Freepik from www.flaticon.com)
Both systems have their pros and their cons. Content-based recommendation systems are limited in their possibilities as the recommended articles will be close to the article on which a set of recommendations is based. For example, a post about a specific feature in 'Google Analytics' will give recommendations based upon similar words in other articles. However, an article about a specific feature in Snowplow, which is a similar analytics tool, is less likely to be recommended. Chances are though that users are interested in both posts as they both cover the theme analytics. Therefore, content-based recommendation systems are not good at finding hidden patterns.
Collaborative filtering outperforms content-based recommendation systems for discovering hidden patterns. Collaborative filtering looks at the reading behaviour of users and not specificly at the content of these articles. So if users reading blog posts about data science are also reading posts about conversion rate optimization (CRO), even when the content of the CRO articles is very different, collaborative filtering will recommend data science readers also CRO articles. The big con of collaborative filtering is that it needs a lot of historical user reading behaviour data in order to find these patterns. Content-based recommendation can be done with none to few historical data and are therefore easier to implement.
Prerequisites for collaborative filtering
In the next two blog posts we are going to implement content-based and collorative filtering and analyse the results. However, in order to be able to do so we first need to set-up some prerequisites. For collaborative filtering this implies implementing a method with which we can measure the articles a user has read. Our colleague Erik Driessen has implemented a method to track user reading behaviour in Google Analytics by using the client ID. Simo Ahava explains in his blog post Storing client ID in Google Analytics how this can be done in an effective manner. Additionaly, because Erik has implemented Enhanced Ecommerce for content, we can track whether a user has fully read an article. Finally, in Google Analytics a custom report can be created which shows the client id and the posts the users has read.
Note that for now no cross-device solution is implemented yet. Therefore, if a user continues reading articles on a different device or removes his cookies, this behaviour cannot be connected to his earlier reading behaviour.
Example of a custom report in Google Analytics which shows reading behaviour on a user level
Prerequisites for content-based
For content-based recommendations we are obviously going to need the content of all TMT articles. There are multiple methods to do so. One of these would be to extract the text of the articles directly from the database. However, as we are Geeks, it is more fun to create a Python script that automatically retrieves the articles and corresponding metrics such as author and category. Therefore we created a Python script that scrapes the articles from the TMT website in two steps. The code for step 1 and step 2 can be found here.
Step 1: Create a list of all TMT articles.
In Python the source of webpages can be loaded with the library urllib2. Using the command urllib2.urlopen(""http://www.themarketingtechnologist.co"") we can therefore load the source of the frontpage of our very own TMT blog. This frontpage always shows the ten most recent posts. Using the BeautifulSoup library we can then easily search through the DOM and extract all article elements with class=""post"" and store them in a Pandas dataframe. Additionally, within each of these elements we can search for author name and tags by searching for the corresponding elements in the DOM.
Because only the ten most recent blog posts are shown on the frontpage we also need to check whether there is an Older posts > button on the bottom of the page for more posts. Again, this can be done by searching for the proper DOM element, i.e. an a element with class=""older-posts"". From the older posts link the URL to the next page can be extracted by using the get function to extract the value from the href attribute. We repeat the above process for each of the pages. In the end, we have a dataframe with the names, tags and author of all articles + a link to the article content.
Step 2: Retrieving the content of each article.
In step 1 we stored a direct link to each article so we can download the full content of each article. There is only one particular problem, i.e. the content of each blog post is loaded via JavaScript. Therefore, if we use urllib2 to load the static source of the article we don't get the content of the article.
In order to execute JavaScript to load the content of the articles we actually need to render the posts by opening it in a web browser. Luckily this can be done using the popular Selenium library. Using a few lines of Python code in combination with Selenium a Firefox browser can be opened, directed to the proper URL and render the page. The DOM can then be searched for the information we want, e.g. the content of a blog post.
Note that because all JavaScript is executed using Selenium this also implies that Google Analytics is executed. Therefore, it is wise to take measures to prevent data pollution. For example, by adding your IP address to the list of filters in GA or by installing the Google Analytics Opt-out Addon by Google. Also note that we do not actually need to visually render each page in a Firefox webbrowser. You can also use a headless driver such as PhantomJS which renders the page in the background without the visual overhead.
That's it for now with respect to setting up the prerequisites. For the next months we collect reading behaviour on a user level for our collaborative filtering model. Therefore, in the next blog post we start by creating a content-based recommendation system and analyse its results.
This was the first article in a series on recommendation engines.
Continue reading with the second article in the series:
A recommendation system for blogs: Content-based similarity (part 2).","[Data Science, python, Recommenders]"
81,Is there time for coffee? Your execution time is ticking in Python!,/progress-timer-in-python/,"
            <p>Last month I was working on a machine learning project. If you make use of grid search to find the optimum parameters, it is nice to know how much time an iterating process costs, so I do not waste my time. In this blog you’ll learn how to:</p>

<ul>
<li>Install the progress bar library in Windows</li>
<li>The disadvantage of the progress bar library</li>
<li>Write your own progress timer class in Python</li>
<li>Use three lines of code to time your code progress</li>
</ul>

<h4 id=""installtheprogressbarlibraryinwindows"">Install the progress bar library in Windows</h4>

<p>Data Science life can be easy sometimes. If you make use of Anaconda (I strongly recommend) then it is the following line of code in de command line:  </p>

<pre><code>conda install progressbar  
</code></pre>

<p>Otherwise you could you do the trick by:  </p>

<pre><code>pip install progressbar  
</code></pre>

<h4 id=""thedisadvantageoftheprogressbarlibrary"">The disadvantage of the progress bar library</h4>

<p>The progress bar library is really nice, but the progress bar library has metrics that are easy to forget. Documentation can be found <a href=""http://progressbar-2.readthedocs.org/en/latest/"">here</a>. Each time I would like to time my code progress, I simply like to initialize, start and finish the job. Now you need for example to initialize the widgets of the progress bar and define the timer. The progress library works as follows:</p>

<pre><code class=""language-prettyprint lang-python prettyprinted""><span class=""com"">#import libraries</span><span class=""pln"">
</span><span class=""kwd"">import</span><span class=""pln""> progressbar </span><span class=""kwd"">as</span><span class=""pln""> pb

</span><span class=""com"">#initialize widgets</span><span class=""pln"">
widgets </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""pun"">[</span><span class=""str"">'Time for loop of 1 000 000 iterations: '</span><span class=""pun"">,</span><span class=""pln""> pb</span><span class=""pun"">.</span><span class=""typ"">Percentage</span><span class=""pun"">(),</span><span class=""pln""> </span><span class=""str"">' '</span><span class=""pun"">,</span><span class=""pln"">  
            pb</span><span class=""pun"">.</span><span class=""typ"">Bar</span><span class=""pun"">(</span><span class=""pln"">marker</span><span class=""pun"">=</span><span class=""pln"">pb</span><span class=""pun"">.</span><span class=""typ"">RotatingMarker</span><span class=""pun"">()),</span><span class=""pln""> </span><span class=""str"">' '</span><span class=""pun"">,</span><span class=""pln""> pb</span><span class=""pun"">.</span><span class=""pln"">ETA</span><span class=""pun"">()]</span><span class=""pln"">
</span><span class=""com"">#initialize timer</span><span class=""pln"">
timer </span><span class=""pun"">=</span><span class=""pln""> pb</span><span class=""pun"">.</span><span class=""typ"">ProgressBar</span><span class=""pun"">(</span><span class=""pln"">widgets</span><span class=""pun"">=</span><span class=""pln"">widgets</span><span class=""pun"">,</span><span class=""pln""> maxval</span><span class=""pun"">=</span><span class=""lit"">1000000</span><span class=""pun"">).</span><span class=""pln"">start</span><span class=""pun"">()</span><span class=""pln"">

</span><span class=""com"">#for loop example</span><span class=""pln"">
</span><span class=""kwd"">for</span><span class=""pln""> i </span><span class=""kwd"">in</span><span class=""pln""> range</span><span class=""pun"">(</span><span class=""lit"">0</span><span class=""pun"">,</span><span class=""lit"">1000000</span><span class=""pun"">):</span><span class=""pln"">  
    </span><span class=""com"">#update</span><span class=""pln"">
    timer</span><span class=""pun"">.</span><span class=""pln"">update</span><span class=""pun"">(</span><span class=""pln"">i</span><span class=""pun"">)</span><span class=""pln"">
</span><span class=""com"">#finish</span><span class=""pln"">
timer</span><span class=""pun"">.</span><span class=""pln"">finish</span><span class=""pun"">()</span><span class=""pln"">  </span></code></pre>

<pre><code class=""language-prettyprint lang-python prettyprinted""><span class=""typ"">Time</span><span class=""pln""> </span><span class=""kwd"">for</span><span class=""pln""> loop of </span><span class=""lit"">1</span><span class=""pln""> </span><span class=""lit"">000</span><span class=""pln""> </span><span class=""lit"">000</span><span class=""pln""> iterations</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""lit"">100</span><span class=""pun"">%</span><span class=""pln""> </span><span class=""pun"">||||||||||||||||||||||||</span><span class=""pln""> </span><span class=""typ"">Time</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""lit"">0</span><span class=""pun"">:</span><span class=""lit"">00</span><span class=""pun"">:</span><span class=""lit"">07</span><span class=""pln"">  </span></code></pre>

<h4 id=""writeyourownprogresstimerclassinpython"">Write your own progress timer class in Python</h4>

<p>Therefore it is easier to write your own class that you can easily use multiple times. First you initialize the progress timer. From now on, when you initialize you only need to give information how many iterations the code takes.  You will also get the opportunity to add a description to the progress timer.</p>

<pre><code class=""language-prettyprint lang-python prettyprinted""><span class=""com"">#import libraries</span><span class=""pln"">
</span><span class=""kwd"">import</span><span class=""pln""> progressbar </span><span class=""kwd"">as</span><span class=""pln""> pb

</span><span class=""com"">#define</span><span class=""pln""> progress timer </span><span class=""kwd"">class</span><span class=""pln"">
</span><span class=""kwd"">class</span><span class=""pln""> progress_timer</span><span class=""pun"">:</span><span class=""pln"">

    </span><span class=""kwd"">def</span><span class=""pln""> __init__</span><span class=""pun"">(</span><span class=""kwd"">self</span><span class=""pun"">,</span><span class=""pln""> n_iter</span><span class=""pun"">,</span><span class=""pln""> description</span><span class=""pun"">=</span><span class=""str"">""Something""</span><span class=""pun"">):</span><span class=""pln"">
        </span><span class=""kwd"">self</span><span class=""pun"">.</span><span class=""pln"">n_iter         </span><span class=""pun"">=</span><span class=""pln""> n_iter
        </span><span class=""kwd"">self</span><span class=""pun"">.</span><span class=""pln"">iter           </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""lit"">0</span><span class=""pln"">
        </span><span class=""kwd"">self</span><span class=""pun"">.</span><span class=""pln"">description    </span><span class=""pun"">=</span><span class=""pln""> description </span><span class=""pun"">+</span><span class=""pln""> </span><span class=""str"">': '</span><span class=""pln"">
        </span><span class=""kwd"">self</span><span class=""pun"">.</span><span class=""pln"">timer          </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""kwd"">None</span><span class=""pln"">
        </span><span class=""kwd"">self</span><span class=""pun"">.</span><span class=""pln"">initialize</span><span class=""pun"">()</span><span class=""pln"">

    </span><span class=""kwd"">def</span><span class=""pln""> initialize</span><span class=""pun"">(</span><span class=""kwd"">self</span><span class=""pun"">):</span><span class=""pln"">
        </span><span class=""com"">#initialize timer</span><span class=""pln"">
        widgets </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""pun"">[</span><span class=""kwd"">self</span><span class=""pun"">.</span><span class=""pln"">description</span><span class=""pun"">,</span><span class=""pln""> pb</span><span class=""pun"">.</span><span class=""typ"">Percentage</span><span class=""pun"">(),</span><span class=""pln""> </span><span class=""str"">' '</span><span class=""pun"">,</span><span class=""pln"">   
                   pb</span><span class=""pun"">.</span><span class=""typ"">Bar</span><span class=""pun"">(</span><span class=""pln"">marker</span><span class=""pun"">=</span><span class=""pln"">pb</span><span class=""pun"">.</span><span class=""typ"">RotatingMarker</span><span class=""pun"">()),</span><span class=""pln""> </span><span class=""str"">' '</span><span class=""pun"">,</span><span class=""pln""> pb</span><span class=""pun"">.</span><span class=""pln"">ETA</span><span class=""pun"">()]</span><span class=""pln"">
        </span><span class=""kwd"">self</span><span class=""pun"">.</span><span class=""pln"">timer </span><span class=""pun"">=</span><span class=""pln""> pb</span><span class=""pun"">.</span><span class=""typ"">ProgressBar</span><span class=""pun"">(</span><span class=""pln"">widgets</span><span class=""pun"">=</span><span class=""pln"">widgets</span><span class=""pun"">,</span><span class=""pln""> maxval</span><span class=""pun"">=</span><span class=""kwd"">self</span><span class=""pun"">.</span><span class=""pln"">n_iter</span><span class=""pun"">).</span><span class=""pln"">start</span><span class=""pun"">()</span><span class=""pln"">

    </span><span class=""kwd"">def</span><span class=""pln""> update</span><span class=""pun"">(</span><span class=""kwd"">self</span><span class=""pun"">,</span><span class=""pln""> q</span><span class=""pun"">=</span><span class=""lit"">1</span><span class=""pun"">):</span><span class=""pln"">
        </span><span class=""com"">#update timer</span><span class=""pln"">
        </span><span class=""kwd"">self</span><span class=""pun"">.</span><span class=""pln"">timer</span><span class=""pun"">.</span><span class=""pln"">update</span><span class=""pun"">(</span><span class=""kwd"">self</span><span class=""pun"">.</span><span class=""pln"">iter</span><span class=""pun"">)</span><span class=""pln"">
        </span><span class=""kwd"">self</span><span class=""pun"">.</span><span class=""pln"">iter </span><span class=""pun"">+=</span><span class=""pln""> q

    </span><span class=""kwd"">def</span><span class=""pln""> finish</span><span class=""pun"">(</span><span class=""kwd"">self</span><span class=""pun"">):</span><span class=""pln"">
        </span><span class=""com"">#end timer</span><span class=""pln"">
        </span><span class=""kwd"">self</span><span class=""pun"">.</span><span class=""pln"">timer</span><span class=""pun"">.</span><span class=""pln"">finish</span><span class=""pun"">()</span></code></pre>

<h4 id=""threelinesofcodetotimeyourcodeprogress"">Three lines of code to time your code progress</h4>

<p>Now it is simple to show the progress of your code. You only need three lines of code to initialize, update and finish the progress timer. In case of a for loop it works as follows:</p>

<pre><code class=""language-prettyprint lang-python prettyprinted""><span class=""com"">#initialize</span><span class=""pln"">
pt </span><span class=""pun"">=</span><span class=""pln""> progress_timer</span><span class=""pun"">(</span><span class=""pln"">description</span><span class=""pun"">=</span><span class=""pln""> </span><span class=""str"">'For loop example'</span><span class=""pun"">,</span><span class=""pln""> n_iter</span><span class=""pun"">=</span><span class=""lit"">1000000</span><span class=""pun"">)</span><span class=""pln"">  
</span><span class=""com"">#for loop example</span><span class=""pln"">
</span><span class=""kwd"">for</span><span class=""pln""> i </span><span class=""kwd"">in</span><span class=""pln""> range</span><span class=""pun"">(</span><span class=""lit"">0</span><span class=""pun"">,</span><span class=""lit"">1000000</span><span class=""pun"">):</span><span class=""pln"">  
    </span><span class=""com"">#update</span><span class=""pln"">
    pt</span><span class=""pun"">.</span><span class=""pln"">update</span><span class=""pun"">()</span><span class=""pln"">
</span><span class=""com"">#finish</span><span class=""pln"">
pt</span><span class=""pun"">.</span><span class=""pln"">finish</span><span class=""pun"">()</span><span class=""pln"">  </span></code></pre>

<p>So you will receive feedback in how much time your function or for loop is finished.  </p>

<pre><code class=""language-prettyprint lang-python prettyprinted""><span class=""typ"">Time</span><span class=""pln""> </span><span class=""kwd"">for</span><span class=""pln""> loop of </span><span class=""lit"">1</span><span class=""pln""> </span><span class=""lit"">000</span><span class=""pln""> </span><span class=""lit"">000</span><span class=""pln""> iterations</span><span class=""pun"">:</span><span class=""pln"">  </span><span class=""lit"">37</span><span class=""pun"">%</span><span class=""pln""> </span><span class=""pun"">|</span><span class=""com"">////////              | ETA:  0:00:04  </span></code></pre>

<p>And finally you will see how much time it took to do the job.  </p>

<pre><code class=""language-prettyprint lang-python prettyprinted""><span class=""typ"">Time</span><span class=""pln""> </span><span class=""kwd"">for</span><span class=""pln""> loop of </span><span class=""lit"">1</span><span class=""pln""> </span><span class=""lit"">000</span><span class=""pln""> </span><span class=""lit"">000</span><span class=""pln""> iterations</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""lit"">100</span><span class=""pun"">%</span><span class=""pln""> </span><span class=""pun"">||||||||||||||||||||||||</span><span class=""pln""> </span><span class=""typ"">Time</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""lit"">0</span><span class=""pun"">:</span><span class=""lit"">00</span><span class=""pun"">:</span><span class=""lit"">07</span><span class=""pln"">  </span></code></pre>

<p>I hope this will help you to perfectly time when to take a coffee. I'm sure there is a lot to improve in this approach, and maybe you got a better way to do execution time tracking. I'd like to hear your ideas!</p>
        ","Last month I was working on a machine learning project. If you make use of grid search to find the optimum parameters, it is nice to know how much time an iterating process costs, so I do not waste my time. In this blog you’ll learn how to:
Install the progress bar library in Windows
The disadvantage of the progress bar library
Write your own progress timer class in Python
Use three lines of code to time your code progress
Install the progress bar library in Windows
Data Science life can be easy sometimes. If you make use of Anaconda (I strongly recommend) then it is the following line of code in de command line:
conda install progressbar  
Otherwise you could you do the trick by:
pip install progressbar  
The disadvantage of the progress bar library
The progress bar library is really nice, but the progress bar library has metrics that are easy to forget. Documentation can be found here. Each time I would like to time my code progress, I simply like to initialize, start and finish the job. Now you need for example to initialize the widgets of the progress bar and define the timer. The progress library works as follows:
#import libraries
import progressbar as pb

#initialize widgets
widgets = ['Time for loop of 1 000 000 iterations: ', pb.Percentage(), ' ',  
            pb.Bar(marker=pb.RotatingMarker()), ' ', pb.ETA()]
#initialize timer
timer = pb.ProgressBar(widgets=widgets, maxval=1000000).start()

#for loop example
for i in range(0,1000000):  
    #update
    timer.update(i)
#finish
timer.finish()  
Time for loop of 1 000 000 iterations: 100% |||||||||||||||||||||||| Time: 0:00:07  
Write your own progress timer class in Python
Therefore it is easier to write your own class that you can easily use multiple times. First you initialize the progress timer. From now on, when you initialize you only need to give information how many iterations the code takes. You will also get the opportunity to add a description to the progress timer.
#import libraries
import progressbar as pb

#define progress timer class
class progress_timer:

    def __init__(self, n_iter, description=""Something""):
        self.n_iter         = n_iter
        self.iter           = 0
        self.description    = description + ': '
        self.timer          = None
        self.initialize()

    def initialize(self):
        #initialize timer
        widgets = [self.description, pb.Percentage(), ' ',   
                   pb.Bar(marker=pb.RotatingMarker()), ' ', pb.ETA()]
        self.timer = pb.ProgressBar(widgets=widgets, maxval=self.n_iter).start()

    def update(self, q=1):
        #update timer
        self.timer.update(self.iter)
        self.iter += q

    def finish(self):
        #end timer
        self.timer.finish()
Three lines of code to time your code progress
Now it is simple to show the progress of your code. You only need three lines of code to initialize, update and finish the progress timer. In case of a for loop it works as follows:
#initialize
pt = progress_timer(description= 'For loop example', n_iter=1000000)  
#for loop example
for i in range(0,1000000):  
    #update
    pt.update()
#finish
pt.finish()  
So you will receive feedback in how much time your function or for loop is finished.
Time for loop of 1 000 000 iterations:  37% |////////              | ETA:  0:00:04  
And finally you will see how much time it took to do the job.
Time for loop of 1 000 000 iterations: 100% |||||||||||||||||||||||| Time: 0:00:07  
I hope this will help you to perfectly time when to take a coffee. I'm sure there is a lot to improve in this approach, and maybe you got a better way to do execution time tracking. I'd like to hear your ideas!","[Data Science, python]"
82,7 things I’ve learned by attending the 2015 Data Visualisation Summit in London,/7-things-ive-learned-by-attending-the-2015-data-visualisation-summit-in-london/,"
            <p>This week, I attended the <a href=""https://theinnovationenterprise.com/summits/data-visualisation-summit-london-2015"">Data Visualisation Summit in London</a>. This post contains my 7 key takeaways from the presentations I attended. For those of you looking for a quick snack, here’s the list:</p>

<ol>
<li>Don’t forget where your data comes from  </li>
<li>How web design is a form of data visualisation  </li>
<li>Automate routine (with Nightingale)  </li>
<li>There is no BI Team  </li>
<li>The power of Topological Analysis (TDA)  </li>
<li>Grow a data visualisation culture  </li>
<li>Visualise in the real world.</li>
</ol>

<p>If you’re interested in the details, read ahead.</p>

<h2 id=""1dontforgetwhereyourdatacomesfrom"">1: Don’t forget where your data comes from</h2>

<p><em>Twitter Data Architecture by Robert Stapenhurst and Filipa Moura, <a href=""https://www.twitter.com"">Twitter</a></em></p>

<p>The first talk was about data architecture of Twitter. The main point was to show what happens to the data before it ends up in the hands of analysts and visualisation experts. Looking at requests, they describe two main types:</p>

<ul>
<li><strong>Anticipated</strong>: how many people saw my tweet?</li>
<li><strong>Unanticipated</strong>: if a football player with a car preference for Mercedes scores during a match, how many people talk about that car brand directly after the goal.</li>
</ul>

<p>The anticipated part is easy to answer, and therefore included in Twitter’s analytics platform. The second question is more complex. For these questions, they’ve made a report builder, allowing users to run custom queries to answer questions.</p>

<p>Besides this, they briefly discussed <a href=""https://analytics.twitter.com"">analytics.twitter.com</a>, which I didn’t know about. It shows you an overview of your twitter stats of the last 28 days. Here’s a view of my data:</p>

<p><img src=""https://themarketingtechnologist-ghost.s3.amazonaws.com/2015/Nov/geek_dataviz_twitterdashb-1447409462673.PNG"" alt=""Twitter Analytics Dashboard"" class=""full-img"">
<em>Sample of the Twitter analytics dashboard of my account</em></p>

<p>It looks like the tweets about the summit made my numbers go up.</p>

<h2 id=""2howwebdesignisaformofdatavisualisation"">2: How web design is a form of data visualisation</h2>

<p><em>Get Creative With Data by Imran Younis, <a href=""http://www.laterooms.com/en/home"">LateRooms.com</a></em></p>

<p>Imran talked about using data to fuel creativity. He says the best way to improve your website is by using small teams that contain every role you need, e.g. a designer, developer and analyst, and use these teams to iterate quickly. The workflow follows the concept of <a href=""http://www.gv.com/sprint/"">the design sprint</a>. It contains five steps:</p>

<ol>
<li><strong>Idea</strong>: use data to identify problems and sketch a possible solution. It’s important to use an hypothesis for your idea.  </li>
<li><strong>Build</strong>: work together with a developer to create a working version of the idea and use your sketch to explain it  </li>
<li><strong>Launch</strong>: GOOB (Go Out Of the Building). With your new version ready to test, go out there and let users test it.  </li>
<li><strong>Learn</strong>: use the feedback gathered to further improve the variation;  </li>
<li><strong>Put it live</strong> (or not): after doing both qualitative tests (users) and onsite testing (a/b testing), see if the data shows you that your solution works as you expected it to work. If so: put it live. In either case, repeat step 1 to further optimise your website.</li>
</ol>

<p>After this, repeat the process. You should always be testing and updating your website. As an example, he shares the frequency of code releases by Amazon: one every 11.6 seconds(!) on average. And less than 50% of the changes actually work.</p>

<p>What was really interesting to me about this talk is the way it shed a new light on web design for me. Though he didn't directly describe as such, web design is a way of data visualisation. If your designs are driven by data, then the website (the visualisation) changes based on user behaviour (the data). Interesting right?</p>

<h2 id=""3automateroutine"">3: Automate routine</h2>

<p><em>Talking Geek and Human: How to Make Your Visualisations Visions Actually Happen by Emily Cadman, <a href=""http://www.ft.com/home/uk"">Financial Times (FT)</a></em></p>

<p>Emily had a great talk about how to make sure your visualisations happen. She started with the struggle that most of the visualisers have: there are quite a few, maybe even too many, day to day tasks that keep us from doing the actual projects we want to do. To solve this at FT, she started a project to automate most of the visualisations for FT. The main challenge was to let everyone create graphs, without allowing them to screw them up visually, since all graph should match the FT design standards. The solution they created is Nightingale. </p>

<p>Nightingale is a tool that allows everyone at FT to create basic graphs according to the FT standards. It allows users to copy and paste data into an input field. After that, it generates the graphs for them. It has two ways to guard the quality of the graphs:</p>

<ul>
<li><strong>Style</strong>: users can’t change the styling (colours, tick layout, etc.) of a graph, making sure that they match the style guide of the FT.</li>
<li><strong>Type of graph</strong>: Nightingale automatically picks a type of graph that’s the best fit for the data set. A set that looks like a trended set of data points, will be a line. A categorized set will turn into a bar chart. You can choose another visualisation, but they deliberately hide them below the fold, only showing the best visualisation by default. </li>
</ul>

<p>It’s a great way to make sure all graphs used in your company’s communications (reports, presentations, social media posts) match the same style guide. The great part? It’s open source, free to use. Here’s the link on GitHub: <a href=""https://github.com/Financial-Times/nightingale"">Financial Times Nightingale</a>. </p>

<h2 id=""4thereisnobiteam"">4: There is no BI Team</h2>

<p><em>Putting Data Visualisation and Analytics into the Hands of Everyone at Skyscanner by Mark Shilton, <a href=""http://www.skyscanner.nl/"">Skyscanner</a></em></p>

<p>Mark’s talk began with the history of typing. When typewriters were invented, there used to be rooms filled with people that typed messages for other people. Nowadays, if you'd tell someone you don't type your own texts, they'd probably laugh at you. Nobody has someone else to do all their typing today. </p>

<p>At Skyscanner, they look at BI in the same way. Skyscanner used to have a separate BI team. They did all the BI work for other departments. But  they’ve changed this. They restructured all teams according to Spotify’s Tribe system. In this system, squads are responsible for a specific part or function of the site, e.g. Flight search. A squad should be able to operate independently. This way they can quickly develop their part of the website.  A squad also contains a BI person. If a BI team would be a separate department, they’d sometimes have to wait on results from another team. For more info on the Tribe system, <a href=""https://dl.dropboxusercontent.com/u/1018963/Articles/SpotifyScaling.pdf"">read Spotify’s whitepaper</a>.</p>

<p>Besides that, they try to educate their company, matching tools to their audience:</p>

<ul>
<li>Use Excel for users who mainly work with Excel.</li>
<li>For more advanced analytics colleagues, use tools like Tableau.</li>
<li>Teach your developers how to analyse data with code, e.g. with Python and Pandas. </li>
</ul>

<p>By teaching everyone how to handle data with graphs in the tools they’re familiar with, you’ll move the focus of their questions. Where people would ask “can you put this in a dashboard for me” before, they now ask “where can I get the data I need”. The main point is here is to educate people. Don’t just do the things for them.</p>

<h2 id=""5thepoweroftopologicalanalysistda"">5: The power of Topological Analysis (TDA)</h2>

<p><em>6 Crazy Things Deep Learning and Topological Data Analysis Can Do With Your Data by Edward Kibardin, <a href=""https://badoo.com/"">Badoo</a></em></p>

<p>A different but really interesting talk. It was completely new to me, so this part might be a bit vague. With deep learning, you throw data into a system, and see how the system clusters your data. With this technique, you get an interesting visualisation showing the cluster of e.g. users. The graphs look like a sort of abstract galaxy plot to me. What’s really interesting is how it can learn from your input. Applying TDA, you’ll add labels to specific clusters, e.g. their common interest.  If you apply labels to three groups and repeat this step about three times, the cluster plot will change like this:</p>

<p><img src=""https://media.licdn.com/mpr/mpr/shrinknp_800_800/AAEAAQAAAAAAAAUMAAAAJGFlYjk2OGEzLTI3ODQtNDZkNS04M2UwLWVmZjc3YzE5M2NhYg.png"" alt=""Cluster Graph by KDnuggets"" class=""full-img"">
<em>Image by <a href=""http://www.kdnuggets.com/2015/11/crazy-deep-learning-topological-data-analysis.html"">KD Nuggets</a></em></p>

<p>As you can see, the complex network has changed to a clear collection of clusters. So by guiding the system by manually grouping clusters, the system automatically identified others by itself. If you'd like to read more about the usage of TDA, <a href=""http://www.kdnuggets.com/2015/11/crazy-deep-learning-topological-data-analysis.html"">read Edward's post on KDnuggets</a>.</p>

<h2 id=""6growadatavisualisationculture"">6: Grow a data visualisation culture</h2>

<p><em>Using Shiny to Visualise Crime and Collision Data by Henry Partridge, <a href=""https://tfl.gov.uk/"">Transport for London</a></em></p>

<p>Shiny is a tool to create visualisations with the R programming language. It allows you to quickly create interactive data visualisations and publish them on the net. The nice thing is that it generates it in HTML, CSS and JavaScript, but doesn't require any knowledge of those languages. </p>

<p>Henry talked about how he learned about Shiny, picked it up with some colleagues to make visualisations, and started spreading the knowledge within his company. There’s now a group of over 50 people at his company using Shiny for visualisation. I loved the way how a group's interests in visualising by code, generated a whole group of people doing the same.</p>

<h2 id=""7visualiseintherealworld"">7: Visualise in the real world.</h2>

<p><em>Information Experience Design by Kevin Walker, <a href=""http://www.rca.ac.uk/"">Royal College of Art</a></em></p>

<p>Kevin, from the Royal College of Art, showed visualisations that were closer to art than to the other graphs that most of us see in our day to day jobs. He showed some great examples of data visualisation in the physical world. Kevin and his students use a four step process to create the visualisations:</p>

<ol>
<li>Decomposition  </li>
<li>Pattern recognition  </li>
<li>Abstraction  </li>
<li>Design</li>
</ol>

<p>A creative visualisation that was created by these four steps was the ‘bell visualisation’. A student used a graph showing travellers in the London Underground over time. Based on the two peaks, one in the beginning of a working day and one at the end, he created two bronze bells. This visualisation created a sound out of data. If you’re interested, here’s a link of a new project by the same guys:  <a href=""http://www.change-ringing.co.uk/"">http://www.change-ringing.co.uk/</a>. Other projects of interesting physical data visualisations can be found here: <a href=""http://ied.rca.ac.uk/"">http://ied.rca.ac.uk/</a>.</p>

<h2 id=""bonusround"">Bonus round</h2>

<p>During some of the talks and the panels at the end of each day, some nice anecdotes, quotes and best practices were discussed. The final takeaway is a list of these:</p>

<ul>
<li><strong>Visualising bad data is like putting lipstick on a pig</strong>. This quote came up while discussing a question about the point of visualising data if the data you’re using is bad to begin with. A nice addition to the quote is that you can at least see that it’s a pig with lipstick. So a visualisation might actually show you your data is bad.</li>
<li>It’s important to <strong>determine the goal of your dashboard</strong>. All data in it should you help you to achieve that goal. </li>
<li><strong>Open up data to everyone</strong>. Make your companies’ data accessible to everyone within your company.  This way everyone can grab the data they want themselves, empowering everyone. The level of data access may vary, e.g. a data engineer has access to raw data, and other people might only have access to aggregated reports. </li>
<li><strong>Process data before you visualise it</strong>. Most visualisation tools are good a visualising data, but not as good at handling the data for it. Because of this, you should do all the data processing before you load into your tool.</li>
</ul>

<h2 id=""thatsawrap"">That’s a wrap</h2>

<p>These are the main takeaways of the data visualisation summit for me. I hoped you learned something from my gist of the summit. If you have any additions, feel free to comment.</p>
        ","This week, I attended the Data Visualisation Summit in London. This post contains my 7 key takeaways from the presentations I attended. For those of you looking for a quick snack, here’s the list:
Don’t forget where your data comes from
How web design is a form of data visualisation
Automate routine (with Nightingale)
There is no BI Team
The power of Topological Analysis (TDA)
Grow a data visualisation culture
Visualise in the real world.
If you’re interested in the details, read ahead.
1: Don’t forget where your data comes from
Twitter Data Architecture by Robert Stapenhurst and Filipa Moura, Twitter
The first talk was about data architecture of Twitter. The main point was to show what happens to the data before it ends up in the hands of analysts and visualisation experts. Looking at requests, they describe two main types:
Anticipated: how many people saw my tweet?
Unanticipated: if a football player with a car preference for Mercedes scores during a match, how many people talk about that car brand directly after the goal.
The anticipated part is easy to answer, and therefore included in Twitter’s analytics platform. The second question is more complex. For these questions, they’ve made a report builder, allowing users to run custom queries to answer questions.
Besides this, they briefly discussed analytics.twitter.com, which I didn’t know about. It shows you an overview of your twitter stats of the last 28 days. Here’s a view of my data:
Sample of the Twitter analytics dashboard of my account
It looks like the tweets about the summit made my numbers go up.
2: How web design is a form of data visualisation
Get Creative With Data by Imran Younis, LateRooms.com
Imran talked about using data to fuel creativity. He says the best way to improve your website is by using small teams that contain every role you need, e.g. a designer, developer and analyst, and use these teams to iterate quickly. The workflow follows the concept of the design sprint. It contains five steps:
Idea: use data to identify problems and sketch a possible solution. It’s important to use an hypothesis for your idea.
Build: work together with a developer to create a working version of the idea and use your sketch to explain it
Launch: GOOB (Go Out Of the Building). With your new version ready to test, go out there and let users test it.
Learn: use the feedback gathered to further improve the variation;
Put it live (or not): after doing both qualitative tests (users) and onsite testing (a/b testing), see if the data shows you that your solution works as you expected it to work. If so: put it live. In either case, repeat step 1 to further optimise your website.
After this, repeat the process. You should always be testing and updating your website. As an example, he shares the frequency of code releases by Amazon: one every 11.6 seconds(!) on average. And less than 50% of the changes actually work.
What was really interesting to me about this talk is the way it shed a new light on web design for me. Though he didn't directly describe as such, web design is a way of data visualisation. If your designs are driven by data, then the website (the visualisation) changes based on user behaviour (the data). Interesting right?
3: Automate routine
Talking Geek and Human: How to Make Your Visualisations Visions Actually Happen by Emily Cadman, Financial Times (FT)
Emily had a great talk about how to make sure your visualisations happen. She started with the struggle that most of the visualisers have: there are quite a few, maybe even too many, day to day tasks that keep us from doing the actual projects we want to do. To solve this at FT, she started a project to automate most of the visualisations for FT. The main challenge was to let everyone create graphs, without allowing them to screw them up visually, since all graph should match the FT design standards. The solution they created is Nightingale.
Nightingale is a tool that allows everyone at FT to create basic graphs according to the FT standards. It allows users to copy and paste data into an input field. After that, it generates the graphs for them. It has two ways to guard the quality of the graphs:
Style: users can’t change the styling (colours, tick layout, etc.) of a graph, making sure that they match the style guide of the FT.
Type of graph: Nightingale automatically picks a type of graph that’s the best fit for the data set. A set that looks like a trended set of data points, will be a line. A categorized set will turn into a bar chart. You can choose another visualisation, but they deliberately hide them below the fold, only showing the best visualisation by default.
It’s a great way to make sure all graphs used in your company’s communications (reports, presentations, social media posts) match the same style guide. The great part? It’s open source, free to use. Here’s the link on GitHub: Financial Times Nightingale.
4: There is no BI Team
Putting Data Visualisation and Analytics into the Hands of Everyone at Skyscanner by Mark Shilton, Skyscanner
Mark’s talk began with the history of typing. When typewriters were invented, there used to be rooms filled with people that typed messages for other people. Nowadays, if you'd tell someone you don't type your own texts, they'd probably laugh at you. Nobody has someone else to do all their typing today.
At Skyscanner, they look at BI in the same way. Skyscanner used to have a separate BI team. They did all the BI work for other departments. But they’ve changed this. They restructured all teams according to Spotify’s Tribe system. In this system, squads are responsible for a specific part or function of the site, e.g. Flight search. A squad should be able to operate independently. This way they can quickly develop their part of the website. A squad also contains a BI person. If a BI team would be a separate department, they’d sometimes have to wait on results from another team. For more info on the Tribe system, read Spotify’s whitepaper.
Besides that, they try to educate their company, matching tools to their audience:
Use Excel for users who mainly work with Excel.
For more advanced analytics colleagues, use tools like Tableau.
Teach your developers how to analyse data with code, e.g. with Python and Pandas.
By teaching everyone how to handle data with graphs in the tools they’re familiar with, you’ll move the focus of their questions. Where people would ask “can you put this in a dashboard for me” before, they now ask “where can I get the data I need”. The main point is here is to educate people. Don’t just do the things for them.
5: The power of Topological Analysis (TDA)
6 Crazy Things Deep Learning and Topological Data Analysis Can Do With Your Data by Edward Kibardin, Badoo
A different but really interesting talk. It was completely new to me, so this part might be a bit vague. With deep learning, you throw data into a system, and see how the system clusters your data. With this technique, you get an interesting visualisation showing the cluster of e.g. users. The graphs look like a sort of abstract galaxy plot to me. What’s really interesting is how it can learn from your input. Applying TDA, you’ll add labels to specific clusters, e.g. their common interest. If you apply labels to three groups and repeat this step about three times, the cluster plot will change like this:
Image by KD Nuggets
As you can see, the complex network has changed to a clear collection of clusters. So by guiding the system by manually grouping clusters, the system automatically identified others by itself. If you'd like to read more about the usage of TDA, read Edward's post on KDnuggets.
6: Grow a data visualisation culture
Using Shiny to Visualise Crime and Collision Data by Henry Partridge, Transport for London
Shiny is a tool to create visualisations with the R programming language. It allows you to quickly create interactive data visualisations and publish them on the net. The nice thing is that it generates it in HTML, CSS and JavaScript, but doesn't require any knowledge of those languages.
Henry talked about how he learned about Shiny, picked it up with some colleagues to make visualisations, and started spreading the knowledge within his company. There’s now a group of over 50 people at his company using Shiny for visualisation. I loved the way how a group's interests in visualising by code, generated a whole group of people doing the same.
7: Visualise in the real world.
Information Experience Design by Kevin Walker, Royal College of Art
Kevin, from the Royal College of Art, showed visualisations that were closer to art than to the other graphs that most of us see in our day to day jobs. He showed some great examples of data visualisation in the physical world. Kevin and his students use a four step process to create the visualisations:
Decomposition
Pattern recognition
Abstraction
Design
A creative visualisation that was created by these four steps was the ‘bell visualisation’. A student used a graph showing travellers in the London Underground over time. Based on the two peaks, one in the beginning of a working day and one at the end, he created two bronze bells. This visualisation created a sound out of data. If you’re interested, here’s a link of a new project by the same guys: http://www.change-ringing.co.uk/. Other projects of interesting physical data visualisations can be found here: http://ied.rca.ac.uk/.
Bonus round
During some of the talks and the panels at the end of each day, some nice anecdotes, quotes and best practices were discussed. The final takeaway is a list of these:
Visualising bad data is like putting lipstick on a pig. This quote came up while discussing a question about the point of visualising data if the data you’re using is bad to begin with. A nice addition to the quote is that you can at least see that it’s a pig with lipstick. So a visualisation might actually show you your data is bad.
It’s important to determine the goal of your dashboard. All data in it should you help you to achieve that goal.
Open up data to everyone. Make your companies’ data accessible to everyone within your company. This way everyone can grab the data they want themselves, empowering everyone. The level of data access may vary, e.g. a data engineer has access to raw data, and other people might only have access to aggregated reports.
Process data before you visualise it. Most visualisation tools are good a visualising data, but not as good at handling the data for it. Because of this, you should do all the data processing before you load into your tool.
That’s a wrap
These are the main takeaways of the data visualisation summit for me. I hoped you learned something from my gist of the summit. If you have any additions, feel free to comment.","[Visualisation, Data, Events, Analytics]"
83,How to use responsive breakpoint analytics to measure design performance,/how_to_use_responsive_breakpoint_analytics_to_measure_design_performance/,"
            <p>I was inspired by <a href=""http://philipwalton.com/articles/measuring-your-sites-responsive-breakpoint-usage/"">this article</a> by Philip Walton. Device information in most web analytics tools is sort of meaningless if you are interested in how your website's responsive design performs in terms of viewport width instead of a specific type of device.</p>

<p>I always kept this idea in the back of my head, and when one of our clients was interested in how their new design was performing in real life we had a nice opportunity to pull this old trick out of our sleeve.</p>

<h2 id=""havingfunwiththemediaquerylistobject"">Having fun with the MediaQueryList object</h2>

<p>The most important player in this game is the <code>MediaQueryList</code> object you can create by using the  <code>window.matchMedia</code> method. <mark>Important side note:</mark> This method is only supported from <code>IE10</code>, <code>IE Mobile 10</code>, <code>Android 3.0</code>, <code>Safari 5.1</code> and <code>Safari Mobile 5</code>.</p>

<p>To get a feeling of what this method does open your console and type: <code>window.matchMedia(""(min-width: 400px)"").matches</code> or <code>window.matchMedia(""(orientation: portrait)"").matches</code> <br>
This should give you a boolean whether the statement matches or not. Play around with this and try to create a statement that applies to your own website. Check the MDN documentation of the MediaQueryList object <a href=""https://developer.mozilla.org/en-US/docs/Web/CSS/Media_Queries/Testing_media_queries"">here</a>.</p>

<h2 id=""listyourwebsitesbreakpoints"">List your website's breakpoints</h2>

<p>So first determine what breakpoints you want to test. Of course your website is fluid and doesn't have <a href=""http://bradfrost.com/blog/post/7-habits-of-highly-effective-media-queries/#content"">device based breakpoints</a>, but may have some interesting changes in layout on smaller (or larger) widths. So you want to have an indication of how these layout changes perform relating to each other. Here is how we set our breakpoints variables:</p>

<pre><code class=""language-pretty lang-js"">var breakpoints = {  
  s: '(max-width: 767px)',
  m: '(min-width: 767px) and (max-width: 959px)',
  l: '(min-width: 960px) and (max-width: 1200px)',
  xl: '(min-width: 1200px)'
};
</code></pre>

<p>It's easy to get an overview of the breakpoints that exist on your website. Open your Google Developer tool and switch to mobile view by clicking on the mobile device icon. At the top of your page you can see in interactive visualisation of your websites breakpoints, which can help by defining a list. See an example of the adobe.com website below.</p>

<p><img src=""http://i.imgur.com/8CGwSP6.jpg"" alt=""You can see that the Adobe website uses a lot of breakpoints"" class=""full-img""></p>

<h2 id=""fireabreakpointevent"">Fire a breakpoint event</h2>

<p>First you have to check if the matchMedia function is supported by the browser to prevent errors.  </p>

<pre><code>// Check if matchMedia API exists
var matchMediaIsPossible = typeof window.matchMedia !== 'undefined';  
</code></pre>

<p>Add your set breakpoint object in a function that checks if the keys matches with the current width of your page. </p>

<pre><code>if (matchMediaIsPossible) {  
  Object.keys(breakpoints).forEach(function(breakpoint) {
    var mediaQuerySize = window.matchMedia(breakpoints[breakpoint]);
    // Set the initial breakpoint on page load.
    if (mediaQuerySize.matches) {
      // Fire your analytics event right here
    }
  });
}
</code></pre>

<p>Ideally you want to add a listener to check if your page gets resized.</p>

<pre><code>mediaQuerySize.addListener(function() {  
      if (mediaQuerySize.matches) {
        // Update your analytics event right here
      }
});
</code></pre>

<p>There you go, this is the basic setup of your breakpoint script. You can see a working example in <a href=""http://codepen.io/noudjes/pen/yvgcL"">this pen</a>. </p>

<h2 id=""measureyourwebsitesversioningoogleanalytics"">Measure your website's version in Google Analytics</h2>

<p><em>In this paragraph, <a href=""http://geek.bluemangointeractive.com/author/erik-driessen/"">Erik Driessen</a>, one of our web analysts, explains how to add this data to Google Analytics.</em></p>

<p>As a next step, you might want to add the data to Google Analytics. Normally, when you want to analyse your website across devices, you’d look at the device category defined by Google. But with the rise of phablets, and support for multitasking on tablets, it’ll become less and less clear what version of your website a user is looking at. </p>

<p>The breakpoint script allows you to add the website’s version to Google Analytics. This way, you ignore the device type, and focus on the different versions your website has. One of our clients had four:</p>

<ul>
<li>Mobile</li>
<li>Tablet portrait</li>
<li>Tablet landscape</li>
<li>Desktop</li>
</ul>

<p>We’ve increased their insights on the website performance, by setting a custom dimension on every first pageview of a sessions.  Here’s a standard JavaScript example:</p>

<pre><code>  //set up breakpoints
  var breakpoints = {
    'mobile': '(max-width: 767px)',
    'tablet portrait': '(min-width: 767px) and (max-width: 959px)',
    'tablet landscape': '(min-width: 960px) and (max-width: 1200px)',
    'desktop': '(min-width: 1200px)'
  };

  // Check if matchMedia API exists
  var matchMediaIsPossible = typeof window.matchMedia !== 'undefined';

  if (matchMediaIsPossible) {
    Object.keys(breakpoints).forEach(function(breakpoint) {
      var mediaQuerySize = window.matchMedia(breakpoints[breakpoint]);
      // Set the initial breakpoint on page load.
      if (mediaQuerySize.matches) {
        ga('set', 'dimension1', breakpoint);
        ga('send', 'event', 'Breakpoint', 'Set', breakpoint, {'nonInteraction': 1});
      }
      // Set updated breakpoint on page resize
      mediaQuerySize.addListener(function() {
        if (mediaQuerySize.matches) {
          ga('set', 'dimension1', breakpoint);
          ga('send', 'event', 'Breakpoint', 'Change', breakpoint, {'nonInteraction': 1});
        }
      });
    });
  } else {
    //set to desktop if matchMedia API is not available
    ga('set', 'dimension1', 'desktop');
    ga('send', 'event', 'Breakpoint', 'Not available', 'desktop', {'nonInteraction': 1});
  }
</code></pre>

<p><em>Code example for setting the breakpoint custom dimension</em></p>

<p>Notice that I don’t use the actual breakpoint labels (e.g. xs), but the associated devices. Labelling the custom dimension with a name that’s easy to understand, makes it easier to use for those who are not familiar with CSS breakpoints. Don't forget to change <code>dimension1</code> to match the custom dimension index in your Google Analytics settings. I also set the dimension to desktop when the matchMedia API is not available. Our data showed that all of these sessions were on desktop.</p>

<p>A powerful report to use this dimension in is the Enhanced Ecommerce report. In this report, you’ll instantly see how your website versions are performing. Let’s look at an example:</p>

<p><img src=""http://i64.tinypic.com/smy5qd.png"" alt=""Enhanced Ecommerce sample report"" class=""full-img""></p>

<p><em>The table from the Enhanced Ecommerce report, broken down by the custom dimension Website Version. The data is anonymized.</em></p>

<p>You can clearly see that mobile performance is lacking behind compared to the desktop performance. Though users might pick a product, and finish their order on desktop, this is a good indication that the flow on mobile could be improved. The benefit of the custom dimension is that we ignore the device a user has, and focus on what CSS defined version of a website the user is looking at.</p>

<h2 id=""conclusion"">Conclusion</h2>

<p>You can set these variables in any way you would like to see in you results. Point is that you can get results on a layout width you would like to see instead of device statistics configured by your analytics tool.</p>

<pre><code class=""language-pretty lang-js"">var layoutToTest = {  
  the one with the special new background image: '(min-width: 320px) and (max-width: 460px)',
};
</code></pre>

<p>Playing around with the MediaQueryList can give you some really useful insights. It gives our clients designer a solid proof to make decisions based on actual live testing. </p>
        ","I was inspired by this article by Philip Walton. Device information in most web analytics tools is sort of meaningless if you are interested in how your website's responsive design performs in terms of viewport width instead of a specific type of device.
I always kept this idea in the back of my head, and when one of our clients was interested in how their new design was performing in real life we had a nice opportunity to pull this old trick out of our sleeve.
Having fun with the MediaQueryList object
The most important player in this game is the MediaQueryList object you can create by using the window.matchMedia method. Important side note: This method is only supported from IE10, IE Mobile 10, Android 3.0, Safari 5.1 and Safari Mobile 5.
To get a feeling of what this method does open your console and type: window.matchMedia(""(min-width: 400px)"").matches or window.matchMedia(""(orientation: portrait)"").matches
This should give you a boolean whether the statement matches or not. Play around with this and try to create a statement that applies to your own website. Check the MDN documentation of the MediaQueryList object here.
List your website's breakpoints
So first determine what breakpoints you want to test. Of course your website is fluid and doesn't have device based breakpoints, but may have some interesting changes in layout on smaller (or larger) widths. So you want to have an indication of how these layout changes perform relating to each other. Here is how we set our breakpoints variables:
var breakpoints = {  
  s: '(max-width: 767px)',
  m: '(min-width: 767px) and (max-width: 959px)',
  l: '(min-width: 960px) and (max-width: 1200px)',
  xl: '(min-width: 1200px)'
};
It's easy to get an overview of the breakpoints that exist on your website. Open your Google Developer tool and switch to mobile view by clicking on the mobile device icon. At the top of your page you can see in interactive visualisation of your websites breakpoints, which can help by defining a list. See an example of the adobe.com website below.
Fire a breakpoint event
First you have to check if the matchMedia function is supported by the browser to prevent errors.
// Check if matchMedia API exists
var matchMediaIsPossible = typeof window.matchMedia !== 'undefined';  
Add your set breakpoint object in a function that checks if the keys matches with the current width of your page.
if (matchMediaIsPossible) {  
  Object.keys(breakpoints).forEach(function(breakpoint) {
    var mediaQuerySize = window.matchMedia(breakpoints[breakpoint]);
    // Set the initial breakpoint on page load.
    if (mediaQuerySize.matches) {
      // Fire your analytics event right here
    }
  });
}
Ideally you want to add a listener to check if your page gets resized.
mediaQuerySize.addListener(function() {  
      if (mediaQuerySize.matches) {
        // Update your analytics event right here
      }
});
There you go, this is the basic setup of your breakpoint script. You can see a working example in this pen.
Measure your website's version in Google Analytics
In this paragraph, Erik Driessen, one of our web analysts, explains how to add this data to Google Analytics.
As a next step, you might want to add the data to Google Analytics. Normally, when you want to analyse your website across devices, you’d look at the device category defined by Google. But with the rise of phablets, and support for multitasking on tablets, it’ll become less and less clear what version of your website a user is looking at.
The breakpoint script allows you to add the website’s version to Google Analytics. This way, you ignore the device type, and focus on the different versions your website has. One of our clients had four:
Mobile
Tablet portrait
Tablet landscape
Desktop
We’ve increased their insights on the website performance, by setting a custom dimension on every first pageview of a sessions. Here’s a standard JavaScript example:
  //set up breakpoints
  var breakpoints = {
    'mobile': '(max-width: 767px)',
    'tablet portrait': '(min-width: 767px) and (max-width: 959px)',
    'tablet landscape': '(min-width: 960px) and (max-width: 1200px)',
    'desktop': '(min-width: 1200px)'
  };

  // Check if matchMedia API exists
  var matchMediaIsPossible = typeof window.matchMedia !== 'undefined';

  if (matchMediaIsPossible) {
    Object.keys(breakpoints).forEach(function(breakpoint) {
      var mediaQuerySize = window.matchMedia(breakpoints[breakpoint]);
      // Set the initial breakpoint on page load.
      if (mediaQuerySize.matches) {
        ga('set', 'dimension1', breakpoint);
        ga('send', 'event', 'Breakpoint', 'Set', breakpoint, {'nonInteraction': 1});
      }
      // Set updated breakpoint on page resize
      mediaQuerySize.addListener(function() {
        if (mediaQuerySize.matches) {
          ga('set', 'dimension1', breakpoint);
          ga('send', 'event', 'Breakpoint', 'Change', breakpoint, {'nonInteraction': 1});
        }
      });
    });
  } else {
    //set to desktop if matchMedia API is not available
    ga('set', 'dimension1', 'desktop');
    ga('send', 'event', 'Breakpoint', 'Not available', 'desktop', {'nonInteraction': 1});
  }
Code example for setting the breakpoint custom dimension
Notice that I don’t use the actual breakpoint labels (e.g. xs), but the associated devices. Labelling the custom dimension with a name that’s easy to understand, makes it easier to use for those who are not familiar with CSS breakpoints. Don't forget to change dimension1 to match the custom dimension index in your Google Analytics settings. I also set the dimension to desktop when the matchMedia API is not available. Our data showed that all of these sessions were on desktop.
A powerful report to use this dimension in is the Enhanced Ecommerce report. In this report, you’ll instantly see how your website versions are performing. Let’s look at an example:
The table from the Enhanced Ecommerce report, broken down by the custom dimension Website Version. The data is anonymized.
You can clearly see that mobile performance is lacking behind compared to the desktop performance. Though users might pick a product, and finish their order on desktop, this is a good indication that the flow on mobile could be improved. The benefit of the custom dimension is that we ignore the device a user has, and focus on what CSS defined version of a website the user is looking at.
Conclusion
You can set these variables in any way you would like to see in you results. Point is that you can get results on a layout width you would like to see instead of device statistics configured by your analytics tool.
var layoutToTest = {  
  the one with the special new background image: '(min-width: 320px) and (max-width: 460px)',
};
Playing around with the MediaQueryList can give you some really useful insights. It gives our clients designer a solid proof to make decisions based on actual live testing.","[Code, Analytics]"
84,How article size helps you understand your content performance,/how-article-size-helps-you-understand-your-content-performance/,"
            <p>Last month, we shared <a href=""http://geek.bluemangointeractive.com/track-content-performance-using-google-analytics-enhanced-ecommerce-report/"">our insights on the enhanced ecommerce for content report</a> that we’ve implemented on GEEK. With the reports, we measure what articles are read, how often posts from a specific author are read, and how many words people read on our website. I’ve also mentioned that we would add article size to give insights on how different article sizes are performing. In this post, we’re focusing on that.</p>

<h2 id=""addingthecustomdimension"">Adding the custom dimension</h2>

<p>As described in the post, we’ve added three different types of article sizes:</p>

<ul>
<li>Small: less than 500 words;</li>
<li>Medium: a word count of 500 or more and less than 1000;</li>
<li>Large: a word count of 1000 words or more.</li>
</ul>

<p>The goal of this update was to see what article size works best for our readers. Because it gives us more info about an article, we've added it as a product dimension in Google Analytics. This way, we can use the enhanced ecommerce metrics and apply them to the article sizes.</p>

<h2 id=""theanalyses"">The analyses</h2>

<p>The first insights we can get from enhanced ecommerce is on read articles. I’ve set up a custom report with the following settings:</p>

<p><img src=""http://i64.tinypic.com/ny762p.png"" alt=""Google Analytics Article Size Custom Report"" class=""full-img""></p>

<p><em>Set up a custom report and set the dimension to article size, and add metrics for revenue (word count) and quantity (number of reads).</em></p>

<p>You would expect to see the highest word count for large articles, and a reversed order for number of reads, as small articles will trigger a full read quicker than large ones. Let’s look at the data for October:</p>

<p><img src=""http://i64.tinypic.com/veoj1c.png"" alt=""Google Analytics Article Size results"" class=""full-img"">
<em>Article size results of October 2015</em></p>

<p>As expected, we see the highest word count for large articles, followed by medium and small articles. We also see an exact reversed order  for quantity, starting with small articles at 816 full reads, and ending with large articles at 663 full reads. Now we have the data to show that our articles perform as we thought they would. It also shows that large articles are read quite well, as they almost double the word count of medium articles, with about 17% less reads. </p>

<h2 id=""amoreinterestinganalyses"">A more interesting analyses</h2>

<p>With enhanced ecommerce implemented, we’re also able to leverage the power of these reports metrics to give more interesting insights. For this report, I’ve added a second tab to the custom report and added the following dimensions and metrics:</p>

<p><img src=""http://i68.tinypic.com/25a669c.png"" alt=""Google Analytics Article Size Flow Custom Report"" class=""full-img""></p>

<p><em>Set up a custom report and set the dimension to article size, and add metrics for product detail views (article opens), cart to detail rate (scroll interaction rate on an article) and buy to detail rate (full read rate).</em></p>

<p>With this report, you’ll get a better understanding on how article sizes perform. I would expect that the longer the article, the bigger the drop off during the stages of reading, e.g.  a lower full read rate compared to small articles. Let’s look at our data for October:</p>

<p><img src=""http://i63.tinypic.com/mbmxcn.png"" alt=""Google Analytics Article Size Flow results"" class=""full-img"">
<em>Article size reading flow results of October 2015</em></p>

<p>We see something interesting here:</p>

<ul>
<li><strong>Article opens</strong>: Small articles tend to have the most article opens (product views), the medium 
and large ones are roughly equal in numbers. This means that small articles might be good ways to get people onto our website and trigger a first read. Or they might be quick snacks for people looking for a short and interesting read.  </li>
<li><strong>Scroll interactions</strong>: The large articles have a slightly lower scroll interaction rate than the other two article size. This indicates that users find the first part of these articles less interesting, since these users start scrolling less often. Large articles have up to 8% less scrolls compared to medium articles. </li>
<li><strong>Full reads</strong>: Looking at full read rate (buy to detail) we clearly see that medium articles have the best performance. Comparing the small articles’ rate to the medium articles’ rate, we see a performance that's 19% lower than the medium ones.</li>
</ul>

<h2 id=""puttingittogether"">Putting it together</h2>

<p>Combining the numbers of the two reports, we can tell that medium articles don’t generate the most read words or most article reads. But looking at quality, focusing on scroll interaction rates and full read rates, medium articles perform better than both small and medium articles. This could indicate that medium articles...</p>

<ul>
<li>... hit the sweet spot for article length; or</li>
<li>... are better written than articles of other sizes.</li>
</ul>

<p>As a closing comment, we can now tell that by adding the custom dimension, we know that all article sizes serve a purpose on GEEK:</p>

<ul>
<li>Small articles generate the most article opens;</li>
<li>Large articles generate the highest number of read words; and</li>
<li>Medium articles are the best performing articles.</li>
</ul>
        ","Last month, we shared our insights on the enhanced ecommerce for content report that we’ve implemented on GEEK. With the reports, we measure what articles are read, how often posts from a specific author are read, and how many words people read on our website. I’ve also mentioned that we would add article size to give insights on how different article sizes are performing. In this post, we’re focusing on that.
Adding the custom dimension
As described in the post, we’ve added three different types of article sizes:
Small: less than 500 words;
Medium: a word count of 500 or more and less than 1000;
Large: a word count of 1000 words or more.
The goal of this update was to see what article size works best for our readers. Because it gives us more info about an article, we've added it as a product dimension in Google Analytics. This way, we can use the enhanced ecommerce metrics and apply them to the article sizes.
The analyses
The first insights we can get from enhanced ecommerce is on read articles. I’ve set up a custom report with the following settings:
Set up a custom report and set the dimension to article size, and add metrics for revenue (word count) and quantity (number of reads).
You would expect to see the highest word count for large articles, and a reversed order for number of reads, as small articles will trigger a full read quicker than large ones. Let’s look at the data for October:
Article size results of October 2015
As expected, we see the highest word count for large articles, followed by medium and small articles. We also see an exact reversed order for quantity, starting with small articles at 816 full reads, and ending with large articles at 663 full reads. Now we have the data to show that our articles perform as we thought they would. It also shows that large articles are read quite well, as they almost double the word count of medium articles, with about 17% less reads.
A more interesting analyses
With enhanced ecommerce implemented, we’re also able to leverage the power of these reports metrics to give more interesting insights. For this report, I’ve added a second tab to the custom report and added the following dimensions and metrics:
Set up a custom report and set the dimension to article size, and add metrics for product detail views (article opens), cart to detail rate (scroll interaction rate on an article) and buy to detail rate (full read rate).
With this report, you’ll get a better understanding on how article sizes perform. I would expect that the longer the article, the bigger the drop off during the stages of reading, e.g. a lower full read rate compared to small articles. Let’s look at our data for October:
Article size reading flow results of October 2015
We see something interesting here:
Article opens: Small articles tend to have the most article opens (product views), the medium and large ones are roughly equal in numbers. This means that small articles might be good ways to get people onto our website and trigger a first read. Or they might be quick snacks for people looking for a short and interesting read.
Scroll interactions: The large articles have a slightly lower scroll interaction rate than the other two article size. This indicates that users find the first part of these articles less interesting, since these users start scrolling less often. Large articles have up to 8% less scrolls compared to medium articles.
Full reads: Looking at full read rate (buy to detail) we clearly see that medium articles have the best performance. Comparing the small articles’ rate to the medium articles’ rate, we see a performance that's 19% lower than the medium ones.
Putting it together
Combining the numbers of the two reports, we can tell that medium articles don’t generate the most read words or most article reads. But looking at quality, focusing on scroll interaction rates and full read rates, medium articles perform better than both small and medium articles. This could indicate that medium articles...
... hit the sweet spot for article length; or
... are better written than articles of other sizes.
As a closing comment, we can now tell that by adding the custom dimension, we know that all article sizes serve a purpose on GEEK:
Small articles generate the most article opens;
Large articles generate the highest number of read words; and
Medium articles are the best performing articles.",[Analytics]
85,How to improve Visual Website Optimizer's Google Tag Manager integration,/improving-visual-website-optimizers-gtm-integration/,"
            <p>If you're running Google Tag Manager on your site, and use Visual Website Optimizer for A/B Testing, you've probably noticed that VWO's standard GTM integration is slightly lacking. <a href=""https://vwo.com/knowledge/integrate-vwo-with-gtm/"">While the integration suggested by VWO does work</a>, you're left with a lot of manual work, as you have to add or change tags for every new test.</p>

<h3 id=""theissue"">The issue</h3>

<p>The main issue with the integration is VWO's use of key/pair values in the dataLayer. For each campaign, Visual Website Optimizer adds 2 objects to GTM's dataLayer:</p>

<ul>
<li><code>Campaign-1: ""Variation-1""</code></li>
<li><code>event: ""VWO""</code></li>
</ul>

<p>The original integration uses the <code>VWO</code> event and fires a tag that sends <code>Variation-1</code> to Google Analytics. If you're using a Custom Dimension that means you could potentially have <code>Variation-1</code> for multiple tests in your dimension, and have no way in tracking which campaign it was. You'll also have to add a new tag for every test you want to run.</p>

<h3 id=""howtoimprove"">How to improve</h3>

<p>Improving the standard integration is easy, and you'll only have to do it once. First, you need a <em>Trigger</em> for <code>event equals VWO</code>, so you can fire a tag when VWO adds data to the dataLayer. Next, make the following <em>custom HTML tag</em>:</p>

<p><img src=""http://i64.tinypic.com/iz3tis.png"" alt=""alt text"" class=""full-img""></p>

<pre><code>&lt;script&gt;  
// jQuery version
$.each(dataLayer,function(i,v){
    for(var k in dataLayer[i]){
      if(/^Campaign/.test(k))
        dataLayer.push({'event':'vwovar','vwoCampVar': k+""_""+dataLayer[i][k]});   
    }
});
&lt;/script&gt;  
</code></pre>

<p>if you're not using jQuery, or don't want your tags to be dependent, use the regular javascript version:  </p>

<pre><code>&lt;script&gt;  
// Regular JS
for (var key in dataLayer) {  
   if (dataLayer.hasOwnProperty(key)) {
       var obj = dataLayer[key];
        for (var prop in obj) {
          if(obj.hasOwnProperty(prop)){
            if(/^Campaign/.test(prop))
                dataLayer.push({'event':'vwovar','vwoCampVar': prop+""_""+obj[prop]});  
          }
       }
    }
}
&lt;/script&gt;  
</code></pre>

<p>For every page that has a test live, you should now have an extra dataLayer object called <code>vwoCampVar</code>; the campaign ánd the variant. Next, create the <code>vwoCampVar</code> variable so you can use it in GTM: <br>
<img src=""http://i63.tinypic.com/nz0qi0.png"" alt=""alt text"" class=""full-img""></p>

<p>You now have a variable in GTM that you can send to Analytics any way you wish, the content of the variable isn't something like <code>Variation-1</code> anymore, but <code>Campaign-1_Variation-1</code>, letting you find the right campaign in Analytics and making sure your reports are accurate. Filtering and creating custom reports is much easier, and you can plot lines like a pro. This is how it will look if you name your Custom Dimension 'VWO Tests'.</p>

<p><img src=""http://i67.tinypic.com/o7ls9j.png"" alt=""alt text""></p>

<p><em>Bonus: if you name your variations in VWO (you should), that name will show up in GA!</em></p>
        ","If you're running Google Tag Manager on your site, and use Visual Website Optimizer for A/B Testing, you've probably noticed that VWO's standard GTM integration is slightly lacking. While the integration suggested by VWO does work, you're left with a lot of manual work, as you have to add or change tags for every new test.
The issue
The main issue with the integration is VWO's use of key/pair values in the dataLayer. For each campaign, Visual Website Optimizer adds 2 objects to GTM's dataLayer:
Campaign-1: ""Variation-1""
event: ""VWO""
The original integration uses the VWO event and fires a tag that sends Variation-1 to Google Analytics. If you're using a Custom Dimension that means you could potentially have Variation-1 for multiple tests in your dimension, and have no way in tracking which campaign it was. You'll also have to add a new tag for every test you want to run.
How to improve
Improving the standard integration is easy, and you'll only have to do it once. First, you need a Trigger for event equals VWO, so you can fire a tag when VWO adds data to the dataLayer. Next, make the following custom HTML tag:
<script>  
// jQuery version
$.each(dataLayer,function(i,v){
    for(var k in dataLayer[i]){
      if(/^Campaign/.test(k))
        dataLayer.push({'event':'vwovar','vwoCampVar': k+""_""+dataLayer[i][k]});   
    }
});
</script>  
if you're not using jQuery, or don't want your tags to be dependent, use the regular javascript version:
<script>  
// Regular JS
for (var key in dataLayer) {  
   if (dataLayer.hasOwnProperty(key)) {
       var obj = dataLayer[key];
        for (var prop in obj) {
          if(obj.hasOwnProperty(prop)){
            if(/^Campaign/.test(prop))
                dataLayer.push({'event':'vwovar','vwoCampVar': prop+""_""+obj[prop]});  
          }
       }
    }
}
</script>  
For every page that has a test live, you should now have an extra dataLayer object called vwoCampVar; the campaign ánd the variant. Next, create the vwoCampVar variable so you can use it in GTM:
You now have a variable in GTM that you can send to Analytics any way you wish, the content of the variable isn't something like Variation-1 anymore, but Campaign-1_Variation-1, letting you find the right campaign in Analytics and making sure your reports are accurate. Filtering and creating custom reports is much easier, and you can plot lines like a pro. This is how it will look if you name your Custom Dimension 'VWO Tests'.
Bonus: if you name your variations in VWO (you should), that name will show up in GA!","[tag management, Analytics, cro]"
86,Start using page types to make your tag management life easier,/start-using-page-types-to-make-your-tag-management-life-easier/,"
            <p>At Blue Mango Interactive, we work with different tag management vendors. All of them have their own version of the data layer. This post is about using the differences between them to your advantage. We’ll look at the page type standard from Qubit’s universal variable data layer and apply it to Google’s data layer for GTM. </p>

<h2 id=""whatarepagetypes"">What are page types?</h2>

<p>Page types group pages in categories based on their functional role. The page types instantly tell you where a user is on your website. As a standard, Qubit uses the following list:</p>

<ul>
<li><strong>home</strong>: for the homepage;</li>
<li><strong>content</strong>: for regular content page, e.g. a blog or about us page;</li>
<li><strong>category</strong>: pages that list products;</li>
<li><strong>product</strong>: product detail pages;</li>
<li><strong>basket</strong>: the shopping basket of your page;</li>
<li><strong>checkout</strong>: the steps between the shopping basket and a transaction;</li>
<li><strong>confirmation</strong>: the transaction page;  and</li>
<li><strong>search</strong>: a search results page.</li>
</ul>

<p>These are  the eight page types Qubit lists in their universal variable documentation. We’ve added Account ourselves because some or our clients' websites have an account environment: </p>

<ul>
<li><strong>account</strong>: the pages where the users manage their accounts, look at order status, etc. The account pages normally require the user to be logged in. </li>
</ul>

<h2 id=""okaynicebuthowdotheyhelp"">Okay nice, but how do they help?</h2>

<p>If you implement a lot of tags, the triggers for tags will vary from easy to complex. Page types take the complexity out of some your triggers. We’ll take category pages as an example. </p>

<h4 id=""withoutpagetypes"">Without page types</h4>

<p>If I have to place a tag on all category pages, I normally check the pages of the client’s website to find the best possible trigger. Normally, I look for a pattern in the URL structure. Unfortunately, they normally don’t have <em>category</em> written in there. They do tend to have a certain pattern, but it’s hard to capture it.</p>

<p>Let’s take a fictional fashion store as an example. The category page structure will probably follow the following format:</p>

<ul>
<li>/category</li>
<li>/category/subcategory</li>
</ul>

<p>Looking at the actual URLs, the category and subcategory are replaced by actual category names:</p>

<ul>
<li>/shoes</li>
<li>/shoes/sneakers</li>
<li>/shoes/boots</li>
<li>/shoes/…</li>
<li>/jackets</li>
<li>/jackets/long</li>
<li>/jackets/short</li>
<li>/jackets/…</li>
</ul>

<p>It’s possible to capture these URL variations with a regex like <code>^\/[a-z]*\/[a-z]*$</code>, but you could end up including other content pages that match the regex as well, for example <code>/blog/our-company-at-the-amsterdam-fashion-week</code>.  To rule these out, you’d have to know all possible category labels.</p>

<h4 id=""withpagetypes"">With page types</h4>

<p>If you’d have page types, all you have to do is check if the page type equals category. Besides triggers, page types also allow you to add extra information about pages in Google Analytics. I'll talk about setting this up later.</p>

<h2 id=""transformingthequbitpagetypestandardtoagoogletagmanagerstandard"">Transforming the Qubit page type standard to a Google Tag Manager standard</h2>

<p>For the implementation of the page types, we’ll start with Qubit’s documentation on their data layer, the universal variable (or uv for short). </p>

<pre><code class=""language- prettyprint lang-js prettyprinted""><span class=""pln"">window</span><span class=""pun"">.</span><span class=""pln"">universal_variable </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">  
  page</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">
    type</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""str"">''</span><span class=""pln"">
  </span><span class=""pun"">}</span><span class=""pln"">
</span><span class=""pun"">}</span></code></pre>

<p>Note that the variable is nested. The uv doesn’t contain a <code>pagetype</code> variable, but a <code>page</code> variable with <code>type</code> nested inside. This allows you to add more page information within the page variable. Read <a href=""http://docs.qubitproducts.com/uv/parameters/page.html"">the Qubit documentation</a> for more ideas for page variables. At GEEK, we use it to store page author and article size information.</p>

<p>Technically, you’d be able to implement Qubit's uv and use it in GTM by setting up a custom JavaScript variable. But since we’re talking about a GTM implementation, we’ll transform it into a dataLayer variable:</p>

<pre><code class=""language- prettyprint lang-js prettyprinted""><span class=""pln"">window</span><span class=""pun"">.</span><span class=""pln"">dataLayer </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""pun"">[{</span><span class=""pln"">  
  page</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">
    type</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""str"">''</span><span class=""pln"">
  </span><span class=""pun"">}</span><span class=""pln"">
</span><span class=""pun"">}];</span></code></pre>

<p>I advise you to implement this part of the data layer in the <code>&lt;head&gt;</code>, so it's available before loading the GTM container in the <code>&lt;body&gt;</code>. If you want to push it to the dataLayer at a later stage, add it like this:</p>

<pre><code class=""language- prettyprint lang-js prettyprinted""><span class=""pln"">window</span><span class=""pun"">.</span><span class=""pln"">dataLayer</span><span class=""pun"">.</span><span class=""pln"">push</span><span class=""pun"">({</span><span class=""pln"">  
  page</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">
    type</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""str"">''</span><span class=""pln"">
  </span><span class=""pun"">}</span><span class=""pln"">
</span><span class=""pun"">});</span></code></pre>

<h2 id=""settingupthegtmvariable"">Setting up the GTM variable</h2>

<p>With page types available in the dataLayer, you’re ready to set up the variable in GTM. This will be a dataLayer variable:</p>

<p><img src=""http://i68.tinypic.com/2uy0oox.png"" alt=""GTM page type variable"" class=""full-img"">
<em>Add a dataLayer variable for page types to start using them in GTM</em></p>

<p>With the variable added, you’re ready to start using the variable in several ways:</p>

<h3 id=""option1triggers"">Option 1: Triggers</h3>

<p>Set up a page type trigger. This allows you to fire tags on specific page types. You could use this to send data to Google Analytics for specific page types, or load feedback or assistance tools more effectively to better guide users through the customer journey:</p>

<p><img src=""http://i68.tinypic.com/zip304.png"" alt=""GTM pagetype trigger"" class=""full-img"">
<em>Add a page type trigger to load tags on specific page types</em></p>

<h3 id=""option2customdimensions"">Option 2: Custom dimensions</h3>

<p>You can also setup page types as  custom dimension. I’ll give you two examples:</p>

<p><strong>Track page types for each pageview</strong></p>

<p>This is a basic one. Add a custom dimension to each pageview on your site. You’ll be able to filter your page reports on the different pagetypes, helping you understand how different types of pages are performing. The setup contains two steps:</p>

<ul>
<li>1: Set a custom dimension in Google Analytics with a hit scope, to make sure a new value is added for every pageview.</li>
<li>2: Add the dimension to your pageview tag.</li>
</ul>

<p><strong>1: Set the custom dimension</strong></p>

<p><img src=""http://i64.tinypic.com/e9dzdg.png"" alt=""GTM pagetype custom dimension"">
<em>Create a hit scope custom dimension in Google Analytics to add page types to each pageview</em></p>

<p><strong>2: Add the custom dimension to the pageview tag</strong>
<img src=""http://i67.tinypic.com/s1ndao.png"" alt=""GTM pagetype custom dimension in GTM"" class=""full-img"">
<em>Add the custom dimension to your GTM pageview tag to add page type data to each pageview</em></p>

<p><strong>Track landing page types</strong></p>

<p>This one is a bit more complex. We normally use landing pages to see how users with specific page entries act on our website. With landing page types, we’ll be able to group these landing page based on their functions. This way we can see how well category landing pages perform compared to a product page entry. Keep in mind that this is only useful for the page types home, content, category and product. The other page types only occur in the later stages of the buying cycle and won’t be a landing page because of it. </p>

<p>The setup contains four steps:</p>

<ul>
<li>1: Set a custom dimension in Google Analytics with a session scope, to set the landing page type for the session of the user.</li>
<li>2: Create an event tag to send the landing page type custom dimension to Google Analytics.</li>
<li>3: Create a referral hostname variable to use in the trigger for the event.</li>
<li>4: Add a landing page trigger to the event so it will only fire on the first pageview of a user on your website.</li>
</ul>

<p><strong>1: Set a custom dimension with session scope</strong>
<img src=""http://i68.tinypic.com/29xjbb8.png"" alt=""GTM landing page type custom dimension in GA"">
<em>Add the custom dimension to your GTM pageview tag to add page type data to each pageview.</em></p>

<p><strong>2: Create an event tag to send the landing page type custom dimension to Google Analytics</strong>
<img src=""http://i63.tinypic.com/288cvwg.png"" alt=""GTM landing page type custom dimension in GTM"" class=""full-img"">
<em>Add the custom dimension to your GTM pageview tag to add page type data to each pageview</em></p>

<p><strong>3: Create a referral hostname variable to use in the trigger for the event</strong>
<img src=""http://i68.tinypic.com/33crn02.png"" alt=""GTM referral hostname variable"" class=""full-img"">
<em>Add a HTTP Referrer hostname variable to your GTM container.</em></p>

<p><strong>4: Add a landing page trigger to trigger the event on the first pageview</strong>
<img src=""http://i63.tinypic.com/2s1m4cl.png"" alt=""GTM landing page trigger"" class=""full-img"">
<em>Add a trigger to the event tag that only fires the tag on the first pageview on your site. On your website, you'll need to change it to match your own hostname.</em></p>

<p>These are two options that show you how you could use page types to your advantage as custom dimensions. </p>

<h2 id=""keeplookingaround"">Keep looking around</h2>

<p>What I wanted to show with this post is that when you see something interesting for tool a, and you’re using tool b, you’re able to apply the learnings from tool a on tool b. So don’t think ‘I don’t have that tool, so I’ll skip that post’, and do think ‘that looks interesting, let’s see how I can apply this to the products I’m using’.</p>
        ","At Blue Mango Interactive, we work with different tag management vendors. All of them have their own version of the data layer. This post is about using the differences between them to your advantage. We’ll look at the page type standard from Qubit’s universal variable data layer and apply it to Google’s data layer for GTM.
What are page types?
Page types group pages in categories based on their functional role. The page types instantly tell you where a user is on your website. As a standard, Qubit uses the following list:
home: for the homepage;
content: for regular content page, e.g. a blog or about us page;
category: pages that list products;
product: product detail pages;
basket: the shopping basket of your page;
checkout: the steps between the shopping basket and a transaction;
confirmation: the transaction page; and
search: a search results page.
These are the eight page types Qubit lists in their universal variable documentation. We’ve added Account ourselves because some or our clients' websites have an account environment:
account: the pages where the users manage their accounts, look at order status, etc. The account pages normally require the user to be logged in.
Okay nice, but how do they help?
If you implement a lot of tags, the triggers for tags will vary from easy to complex. Page types take the complexity out of some your triggers. We’ll take category pages as an example.
Without page types
If I have to place a tag on all category pages, I normally check the pages of the client’s website to find the best possible trigger. Normally, I look for a pattern in the URL structure. Unfortunately, they normally don’t have category written in there. They do tend to have a certain pattern, but it’s hard to capture it.
Let’s take a fictional fashion store as an example. The category page structure will probably follow the following format:
/category
/category/subcategory
Looking at the actual URLs, the category and subcategory are replaced by actual category names:
/shoes
/shoes/sneakers
/shoes/boots
/shoes/…
/jackets
/jackets/long
/jackets/short
/jackets/…
It’s possible to capture these URL variations with a regex like ^\/[a-z]*\/[a-z]*$, but you could end up including other content pages that match the regex as well, for example /blog/our-company-at-the-amsterdam-fashion-week. To rule these out, you’d have to know all possible category labels.
With page types
If you’d have page types, all you have to do is check if the page type equals category. Besides triggers, page types also allow you to add extra information about pages in Google Analytics. I'll talk about setting this up later.
Transforming the Qubit page type standard to a Google Tag Manager standard
For the implementation of the page types, we’ll start with Qubit’s documentation on their data layer, the universal variable (or uv for short).
window.universal_variable = {  
  page: {
    type: ''
  }
}
Note that the variable is nested. The uv doesn’t contain a pagetype variable, but a page variable with type nested inside. This allows you to add more page information within the page variable. Read the Qubit documentation for more ideas for page variables. At GEEK, we use it to store page author and article size information.
Technically, you’d be able to implement Qubit's uv and use it in GTM by setting up a custom JavaScript variable. But since we’re talking about a GTM implementation, we’ll transform it into a dataLayer variable:
window.dataLayer = [{  
  page: {
    type: ''
  }
}];
I advise you to implement this part of the data layer in the <head>, so it's available before loading the GTM container in the <body>. If you want to push it to the dataLayer at a later stage, add it like this:
window.dataLayer.push({  
  page: {
    type: ''
  }
});
Setting up the GTM variable
With page types available in the dataLayer, you’re ready to set up the variable in GTM. This will be a dataLayer variable:
Add a dataLayer variable for page types to start using them in GTM
With the variable added, you’re ready to start using the variable in several ways:
Option 1: Triggers
Set up a page type trigger. This allows you to fire tags on specific page types. You could use this to send data to Google Analytics for specific page types, or load feedback or assistance tools more effectively to better guide users through the customer journey:
Add a page type trigger to load tags on specific page types
Option 2: Custom dimensions
You can also setup page types as custom dimension. I’ll give you two examples:
Track page types for each pageview
This is a basic one. Add a custom dimension to each pageview on your site. You’ll be able to filter your page reports on the different pagetypes, helping you understand how different types of pages are performing. The setup contains two steps:
1: Set a custom dimension in Google Analytics with a hit scope, to make sure a new value is added for every pageview.
2: Add the dimension to your pageview tag.
1: Set the custom dimension
Create a hit scope custom dimension in Google Analytics to add page types to each pageview
2: Add the custom dimension to the pageview tag
Add the custom dimension to your GTM pageview tag to add page type data to each pageview
Track landing page types
This one is a bit more complex. We normally use landing pages to see how users with specific page entries act on our website. With landing page types, we’ll be able to group these landing page based on their functions. This way we can see how well category landing pages perform compared to a product page entry. Keep in mind that this is only useful for the page types home, content, category and product. The other page types only occur in the later stages of the buying cycle and won’t be a landing page because of it.
The setup contains four steps:
1: Set a custom dimension in Google Analytics with a session scope, to set the landing page type for the session of the user.
2: Create an event tag to send the landing page type custom dimension to Google Analytics.
3: Create a referral hostname variable to use in the trigger for the event.
4: Add a landing page trigger to the event so it will only fire on the first pageview of a user on your website.
1: Set a custom dimension with session scope
Add the custom dimension to your GTM pageview tag to add page type data to each pageview.
2: Create an event tag to send the landing page type custom dimension to Google Analytics
Add the custom dimension to your GTM pageview tag to add page type data to each pageview
3: Create a referral hostname variable to use in the trigger for the event
Add a HTTP Referrer hostname variable to your GTM container.
4: Add a landing page trigger to trigger the event on the first pageview
Add a trigger to the event tag that only fires the tag on the first pageview on your site. On your website, you'll need to change it to match your own hostname.
These are two options that show you how you could use page types to your advantage as custom dimensions.
Keep looking around
What I wanted to show with this post is that when you see something interesting for tool a, and you’re using tool b, you’re able to apply the learnings from tool a on tool b. So don’t think ‘I don’t have that tool, so I’ll skip that post’, and do think ‘that looks interesting, let’s see how I can apply this to the products I’m using’.",[Analytics]
87,Huh? Does DCM use the creative's source code for its validation?,/dcm-uses-source-code-for-their-validation/,"
            <p>We serve most of our ads using Appnexus, but now and then we need to use a different adserver. Recently we wanted to upload our creative to Google's DoubleClick Campaign Manager (DCM), part of the DoubleClick Digital Marketing platform. DCM has quite a lot of requirements for the creatives that are uploaded. For istance, you have to put all the assets in the root of the ZIP, all external assets need to be loaded over HTTPS and they do not allow 404's. These requirements actually make sense. </p>

<p>My initial upload was declined by DCM. The reason for the rejection was a missing image, <em>transparent.png</em>. I was quite suprised by it, because I've checked the creative against all their requirements. </p>

<p>So I started looking for <em>transparent.png</em> in to the source code of the creative, and ran into this code snippet:</p>

<pre><code class=""language-prettyprint lang-javascript prettyprinted""><span class=""kwd"">if</span><span class=""pln""> </span><span class=""pun"">(</span><span class=""pln"">splash</span><span class=""pun"">.</span><span class=""pln"">isPreviewer</span><span class=""pun"">)</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">  
  elm</span><span class=""pun"">.</span><span class=""pln"">style</span><span class=""pun"">.</span><span class=""pln"">backgroundImage </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""str"">'url(/images/transparent.png)'</span><span class=""pun"">;</span><span class=""pln"">
</span><span class=""pun"">}</span></code></pre>

<p>The weird thing is that this code never executes in a production envirnonment, because <code>splash.isPreviewer</code> is only evaluated to <code>true</code> when the code is executed in our own tool Splash.Create. Does this mean that DCM scans the source code to validate the uploaded creative, instead of looking what code is actually executed in the browser? Just to verify this, I changed the code to this:  </p>

<pre><code class=""language-prettyprint lang-javascript prettyprinted""><span class=""kwd"">if</span><span class=""pln""> </span><span class=""pun"">(</span><span class=""kwd"">false</span><span class=""pun"">)</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">  
  elm</span><span class=""pun"">.</span><span class=""pln"">style</span><span class=""pun"">.</span><span class=""pln"">backgroundImage </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""str"">'url(/images/transparent.png)'</span><span class=""pun"">;</span><span class=""pln"">
</span><span class=""pun"">}</span></code></pre>

<p>Yep. Still no luck. It looks like they just run a regular expression on the source to identify images and see if the matched assets exist in the uploaded ZIP. I'm curious why they have chosen this approach, mostly because the increase of dynamic content in creatives where the initial source code can potentially be totally different for every viewer. </p>

<p>A few months ago I also got a weird error message in DCM saying ""<em>I can't use the variable <code>u</code> in the source code</em>"". I never understood what was going on there. </p>

<p>My overall feeling is that their validation process is very strict and over-automated. Over a year ago I've read some docs by Google in which they (of course) recommend using their HTML5 authoring tool <a href=""https://www.google.com/webdesigner/"">Google Web Designer</a>, and maybe DCM's validation process is optimised for creatives that are built with GWD as well. </p>
        ","We serve most of our ads using Appnexus, but now and then we need to use a different adserver. Recently we wanted to upload our creative to Google's DoubleClick Campaign Manager (DCM), part of the DoubleClick Digital Marketing platform. DCM has quite a lot of requirements for the creatives that are uploaded. For istance, you have to put all the assets in the root of the ZIP, all external assets need to be loaded over HTTPS and they do not allow 404's. These requirements actually make sense.
My initial upload was declined by DCM. The reason for the rejection was a missing image, transparent.png. I was quite suprised by it, because I've checked the creative against all their requirements.
So I started looking for transparent.png in to the source code of the creative, and ran into this code snippet:
if (splash.isPreviewer) {  
  elm.style.backgroundImage = 'url(/images/transparent.png)';
}
The weird thing is that this code never executes in a production envirnonment, because splash.isPreviewer is only evaluated to true when the code is executed in our own tool Splash.Create. Does this mean that DCM scans the source code to validate the uploaded creative, instead of looking what code is actually executed in the browser? Just to verify this, I changed the code to this:
if (false) {  
  elm.style.backgroundImage = 'url(/images/transparent.png)';
}
Yep. Still no luck. It looks like they just run a regular expression on the source to identify images and see if the matched assets exist in the uploaded ZIP. I'm curious why they have chosen this approach, mostly because the increase of dynamic content in creatives where the initial source code can potentially be totally different for every viewer.
A few months ago I also got a weird error message in DCM saying ""I can't use the variable u in the source code"". I never understood what was going on there.
My overall feeling is that their validation process is very strict and over-automated. Over a year ago I've read some docs by Google in which they (of course) recommend using their HTML5 authoring tool Google Web Designer, and maybe DCM's validation process is optimised for creatives that are built with GWD as well.",[adserving]
88,React Router: a comprehensive introduction,/react-router-an-introduction/,"
            <p>When you need to route between different parts of your React application, you'll probably need a router. The most popular choice to do this in React is <a href=""https://github.com/rackt/react-router"">React Router</a>. If you are familiar with routing in Ember, you'll get your routing up and running with React Router in no time. </p>

<p>In the following introduction to React Router I assume you already have a development environment up and running and you are familiar with React and CommonJS modules. It uses React 0.13. </p>

<h3 id=""routecomponents"">Route components</h3>

<p>There are 4 types of route components available with React Router:</p>

<ul>
<li>Route</li>
<li>DefaultRoute </li>
<li>NotFoundRoute</li>
<li>RedirectRoute</li>
</ul>

<p>We'll discuss all four in this post. </p>

<h3 id=""settingupthebasics"">Setting up the basics</h3>

<p>Let's install react-router using npm:  </p>

<pre><code>npm install react-router --save  
</code></pre>

<p>Now we need to setup our page. We need an index.html file that references the (not yet existing) <code>main.js</code> and it should have an div element with an ID of <code>app</code>.</p>

<pre><code class=""language-prettyprint lang-html prettyprinted""><span class=""tag"">&lt;html&gt;</span><span class=""pln"">  
    </span><span class=""tag"">&lt;head&gt;</span><span class=""pln"">
        </span><span class=""tag"">&lt;title&gt;</span><span class=""pln"">Our app</span><span class=""tag"">&lt;/title&gt;</span><span class=""pln"">
    </span><span class=""tag"">&lt;/head&gt;</span><span class=""pln"">
    </span><span class=""tag"">&lt;body&gt;</span><span class=""pln"">
        </span><span class=""tag"">&lt;div</span><span class=""pln""> </span><span class=""atn"">id</span><span class=""pun"">=</span><span class=""atv"">""app""</span><span class=""pln""> </span><span class=""tag"">/&gt;</span><span class=""pln"">
        </span><span class=""tag"">&lt;script</span><span class=""pln""> </span><span class=""atn"">href</span><span class=""pun"">=</span><span class=""atv"">""main.js""</span><span class=""tag"">&gt;&lt;/script&gt;</span><span class=""pln""> 
    </span><span class=""tag"">&lt;/body&gt;</span><span class=""pln"">
</span><span class=""tag"">&lt;/html&gt;</span><span class=""pln"">  </span></code></pre>

<p>We place reference to main.js at the bottom of the body tag, just to be sure our DOM is ready for our app to bootstrap. </p>

<p>As for React components, we only need a simple high level container for our routes. Let's call it app.js. It only needs to contain some basic React boilerplate code. We use the ES6 syntax at Blue Mango, so it looks something like this:</p>

<pre><code class=""language-prettyprint lang-javascript prettyprinted""><span class=""kwd"">var</span><span class=""pln""> </span><span class=""typ"">React</span><span class=""pln""> </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""kwd"">require</span><span class=""pun"">(</span><span class=""str"">'react'</span><span class=""pun"">);</span><span class=""pln"">

</span><span class=""kwd"">export</span><span class=""pln""> </span><span class=""kwd"">class</span><span class=""pln""> ourApp </span><span class=""kwd"">extends</span><span class=""pln""> </span><span class=""typ"">React</span><span class=""pun"">.</span><span class=""typ"">Component</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">  
  constructor</span><span class=""pun"">(</span><span class=""pln"">props</span><span class=""pun"">)</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">
    </span><span class=""kwd"">super</span><span class=""pun"">(</span><span class=""pln"">props</span><span class=""pun"">);</span><span class=""pln"">
  </span><span class=""pun"">}</span><span class=""pln"">
  render</span><span class=""pun"">()</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">
    </span><span class=""kwd"">return</span><span class=""pln""> </span><span class=""pun"">(</span><span class=""pln"">
      </span><span class=""str"">&lt;div&gt;</span><span class=""pln"">
        </span><span class=""str"">&lt;h1&gt;</span><span class=""typ"">Welcome</span><span class=""pln""> to </span><span class=""kwd"">our</span><span class=""pln""> routed page</span><span class=""pun"">&lt;/</span><span class=""pln"">h1</span><span class=""pun"">&gt;</span><span class=""pln"">
        </span><span class=""str"">&lt;p&gt;</span><span class=""typ"">We</span><span class=""pln""> hope you have great time</span><span class=""pun"">.&lt;/</span><span class=""pln"">p</span><span class=""pun"">&gt;</span><span class=""pln"">
      </span><span class=""pun"">&lt;/</span><span class=""pln"">div</span><span class=""pun"">&gt;</span><span class=""pln"">
    </span><span class=""pun"">);</span><span class=""pln"">
  </span><span class=""pun"">}</span><span class=""pln"">
</span><span class=""pun"">}</span></code></pre>

<p>We save this file at './components/app.js'. We'll create the referenced main.js file later. </p>

<h3 id=""defaultroute"">Default route</h3>

<p>Let's dive in directly. In most cases, the routes are defined in a global file called routes.js, and it exists in the root of our source folder. </p>

<p>First load the React Core and React Router:  </p>

<pre><code class=""language-prettyprint lang-javascript prettyprinted""><span class=""kwd"">var</span><span class=""pln""> </span><span class=""typ"">React</span><span class=""pln""> </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""kwd"">require</span><span class=""pun"">(</span><span class=""str"">'react'</span><span class=""pun"">);</span><span class=""pln"">  
</span><span class=""kwd"">var</span><span class=""pln""> </span><span class=""typ"">Router</span><span class=""pln""> </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""kwd"">require</span><span class=""pun"">(</span><span class=""str"">'react-router'</span><span class=""pun"">);</span><span class=""pln"">  </span></code></pre>

<p>Next up, we'll need to define what should do when someone opens our page without adding any segments to the url. This is where the DefaultRoute comes in. We also need to load the Route component, because it forms the base of every other Route in our application.  </p>

<pre><code class=""language-prettyprint lang-javascript prettyprinted""><span class=""kwd"">var</span><span class=""pln""> </span><span class=""typ"">DefaultRoute</span><span class=""pln""> </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""typ"">Router</span><span class=""pun"">.</span><span class=""typ"">DefaultRoute</span><span class=""pun"">;</span><span class=""pln"">  
</span><span class=""kwd"">var</span><span class=""pln""> </span><span class=""typ"">Route</span><span class=""pln""> </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""typ"">Router</span><span class=""pun"">.</span><span class=""typ"">Route</span><span class=""pun"">;</span><span class=""pln"">  </span></code></pre>

<p>Now that we have our core dependencies in place, we can start defining our routes. We define our routes in a nested, XML-like way, making it pretty declarative:</p>

<pre><code class=""language-prettyprint lang-javascript prettyprinted""><span class=""kwd"">var</span><span class=""pln""> routes </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""pun"">(</span><span class=""pln"">  
    </span><span class=""pun"">&lt;</span><span class=""typ"">Route</span><span class=""pln""> name</span><span class=""pun"">=</span><span class=""str"">""ourApp""</span><span class=""pln""> path</span><span class=""pun"">=</span><span class=""str"">""/""</span><span class=""pln""> handler</span><span class=""pun"">={</span><span class=""kwd"">require</span><span class=""pun"">(</span><span class=""str"">'./components/ourApp.js'</span><span class=""pun"">)}&gt;</span><span class=""pln"">
           </span><span class=""pun"">&lt;</span><span class=""typ"">DefaultRoute</span><span class=""pln""> handler</span><span class=""pun"">={</span><span class=""kwd"">require</span><span class=""pun"">(</span><span class=""str"">'./components/home.js'</span><span class=""pun"">)}</span><span class=""pln""> </span><span class=""pun"">/&gt;</span><span class=""pln"">
    </span><span class=""pun"">&lt;/</span><span class=""typ"">Route</span><span class=""pun"">&gt;</span><span class=""pln"">
</span><span class=""pun"">)</span></code></pre>

<p>Let's see what is going on over here.</p>

<ol>
<li>We define a base route called <code>ourApp</code>. Because we've set the path to <code>/</code> it will match all our routes.  </li>
<li>In the <code>handler</code> attribute of this route, we define what component should be loaded when the route is matched.  </li>
<li>We nest every other route in our base route. The only required attribute for a route is the <code>handler</code> attribute. </li>
</ol>

<p>You see I used <code>require</code> to define the handler. Of course we can also make a reference to the component and pass in that reference:</p>

<pre><code class=""language-prettyprint lang-javascript prettyprinted""><span class=""kwd"">var</span><span class=""pln""> </span><span class=""typ"">Home</span><span class=""pln""> </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""kwd"">require</span><span class=""pun"">(</span><span class=""str"">'./components/home.js'</span><span class=""pun"">);</span><span class=""pln"">  
</span><span class=""kwd"">var</span><span class=""pln""> routes </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""pun"">(</span><span class=""pln"">  
    </span><span class=""pun"">&lt;</span><span class=""typ"">Route</span><span class=""pln""> name</span><span class=""pun"">=</span><span class=""str"">""ourApp""</span><span class=""pln""> path</span><span class=""pun"">=</span><span class=""str"">""/""</span><span class=""pln""> handler</span><span class=""pun"">={</span><span class=""kwd"">require</span><span class=""pun"">(</span><span class=""str"">'./components/ourApp.js'</span><span class=""pun"">)}&gt;</span><span class=""pln"">
           </span><span class=""pun"">&lt;</span><span class=""typ"">DefaultRoute</span><span class=""pln""> handler</span><span class=""pun"">={</span><span class=""typ"">Home</span><span class=""pun"">}</span><span class=""pln""> </span><span class=""pun"">/&gt;</span><span class=""pln"">
    </span><span class=""pun"">&lt;/</span><span class=""typ"">Route</span><span class=""pun"">&gt;</span><span class=""pln"">
</span><span class=""pun"">)</span></code></pre>

<p>We'll see how we can use this route in our components later on in this post. </p>

<h3 id=""abasicroute"">A basic route</h3>

<p>Ok. That one default route is not impressive, it is? We want some specific ones. Let's say we want to have the /contact path to head over to our Contact.js component. Easy, let's add a new Route component:</p>

<pre><code class=""language-prettyprint lang-javascript prettyprinted""><span class=""kwd"">var</span><span class=""pln""> routes </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""pun"">(</span><span class=""pln"">  
    </span><span class=""pun"">&lt;</span><span class=""typ"">Route</span><span class=""pln""> name</span><span class=""pun"">=</span><span class=""str"">""ourApp""</span><span class=""pln""> path</span><span class=""pun"">=</span><span class=""str"">""/""</span><span class=""pln""> handler</span><span class=""pun"">={</span><span class=""kwd"">require</span><span class=""pun"">(</span><span class=""str"">'./components/ourApp.js'</span><span class=""pun"">)}&gt;</span><span class=""pln"">
        </span><span class=""pun"">&lt;</span><span class=""typ"">DefaultRoute</span><span class=""pln""> handler</span><span class=""pun"">={</span><span class=""kwd"">require</span><span class=""pun"">(</span><span class=""str"">'./components/home.js'</span><span class=""pun"">)}</span><span class=""pln""> </span><span class=""pun"">/&gt;</span><span class=""pln"">
        </span><span class=""pun"">&lt;</span><span class=""typ"">Route</span><span class=""pln""> name</span><span class=""pun"">=</span><span class=""str"">""contact""</span><span class=""pln""> handler</span><span class=""pun"">={</span><span class=""kwd"">require</span><span class=""pun"">(./</span><span class=""pln"">componenents</span><span class=""pun"">/</span><span class=""pln"">contact</span><span class=""pun"">.</span><span class=""pln"">js</span><span class=""pun"">)}</span><span class=""pln""> </span><span class=""pun"">/&gt;</span><span class=""pln"">
    </span><span class=""pun"">&lt;/</span><span class=""typ"">Route</span><span class=""pun"">&gt;</span><span class=""pln"">
</span><span class=""pun"">)</span></code></pre>

<p>Since we gave our Route's <code>name</code> attribute a value of <em>contact</em>, <em>contact</em> is now also the path of that route. If we want the name to differ from the path, you can explicitly define the path attribute. If you want the path to be <em>contact-us</em>, your code should look like this:</p>

<pre><code class=""language-prettyprint lang-javascript prettyprinted""><span class=""pun"">...</span><span class=""pln"">
</span><span class=""pun"">&lt;</span><span class=""typ"">Route</span><span class=""pln""> name</span><span class=""pun"">=</span><span class=""str"">""contact""</span><span class=""pln""> path</span><span class=""pun"">=</span><span class=""str"">""contact-us""</span><span class=""pln""> handler</span><span class=""pun"">={</span><span class=""kwd"">require</span><span class=""pun"">(./</span><span class=""pln"">componenents</span><span class=""pun"">/</span><span class=""pln"">contact</span><span class=""pun"">.</span><span class=""pln"">js</span><span class=""pun"">)}</span><span class=""pln""> </span><span class=""pun"">/&gt;</span><span class=""pln"">  
</span><span class=""pun"">...</span></code></pre>

<h3 id=""usingroutehandler"">Using RouteHandler</h3>

<p>Now, how do we show the correct Route component in our application? Meet the <code>RouteHandler</code> component. The RouteHandler is the placeholder for your routed pages.</p>

<p>First, let's reference the RouteHandler in the component in which we want to show the routed pages. In our example this is <code>component/app.js</code>.</p>

<pre><code class=""language-prettyprint lang-javascript prettyprinted""><span class=""kwd"">var</span><span class=""pln""> </span><span class=""typ"">RouteHandler</span><span class=""pln""> </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""kwd"">require</span><span class=""pun"">(</span><span class=""str"">'react-router'</span><span class=""pun"">).</span><span class=""typ"">RouteHandler</span><span class=""pun"">;</span><span class=""pln"">  </span></code></pre>

<p>Now let's remove the hardcoded content and replace it with the RouteHandler component:</p>

<pre><code class=""language-prettyprint lang-javascript prettyprinted""><span class=""kwd"">var</span><span class=""pln""> </span><span class=""typ"">React</span><span class=""pln""> </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""kwd"">require</span><span class=""pun"">(</span><span class=""str"">'react'</span><span class=""pun"">);</span><span class=""pln"">  
</span><span class=""kwd"">var</span><span class=""pln""> </span><span class=""typ"">RouteHandler</span><span class=""pln""> </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""kwd"">require</span><span class=""pun"">(</span><span class=""str"">'react-router'</span><span class=""pun"">).</span><span class=""typ"">RouteHandler</span><span class=""pun"">;</span><span class=""pln"">

</span><span class=""kwd"">export</span><span class=""pln""> </span><span class=""kwd"">class</span><span class=""pln""> ourApp </span><span class=""kwd"">extends</span><span class=""pln""> </span><span class=""typ"">React</span><span class=""pun"">.</span><span class=""typ"">Component</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">  
  constructor</span><span class=""pun"">(</span><span class=""pln"">props</span><span class=""pun"">)</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">
    </span><span class=""kwd"">super</span><span class=""pun"">(</span><span class=""pln"">props</span><span class=""pun"">);</span><span class=""pln"">
  </span><span class=""pun"">}</span><span class=""pln"">
  render</span><span class=""pun"">()</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">
    </span><span class=""kwd"">return</span><span class=""pln""> </span><span class=""pun"">(</span><span class=""pln"">
      </span><span class=""pun"">&lt;</span><span class=""typ"">RouteHandler</span><span class=""pln""> </span><span class=""pun"">/&gt;</span><span class=""pln"">
    </span><span class=""pun"">);</span><span class=""pln"">
  </span><span class=""pun"">}</span><span class=""pln"">
</span><span class=""pun"">}</span></code></pre>

<p>Next up, we need to create a main.js that acts as an entry point for our application. This file bootstraps the router:</p>

<pre><code class=""language-prettyprint lang-javascript prettyprinted""><span class=""kwd"">var</span><span class=""pln""> </span><span class=""typ"">React</span><span class=""pln""> </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""kwd"">require</span><span class=""pun"">(</span><span class=""str"">'react'</span><span class=""pun"">);</span><span class=""pln"">  
</span><span class=""kwd"">var</span><span class=""pln""> </span><span class=""typ"">Router</span><span class=""pln""> </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""kwd"">require</span><span class=""pun"">(</span><span class=""str"">'react-router'</span><span class=""pun"">);</span><span class=""pln"">  
</span><span class=""kwd"">var</span><span class=""pln""> routes </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""kwd"">require</span><span class=""pun"">(</span><span class=""str"">'./routes'</span><span class=""pun"">);</span><span class=""pln"">

</span><span class=""typ"">Router</span><span class=""pun"">.</span><span class=""pln"">run</span><span class=""pun"">(</span><span class=""pln"">routes</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""kwd"">function</span><span class=""pun"">(</span><span class=""typ"">Handler</span><span class=""pun"">)</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">  
    </span><span class=""typ"">React</span><span class=""pun"">.</span><span class=""pln"">render</span><span class=""pun"">(&lt;</span><span class=""typ"">Handler</span><span class=""pln""> </span><span class=""pun"">/&gt;,</span><span class=""pln""> document</span><span class=""pun"">.</span><span class=""pln"">getElementById</span><span class=""pun"">(</span><span class=""str"">'app'</span><span class=""pun"">);</span><span class=""pln"">
</span><span class=""pun"">});</span></code></pre>

<p>When we run our indx.html, we should see our routes working. </p>

<h3 id=""parameters"">Parameters</h3>

<p>Not all routes are as static as our previous contact page example. Let's switch to a more complex, classic example of working with a collection of employees. What if we want to show the details of a specific employee by its employee ID? </p>

<pre><code class=""language-prettyprint lang-javascript prettyprinted""><span class=""tag"">&lt;Route</span><span class=""pln""> </span><span class=""atn"">name</span><span class=""pun"">=</span><span class=""atv"">""ourApp""</span><span class=""pln""> </span><span class=""atn"">path</span><span class=""pun"">=</span><span class=""atv"">""/""</span><span class=""pln""> </span><span class=""atn"">handler</span><span class=""pun"">=</span><span class=""atv"">{require('./components/app.js'</span><span class=""pln"">)}</span><span class=""tag"">&gt;</span><span class=""pln"">  
    </span><span class=""tag"">&lt;Route</span><span class=""pln""> </span><span class=""atn"">name</span><span class=""pun"">=</span><span class=""atv"">""employeeDetail""</span><span class=""pln""> </span><span class=""atn"">path</span><span class=""pun"">=</span><span class=""atv"">""/employees/:empoyeeId""</span><span class=""pln""> </span><span class=""atn"">handler</span><span class=""pun"">=</span><span class=""atv"">{require('./components/empoyee.js'</span><span class=""pln"">)} </span><span class=""tag"">/&gt;</span><span class=""pln"">
</span><span class=""tag"">&lt;/Route&gt;</span><span class=""pln"">  </span></code></pre>

<p>Now we can simply retrieve the passed employee ID using the <code>props.params.employeeId</code> property in our Employee component:  </p>

<pre><code class=""language-prettyprint lang-javascript prettyprinted""><span class=""kwd"">class</span><span class=""pln""> </span><span class=""typ"">Employee</span><span class=""pln""> </span><span class=""kwd"">extends</span><span class=""pln""> </span><span class=""typ"">React</span><span class=""pun"">.</span><span class=""typ"">Component</span><span class=""pun"">{</span><span class=""pln"">  
  constructor</span><span class=""pun"">(</span><span class=""pln"">props</span><span class=""pun"">){</span><span class=""pln"">
    </span><span class=""kwd"">super</span><span class=""pun"">(</span><span class=""pln"">props</span><span class=""pun"">);</span><span class=""pln"">
    </span><span class=""com"">// Here we have access to this.props.params.employeeId</span><span class=""pln"">
  </span><span class=""pun"">}</span><span class=""pln"">
</span><span class=""pun"">}</span></code></pre>

<h3 id=""linktoaroute"">Link to a Route</h3>

<p>React Router offers an abstraction over links called <code>Link</code>. Link allows you to leverage the routes you've defined, so you don't have to repeat them every time you want to link to a given page.</p>

<p>Let's say we want to link to an employee with an ID of 1. This page URL looks like <code>/employees/1</code>. </p>

<p>Remember we added a route for this page that has has a placeholder for the employee ID and the name <code>employeeDetail</code>: </p>

<pre><code class=""language-prettyprint lang-javascript prettyprinted""><span class=""tag"">&lt;Route</span><span class=""pln""> </span><span class=""atn"">name</span><span class=""pun"">=</span><span class=""atv"">""employeeDetail""</span><span class=""pln""> </span><span class=""atn"">path</span><span class=""pun"">=</span><span class=""atv"">""/employees/:empoyeeId""</span><span class=""pln""> </span><span class=""atn"">handler</span><span class=""pun"">=</span><span class=""atv"">{require('./components/empoyee.js'</span><span class=""pln"">)} </span><span class=""tag"">/&gt;</span><span class=""pln"">  </span></code></pre>

<p>Having defined a name gives us an easy hook for referencing this route in our links. To create a link that points to this route, we can write some JSX that uses the Link component. </p>

<pre><code class=""language-prettyprint lang-javascript prettyprinted""><span class=""tag"">&lt;Link</span><span class=""pln""> </span><span class=""atn"">to</span><span class=""pun"">=</span><span class=""atv"">""employeeDetail""</span><span class=""pln""> </span><span class=""atn"">params</span><span class=""pun"">=</span><span class=""atv"">{{employeeId:</span><span class=""pln""> 1}}</span><span class=""tag"">&gt;</span><span class=""pln"">Go to employee #1</span><span class=""tag"">&lt;/Link&gt;</span><span class=""pln"">  </span></code></pre>

<p>We've set the <code>to</code> property to <code>employeeDetail</code>, because that's the name of the route we want to use. <code>params</code> can be a JSON object of data that we can use in the URL. When this JSX is actually compiled the following normal anchor will be generated: </p>

<pre><code class=""language-prettyprint lang-html prettyprinted""><span class=""tag"">&lt;a</span><span class=""pln""> </span><span class=""atn"">href</span><span class=""pun"">=</span><span class=""atv"">""/employees/1""</span><span class=""tag"">&gt;</span><span class=""pln"">Go to employee #1</span><span class=""tag"">&lt;/a&gt;</span><span class=""pln"">  </span></code></pre>

<p>The big win here is that it's common to create multiple links to any given resource in your app. The Link component effectively normalizes your links by abstracting the paths, and simply let's you reference your routes by name. Nice, huh?</p>

<h3 id=""pagenotfoundroute"">'Page Not Found' Route</h3>

<p>What about 404's? When someone enters a URL that doesn't exist, it would be nice if our application showed a friendly error page. </p>

<p>First, of course, let's create a page that you want to display as a 404. After this, let's go over to your routes file and define a 'not found' route. First we need to make a reference to the NotFoundRoute component that comes with React Router. </p>

<pre><code class=""language-prettyprint lang-javascript prettyprinted""><span class=""kwd"">var</span><span class=""pln""> </span><span class=""typ"">NotFoundRoute</span><span class=""pln""> </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""typ"">Router</span><span class=""pun"">.</span><span class=""typ"">NotFoundRoute</span><span class=""pun"">;</span><span class=""pln"">  </span></code></pre>

<p>Now you can use the NotFoundRoute component in your routes, just like the normal Route object. We don't have to define a name for it, but we do need to define a handler:</p>

<pre><code class=""language-prettyprint lang-javascript prettyprinted""><span class=""kwd"">var</span><span class=""pln""> routes </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""pun"">(</span><span class=""pln"">  
    </span><span class=""pun"">&lt;</span><span class=""typ"">Route</span><span class=""pln""> name</span><span class=""pun"">=</span><span class=""str"">""ourApp""</span><span class=""pln""> path</span><span class=""pun"">=</span><span class=""str"">""/""</span><span class=""pln""> handler</span><span class=""pun"">={</span><span class=""kwd"">require</span><span class=""pun"">(</span><span class=""str"">'./components/ourApp.js'</span><span class=""pun"">)}&gt;</span><span class=""pln"">
        </span><span class=""pun"">..</span><span class=""pln""> your other routes </span><span class=""pun"">...</span><span class=""pln"">
        </span><span class=""pun"">&lt;</span><span class=""typ"">NotFoundRoute</span><span class=""pln""> handler</span><span class=""pun"">={</span><span class=""kwd"">require</span><span class=""pun"">(</span><span class=""str"">'./components/notFound.js'</span><span class=""pun"">)}</span><span class=""pln""> </span><span class=""pun"">/&gt;</span><span class=""pln"">
    </span><span class=""pun"">&lt;/</span><span class=""typ"">Route</span><span class=""pun"">&gt;</span><span class=""pln"">
</span><span class=""pun"">)</span></code></pre>

<p>This gives us an easy way to handle 404's using React Router. You can also use an aterisk (*) as a path to match all routes that are not defined in the routes configuration. </p>

<pre><code class=""language-prettyprint lang-javascript prettyprinted""><span class=""tag"">&lt;Route</span><span class=""pln""> </span><span class=""atn"">path</span><span class=""pun"">=</span><span class=""atv"">""*""</span><span class=""pln""> </span><span class=""atn"">handler</span><span class=""pun"">=</span><span class=""atv"">{require('./components/notFound.js'</span><span class=""pln"">)} </span><span class=""tag"">/&gt;</span><span class=""pln"">  </span></code></pre>

<p>Make sure this is the last route in your route configuration. </p>

<h3 id=""redirectroute"">Redirect Route</h3>

<p>In the previous section, I discussed 404's. In case an old URL is replaced by a new one, it would be nice to redirect the user to the new page instead of showing a 404 on the old URL. </p>

<p>Luckily, this is very simple with React Router. To do this, we need to reference the Redirect component in your routes file:</p>

<pre><code class=""language-prettyprint lang-javascript prettyprinted""><span class=""kwd"">var</span><span class=""pln""> </span><span class=""typ"">Redirect</span><span class=""pln""> </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""typ"">Router</span><span class=""pun"">.</span><span class=""typ"">Redirect</span><span class=""pun"">;</span><span class=""pln"">  </span></code></pre>

<p>A redirect Route looks like this:  </p>

<pre><code class=""language-prettyprint lang-javascript prettyprinted""><span class=""tag"">&lt;Redirect</span><span class=""pln""> </span><span class=""atn"">from</span><span class=""pun"">=</span><span class=""atv"">'old-path'</span><span class=""pln""> </span><span class=""atn"">to</span><span class=""pun"">=</span><span class=""atv"">'new-path'</span><span class=""pln""> </span><span class=""tag"">/&gt;</span><span class=""pln"">  </span></code></pre>

<p>You can include your query string parameters in the <code>from</code> attribute too, but this is not necessary because they will be passed to the new path anyway. </p>

<p>The <code>from</code> attribute can also include dynamic segments, so you can use an * to redirect segment that isn't found. The next example will redirect all pages under <code>old-path</code> to the <code>new-path</code>:</p>

<pre><code class=""language-prettyprint lang-javascript prettyprinted""><span class=""tag"">&lt;Redirect</span><span class=""pln""> </span><span class=""atn"">from</span><span class=""pun"">=</span><span class=""atv"">'old-path/*'</span><span class=""pln""> </span><span class=""atn"">to</span><span class=""pun"">=</span><span class=""atv"">'new-path'</span><span class=""pln""> </span><span class=""tag"">/&gt;</span><span class=""pln"">  </span></code></pre>

<h3 id=""nestedroutes"">Nested routes</h3>

<p>As our applications grow, there will be a demand for nested routes. Let's say we want to have an images section in our employee detail view. We can simply nest this route in our existing employee route:</p>

<pre><code class=""language-prettyprint lang-javascript prettyprinted""><span class=""tag"">&lt;Route</span><span class=""pln""> </span><span class=""atn"">name</span><span class=""pun"">=</span><span class=""atv"">""ourApp""</span><span class=""pln""> </span><span class=""atn"">path</span><span class=""pun"">=</span><span class=""atv"">""/""</span><span class=""pln""> </span><span class=""atn"">handler</span><span class=""pun"">=</span><span class=""atv"">{require('./components/app.js'</span><span class=""pln"">)}</span><span class=""tag"">&gt;</span><span class=""pln"">  
    </span><span class=""tag"">&lt;Route</span><span class=""pln""> </span><span class=""atn"">name</span><span class=""pun"">=</span><span class=""atv"">""employeeDetail""</span><span class=""pln""> </span><span class=""atn"">path</span><span class=""pun"">=</span><span class=""atv"">""/employees/:empoyeeId""</span><span class=""pln""> </span><span class=""atn"">handler</span><span class=""pun"">=</span><span class=""atv"">{require('./components/empoyee.js'</span><span class=""pln"">)}</span><span class=""tag"">&gt;</span><span class=""pln"">
        </span><span class=""tag"">&lt;Route</span><span class=""pln""> </span><span class=""atn"">name</span><span class=""pun"">=</span><span class=""atv"">""employeeImages""</span><span class=""pln""> </span><span class=""atn"">path</span><span class=""pun"">=</span><span class=""atv"">""images""</span><span class=""pln""> </span><span class=""atn"">handler</span><span class=""pun"">=</span><span class=""atv"">{require('./components/empoyee_images.js'</span><span class=""pln"">)}</span><span class=""tag"">&gt;</span><span class=""pln"">
    </span><span class=""tag"">&lt;/Route&gt;</span><span class=""pln"">
</span><span class=""tag"">&lt;/Route&gt;</span><span class=""pln"">  </span></code></pre>

<p>To make this work, we need to add an extra <code>RouteHandler</code> in employee.js where the nested images page will be rendered:</p>

<pre><code class=""language-prettyprint lang-javascript prettyprinted""><span class=""kwd"">export</span><span class=""pln""> </span><span class=""kwd"">class</span><span class=""pln""> </span><span class=""typ"">Employee</span><span class=""pln""> </span><span class=""kwd"">extends</span><span class=""pln""> </span><span class=""typ"">React</span><span class=""pun"">.</span><span class=""typ"">Component</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">  
  constructor</span><span class=""pun"">(</span><span class=""pln"">props</span><span class=""pun"">)</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">
    </span><span class=""kwd"">super</span><span class=""pun"">(</span><span class=""pln"">props</span><span class=""pun"">);</span><span class=""pln"">
  </span><span class=""pun"">}</span><span class=""pln"">
  render</span><span class=""pun"">()</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">
    </span><span class=""kwd"">return</span><span class=""pln""> </span><span class=""pun"">(</span><span class=""pln"">
      </span><span class=""str"">&lt;div&gt;</span><span class=""pln"">
        </span><span class=""str"">&lt;h1&gt;</span><span class=""typ"">Employee</span><span class=""pun"">&lt;/</span><span class=""pln"">h1</span><span class=""pun"">&gt;</span><span class=""pln"">
        </span><span class=""pun"">&lt;</span><span class=""typ"">RouteHandler</span><span class=""pln""> </span><span class=""pun"">/&gt;</span><span class=""pln"">
      </span><span class=""pun"">&lt;/</span><span class=""pln"">div</span><span class=""pun"">&gt;</span><span class=""pln"">
    </span><span class=""pun"">);</span><span class=""pln"">
  </span><span class=""pun"">}</span><span class=""pln"">
</span><span class=""pun"">}</span></code></pre>

<p>Now we should be able to visit <code>/employees/1/images</code>.</p>

<h3 id=""whataboutquerystringparameters"">What about query string parameters?</h3>

<p>You might want to pass some additional information through the query string. Easy!</p>

<pre><code class=""language-prettyprint lang-javascript prettyprinted""><span class=""tag"">&lt;Route</span><span class=""pln""> </span><span class=""atn"">name</span><span class=""pun"">=</span><span class=""atv"">""search""</span><span class=""pln""> </span><span class=""atn"">path</span><span class=""pun"">=</span><span class=""atv"">""?query:query&amp;limit=:limit&amp;pageSize=:pageSize""</span><span class=""pln""> </span><span class=""atn"">handler</span><span class=""pun"">=</span><span class=""atv"">{SearchDisplay}</span><span class=""tag"">/&gt;</span><span class=""pln"">  </span></code></pre>

<p>Now you can easily get these values in your component using <code>router.getCurrentQuery()</code>:</p>

<pre><code class=""language-prettyprint lang-javascript prettyprinted""><span class=""kwd"">var</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln""> query</span><span class=""pun"">,</span><span class=""pln""> limit</span><span class=""pun"">,</span><span class=""pln""> pageSize</span><span class=""pun"">}</span><span class=""pln""> </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""kwd"">this</span><span class=""pun"">.</span><span class=""pln"">context</span><span class=""pun"">.</span><span class=""pln"">router</span><span class=""pun"">.</span><span class=""pln"">getCurrentQuery</span><span class=""pun"">();</span><span class=""pln"">  </span></code></pre>

<p>You have to make sure that you included <code>router</code> in your <code>contextTypes</code>. </p>

<pre><code>class MyComp extends React.Component{  
  constructor(props, context){
    super(props);
    // here you have access to context.router.getCurrentQuery()
  }
}

MyComp.contextTypes = {  
    router: React.PropTypes.func.isRequired
};
</code></pre>

<h3 id=""conclusion"">Conclusion</h3>

<p>React Router is a complete routing solution for your React application. We discussed client-side routing in this article, but you can also use React Router in a server-side or shared environment (<a href=""http://ponyfoo.com/articles/universal-routing-react-es6"">Universal Routing in React with ES6</a>). You might also want to check the <a href=""https://www.youtube.com/watch?v=XZfvW1a8Xac"">react-router increases your productivity video</a> recordered at React Conf 2015. You can also <a href=""http://jamesknelson.com/routing-with-raw-react/"">implement routing without React Router</a>, of course. </p>

<p><strong>Note</strong>: After Facebook recently released React 0.14 we'll also need to update this post (see comments). </p>
        ","When you need to route between different parts of your React application, you'll probably need a router. The most popular choice to do this in React is React Router. If you are familiar with routing in Ember, you'll get your routing up and running with React Router in no time.
In the following introduction to React Router I assume you already have a development environment up and running and you are familiar with React and CommonJS modules. It uses React 0.13.
Route components
There are 4 types of route components available with React Router:
Route
DefaultRoute
NotFoundRoute
RedirectRoute
We'll discuss all four in this post.
Setting up the basics
Let's install react-router using npm:
npm install react-router --save  
Now we need to setup our page. We need an index.html file that references the (not yet existing) main.js and it should have an div element with an ID of app.
<html>  
    <head>
        <title>Our app</title>
    </head>
    <body>
        <div id=""app"" />
        <script href=""main.js""></script> 
    </body>
</html>  
We place reference to main.js at the bottom of the body tag, just to be sure our DOM is ready for our app to bootstrap.
As for React components, we only need a simple high level container for our routes. Let's call it app.js. It only needs to contain some basic React boilerplate code. We use the ES6 syntax at Blue Mango, so it looks something like this:
var React = require('react');

export class ourApp extends React.Component {  
  constructor(props) {
    super(props);
  }
  render() {
    return (
      <div>
        <h1>Welcome to our routed page</h1>
        <p>We hope you have great time.</p>
      </div>
    );
  }
}
We save this file at './components/app.js'. We'll create the referenced main.js file later.
Default route
Let's dive in directly. In most cases, the routes are defined in a global file called routes.js, and it exists in the root of our source folder.
First load the React Core and React Router:
var React = require('react');  
var Router = require('react-router');  
Next up, we'll need to define what should do when someone opens our page without adding any segments to the url. This is where the DefaultRoute comes in. We also need to load the Route component, because it forms the base of every other Route in our application.
var DefaultRoute = Router.DefaultRoute;  
var Route = Router.Route;  
Now that we have our core dependencies in place, we can start defining our routes. We define our routes in a nested, XML-like way, making it pretty declarative:
var routes = (  
    <Route name=""ourApp"" path=""/"" handler={require('./components/ourApp.js')}>
           <DefaultRoute handler={require('./components/home.js')} />
    </Route>
)
Let's see what is going on over here.
We define a base route called ourApp. Because we've set the path to / it will match all our routes.
In the handler attribute of this route, we define what component should be loaded when the route is matched.
We nest every other route in our base route. The only required attribute for a route is the handler attribute.
You see I used require to define the handler. Of course we can also make a reference to the component and pass in that reference:
var Home = require('./components/home.js');  
var routes = (  
    <Route name=""ourApp"" path=""/"" handler={require('./components/ourApp.js')}>
           <DefaultRoute handler={Home} />
    </Route>
)
We'll see how we can use this route in our components later on in this post.
A basic route
Ok. That one default route is not impressive, it is? We want some specific ones. Let's say we want to have the /contact path to head over to our Contact.js component. Easy, let's add a new Route component:
var routes = (  
    <Route name=""ourApp"" path=""/"" handler={require('./components/ourApp.js')}>
        <DefaultRoute handler={require('./components/home.js')} />
        <Route name=""contact"" handler={require(./componenents/contact.js)} />
    </Route>
)
Since we gave our Route's name attribute a value of contact, contact is now also the path of that route. If we want the name to differ from the path, you can explicitly define the path attribute. If you want the path to be contact-us, your code should look like this:
...
<Route name=""contact"" path=""contact-us"" handler={require(./componenents/contact.js)} />  
...
Using RouteHandler
Now, how do we show the correct Route component in our application? Meet the RouteHandler component. The RouteHandler is the placeholder for your routed pages.
First, let's reference the RouteHandler in the component in which we want to show the routed pages. In our example this is component/app.js.
var RouteHandler = require('react-router').RouteHandler;  
Now let's remove the hardcoded content and replace it with the RouteHandler component:
var React = require('react');  
var RouteHandler = require('react-router').RouteHandler;

export class ourApp extends React.Component {  
  constructor(props) {
    super(props);
  }
  render() {
    return (
      <RouteHandler />
    );
  }
}
Next up, we need to create a main.js that acts as an entry point for our application. This file bootstraps the router:
var React = require('react');  
var Router = require('react-router');  
var routes = require('./routes');

Router.run(routes, function(Handler) {  
    React.render(<Handler />, document.getElementById('app');
});
When we run our indx.html, we should see our routes working.
Parameters
Not all routes are as static as our previous contact page example. Let's switch to a more complex, classic example of working with a collection of employees. What if we want to show the details of a specific employee by its employee ID?
<Route name=""ourApp"" path=""/"" handler={require('./components/app.js')}>  
    <Route name=""employeeDetail"" path=""/employees/:empoyeeId"" handler={require('./components/empoyee.js')} />
</Route>  
Now we can simply retrieve the passed employee ID using the props.params.employeeId property in our Employee component:
class Employee extends React.Component{  
  constructor(props){
    super(props);
    // Here we have access to this.props.params.employeeId
  }
}
Link to a Route
React Router offers an abstraction over links called Link. Link allows you to leverage the routes you've defined, so you don't have to repeat them every time you want to link to a given page.
Let's say we want to link to an employee with an ID of 1. This page URL looks like /employees/1.
Remember we added a route for this page that has has a placeholder for the employee ID and the name employeeDetail:
<Route name=""employeeDetail"" path=""/employees/:empoyeeId"" handler={require('./components/empoyee.js')} />  
Having defined a name gives us an easy hook for referencing this route in our links. To create a link that points to this route, we can write some JSX that uses the Link component.
<Link to=""employeeDetail"" params={{employeeId: 1}}>Go to employee #1</Link>  
We've set the to property to employeeDetail, because that's the name of the route we want to use. params can be a JSON object of data that we can use in the URL. When this JSX is actually compiled the following normal anchor will be generated:
<a href=""/employees/1"">Go to employee #1</a>  
The big win here is that it's common to create multiple links to any given resource in your app. The Link component effectively normalizes your links by abstracting the paths, and simply let's you reference your routes by name. Nice, huh?
'Page Not Found' Route
What about 404's? When someone enters a URL that doesn't exist, it would be nice if our application showed a friendly error page.
First, of course, let's create a page that you want to display as a 404. After this, let's go over to your routes file and define a 'not found' route. First we need to make a reference to the NotFoundRoute component that comes with React Router.
var NotFoundRoute = Router.NotFoundRoute;  
Now you can use the NotFoundRoute component in your routes, just like the normal Route object. We don't have to define a name for it, but we do need to define a handler:
var routes = (  
    <Route name=""ourApp"" path=""/"" handler={require('./components/ourApp.js')}>
        .. your other routes ...
        <NotFoundRoute handler={require('./components/notFound.js')} />
    </Route>
)
This gives us an easy way to handle 404's using React Router. You can also use an aterisk (*) as a path to match all routes that are not defined in the routes configuration.
<Route path=""*"" handler={require('./components/notFound.js')} />  
Make sure this is the last route in your route configuration.
Redirect Route
In the previous section, I discussed 404's. In case an old URL is replaced by a new one, it would be nice to redirect the user to the new page instead of showing a 404 on the old URL.
Luckily, this is very simple with React Router. To do this, we need to reference the Redirect component in your routes file:
var Redirect = Router.Redirect;  
A redirect Route looks like this:
<Redirect from='old-path' to='new-path' />  
You can include your query string parameters in the from attribute too, but this is not necessary because they will be passed to the new path anyway.
The from attribute can also include dynamic segments, so you can use an * to redirect segment that isn't found. The next example will redirect all pages under old-path to the new-path:
<Redirect from='old-path/*' to='new-path' />  
Nested routes
As our applications grow, there will be a demand for nested routes. Let's say we want to have an images section in our employee detail view. We can simply nest this route in our existing employee route:
<Route name=""ourApp"" path=""/"" handler={require('./components/app.js')}>  
    <Route name=""employeeDetail"" path=""/employees/:empoyeeId"" handler={require('./components/empoyee.js')}>
        <Route name=""employeeImages"" path=""images"" handler={require('./components/empoyee_images.js')}>
    </Route>
</Route>  
To make this work, we need to add an extra RouteHandler in employee.js where the nested images page will be rendered:
export class Employee extends React.Component {  
  constructor(props) {
    super(props);
  }
  render() {
    return (
      <div>
        <h1>Employee</h1>
        <RouteHandler />
      </div>
    );
  }
}
Now we should be able to visit /employees/1/images.
What about query string parameters?
You might want to pass some additional information through the query string. Easy!
<Route name=""search"" path=""?query:query&limit=:limit&pageSize=:pageSize"" handler={SearchDisplay}/>  
Now you can easily get these values in your component using router.getCurrentQuery():
var { query, limit, pageSize} = this.context.router.getCurrentQuery();  
You have to make sure that you included router in your contextTypes.
class MyComp extends React.Component{  
  constructor(props, context){
    super(props);
    // here you have access to context.router.getCurrentQuery()
  }
}

MyComp.contextTypes = {  
    router: React.PropTypes.func.isRequired
};
Conclusion
React Router is a complete routing solution for your React application. We discussed client-side routing in this article, but you can also use React Router in a server-side or shared environment (Universal Routing in React with ES6). You might also want to check the react-router increases your productivity video recordered at React Conf 2015. You can also implement routing without React Router, of course.
Note: After Facebook recently released React 0.14 we'll also need to update this post (see comments).","[reactjs, Code]"
89,Improve client acquisition insights using a single line of code,/improve-client-acquisition-insights-using-a-single-line-of-code/,"
            <p>As a web analyst with a technical background, I sometimes have ideas that are a mix of marketing knowhow and my basic JavaScript knowledge. This post is about one of them. It'll show you how to measure what type of clients buy on your website. Are they new, or existing clients? It includes a how-to guide to set this up in Google Tag Manager.</p>

<h4 id=""referrals"">Referrals</h4>

<p>Page referral information contains the URL of the previous page. For example: when I visit website-a.com, and click to website-b.com, my referral on the second page will be website-a.com. This is how Google Analytics tracks referrals source information. The referral is easy to get with JavaScript:</p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""pln"">document</span><span class=""pun"">.</span><span class=""pln"">referrer  </span></code></pre>

<p>So how can we use this simple code snippet to improve client acquisition insights?</p>

<h4 id=""improveclientacquisitioninsightswithreferrals"">Improve client acquisition insights with referrals</h4>

<p>You can use referral information to track the previous page from another domain, or within the same domain. The latter option is the one we will use to gain more info about client acquisition. Let's use a standard webshop as an example.</p>

<p>On a webshop, users generally can go through one of the following two flows when ordering:</p>

<ul>
<li>Basket &gt; Login &gt; Overview &gt; Transaction; or</li>
<li>Basket &gt; Register &gt; Overview &gt; Transaction.</li>
</ul>

<p>The difference between the two flows is step 2: the login or register page. On the overview page, we can use the referral information to tell us if a visitor is an existing, or new customer:</p>

<p><strong>Basic JavaScript code</strong></p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""kwd"">if</span><span class=""pun"">(</span><span class=""pln"">document</span><span class=""pun"">.</span><span class=""pln"">referrer </span><span class=""pun"">===</span><span class=""pln""> </span><span class=""str"">'https://www.example.com/register'</span><span class=""pun"">){</span><span class=""pln"">  
  </span><span class=""com"">//track new customer</span><span class=""pln"">
</span><span class=""pun"">}</span><span class=""pln""> </span><span class=""kwd"">else</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">
  </span><span class=""com"">//track existing customer</span><span class=""pln"">
</span><span class=""pun"">}</span></code></pre>

<p>With this code snippet, you can measure the type of customer that visit your page.</p>

<h4 id=""implementation"">Implementation</h4>

<p>The implementation of the script is key. You'll only want to load the script on the page after the register page. In case of the example flow, the overview page. To give you an example, here's a possible Google Tag Manager setup (no dataLayer changes required):</p>

<p><strong>1: GTM Variable</strong></p>

<p>Step one is setting up the GTM variable. We'll use a Custom JavaScript tag. Make sure to change the referral to the correct one of your website. <br>
<img src=""http://i59.tinypic.com/2rqdls4.png"" alt=""GTM Client Type Variable"" class=""full-img""></p>

<p><strong>2: GTM Tag</strong></p>

<p>Step two is setting up the GTM tag. We'll use a Google (Universal) Analytics event tag. You can set it as the event label, add a custom dimension for it or set both. Make sure to change the custom dimension index to match it with <a href=""https://support.google.com/analytics/answer/2709829?hl=en"">your Google Analytics setup</a>. <br>
<img src=""http://i59.tinypic.com/sbiky9.png"" alt=""GTM Client Type Tag"" class=""full-img""></p>

<p><strong>3: GTM Trigger</strong></p>

<p>Step three is to set up a trigger that matches with the right page. In our example case, this is the /overview page. <br>
<img src=""http://i58.tinypic.com/21njfog.png"" alt=""GTM Client Type Trigger"" class=""full-img""></p>

<p><strong>Done</strong></p>

<p>With these three things set up, you're ready to measure your customer types without the need for developers. Keep in mind to:</p>

<ul>
<li>Find the right pages that define your new or existing customer rule;</li>
<li>Set the right referrer in the variable; and</li>
<li>Set the right page that triggers the tag.</li>
</ul>

<p>You can use the referral information in other cases as well. For example: you could use it to track the previous page of a product detail page, to track how people land there. </p>

<h4 id=""whatdidwelearnfromthis"">What did we learn from this</h4>

<p>The main point of this post is 'hybrid knowledge'. If you teach yourself marketing as a developer, it becomes easier to translate a marketer's question into a technological answer. It works the other way around as well. If you're a marketer and invest in coding, you know what's possible on a technical level. You don't need to be a great developer, but you should be able to write down what you need theoretically. </p>

<p>This way, the world of marketing and development will find it easier to understand each other. And together, you'll grow to a higher level.</p>
        ","As a web analyst with a technical background, I sometimes have ideas that are a mix of marketing knowhow and my basic JavaScript knowledge. This post is about one of them. It'll show you how to measure what type of clients buy on your website. Are they new, or existing clients? It includes a how-to guide to set this up in Google Tag Manager.
Referrals
Page referral information contains the URL of the previous page. For example: when I visit website-a.com, and click to website-b.com, my referral on the second page will be website-a.com. This is how Google Analytics tracks referrals source information. The referral is easy to get with JavaScript:
document.referrer  
So how can we use this simple code snippet to improve client acquisition insights?
Improve client acquisition insights with referrals
You can use referral information to track the previous page from another domain, or within the same domain. The latter option is the one we will use to gain more info about client acquisition. Let's use a standard webshop as an example.
On a webshop, users generally can go through one of the following two flows when ordering:
Basket > Login > Overview > Transaction; or
Basket > Register > Overview > Transaction.
The difference between the two flows is step 2: the login or register page. On the overview page, we can use the referral information to tell us if a visitor is an existing, or new customer:
Basic JavaScript code
if(document.referrer === 'https://www.example.com/register'){  
  //track new customer
} else {
  //track existing customer
}
With this code snippet, you can measure the type of customer that visit your page.
Implementation
The implementation of the script is key. You'll only want to load the script on the page after the register page. In case of the example flow, the overview page. To give you an example, here's a possible Google Tag Manager setup (no dataLayer changes required):
1: GTM Variable
Step one is setting up the GTM variable. We'll use a Custom JavaScript tag. Make sure to change the referral to the correct one of your website.
2: GTM Tag
Step two is setting up the GTM tag. We'll use a Google (Universal) Analytics event tag. You can set it as the event label, add a custom dimension for it or set both. Make sure to change the custom dimension index to match it with your Google Analytics setup.
3: GTM Trigger
Step three is to set up a trigger that matches with the right page. In our example case, this is the /overview page.
Done
With these three things set up, you're ready to measure your customer types without the need for developers. Keep in mind to:
Find the right pages that define your new or existing customer rule;
Set the right referrer in the variable; and
Set the right page that triggers the tag.
You can use the referral information in other cases as well. For example: you could use it to track the previous page of a product detail page, to track how people land there.
What did we learn from this
The main point of this post is 'hybrid knowledge'. If you teach yourself marketing as a developer, it becomes easier to translate a marketer's question into a technological answer. It works the other way around as well. If you're a marketer and invest in coding, you know what's possible on a technical level. You don't need to be a great developer, but you should be able to write down what you need theoretically.
This way, the world of marketing and development will find it easier to understand each other. And together, you'll grow to a higher level.",[Analytics]
90,Track content performance using Google Analytics Enhanced Ecommerce report,/track-content-performance-using-google-analytics-enhanced-ecommerce-report/,"
            <p>In a post from June – <a href=""http://geek.bluemangointeractive.com/5-takeaways-from-the-google-analytics-user-conference-in-amsterdam/"">5 key takeaways from the Google Analytics User Conference</a> – we mentioned the use of GA’s enhanced ecommerce report for content (credits for the idea go to <a href=""http://www.simoahava.com/analytics/track-content-enhanced-ecommerce/"">Simo Ahava</a>). And more importantly, told your, our reader, that we were going to implement it on GEEK. So a month ago we did, and after our first full month of measuring, we’d like to share our results.</p>

<h4 id=""useshoppingbehaviourtotrackreadingbehaviour"">Use shopping behaviour to track reading behaviour</h4>

<p>Enhanced ecommerce has some nice reports. The first one is <em>Shopping behaviour</em>. This shows the your shop performance in five steps:</p>

<ul>
<li>All sessions;</li>
<li>Product views: sessions  where a visitor sees a product detail page;</li>
<li>Add to carts: sessions where a product is added to the shopping cart;</li>
<li>Checkout: sessions that enter the checkout; and</li>
<li>Transactions: sessions that have a successful transaction.</li>
</ul>

<p>For e-commerce, this is really handy, but for content not so much. Luckily, following the guide of Simo Ahava, it translates to content easily:</p>

<ul>
<li>All sessions;</li>
<li>Product views &gt; Article views: sessions where a visitor opens an article page;</li>
<li>Add to carts &gt; Start scrolling: sessions where users have a scroll on an article page;</li>
<li>Checkout  &gt; percentage read: sessions where users have scrolled at least 25%;</li>
<li>Transactions &gt; full reads: sessions where users have a full read.</li>
</ul>

<p>We’ve set the full reads to a scroll reach of at least 75% and a time on article of at least 60 seconds. We use a 75% scroll to ignore the comments and suggested posts part of the article. The time limit is used to ignore scanners that don’t actually read the full article.</p>

<p>For GEEK, this resulted in the following graph:</p>

<p><img src=""http://i62.tinypic.com/dotjz6.png"" alt=""GEEK shopping behaviour"" class=""full-img""></p>

<p>This shows us that for GEEK, content consumption is pretty well, and that about 40% of visitors read an article. </p>

<h4 id=""usecheckoutbehaviourtotrackscrollbehaviour"">Use checkout behaviour to track scroll behaviour</h4>

<p>Besides the shop behaviour flow, enhanced ecommerce allows you to set up a custom checkout report. With this report, you can drill down into checkout behaviour. Normally, this would contain all the steps between add to cart and transaction, for example:</p>

<ul>
<li>Personal details;</li>
<li>Payment details; and</li>
<li>Order overview.</li>
</ul>

<p>For reading behaviour, we’ll use this te measure scroll reach on article pages:</p>

<ul>
<li>25% read;</li>
<li>50% read;</li>
<li>75% read; and</li>
<li>100% read.</li>
</ul>

<p>With this, we can easily see what’s the main dropoff point of our articles. If, for example, a lot of sessions leave the page after a 50% read, our articles might be too long. Let’s take a look at the actual example from GEEK:</p>

<p><img src=""http://i61.tinypic.com/2cmw502.png"" alt=""GEEK checkout behaviour"" class=""full-img""></p>

<p>As you can see, our main dropoff point is at the 100% read mark. This tells us that 75% of sessions leave an article between the 75% and 100% read mark. We even have an increase to transactions after that. </p>

<p>As mentioned before, we ignore the comments and suggested reads part for the full read, though the 75% mark might not be accurate for the longer articles. If you want to make it more accurate, you’ll want measure the scroll reach for just the article part of a page.</p>

<h4 id=""userevenuetomeasurewordcount"">Use revenue to measure word count</h4>

<p>Another clever use of the report for content, setting the word count as revenue. This gives you insights in:</p>

<ul>
<li>How many words do people read?</li>
<li>How many words do people read on average?</li>
<li>How many read words does an article generate?</li>
<li>What’s the best performing article looking the amount of reads?</li>
</ul>

<p>To clarify tings, here’s an example of product, or rather article, performance report of GEEK:</p>

<p><img src=""http://i62.tinypic.com/11771up.png"" alt=""GEEK article performance"" class=""full-img""></p>

<p>First of all, you can instantly tell how many words were read on your blog. For GEEK, september generated over 1.000.000 read words (Booyah!). To answer the questions above with this report, try the following:</p>

<ul>
<li>1: How many words do people read? Easy, just look at the product revenue on the top left of the table.</li>
<li>2: How many words do people read on average? Again, easy. Use the average price on top of the table.</li>
<li>3: How many read words does an article generate? Filter the table on revenue and the articles are listed on total words read.</li>
<li>4: What’s the best performing article looking the amount of reads? All you have to do is filter on quantity. </li>
</ul>

<p>This way you can see which content works. For example: a large post with a few reads may generate quite some read words. This might indicate that it’s a good post for small audience. If a short article generates a lot of reads, it might be an quick snack that's easy to ready for a lot of readers. </p>

<h4 id=""bringintheauthors"">Bring in the authors</h4>

<p>An extra option when using enhanced ecommerce is the use of custom product dimensions. For content, these translate to custom article dimensions.</p>

<p>We’ve currently set this up for authors. Although we think writing for GEEK isn’t a competition, it’s nice to see how authors are performing. </p>

<p>Let’s look at a custom report for authors:</p>

<p><img src=""http://i57.tinypic.com/28lgupv.png"" alt=""GEEK author performance"" class=""full-img""></p>

<p>First things first: props to Siebe for being the most read author of September. </p>

<p>Secondly: what's interesting for us is the number two spot of Erwin. He wrote just one post at the time of writing this post and is at the number 2 spot of authors. Compare that to my rank, it’s at the 6th spot. I write quite some shorter articles. What does this tell us? It could be a number of things:</p>

<ul>
<li>Long articles work better than short ones;</li>
<li>Google OAuth 2 is a more popular topic than analytics;</li>
<li>Erwin has a better style of writing that I do.</li>
</ul>

<p>What’s certain is that this data helps you understand content performance, and find ways to make it better.</p>

<h4 id=""nextsteps"">Next steps</h4>

<p>The next step is to add more custom data to the report. One thing we’re adding is a size treshhold:</p>

<ul>
<li>0-500 words : small article;</li>
<li>500-1000 words: medium article;</li>
<li>1000+ words: large article.</li>
</ul>

<p>This will make it easier for us to tell if long posts work better than short ones. The change is live starting today. We'll write short update on our findings next month.</p>

<p>What would you add to this data set if you had the chance? Let us know in the comments and we might actually test it on GEEK. </p>

<p><em>UPDATE: on November 2, 2015 we've published a post on article size. <a href=""http://geek.bluemangointeractive.com/how-article-size-helps-you-understand-your-content-performance/"">Read it here.</a></em></p>
        ","In a post from June – 5 key takeaways from the Google Analytics User Conference – we mentioned the use of GA’s enhanced ecommerce report for content (credits for the idea go to Simo Ahava). And more importantly, told your, our reader, that we were going to implement it on GEEK. So a month ago we did, and after our first full month of measuring, we’d like to share our results.
Use shopping behaviour to track reading behaviour
Enhanced ecommerce has some nice reports. The first one is Shopping behaviour. This shows the your shop performance in five steps:
All sessions;
Product views: sessions where a visitor sees a product detail page;
Add to carts: sessions where a product is added to the shopping cart;
Checkout: sessions that enter the checkout; and
Transactions: sessions that have a successful transaction.
For e-commerce, this is really handy, but for content not so much. Luckily, following the guide of Simo Ahava, it translates to content easily:
All sessions;
Product views > Article views: sessions where a visitor opens an article page;
Add to carts > Start scrolling: sessions where users have a scroll on an article page;
Checkout > percentage read: sessions where users have scrolled at least 25%;
Transactions > full reads: sessions where users have a full read.
We’ve set the full reads to a scroll reach of at least 75% and a time on article of at least 60 seconds. We use a 75% scroll to ignore the comments and suggested posts part of the article. The time limit is used to ignore scanners that don’t actually read the full article.
For GEEK, this resulted in the following graph:
This shows us that for GEEK, content consumption is pretty well, and that about 40% of visitors read an article.
Use checkout behaviour to track scroll behaviour
Besides the shop behaviour flow, enhanced ecommerce allows you to set up a custom checkout report. With this report, you can drill down into checkout behaviour. Normally, this would contain all the steps between add to cart and transaction, for example:
Personal details;
Payment details; and
Order overview.
For reading behaviour, we’ll use this te measure scroll reach on article pages:
25% read;
50% read;
75% read; and
100% read.
With this, we can easily see what’s the main dropoff point of our articles. If, for example, a lot of sessions leave the page after a 50% read, our articles might be too long. Let’s take a look at the actual example from GEEK:
As you can see, our main dropoff point is at the 100% read mark. This tells us that 75% of sessions leave an article between the 75% and 100% read mark. We even have an increase to transactions after that.
As mentioned before, we ignore the comments and suggested reads part for the full read, though the 75% mark might not be accurate for the longer articles. If you want to make it more accurate, you’ll want measure the scroll reach for just the article part of a page.
Use revenue to measure word count
Another clever use of the report for content, setting the word count as revenue. This gives you insights in:
How many words do people read?
How many words do people read on average?
How many read words does an article generate?
What’s the best performing article looking the amount of reads?
To clarify tings, here’s an example of product, or rather article, performance report of GEEK:
First of all, you can instantly tell how many words were read on your blog. For GEEK, september generated over 1.000.000 read words (Booyah!). To answer the questions above with this report, try the following:
1: How many words do people read? Easy, just look at the product revenue on the top left of the table.
2: How many words do people read on average? Again, easy. Use the average price on top of the table.
3: How many read words does an article generate? Filter the table on revenue and the articles are listed on total words read.
4: What’s the best performing article looking the amount of reads? All you have to do is filter on quantity.
This way you can see which content works. For example: a large post with a few reads may generate quite some read words. This might indicate that it’s a good post for small audience. If a short article generates a lot of reads, it might be an quick snack that's easy to ready for a lot of readers.
Bring in the authors
An extra option when using enhanced ecommerce is the use of custom product dimensions. For content, these translate to custom article dimensions.
We’ve currently set this up for authors. Although we think writing for GEEK isn’t a competition, it’s nice to see how authors are performing.
Let’s look at a custom report for authors:
First things first: props to Siebe for being the most read author of September.
Secondly: what's interesting for us is the number two spot of Erwin. He wrote just one post at the time of writing this post and is at the number 2 spot of authors. Compare that to my rank, it’s at the 6th spot. I write quite some shorter articles. What does this tell us? It could be a number of things:
Long articles work better than short ones;
Google OAuth 2 is a more popular topic than analytics;
Erwin has a better style of writing that I do.
What’s certain is that this data helps you understand content performance, and find ways to make it better.
Next steps
The next step is to add more custom data to the report. One thing we’re adding is a size treshhold:
0-500 words : small article;
500-1000 words: medium article;
1000+ words: large article.
This will make it easier for us to tell if long posts work better than short ones. The change is live starting today. We'll write short update on our findings next month.
What would you add to this data set if you had the chance? Let us know in the comments and we might actually test it on GEEK.
UPDATE: on November 2, 2015 we've published a post on article size. Read it here.",[Analytics]
91,Slashception with regexp_extract in Hive,/slashception-with-regexp_extract-in-hive/,"
            <p>As a Data Scientist I frequently need to work with regular expressions. Though the capabilities and power of regular expressions are enormous, I just cannot seem to like them a lot. That is because when they do not function as expected they can be a really time-consuming nightmare. In this blogpost I will describe the hours I lost last week because of something I now call <em>slashception</em>.</p>

<h4 id=""jsonception"">JSONception</h4>

<p>On most of our clients websites we have our own datalogger <a href=""http://snowplowanalytics.com/"">Snowplow</a> running next to the Google or Adobe Analytics implementation. Snowplow enables us to do much more in depth analysis on log level data than we are able to do with only Google or Adobe Analytics. For a particular project Snowplow was storing a JSON-object in its database. This JSON-object contained a key-value pair for which the value was another JSON-object. This nested JSON-object was however stored as a string value. To ensure that the whole JSON-object is syntactically correct, the string-formatted JSON-object contains several backslash-characters (\) to escape the quote-characters (“). An illustration of the JSON-object we are talking about is given below:</p>

<pre><code class=""language-prettyprint lang-json prettyprinted""><span class=""pun"">{</span><span class=""pln"">  
   </span><span class=""str"">""key1""</span><span class=""pun"">:</span><span class=""str"">""value1""</span><span class=""pun"">,</span><span class=""pln"">
   </span><span class=""str"">""key2""</span><span class=""pun"">:{</span><span class=""pln"">  
      </span><span class=""str"">""settings""</span><span class=""pun"">:{</span><span class=""pln"">  
         </span><span class=""str"">""setting1""</span><span class=""pun"">:</span><span class=""kwd"">true</span><span class=""pun"">,</span><span class=""pln"">
         </span><span class=""str"">""setting2""</span><span class=""pun"">:</span><span class=""str"">""{\""client\"":{\""id\"":\""26480999\"",\""name\"":\""Thom\"",\""age\"":\""24\""}}""</span><span class=""pln"">
      </span><span class=""pun"">}</span><span class=""pln"">
   </span><span class=""pun"">}</span><span class=""pln"">
</span><span class=""pun"">}</span></code></pre>

<h4 id=""whatsinaname"">What's in a name?</h4>

<p>For our analysis we were interested in the value in the key 'name'. In the example above that value would have been <code>Thom</code> (<em>just a random name for illustration purposes... or maybe not thát random.</em>). Our idea was to use the <a href=""https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF#LanguageManualUDF-StringFunctions"">regexp_extract</a> function in Hive to extract the name key-value pair and store the value in a new column denoted by 'name'. In that way, we could use the name column in all our subsequent queries on the database. Confident and with high hopes we ran the following Hive query:</p>

<pre><code class=""language-prettyprint lang-sql prettyprinted""><span class=""pln"">insert overwrite table data partition</span><span class=""pun"">(</span><span class=""pln"">run</span><span class=""pun"">)</span><span class=""pln"">  
</span><span class=""kwd"">select</span><span class=""pln"">  
original_json_object</span><span class=""pun"">,</span><span class=""pln"">  
regexp_extract</span><span class=""pun"">(</span><span class=""pln"">original_json_object</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""str"">'name\\"":\\""(.*?)\\'</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""lit"">1</span><span class=""pun"">)</span><span class=""pln""> </span><span class=""kwd"">as</span><span class=""pln""> name</span><span class=""pun"">,</span><span class=""pln"">  
run  
</span><span class=""kwd"">from</span><span class=""pln""> data</span><span class=""pun"">;</span><span class=""pln"">  </span></code></pre>

<p>Note that the regular expression we used was <code>regexp_extract(original_json_object, 'name\\"":\\""(.*?)\\', 1)</code>. We used two slashes, because we have to escape the protected backslash character in regular expressions. The first backslash tells the regular expression we want to use the second backslash character literally. Unfortunately, this regular expression did not do the trick as it returned <code></code>...</p>

<p>Confused by the result of <code>NULL</code> instead of <code>Thom</code> we called for help from the internet.  <a href=""https://regex101.com/"">Regex101.com</a> is a site we regularly use to create and test our regular expressions. However, also Regex101 said that this regular expression was correct. That is, Regex101 stated that the test string <code>name\"":\""Thom\""</code> is a match to the regular expression <code>name\\"":\\""(.*?)\\""</code>. After spending already too much time looking for the solution on the internet and its community, it was time to call in the colleagues. Even they hadn't experienced this issue before and mentioned that two backslashes should be enough. I kind of suspect that the first colleague that reads this post immediately knows the solution to this problem but that he or she wasn't around at the time. </p>

<h4 id=""weretakingtheregextoisengard"">We're taking the regex to Isengard</h4>

<p>As a last resort we applied a trial and error approach. That is, we let our knowledge about escaping with slashes in regular expressions go and just tried what would happen if we widened our search, i.e. <code>regexp_extract(original_json_object, 'name(.*?),', 1)</code>. Funny enough, this returned <code>\"":\""Thom\""</code>. Now we were getting somewhere. The regexp_extract function was function correctly, we only needed to narrow down the search.</p>

<p>The next try was <code>regexp_extract(original_json_object, 'name\(.*?),', 1)</code>. Note that we did not escape the backslash as this didn't work in our earlier attempts and we are now doing trial runs  to locate the problem. The output of this regexp_extract function was <code>\"":\""Thom\""</code>. So after adding the extra slash we still got the same output. It was almost like our added slash disappeared... We tried adding an additional slash then! Unfortunately, <code>regexp_extract(original_json_object, 'name\\(.*?),', 1)</code> returned an error:  </p>

<pre><code class=""language-prettyprint lang-java prettyprinted""><span class=""typ"">Caused</span><span class=""pln""> </span><span class=""kwd"">by</span><span class=""pun"">:</span><span class=""pln""> java</span><span class=""pun"">.</span><span class=""pln"">util</span><span class=""pun"">.</span><span class=""pln"">regex</span><span class=""pun"">.</span><span class=""typ"">PatternSyntaxException</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""typ"">Unmatched</span><span class=""pln""> closing </span><span class=""str"">')'</span><span class=""pln""> near index </span><span class=""lit"">16</span><span class=""pln"">  
name\(</span><span class=""pun"">.*?),</span><span class=""pln"">  
                </span><span class=""pun"">^</span><span class=""pln"">
    at java</span><span class=""pun"">.</span><span class=""pln"">util</span><span class=""pun"">.</span><span class=""pln"">regex</span><span class=""pun"">.</span><span class=""typ"">Pattern</span><span class=""pun"">.</span><span class=""pln"">error</span><span class=""pun"">(</span><span class=""typ"">Pattern</span><span class=""pun"">.</span><span class=""pln"">java</span><span class=""pun"">:</span><span class=""lit"">1924</span><span class=""pun"">)</span><span class=""pln"">
    at java</span><span class=""pun"">.</span><span class=""pln"">util</span><span class=""pun"">.</span><span class=""pln"">regex</span><span class=""pun"">.</span><span class=""typ"">Pattern</span><span class=""pun"">.</span><span class=""pln"">compile</span><span class=""pun"">(</span><span class=""typ"">Pattern</span><span class=""pun"">.</span><span class=""pln"">java</span><span class=""pun"">:</span><span class=""lit"">1669</span><span class=""pun"">)</span><span class=""pln"">
    at java</span><span class=""pun"">.</span><span class=""pln"">util</span><span class=""pun"">.</span><span class=""pln"">regex</span><span class=""pun"">.</span><span class=""typ"">Pattern</span><span class=""pun"">.&lt;</span><span class=""pln"">init</span><span class=""pun"">&gt;(</span><span class=""typ"">Pattern</span><span class=""pun"">.</span><span class=""pln"">java</span><span class=""pun"">:</span><span class=""lit"">1337</span><span class=""pun"">)</span><span class=""pln"">
    at java</span><span class=""pun"">.</span><span class=""pln"">util</span><span class=""pun"">.</span><span class=""pln"">regex</span><span class=""pun"">.</span><span class=""typ"">Pattern</span><span class=""pun"">.</span><span class=""pln"">compile</span><span class=""pun"">(</span><span class=""typ"">Pattern</span><span class=""pun"">.</span><span class=""pln"">java</span><span class=""pun"">:</span><span class=""lit"">1022</span><span class=""pun"">)</span><span class=""pln"">
    at org</span><span class=""pun"">.</span><span class=""pln"">apache</span><span class=""pun"">.</span><span class=""pln"">hadoop</span><span class=""pun"">.</span><span class=""pln"">hive</span><span class=""pun"">.</span><span class=""pln"">ql</span><span class=""pun"">.</span><span class=""pln"">udf</span><span class=""pun"">.</span><span class=""typ"">UDFRegExpExtract</span><span class=""pun"">.</span><span class=""pln"">evaluate</span><span class=""pun"">(</span><span class=""typ"">UDFRegExpExtract</span><span class=""pun"">.</span><span class=""pln"">java</span><span class=""pun"">:</span><span class=""lit"">51</span><span class=""pun"">)</span><span class=""pln"">
    </span><span class=""pun"">...</span><span class=""pln""> </span><span class=""lit"">23</span><span class=""pln""> more</span></code></pre>

<p>If you look at the error message you see that the regular expression used by Hive was <code>name\(.*?),</code>. So the second slash disappeared and only one slash was used. As discussed before, our regular expression knowledge tells us that this regular expression will not work. That is because the backslash now escapes the opening bracket and thus states that this opening bracket has to be interpreted literally. Therefore, the closing bracket has no matching opening bracket anymore and the regular expression crashes.</p>

<h4 id=""slashception"">Slashception</h4>

<p>So if two slashes translate to one slash in the regular expression, what happens when we use four then? That should translate to the regular expression <code>name\\(.*?),</code>. Therefore we tried <code>regexp_extract(original_json_object, 'name\\\\(.*?),', 1)</code> which returned <code>"":\""Thom\""</code>. Hurray, we found the solution to deal with the first slash: <strong>four slashes</strong>. Using the same logic we then used <code>regexp_extract(original_json_object, 'name\\\\:\\\\""(.*?)\\\\""', 1)</code> which returned <code>Thom</code>. Hurrah!</p>

<h4 id=""whytellmewhy"">Why tell me why</h4>

<p>We were able to succesfully complete the task using the latter regexp_extract function. A few hours later I was still wondering though why we needed four slashes to escape one single slash. Because we now knew the solution to the problem it was much easier to find other people with the same problem via Google. This <a href=""http://stackoverflow.com/questions/18875852/why-string-replaceall-in-java-requires-4-slashes-in-regex-to-actually-r"">Stack Overflow post</a> perfectly describes why we needed four slashes:</p>

<blockquote>
  <p>You need to escape twice, once for Java, once for the regex. Java code is <code>\\\\</code> makes a regex string of <code>\\</code>, i.e. two chars. But the regex needs an escape too, so it turns into <code>\</code>, i.e. one symbol <a href=""http://stackoverflow.com/users/57695/peter-lawrey"">~Peter Lawrey</a></p>
</blockquote>

<p>and additionally</p>

<blockquote>
  <p>@Peter Lawrey's answer describes the mechanics. Basically, the ""problem"" is that backslash is an escape character in both Java string literals, and in the mini-language of regexes. So when you use a string literal to represent a regex, there are two sets of escaping to consider, depending on what you want the regex to mean. <a href=""http://stackoverflow.com/users/139985/stephen-c"">~Stephen C</a></p>
</blockquote>

<p>If you found this blogpost because you are experiencing a similar problem, I hope we saved you a lot of time!</p>
        ","As a Data Scientist I frequently need to work with regular expressions. Though the capabilities and power of regular expressions are enormous, I just cannot seem to like them a lot. That is because when they do not function as expected they can be a really time-consuming nightmare. In this blogpost I will describe the hours I lost last week because of something I now call slashception.
JSONception
On most of our clients websites we have our own datalogger Snowplow running next to the Google or Adobe Analytics implementation. Snowplow enables us to do much more in depth analysis on log level data than we are able to do with only Google or Adobe Analytics. For a particular project Snowplow was storing a JSON-object in its database. This JSON-object contained a key-value pair for which the value was another JSON-object. This nested JSON-object was however stored as a string value. To ensure that the whole JSON-object is syntactically correct, the string-formatted JSON-object contains several backslash-characters (\) to escape the quote-characters (“). An illustration of the JSON-object we are talking about is given below:
{  
   ""key1"":""value1"",
   ""key2"":{  
      ""settings"":{  
         ""setting1"":true,
         ""setting2"":""{\""client\"":{\""id\"":\""26480999\"",\""name\"":\""Thom\"",\""age\"":\""24\""}}""
      }
   }
}
What's in a name?
For our analysis we were interested in the value in the key 'name'. In the example above that value would have been Thom (just a random name for illustration purposes... or maybe not thát random.). Our idea was to use the regexp_extract function in Hive to extract the name key-value pair and store the value in a new column denoted by 'name'. In that way, we could use the name column in all our subsequent queries on the database. Confident and with high hopes we ran the following Hive query:
insert overwrite table data partition(run)  
select  
original_json_object,  
regexp_extract(original_json_object, 'name\\"":\\""(.*?)\\', 1) as name,  
run  
from data;  
Note that the regular expression we used was regexp_extract(original_json_object, 'name\\"":\\""(.*?)\\', 1). We used two slashes, because we have to escape the protected backslash character in regular expressions. The first backslash tells the regular expression we want to use the second backslash character literally. Unfortunately, this regular expression did not do the trick as it returned ...
Confused by the result of NULL instead of Thom we called for help from the internet. Regex101.com is a site we regularly use to create and test our regular expressions. However, also Regex101 said that this regular expression was correct. That is, Regex101 stated that the test string name\"":\""Thom\"" is a match to the regular expression name\\"":\\""(.*?)\\"". After spending already too much time looking for the solution on the internet and its community, it was time to call in the colleagues. Even they hadn't experienced this issue before and mentioned that two backslashes should be enough. I kind of suspect that the first colleague that reads this post immediately knows the solution to this problem but that he or she wasn't around at the time.
We're taking the regex to Isengard
As a last resort we applied a trial and error approach. That is, we let our knowledge about escaping with slashes in regular expressions go and just tried what would happen if we widened our search, i.e. regexp_extract(original_json_object, 'name(.*?),', 1). Funny enough, this returned \"":\""Thom\"". Now we were getting somewhere. The regexp_extract function was function correctly, we only needed to narrow down the search.
The next try was regexp_extract(original_json_object, 'name\(.*?),', 1). Note that we did not escape the backslash as this didn't work in our earlier attempts and we are now doing trial runs to locate the problem. The output of this regexp_extract function was \"":\""Thom\"". So after adding the extra slash we still got the same output. It was almost like our added slash disappeared... We tried adding an additional slash then! Unfortunately, regexp_extract(original_json_object, 'name\\(.*?),', 1) returned an error:
Caused by: java.util.regex.PatternSyntaxException: Unmatched closing ')' near index 16  
name\(.*?),  
                ^
    at java.util.regex.Pattern.error(Pattern.java:1924)
    at java.util.regex.Pattern.compile(Pattern.java:1669)
    at java.util.regex.Pattern.<init>(Pattern.java:1337)
    at java.util.regex.Pattern.compile(Pattern.java:1022)
    at org.apache.hadoop.hive.ql.udf.UDFRegExpExtract.evaluate(UDFRegExpExtract.java:51)
    ... 23 more
If you look at the error message you see that the regular expression used by Hive was name\(.*?),. So the second slash disappeared and only one slash was used. As discussed before, our regular expression knowledge tells us that this regular expression will not work. That is because the backslash now escapes the opening bracket and thus states that this opening bracket has to be interpreted literally. Therefore, the closing bracket has no matching opening bracket anymore and the regular expression crashes.
Slashception
So if two slashes translate to one slash in the regular expression, what happens when we use four then? That should translate to the regular expression name\\(.*?),. Therefore we tried regexp_extract(original_json_object, 'name\\\\(.*?),', 1) which returned "":\""Thom\"". Hurray, we found the solution to deal with the first slash: four slashes. Using the same logic we then used regexp_extract(original_json_object, 'name\\\\:\\\\""(.*?)\\\\""', 1) which returned Thom. Hurrah!
Why tell me why
We were able to succesfully complete the task using the latter regexp_extract function. A few hours later I was still wondering though why we needed four slashes to escape one single slash. Because we now knew the solution to the problem it was much easier to find other people with the same problem via Google. This Stack Overflow post perfectly describes why we needed four slashes:
You need to escape twice, once for Java, once for the regex. Java code is \\\\ makes a regex string of \\, i.e. two chars. But the regex needs an escape too, so it turns into \, i.e. one symbol ~Peter Lawrey
and additionally
@Peter Lawrey's answer describes the mechanics. Basically, the ""problem"" is that backslash is an escape character in both Java string literals, and in the mini-language of regexes. So when you use a string literal to represent a regex, there are two sets of escaping to consider, depending on what you want the regex to mean. ~Stephen C
If you found this blogpost because you are experiencing a similar problem, I hope we saved you a lot of time!","[Data Science, Hive]"
92,Is Apple mocking Google's Alphabet announcement?,/is-apple-mocking-googles-alphabet-announcement/,"
            <p>This post is not about code, analytics or data. It's about tech. And something I've noticed after Apple's September 9 event. </p>

<p>On August 10, Google announced an organizational restructuring. They announced <a href=""http://abc.xyz"">Alphabet</a>. On September 9, Apple announced, amongst other devices, <a href=""http://www.apple.com/ipad-pro/"">the iPad Pro</a>. Today, nobody is talking about Apple (possibly) mocking Google’s announcement. </p>

<p>Google’s announcement surprised a lot of us. Speculated reasons for the change were ranging from financial benefits, to shifting innovation priorities. The latter one probably is the main reason (more on that on <a href=""http://www.forbes.com/sites/jacobmorgan/2015/08/14/why-googles-recent-alphabet-announcement-is-pure-genius/"">forbes.com</a>). </p>

<p>After the announcement, there were quite some posts  about Microsoft mocking Alphabet, with <a href=""http://abc.wtf"">abc.wtf</a> redirecting to the Bing landing page. Strangely no one either noticed Apple’s joke last week, or this one is just really farfetched. Anyway, here it goes:</p>

<h3 id=""applemockingalphabet"">Apple mocking Alphabet</h3>

<p>Apple announced the iPad Pro. They’ve added an optional Smart Keyboard. Here’s what the keyboard page reads:</p>

<blockquote>
  <p>The only thing we didn't reinvent was the alphabet.</p>
</blockquote>

<p>Or as shown on the Apple website:</p>

<p><img src=""http://i58.tinypic.com/2i1p0es.png"" alt=""Apple's Smart Keyboard page mocking Google's Alphabet announcment?"" class=""full-img"">
<em>Screenshot from 17 sept 2015 - <a href=""http://www.apple.com/smart-keyboard/"">iPad Pro Smart Keyboard page</a></em></p>

<p>So what do you think: is this just Apple being Apple? Or is it a subtle way of mocking Google? </p>
        ","This post is not about code, analytics or data. It's about tech. And something I've noticed after Apple's September 9 event.
On August 10, Google announced an organizational restructuring. They announced Alphabet. On September 9, Apple announced, amongst other devices, the iPad Pro. Today, nobody is talking about Apple (possibly) mocking Google’s announcement.
Google’s announcement surprised a lot of us. Speculated reasons for the change were ranging from financial benefits, to shifting innovation priorities. The latter one probably is the main reason (more on that on forbes.com).
After the announcement, there were quite some posts about Microsoft mocking Alphabet, with abc.wtf redirecting to the Bing landing page. Strangely no one either noticed Apple’s joke last week, or this one is just really farfetched. Anyway, here it goes:
Apple mocking Alphabet
Apple announced the iPad Pro. They’ve added an optional Smart Keyboard. Here’s what the keyboard page reads:
The only thing we didn't reinvent was the alphabet.
Or as shown on the Apple website:
Screenshot from 17 sept 2015 - iPad Pro Smart Keyboard page
So what do you think: is this just Apple being Apple? Or is it a subtle way of mocking Google?",[tech]
93,The GAM approach to spend your money more efficiently!,/the-gam-approach-to-spend-your-money-more-efficiently/,"
            <p>In an <a href=""http://geek.bluemangointeractive.com/optimize-media-spends-using-s-response-curves/"">earlier blogpost</a> we described how Blue Mango Interactive optimizes the media spend of clients using S-curves. S-curves are used to find the S-shaped relationship of a particular media driver on a KPI such as sales. Moreover, when a S-curve is obtained, we can determine the optimal point that prevents under- or overspending. Hence, <em>we spend our money more efficiently</em>! The previous method however required quite some manual steps and hassle. Inspired by an awesome blogpost (<a href=""http://multithreaded.stitchfix.com/blog/2015/07/30/gam/"">GAM: The Predictive Modeling Silver Bullet</a>) we return this time with a brand new state-of-the art modelling technique: <strong>GAM!</strong></p>

<p><img src=""http://i57.tinypic.com/51bihw.png"" alt=""Example of a S-shaped response curve for the effect of radio on sales"" class=""full-img""></p>

<p>The previous blog post described how an ordinary least-squares (OLS) regression can be used to find the S-curve of a particular media driver. In a fictional example we estimated the S-curve for radio in terms of <a href=""https://en.wikipedia.org/wiki/Gross_rating_point"">GRPs</a>. The OLS regression technique comes from the family of generalized linear modelling (<a href=""https://en.wikipedia.org/wiki/Generalized_linear_model"">GLM</a>) techniques. One of the reasons GLM techniques are so popular nowadays is that they provide an <strong>interpretable</strong> and <strong>computationally fast</strong> method to find the effect of independent variables (e.g. years of education, age) on a dependent variable (e.g. wage). For example, an OLS regression could return that the relationship between education and wage is that for every year of education followed you’ll earn €500 p/m more. Adding more variables to this OLS regression such as age and gender will result in their quantified effects on wage. </p>

<h4 id=""thelisforlinearrelationships"">The L is for linear relationships</h4>

<p>Unfortunately, as the name already suggests, OLS typically only returns linear relationships between the independent variables (e.g. years of education) and dependent variable (e.g. wage). Using the previous wage example, one year of education would imply earning €500 p/m, but ten years of education would imply earning €5000 p/m. Linear relationships are however very limited when modelling nonlinear relationships. For example, assume that the first year of education results in earning €500 p/m more, but the second year only gives you €400 more on top of that, and the third year €300, and so on... If the marginal effect of every additional year of education is decreasing like this, then a really worse fit is obtained when using OLS. <br>
In practice, such non-linear relationships are often tackled by applying transformations on the data. For example, it is possible to capture the diminishing effect of each additional year of education by applying a square root on the years of education. Assuming that after a square root transformation on education the quantified relationship between education and wage is still €500, then ten years of education would imply earning √<span style=""text-decoration: overline"">10</span> = 3.16 × 500 = €1550 p/m. </p>

<h4 id=""ivegot99problemsandlinearrelationshipsaintone"">I’ve got 99 problems and linear relationships ain’t one</h4>

<p>So, what is the problem if data transformations can be used to model non-linear relationships using GLM techniques? Well, in most cases, we have to perform many OLS regressions to find the best-fitting data transformation. This process involves trying a lot of different transformations. For example, we need to check whether the transformation x<sup>0.4</sup> fits the data better than x<sup>0.6</sup> or x<sup>0.5</sup> . Moreover, we might be <a href=""http://scikit-learn.org/stable/auto_examples/plot_underfitting_overfitting.html"">overfitting</a> our data. It could be that the true relationship is y=x<sup>0.5</sup>, but that y=x<sup>0.54</sup> fits our random sample dataset better. Preferably, we need some a priori knowledge about the type of transformation we need. </p>

<p>In the previous blog post we described a method that finds the S-response curve of a media driver in several steps. Assume that we want to estimate the S-shaped effect of radio GRPs. This required the following steps:</p>

<ol>
<li>The continuous radio GRP variable was replaced by dummies, each representing a specific continuous interval.  </li>
<li>An OLS regression was used to estimate the effect of each dummy and thus of each interval.  </li>
<li>A S-curve was then estimated that fits with the estimated effects of each interval.  </li>
<li>A S-curve transformation was then applied to transform the continuous radio variable.  </li>
<li>Finally, the OLS regression was performed again. This time however, the transformed continuous radio variable was used instead of the interval dummies. Because the radio variable is transformed, the coefficient returned by the OLS regression now didn’t denote a linear relationship anymore, but a S-shaped relationship.</li>
</ol>

<p>Wouldn’t it be nice if we could skip all these (manual) steps and use a more mathematical approach to find the best-fitting S-curve? <strong>GAM modelling to the rescue!</strong> <br>
<img src=""http://i62.tinypic.com/2a6t9w9.jpg"" alt=""GAM modelling to the rescue"" class=""full-img"">
(<a href=""https://www.flickr.com/photos/coastguardnews/3052063048/"">Photo by Coast Guard News on Flickr</a>)</p>

<h4 id=""gammodellingtotherescue"">GAM modelling to the rescue!</h4>

<p>Generalized additive models (GAM) is an additive modelling technique where the effect of the dependent variables (e.g. wage) is captured through smooth functions on the independent variables (e.g. years of education, age, gender). Note that these smooth functions do not need to be linear as is the case in GLMs! An example of variables in a GAM model is given below, where s<sub>1</sub> and s<sub>2</sub> are smooth non-linear functions with respective input x<sub>1</sub> and x<sub>2</sub>. Note that s<sub>3</sub> is a smooth linear function as is normally returned by OLS.</p>

<p><img src=""http://i62.tinypic.com/20qghhg.png"" alt=""Example of flexible smooth (non-)linear functions."" class=""full-img""></p>

<p>GAM models therefore have the same easy and intuitive interpretation property of OLS models, but also have the flexibility to model nonlinear relationships. The latter makes it possible to find hidden patterns in our data, which would have gone unnoticed otherwise. Additionally, GAM uses a regularization parameter to prevent overfitting the data!</p>

<p>I would like to refer again to the awesome blogpost about GAM modelling (<a href=""http://multithreaded.stitchfix.com/blog/2015/07/30/gam/"">GAM: The Predictive Modeling Silver Bullet</a>) for the mathematical point of view. Furthermore it also explains how, for example, the best-fitting smooth functions are obtained using an algorithm. Additionally, it also explains how GAM prevents overfitting using a regularization parameter. In the remainder of this blog post, I would like to focus on the advantages and disadvantages of using GAM models to find the S-response curve for a fictional radio example.</p>

<h4 id=""rversuspython"">R versus Python</h4>

<p>Let’s consider again the radio example of the <a href=""http://geek.bluemangointeractive.com/optimize-media-spends-using-s-response-curves/"">previous blogpost</a>. This time, however, we switch to R as programming language. That is because Python does not yet provide a good library for GAM modelling. <a href=""http://statsmodels.sourceforge.net/"">Statsmodels</a> does contain GAM modelling in its sandbox functionality, but GAM modelling in R is more advanced and widely supported. The two main packages in R that can be used to fit generalized additive models are <em>gam</em> and <em>mgcv</em>. We use <a href=""http://people.bath.ac.uk/sw283/mgcv/"">mgcv</a> because it uses a more general approach. The R code to create a fictional dataset with sales and radio GRPs can be found on <a href=""https://github.com/thomhopmans"">Github</a>.</p>

<p>Note that the number of sales on any given day depends on the day of the week (monday,..., sunday), the number of radio GRPs on that day and some normally distributed noise. Moreover, the effect of radio GRPs on sales is logistically distributed and thus follows an S-shape. </p>

<h3 id=""letthegamesbegin"">Let the GAM(es) begin</h3>

<p>We can now formulate the problem as a GAM problem by </p>

<p><img src=""http://i58.tinypic.com/3020sjb.gif"" alt=""Equation (1)""></p>

<p>where x<sub>monday</sub>,…,x<sub>saturday</sub> are day dummy variables and s<sub>1</sub>(x) is a smooth function. The <em>mcgv</em> package in R is used to solve the above GAM problem.  </p>

<pre><code class=""language-prettyprint lang-r prettyprinted""><span class=""pln"">  </span><span class=""com""># Initialize GAM model with 1 smooth function for radio_grp</span><span class=""pln"">
  b1 </span><span class=""pun"">&lt;-</span><span class=""pln""> mgcv</span><span class=""pun"">::</span><span class=""pln"">gam</span><span class=""pun"">(</span><span class=""pln"">sales_total </span><span class=""pun"">~</span><span class=""pln""> s</span><span class=""pun"">(</span><span class=""pln"">radio_grp</span><span class=""pun"">,</span><span class=""pln""> bs</span><span class=""pun"">=</span><span class=""str"">'ps'</span><span class=""pun"">,</span><span class=""pln""> sp</span><span class=""pun"">=</span><span class=""lit"">0.5</span><span class=""pun"">)</span><span class=""pln"">
                  </span><span class=""pun"">+</span><span class=""pln""> seasonality_monday </span><span class=""pun"">+</span><span class=""pln""> seasonality_tuesday 
                  </span><span class=""pun"">+</span><span class=""pln""> seasonality_wednesday </span><span class=""pun"">+</span><span class=""pln""> seasonality_thursday 
                  </span><span class=""pun"">+</span><span class=""pln""> seasonality_friday </span><span class=""pun"">+</span><span class=""pln""> seasonality_saturday</span><span class=""pun"">,</span><span class=""pln""> 
                  data</span><span class=""pun"">=</span><span class=""pln"">dat</span><span class=""pun"">)</span><span class=""pln"">

  </span><span class=""com""># Output model results and store intercept for plotting later on</span><span class=""pln"">
  summary_model      </span><span class=""pun"">&lt;-</span><span class=""pln""> summary</span><span class=""pun"">(</span><span class=""pln"">b1</span><span class=""pun"">)</span><span class=""pln"">
  model_coefficients </span><span class=""pun"">&lt;-</span><span class=""pln""> summary_model$p</span><span class=""pun"">.</span><span class=""pln"">table
  model_intercept    </span><span class=""pun"">&lt;-</span><span class=""pln""> model_coefficients</span><span class=""pun"">[</span><span class=""str"">""(Intercept)""</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""lit"">1</span><span class=""pun"">]</span><span class=""pln"">


  </span><span class=""com""># Plot the smooth predictor function to obtain the radio response curve</span><span class=""pln"">
  p    </span><span class=""pun"">&lt;-</span><span class=""pln""> predict</span><span class=""pun"">(</span><span class=""pln"">b1</span><span class=""pun"">,</span><span class=""pln""> type</span><span class=""pun"">=</span><span class=""str"">""lpmatrix""</span><span class=""pun"">)</span><span class=""pln"">
  beta </span><span class=""pun"">&lt;-</span><span class=""pln""> coef</span><span class=""pun"">(</span><span class=""pln"">b1</span><span class=""pun"">)[</span><span class=""pln"">grepl</span><span class=""pun"">(</span><span class=""str"">""radio_grp""</span><span class=""pun"">,</span><span class=""pln""> names</span><span class=""pun"">(</span><span class=""pln"">coef</span><span class=""pun"">(</span><span class=""pln"">b1</span><span class=""pun"">)))]</span><span class=""pln"">
  s    </span><span class=""pun"">&lt;-</span><span class=""pln""> p</span><span class=""pun"">[,</span><span class=""pln"">grepl</span><span class=""pun"">(</span><span class=""str"">""radio_grp""</span><span class=""pun"">,</span><span class=""pln""> colnames</span><span class=""pun"">(</span><span class=""pln"">p</span><span class=""pun"">))]</span><span class=""pln""> </span><span class=""pun"">%*%</span><span class=""pln""> beta </span><span class=""pun"">+</span><span class=""pln""> model_intercept</span></code></pre>

<p>The above code returns the following summary:  </p>

<pre><code>Family: gaussian  
Link function: identity 

Formula:  
sales_total ~ s(radio_grp, bs = ""ps"", sp = 0.5) + seasonality_monday +  
    seasonality_tuesday + seasonality_wednesday + seasonality_thursday + 
    seasonality_friday + seasonality_saturday

Parametric coefficients:  
                      Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)             1.5687     0.1325   11.84   &lt;2e-16 ***
seasonality_monday      3.8044     0.1840   20.68   &lt;2e-16 ***  
seasonality_tuesday     3.0623     0.1830   16.74   &lt;2e-16 ***  
seasonality_wednesday   4.0863     0.1839   22.22   &lt;2e-16 ***  
seasonality_thursday    5.0454     0.1829   27.58   &lt;2e-16 ***  
seasonality_friday      6.1710     0.1849   33.38   &lt;2e-16 ***  
seasonality_saturday    7.8875     0.1839   42.89   &lt;2e-16 ***  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Approximate significance of smooth terms:  
               edf Ref.df    F p-value    
s(radio_grp) 3.848  4.571 1032  &lt;2e-16 ***  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

R-sq.(adj) =   0.99   Deviance explained = 99.1%  
GCV = 0.22855  Scale est. = 0.201     n = 90  
</code></pre>

<p>This summary obviously looks different than when using OLS. However, it still returns statistics for the dummy coefficients  such as the estimate, standard error and significance. The statistics for the dummy coefficients are interpreted in a similar manner as in OLS (so in this case Tuesday sales are three units higher compared to Sunday and all coefficients are statistically significant positive). The effect of radio GRPs is difficult to explain from this summary, but a visualization is very effective though! The code for this visualization can again be found on <a href=""https://github.com/thomhopmans"">Github</a>.</p>

<p><img src=""http://i62.tinypic.com/34gues7.png"" alt=""S-curve visualization"" class=""full-img""></p>

<p>Well, look at this… the GAM model almost perfectly captured the S-shaped effect of radio on sales! And that without all the hassle and extra steps we needed in the previous blog. GAM modelling is therefore a really awesome technique to estimate nonlinear relationships. But, before we jump to conclusions, lets first do some sense checks.</p>

<h4 id=""farbeitfrommetoeverletmycommonsensegetinthewayofmystupidityisaywepresson"">“Far be it from me to ever let my common sense get in the way of my stupidity. I say we press on.”</h4>

<p>The fictional dataset we created above is a somewhat ideal scenario: many radio observations and little noise/variance in the number of sales. So what happens if we use GAM modelling on datasets with less/more observations and less/more noise? The figure below shows the S-curve obtained using GAM for such different datasets. <br>
<img src=""http://i58.tinypic.com/wc26gn.png"" alt=""The S-curve for different amounts of noise and radio observations"" class=""full-img"">
It is not surprising that as the dataset contains more noise or less radio observations the estimated relationship of radio on sales fits worse. However, overall, it still provides a relatively accurate estimation. Even when the noise (standard deviation on the sales per day) is very high, the estimated curve still shows the S-curve effect of increasing returns at first and diminishing returns thereafter.</p>

<p>GAM modelling is therefore also useful when we have noisy data or few variable observations. We want to perform a second sense check however. That is, how does GAM perform when the observations are not well-balanced. For example when we have few observations with high GRP values. Note that this is often the case in practice because of obvious budget constraints. Therefore, we created a fictional dataset where the majority of the radio observations lies below 5 GRPs and only a few above. This was done by taking random radio GRP values from a N(4, 2)-distribution instead of the U[0,10] we used earlier. The figure below shows the estimated curve by the GAM model for this dataset with again variations in noise and observations. <br>
<img src=""http://i59.tinypic.com/20j1b8z.png"" alt=""The S-curve for different amounts of noise and radio observations where the radio observations are clustered more at the beginning of the curve."" class=""full-img"">
We see that the GAM model now has more trouble to find the S-curved relationship of radio on sales. As the majority of the radio grp samples are clustered at the lower part of the curve, GAM has no troubles to estimate the increasing returns effect at the beginning of the curve. However, because of the few samples at the top of the curve, GAM has some troubles to estimate the decreasing returns effect after the <a href=""http://mathworld.wolfram.com/InflectionPoint.html"">inflection point</a>. Especially for the datasets with much noise GAM experiences difficulties and tries to linearly extrapolate the effect. After two sense checks though we can conclude that GAM modelling provides an excellent method to estimate S-curves for media mix modelling. Be careful though when the observations of the variable you want to model are really dense or clustered around a few points. As GAM modelling is not restricted to S-shaped relationships that could lead to strange curves. On the other hand, GAM modelling definitely provides more freedom in the relationships we can model. Additionally it prevents all the manual steps and hassle we needed in our previous blogpost. So, happy modelling and have a GAM time! :)</p>
        ","In an earlier blogpost we described how Blue Mango Interactive optimizes the media spend of clients using S-curves. S-curves are used to find the S-shaped relationship of a particular media driver on a KPI such as sales. Moreover, when a S-curve is obtained, we can determine the optimal point that prevents under- or overspending. Hence, we spend our money more efficiently! The previous method however required quite some manual steps and hassle. Inspired by an awesome blogpost (GAM: The Predictive Modeling Silver Bullet) we return this time with a brand new state-of-the art modelling technique: GAM!
The previous blog post described how an ordinary least-squares (OLS) regression can be used to find the S-curve of a particular media driver. In a fictional example we estimated the S-curve for radio in terms of GRPs. The OLS regression technique comes from the family of generalized linear modelling (GLM) techniques. One of the reasons GLM techniques are so popular nowadays is that they provide an interpretable and computationally fast method to find the effect of independent variables (e.g. years of education, age) on a dependent variable (e.g. wage). For example, an OLS regression could return that the relationship between education and wage is that for every year of education followed you’ll earn €500 p/m more. Adding more variables to this OLS regression such as age and gender will result in their quantified effects on wage.
The L is for linear relationships
Unfortunately, as the name already suggests, OLS typically only returns linear relationships between the independent variables (e.g. years of education) and dependent variable (e.g. wage). Using the previous wage example, one year of education would imply earning €500 p/m, but ten years of education would imply earning €5000 p/m. Linear relationships are however very limited when modelling nonlinear relationships. For example, assume that the first year of education results in earning €500 p/m more, but the second year only gives you €400 more on top of that, and the third year €300, and so on... If the marginal effect of every additional year of education is decreasing like this, then a really worse fit is obtained when using OLS.
In practice, such non-linear relationships are often tackled by applying transformations on the data. For example, it is possible to capture the diminishing effect of each additional year of education by applying a square root on the years of education. Assuming that after a square root transformation on education the quantified relationship between education and wage is still €500, then ten years of education would imply earning √10 = 3.16 × 500 = €1550 p/m.
I’ve got 99 problems and linear relationships ain’t one
So, what is the problem if data transformations can be used to model non-linear relationships using GLM techniques? Well, in most cases, we have to perform many OLS regressions to find the best-fitting data transformation. This process involves trying a lot of different transformations. For example, we need to check whether the transformation x0.4 fits the data better than x0.6 or x0.5 . Moreover, we might be overfitting our data. It could be that the true relationship is y=x0.5, but that y=x0.54 fits our random sample dataset better. Preferably, we need some a priori knowledge about the type of transformation we need.
In the previous blog post we described a method that finds the S-response curve of a media driver in several steps. Assume that we want to estimate the S-shaped effect of radio GRPs. This required the following steps:
The continuous radio GRP variable was replaced by dummies, each representing a specific continuous interval.
An OLS regression was used to estimate the effect of each dummy and thus of each interval.
A S-curve was then estimated that fits with the estimated effects of each interval.
A S-curve transformation was then applied to transform the continuous radio variable.
Finally, the OLS regression was performed again. This time however, the transformed continuous radio variable was used instead of the interval dummies. Because the radio variable is transformed, the coefficient returned by the OLS regression now didn’t denote a linear relationship anymore, but a S-shaped relationship.
Wouldn’t it be nice if we could skip all these (manual) steps and use a more mathematical approach to find the best-fitting S-curve? GAM modelling to the rescue!
(Photo by Coast Guard News on Flickr)
GAM modelling to the rescue!
Generalized additive models (GAM) is an additive modelling technique where the effect of the dependent variables (e.g. wage) is captured through smooth functions on the independent variables (e.g. years of education, age, gender). Note that these smooth functions do not need to be linear as is the case in GLMs! An example of variables in a GAM model is given below, where s1 and s2 are smooth non-linear functions with respective input x1 and x2. Note that s3 is a smooth linear function as is normally returned by OLS.
GAM models therefore have the same easy and intuitive interpretation property of OLS models, but also have the flexibility to model nonlinear relationships. The latter makes it possible to find hidden patterns in our data, which would have gone unnoticed otherwise. Additionally, GAM uses a regularization parameter to prevent overfitting the data!
I would like to refer again to the awesome blogpost about GAM modelling (GAM: The Predictive Modeling Silver Bullet) for the mathematical point of view. Furthermore it also explains how, for example, the best-fitting smooth functions are obtained using an algorithm. Additionally, it also explains how GAM prevents overfitting using a regularization parameter. In the remainder of this blog post, I would like to focus on the advantages and disadvantages of using GAM models to find the S-response curve for a fictional radio example.
R versus Python
Let’s consider again the radio example of the previous blogpost. This time, however, we switch to R as programming language. That is because Python does not yet provide a good library for GAM modelling. Statsmodels does contain GAM modelling in its sandbox functionality, but GAM modelling in R is more advanced and widely supported. The two main packages in R that can be used to fit generalized additive models are gam and mgcv. We use mgcv because it uses a more general approach. The R code to create a fictional dataset with sales and radio GRPs can be found on Github.
Note that the number of sales on any given day depends on the day of the week (monday,..., sunday), the number of radio GRPs on that day and some normally distributed noise. Moreover, the effect of radio GRPs on sales is logistically distributed and thus follows an S-shape.
Let the GAM(es) begin
We can now formulate the problem as a GAM problem by
where xmonday,…,xsaturday are day dummy variables and s1(x) is a smooth function. The mcgv package in R is used to solve the above GAM problem.
  # Initialize GAM model with 1 smooth function for radio_grp
  b1 <- mgcv::gam(sales_total ~ s(radio_grp, bs='ps', sp=0.5)
                  + seasonality_monday + seasonality_tuesday 
                  + seasonality_wednesday + seasonality_thursday 
                  + seasonality_friday + seasonality_saturday, 
                  data=dat)

  # Output model results and store intercept for plotting later on
  summary_model      <- summary(b1)
  model_coefficients <- summary_model$p.table
  model_intercept    <- model_coefficients[""(Intercept)"", 1]


  # Plot the smooth predictor function to obtain the radio response curve
  p    <- predict(b1, type=""lpmatrix"")
  beta <- coef(b1)[grepl(""radio_grp"", names(coef(b1)))]
  s    <- p[,grepl(""radio_grp"", colnames(p))] %*% beta + model_intercept
The above code returns the following summary:
Family: gaussian  
Link function: identity 

Formula:  
sales_total ~ s(radio_grp, bs = ""ps"", sp = 0.5) + seasonality_monday +  
    seasonality_tuesday + seasonality_wednesday + seasonality_thursday + 
    seasonality_friday + seasonality_saturday

Parametric coefficients:  
                      Estimate Std. Error t value Pr(>|t|)    
(Intercept)             1.5687     0.1325   11.84   <2e-16 ***
seasonality_monday      3.8044     0.1840   20.68   <2e-16 ***  
seasonality_tuesday     3.0623     0.1830   16.74   <2e-16 ***  
seasonality_wednesday   4.0863     0.1839   22.22   <2e-16 ***  
seasonality_thursday    5.0454     0.1829   27.58   <2e-16 ***  
seasonality_friday      6.1710     0.1849   33.38   <2e-16 ***  
seasonality_saturday    7.8875     0.1839   42.89   <2e-16 ***  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Approximate significance of smooth terms:  
               edf Ref.df    F p-value    
s(radio_grp) 3.848  4.571 1032  <2e-16 ***  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

R-sq.(adj) =   0.99   Deviance explained = 99.1%  
GCV = 0.22855  Scale est. = 0.201     n = 90  
This summary obviously looks different than when using OLS. However, it still returns statistics for the dummy coefficients such as the estimate, standard error and significance. The statistics for the dummy coefficients are interpreted in a similar manner as in OLS (so in this case Tuesday sales are three units higher compared to Sunday and all coefficients are statistically significant positive). The effect of radio GRPs is difficult to explain from this summary, but a visualization is very effective though! The code for this visualization can again be found on Github.
Well, look at this… the GAM model almost perfectly captured the S-shaped effect of radio on sales! And that without all the hassle and extra steps we needed in the previous blog. GAM modelling is therefore a really awesome technique to estimate nonlinear relationships. But, before we jump to conclusions, lets first do some sense checks.
“Far be it from me to ever let my common sense get in the way of my stupidity. I say we press on.”
The fictional dataset we created above is a somewhat ideal scenario: many radio observations and little noise/variance in the number of sales. So what happens if we use GAM modelling on datasets with less/more observations and less/more noise? The figure below shows the S-curve obtained using GAM for such different datasets.
It is not surprising that as the dataset contains more noise or less radio observations the estimated relationship of radio on sales fits worse. However, overall, it still provides a relatively accurate estimation. Even when the noise (standard deviation on the sales per day) is very high, the estimated curve still shows the S-curve effect of increasing returns at first and diminishing returns thereafter.
GAM modelling is therefore also useful when we have noisy data or few variable observations. We want to perform a second sense check however. That is, how does GAM perform when the observations are not well-balanced. For example when we have few observations with high GRP values. Note that this is often the case in practice because of obvious budget constraints. Therefore, we created a fictional dataset where the majority of the radio observations lies below 5 GRPs and only a few above. This was done by taking random radio GRP values from a N(4, 2)-distribution instead of the U[0,10] we used earlier. The figure below shows the estimated curve by the GAM model for this dataset with again variations in noise and observations.
We see that the GAM model now has more trouble to find the S-curved relationship of radio on sales. As the majority of the radio grp samples are clustered at the lower part of the curve, GAM has no troubles to estimate the increasing returns effect at the beginning of the curve. However, because of the few samples at the top of the curve, GAM has some troubles to estimate the decreasing returns effect after the inflection point. Especially for the datasets with much noise GAM experiences difficulties and tries to linearly extrapolate the effect. After two sense checks though we can conclude that GAM modelling provides an excellent method to estimate S-curves for media mix modelling. Be careful though when the observations of the variable you want to model are really dense or clustered around a few points. As GAM modelling is not restricted to S-shaped relationships that could lead to strange curves. On the other hand, GAM modelling definitely provides more freedom in the relationships we can model. Additionally it prevents all the manual steps and hassle we needed in our previous blogpost. So, happy modelling and have a GAM time! :)","[Data Science, R]"
94,Building a URL validator with Amazon API Gateway,/building-a-url-validator-with-amazon-api-gateway/,"
            <p>In this turorial we’re going to create a cheap, zero-admin API on Amazon AWS which accesses a Lambda function to check if an URL is valid and is made public available via an Amazon S3 hosted static webpage. Before we start, let's get talk a bit about the core concepts.</p>

<h3 id=""coreconceptswithinamazonaws"">Core concepts within Amazon AWS</h3>

<h4 id=""amazonapigateway"">Amazon API Gateway</h4>

<p>This July, Amazon released Amazon API Gateway, a managed service that allows developers to build easy scalable APIs. AWS enables APIs to be created that can act as a 'front door' to access data, business logic, or functionality from backend services, such as applications running on Amazon EC2 or code running on AWS Lambda. In order to successfully host an API backend there must be a supporting infrastructure, which provides security, manages traffic, implements monitoring and provides other essential foundational services. Amazon API Gateway provides this infrastructure, and handles the tasks involved in accepting and processing up to 'hundreds of thousands' of concurrent API calls, including traffic management, authorisation and access control, monitoring, and API version management.</p>

<h4 id=""amazonlambda"">Amazon Lambda</h4>

<p>A bit earlier, in April, Amazon launched AWS Lambda as a zero-admin compute platform. You don’t have to run Amazon Elastic Compute Cloud(EC2) instances, think about scale, or worry about fault tolerance. You simply create a Lambda function (using Java or Node.js) and connect the function to your AWS resources in this case your Amazon API.</p>

<h4 id=""s3"">S3</h4>

<p>Amazon Simple Storage Service is Amazons alternative for cloud storage. Amazon S3 has a simple web services interface that you can use to store and retrieve any amount of data. You can access S3 directly or via 1 of the other Amazon services.</p>

<p>How we need to tie them together, we’ll explain in the diagram below.</p>

<p><img src=""http://s27.postimg.org/o3texs0vn/diagram_amazon_api.pnhttp://s27.postimg.org/o3texs0vn/diagram_amazon_api.png"" alt=""Show diagram""></p>

<p>Which illustrates the following steps <br>
1. First the client loads the webpage from the s3 bucket <br>
2. Types an url to validate and clicks submit <br>
3. The data is posted to the API Gateway <br>
4. The API Gateway passes the information through to the Lambda function <br>
5. The lambda functions does his magic and returns a result to the API Gateway <br>
6. The API gateway passes the result back to the client</p>

<h3 id=""soletsgetstarted"">So let's get started!</h3>

<p>These are our steps:</p>

<ol>
<li>Create API Gateway API  </li>
<li>Create a basic Lambda function  </li>
<li>Connect the API to the Lambda Function  </li>
<li>Deploy the API  </li>
<li>Test the Basic API  </li>
<li>Extend the API  </li>
<li>Create, upload and test a URL validation Lambda Function in Node.js  </li>
<li>Build a static front-end  </li>
<li>Enable Cross-origin resource sharing (CORS)</li>
</ol>

<h4 id=""1createapigatewayapi"">1. Create API Gateway API</h4>

<ol>
<li>Login in to <a href=""https://console.aws.amazon.com/apigateway"">amazon API Gateway</a> via the console. The first time you will get a welcome message, otherwise you’ll get the API console homepage  </li>
<li><p>Create a new API by clicking the “Create API” button <br>
a. Fill in a name and description for the API and click “Create API”. You will get the following screen: 
<img src=""http://s10.postimg.org/jxlsto22h/API.jpg"" alt=""API"" title=""""></p>

<p>b. We are going to create a new method, by clicking “Create Method”.</p>

<p>c. Select “Post” in the dropdown, since we are going to be posting url’s to this service from a static website. You will get the following screen:
<img src=""http://s15.postimg.org/acnsj8xtn/method.jpg"" alt=""API method"" title=""""></p>

<p>d. Select Lambda Function for “Integration type” and a “Lambda region”, since we didn’t create any lambda functions before we will get the following alert:
<img src=""http://s10.postimg.org/t6skhj0c9/warning.png"" alt=""Lambda warning"" title=""""></p></li>
<li><p>We're ready to create our first Lambda function</p></li>
</ol>

<h4 id=""2createabasiclambdafunction"">2. Create a basic Lambda function</h4>

<ol>
<li>Click “Create a Lambda Function” in the alert  </li>
<li>Click “Skip” in the “Select blueprint” screen  </li>
<li>New function <br>
a. Type a name, description and pick Node.js as the runtime <br>
b. Select “edit code inline” for now and past the following code, this will return the text “hello world” if you call the Lambda function:  </li>
</ol>

<pre><code class=""language- "">console.log('Loading event');  
exports.handler = function(event, context) {  
    context.done(null, {""Hello"":""World""});
};
</code></pre>

<p>c. Keep index.handler as the handler <br>
d. For Role we are going to create a new “* Basic execution role”, this will show the following screen, Leave the settings as they are for now and click “Allow” <br>
    <img src=""http://s10.postimg.org/crbo88zg9/role.png"" alt=""Lambda warning"" title=""""></p>

<p>e.      Keep the advanced settings as they are, click next, review the settings and click “Create function”</p>

<ol>
<li>Test your function, by clicking “Test”, leave template as is and click “submit”, the output under “Execution result” will show {“Hello “: “World”}</li>
</ol>

<h4 id=""3connecttheapitothelambdafunction"">3. Connect the API to the Lambda Function</h4>

<ol>
<li>Return to API gateway  </li>
<li>In Lambda Function start typing the name of the Lambda function you created, and select it in the dropdown and click “Save”.  </li>
<li>Click “Ok” in the “Add Permission to Lambda Function” modal  </li>
<li>Click the method “Post” under “/”, you’ll get the following screen: <br>
<img src=""http://s17.postimg.org/olyrizx67/API_screen.jpg"" alt=""Lambda warning"" title=""""></li>
<li>Test the API by clicking “Test” above the client-box in the “Method execution” screen.        Click “Test” again and the response body will show {“Hello “: “World”}</li>
</ol>

<h4 id=""4deploytheapi"">4. Deploy the API</h4>

<ol>
<li>In the resources screen, click “Deploy API”  </li>
<li>For Deployment stage, choose New Stage and type a name.  </li>
<li>Click Deploy.  </li>
<li>Copy the url</li>
</ol>

<h4 id=""5testthebasicapi"">5. Test the Basic API</h4>

<ol>
<li>Use a tool, I use Postman for chrome, to create a post request  </li>
<li><a href=""https://[my-api-id].execute-api.[region-id].amazonaws.com/[api-name"">https://[my-api-id].execute-api.[region-id].amazonaws.com/[api-name</a>]  </li>
<li>Set the header to Content-Type: application/json  </li>
<li>Send the request, again the result should be {  ""Hello"": ""World""}</li>
</ol>

<p>We created the skeleton of our Amazon API Gateway. Now we we’re ready to build our URL validator</p>

<h4 id=""5createamoreadvancedlambdafunctioninnodejs"">5. Create a more advanced Lambda Function in Node.js</h4>

<ol>
<li>Create the node-js app  in a directory on your computer, let’s say “urlvalidator”  </li>
<li>We use valid-url (<a href=""https://www.npmjs.com/package/valid-url"">https://www.npmjs.com/package/valid-url</a>) as a the module to validate if the supplied url is valid.Install by “npm install valid-url” in the directory  </li>
<li>Create a file index.js  </li>
<li>Copy or type the following text in index.js:  </li>
</ol>

<pre><code>var validUrl = require('valid-url');  
exports.handler = function(event, context) {  
    var url = event.url;
    if (validUrl.isWebUri(url)){
        context.succeed('Looks like an URI');
    } else {
        context.fail('Not a valid URI');
    }
};
</code></pre>

<p>
5. Package all the contents of the directory (index.js + node_modules) into a zip-file <br>
6. Go back to Amazon Lambda <br>
7. Select the previous created Lambda Function, you will get this screen: <br>
    <img src=""http://s28.postimg.org/a9k6rver1/upload_zip.jpg"" alt=""upload zip"" title="""" class=""full-img"">
8.       Select “Upload a .ZIP file”, select the zip and click “Save” <br>
9.       In the dropdown “Actions” select “configure sample event” <br>
10.   Type {  ""url"":”<a href=""http://www.bluemango.nl"">http://www.bluemango.nl</a> “}  as a sample and click “Submit”, the result should be ""Looks like an URI"" <br>
11.   Now change the sample event to {  ""url"":”<a href=""http://www.blueman|go.nl“}"">http://www.blueman|go.nl“}</a>  , the result should be “not a valid URI” <br>
12.   The Lambda function now validates if the url contains valid characters (Note: there’s a lot more room for improvement in the validation) <br>
13.   Go back to Amazon API Gateway <br>
14.   Select the previous created API and the post method <br>
15.   Click “Test” above client-box in the Method Execution Screen <br>
16.   Type {""url"":""<a href=""http://www.bluemango.nl"">http://www.bluemango.nl</a>"" } in the Request Body, and click “Test”,the result in the Response Body should be ""Looks like an URI"" <br>
17.   You can test the API again with the tool to create a post request. (Note: the body should be raw, json and have the following format {""url"":""<a href=""http://www.bluemango.nl"">http://www.bluemango.nl</a>""}) <br>
18.   Your API is now ready to use</p>

<h4 id=""6buildastaticfrontend"">6. Build a static front-end</h4>

<p>Build a static page to serve as a input form to the API <br>
1. Create an index.html with a form and a jquery function to post to the API <br>
2. Code for the Post</p>

<pre><code>var frm = $('#validateUrl');

frm.submit(function (ev) {  
    var url = $('#url');
    $.ajax({
        type: 'POST',
        url: '[url to the wepAPI]',
        contentType : 'application/json',
        data: JSON.stringify({ ""url"": url.val()}),
        success: function (data) {
            if(data.errorMessage)
                alert('' + data.errorMessage);
            else      
                alert('' + data);
        },
        error: function (data) {
            alert('error:' + data.errorMessage );
        }
    });
    ev.preventDefault();
});
</code></pre>

<p>3.Upload the page to S3 <br>
4. Go to <a href=""https://console.aws.amazon.com/s3/"">https://console.aws.amazon.com/s3/</a>. <br>
5. Create a new bucket, I choose url-validator. <br>
6. Go to the properties of the bucket. <br>
7. Click Enable Static Website Hosting and check “Enable website hosting”. <br>
8. Type “index.html” for the Index Document, click “Save”. <br>
9. upload the index,html to to bucket.</p>

<h3 id=""7enablecrossoriginresourcesharingcors"">7. Enable Cross-origin resource sharing (CORS)</h3>

<p>We need to enable CORS before we can call the webservice from the static index.html page <br>
1. Go to amazon web api, click the URL-Validator API <br>
2. Click the Method Response-box, Click Create Method, select “Option” in the dropdown <br>
3. Select “Lambda Function” and select the same Function as before, click Save <br>
4. Open the HTTP Status 200 <br>
5. Add three headers, “Access-Control-Allow-Headers”, “Access-Control-Allow-Methods”, “Access-Control-Allow-Origin”. <br>
6. Go back to “Method Execution” and click the “Integration Response” box <br>
7. Open the Method response status 200 and open the Header Mappings <br>
8. For Access-Control-Allow-Headers type “'Content-Type,X-Amz-Date,Authorization'”, for Access-Control-Allow-Methods type “'POST'”, for Access-Control-Allow-Origin type “' * '” (Note: be aware for the single qoutes) <br>
9. Now we need to do the same for the Post method, so click the Method Response-box, go to the POST method and repeat step 3 through 8 <br>
10. Deploy the API again <br>
11. Go to the url of your s3 bucket, fill in the form and click “Submit” <br>
12. Success!</p>

<h3 id=""conclusion"">Conclusion</h3>

<p>The Amazon API Gateway shows great promise, it does what it states, it's a gateway to their other services. The API is very scalable it can handle a million calls very smoothly. But for now just use it if you need a quick, easy API for a prototype. Don't immediately rewrite/rebuild your SaaS to use the Amazon API Gateway. It lacks workflow and testing functionality, development is only in Node or Java and setting up is still a lot of work (things like authorisation, headers, CORS is rather tedious).</p>

<p>Note: The running cost are very doable for. Running the example we build: it would cost us approx. $4.00 a month for a million calls</p>
        ","In this turorial we’re going to create a cheap, zero-admin API on Amazon AWS which accesses a Lambda function to check if an URL is valid and is made public available via an Amazon S3 hosted static webpage. Before we start, let's get talk a bit about the core concepts.
Core concepts within Amazon AWS
Amazon API Gateway
This July, Amazon released Amazon API Gateway, a managed service that allows developers to build easy scalable APIs. AWS enables APIs to be created that can act as a 'front door' to access data, business logic, or functionality from backend services, such as applications running on Amazon EC2 or code running on AWS Lambda. In order to successfully host an API backend there must be a supporting infrastructure, which provides security, manages traffic, implements monitoring and provides other essential foundational services. Amazon API Gateway provides this infrastructure, and handles the tasks involved in accepting and processing up to 'hundreds of thousands' of concurrent API calls, including traffic management, authorisation and access control, monitoring, and API version management.
Amazon Lambda
A bit earlier, in April, Amazon launched AWS Lambda as a zero-admin compute platform. You don’t have to run Amazon Elastic Compute Cloud(EC2) instances, think about scale, or worry about fault tolerance. You simply create a Lambda function (using Java or Node.js) and connect the function to your AWS resources in this case your Amazon API.
S3
Amazon Simple Storage Service is Amazons alternative for cloud storage. Amazon S3 has a simple web services interface that you can use to store and retrieve any amount of data. You can access S3 directly or via 1 of the other Amazon services.
How we need to tie them together, we’ll explain in the diagram below.
Which illustrates the following steps
1. First the client loads the webpage from the s3 bucket
2. Types an url to validate and clicks submit
3. The data is posted to the API Gateway
4. The API Gateway passes the information through to the Lambda function
5. The lambda functions does his magic and returns a result to the API Gateway
6. The API gateway passes the result back to the client
So let's get started!
These are our steps:
Create API Gateway API
Create a basic Lambda function
Connect the API to the Lambda Function
Deploy the API
Test the Basic API
Extend the API
Create, upload and test a URL validation Lambda Function in Node.js
Build a static front-end
Enable Cross-origin resource sharing (CORS)
1. Create API Gateway API
Login in to amazon API Gateway via the console. The first time you will get a welcome message, otherwise you’ll get the API console homepage
Create a new API by clicking the “Create API” button
a. Fill in a name and description for the API and click “Create API”. You will get the following screen:
b. We are going to create a new method, by clicking “Create Method”.
c. Select “Post” in the dropdown, since we are going to be posting url’s to this service from a static website. You will get the following screen:
d. Select Lambda Function for “Integration type” and a “Lambda region”, since we didn’t create any lambda functions before we will get the following alert:
We're ready to create our first Lambda function
2. Create a basic Lambda function
Click “Create a Lambda Function” in the alert
Click “Skip” in the “Select blueprint” screen
New function
a. Type a name, description and pick Node.js as the runtime
b. Select “edit code inline” for now and past the following code, this will return the text “hello world” if you call the Lambda function:
console.log('Loading event');  
exports.handler = function(event, context) {  
    context.done(null, {""Hello"":""World""});
};
c. Keep index.handler as the handler
d. For Role we are going to create a new “* Basic execution role”, this will show the following screen, Leave the settings as they are for now and click “Allow”
e. Keep the advanced settings as they are, click next, review the settings and click “Create function”
Test your function, by clicking “Test”, leave template as is and click “submit”, the output under “Execution result” will show {“Hello “: “World”}
3. Connect the API to the Lambda Function
Return to API gateway
In Lambda Function start typing the name of the Lambda function you created, and select it in the dropdown and click “Save”.
Click “Ok” in the “Add Permission to Lambda Function” modal
Click the method “Post” under “/”, you’ll get the following screen:
Test the API by clicking “Test” above the client-box in the “Method execution” screen. Click “Test” again and the response body will show {“Hello “: “World”}
4. Deploy the API
In the resources screen, click “Deploy API”
For Deployment stage, choose New Stage and type a name.
Click Deploy.
Copy the url
5. Test the Basic API
Use a tool, I use Postman for chrome, to create a post request
https://[my-api-id].execute-api.[region-id].amazonaws.com/[api-name]
Set the header to Content-Type: application/json
Send the request, again the result should be { ""Hello"": ""World""}
We created the skeleton of our Amazon API Gateway. Now we we’re ready to build our URL validator
5. Create a more advanced Lambda Function in Node.js
Create the node-js app in a directory on your computer, let’s say “urlvalidator”
We use valid-url (https://www.npmjs.com/package/valid-url) as a the module to validate if the supplied url is valid.Install by “npm install valid-url” in the directory
Create a file index.js
Copy or type the following text in index.js:
var validUrl = require('valid-url');  
exports.handler = function(event, context) {  
    var url = event.url;
    if (validUrl.isWebUri(url)){
        context.succeed('Looks like an URI');
    } else {
        context.fail('Not a valid URI');
    }
};
5. Package all the contents of the directory (index.js + node_modules) into a zip-file
6. Go back to Amazon Lambda
7. Select the previous created Lambda Function, you will get this screen:
8. Select “Upload a .ZIP file”, select the zip and click “Save”
9. In the dropdown “Actions” select “configure sample event”
10. Type { ""url"":”http://www.bluemango.nl “} as a sample and click “Submit”, the result should be ""Looks like an URI""
11. Now change the sample event to { ""url"":”http://www.blueman|go.nl“} , the result should be “not a valid URI”
12. The Lambda function now validates if the url contains valid characters (Note: there’s a lot more room for improvement in the validation)
13. Go back to Amazon API Gateway
14. Select the previous created API and the post method
15. Click “Test” above client-box in the Method Execution Screen
16. Type {""url"":""http://www.bluemango.nl"" } in the Request Body, and click “Test”,the result in the Response Body should be ""Looks like an URI""
17. You can test the API again with the tool to create a post request. (Note: the body should be raw, json and have the following format {""url"":""http://www.bluemango.nl""})
18. Your API is now ready to use
6. Build a static front-end
Build a static page to serve as a input form to the API
1. Create an index.html with a form and a jquery function to post to the API
2. Code for the Post
var frm = $('#validateUrl');

frm.submit(function (ev) {  
    var url = $('#url');
    $.ajax({
        type: 'POST',
        url: '[url to the wepAPI]',
        contentType : 'application/json',
        data: JSON.stringify({ ""url"": url.val()}),
        success: function (data) {
            if(data.errorMessage)
                alert('' + data.errorMessage);
            else      
                alert('' + data);
        },
        error: function (data) {
            alert('error:' + data.errorMessage );
        }
    });
    ev.preventDefault();
});
3.Upload the page to S3
4. Go to https://console.aws.amazon.com/s3/.
5. Create a new bucket, I choose url-validator.
6. Go to the properties of the bucket.
7. Click Enable Static Website Hosting and check “Enable website hosting”.
8. Type “index.html” for the Index Document, click “Save”.
9. upload the index,html to to bucket.
7. Enable Cross-origin resource sharing (CORS)
We need to enable CORS before we can call the webservice from the static index.html page
1. Go to amazon web api, click the URL-Validator API
2. Click the Method Response-box, Click Create Method, select “Option” in the dropdown
3. Select “Lambda Function” and select the same Function as before, click Save
4. Open the HTTP Status 200
5. Add three headers, “Access-Control-Allow-Headers”, “Access-Control-Allow-Methods”, “Access-Control-Allow-Origin”.
6. Go back to “Method Execution” and click the “Integration Response” box
7. Open the Method response status 200 and open the Header Mappings
8. For Access-Control-Allow-Headers type “'Content-Type,X-Amz-Date,Authorization'”, for Access-Control-Allow-Methods type “'POST'”, for Access-Control-Allow-Origin type “' * '” (Note: be aware for the single qoutes)
9. Now we need to do the same for the Post method, so click the Method Response-box, go to the POST method and repeat step 3 through 8
10. Deploy the API again
11. Go to the url of your s3 bucket, fill in the form and click “Submit”
12. Success!
Conclusion
The Amazon API Gateway shows great promise, it does what it states, it's a gateway to their other services. The API is very scalable it can handle a million calls very smoothly. But for now just use it if you need a quick, easy API for a prototype. Don't immediately rewrite/rebuild your SaaS to use the Amazon API Gateway. It lacks workflow and testing functionality, development is only in Node or Java and setting up is still a lot of work (things like authorisation, headers, CORS is rather tedious).
Note: The running cost are very doable for. Running the example we build: it would cost us approx. $4.00 a month for a million calls",[Code]
95,Catch React errors and log them to Sentry,/log-react-errors-to-sentry/,"
            <p>We use the amazing <a href=""http://www.getsentry.com"">Sentry</a> tool to track both our frontend and backend errors. It's pretty straighforward to setup Sentry for code that runs in the browser, and it captures all uncatched errors out of the box:  </p>

<pre><code class=""language-prettyprint lang-javascript prettyprinted""><span class=""typ"">Raven</span><span class=""pun"">.</span><span class=""pln"">config</span><span class=""pun"">(</span><span class=""str"">'[our key]'</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">  
    collectWindowErrors</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""kwd"">true</span><span class=""pun"">,</span><span class=""pln"">
    fetchContext</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""kwd"">true</span><span class=""pun"">,</span><span class=""pln"">
    linesOfContext</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""lit"">40</span><span class=""pln"">
</span><span class=""pun"">}).</span><span class=""pln"">install</span><span class=""pun"">();</span></code></pre>

<p>Easy. But recentenly we switched from AngularJS to ReactJS as our framework of choice, and we use <a href=""https://github.com/kriasoft/react-starter-kit"">React Starter Kit</a> as a starting point. When an error is thrown during the Reat lifecycle it looks something likes this:  </p>

<pre><code>Unhandled promise rejection TypeError: Cannot read property 'setState' of undefined  
</code></pre>

<p>Aj. This is an async error, and Sentry does not automatically log this error. It turns out they rely heavily on promises (using <a href=""https://github.com/jakearchibald/es6-promise"">ES6 Promise</a> and it looks like they do not really handle these errors nicely. I did some heavily googlin' on this subject, but I could not find a proper in-framework solution to catch these errors. </p>

<p>The only solution that works for us is to override the <code>error</code> method of <code>console</code> and call Sentry from there.  </p>

<pre><code class=""language-prettyprint lang-javascript prettyprinted""><span class=""kwd"">var</span><span class=""pln""> originalConsoleError </span><span class=""pun"">=</span><span class=""pln""> console</span><span class=""pun"">.</span><span class=""pln"">error</span><span class=""pun"">;</span><span class=""pln"">  
console</span><span class=""pun"">.</span><span class=""pln"">error </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""kwd"">function</span><span class=""pun"">(</span><span class=""pln"">message</span><span class=""pun"">,</span><span class=""pln""> error</span><span class=""pun"">)</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">  
    </span><span class=""typ"">Raven</span><span class=""pun"">.</span><span class=""pln"">captureException</span><span class=""pun"">(</span><span class=""pln"">error</span><span class=""pun"">);</span><span class=""pln"">
    originalConsoleError</span><span class=""pun"">.</span><span class=""pln"">apply</span><span class=""pun"">(</span><span class=""kwd"">this</span><span class=""pun"">,</span><span class=""pln""> arguments</span><span class=""pun"">)</span><span class=""pln"">
</span><span class=""pun"">}</span></code></pre>

<p>I can't imagine this is really the way to go, but for now it works. I wonder if there is a better way to handle these async errors..</p>
        ","We use the amazing Sentry tool to track both our frontend and backend errors. It's pretty straighforward to setup Sentry for code that runs in the browser, and it captures all uncatched errors out of the box:
Raven.config('[our key]', {  
    collectWindowErrors: true,
    fetchContext: true,
    linesOfContext: 40
}).install();
Easy. But recentenly we switched from AngularJS to ReactJS as our framework of choice, and we use React Starter Kit as a starting point. When an error is thrown during the Reat lifecycle it looks something likes this:
Unhandled promise rejection TypeError: Cannot read property 'setState' of undefined  
Aj. This is an async error, and Sentry does not automatically log this error. It turns out they rely heavily on promises (using ES6 Promise and it looks like they do not really handle these errors nicely. I did some heavily googlin' on this subject, but I could not find a proper in-framework solution to catch these errors.
The only solution that works for us is to override the error method of console and call Sentry from there.
var originalConsoleError = console.error;  
console.error = function(message, error) {  
    Raven.captureException(error);
    originalConsoleError.apply(this, arguments)
}
I can't imagine this is really the way to go, but for now it works. I wonder if there is a better way to handle these async errors..",[Code]
96,Force Express to handle all requests over HTTPS,/force-express-to-handle-requests-over-https/,"
            <p>If your Express-powered Node app sends user information over the wire it's recommended to use a secure connection. Because of this you might want to force all your requests to run over <em>https</em>, even when someone explicitly opens your app using <em>http</em>. Luckily this is pretty easy using a Express <a href=""http://expressjs.com/4x/api.html#app.use"">middleware function</a>. </p>

<pre><code class=""language-prettyprint lang-javascript prettyprinted""><span class=""pln"">server</span><span class=""pun"">.</span><span class=""kwd"">use</span><span class=""pun"">(</span><span class=""str"">'/path'</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""kwd"">function</span><span class=""pun"">(</span><span class=""pln"">req</span><span class=""pun"">,</span><span class=""pln""> res</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""kwd"">next</span><span class=""pun"">)</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">  
  </span><span class=""kwd"">if</span><span class=""pun"">(!</span><span class=""pln"">req</span><span class=""pun"">.</span><span class=""pln"">secure</span><span class=""pun"">)</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">
    </span><span class=""kwd"">var</span><span class=""pln""> secureUrl </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""str"">""https://""</span><span class=""pln""> </span><span class=""pun"">+</span><span class=""pln""> req</span><span class=""pun"">.</span><span class=""pln"">headers</span><span class=""pun"">[</span><span class=""str"">'host'</span><span class=""pun"">]</span><span class=""pln""> </span><span class=""pun"">+</span><span class=""pln""> req</span><span class=""pun"">.</span><span class=""pln"">url</span><span class=""pun"">;</span><span class=""pln""> 
    res</span><span class=""pun"">.</span><span class=""pln"">writeHead</span><span class=""pun"">(</span><span class=""lit"">301</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln""> </span><span class=""str"">""Location""</span><span class=""pun"">:</span><span class=""pln"">  secureUrl </span><span class=""pun"">});</span><span class=""pln"">
    res</span><span class=""pun"">.</span><span class=""kwd"">end</span><span class=""pun"">();</span><span class=""pln"">
  </span><span class=""pun"">}</span><span class=""pln"">
  </span><span class=""kwd"">next</span><span class=""pun"">();</span><span class=""pln"">
</span><span class=""pun"">});</span></code></pre>

<p>If the current protocol is not <em>https</em>, the function stops the request-response cycle and returns a 301 response that redirects to the secure version of the current url. If the current url is secure, the function calls <code>next()</code> to pass control to the next middleware. Without this the app will be left hanging.</p>

<p>When you want to mount a middleware for every request, you can ommit the first argument:  </p>

<pre><code class=""language-prettyprint lang-javascript prettyprinted""><span class=""pln"">server</span><span class=""pun"">.</span><span class=""kwd"">use</span><span class=""pun"">(</span><span class=""kwd"">function</span><span class=""pun"">(</span><span class=""pln"">req</span><span class=""pun"">,</span><span class=""pln""> res</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""kwd"">next</span><span class=""pun"">)</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">  
 </span><span class=""pun"">...</span><span class=""pln"">
</span><span class=""pun"">});</span></code></pre>

<p>When working locally, your server may not have a SSL certificate installed. I think it's a good idea to add a environment conditional too. </p>

<pre><code class=""language-prettyprint lang-javascript prettyprinted""><span class=""kwd"">if</span><span class=""pun"">(!</span><span class=""pln"">req</span><span class=""pun"">.</span><span class=""pln"">secure </span><span class=""pun"">&amp;&amp;</span><span class=""pln""> process</span><span class=""pun"">.</span><span class=""pln"">env</span><span class=""pun"">.</span><span class=""pln"">NODE_ENV </span><span class=""pun"">===</span><span class=""pln""> </span><span class=""str"">'production'</span><span class=""pun"">)</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">  
</span><span class=""pun"">...</span><span class=""pln"">
</span><span class=""pun"">}</span></code></pre>

<p>Middleware functions are executed sequentially, therefore the order of middleware inclusion is important. Make sure your HTTPS middleware function is included before all the other middleware functions and routes. </p>

<p><strong>Update</strong>: If you use a DNS redirect, <code>req.secure</code> may not work correctly and cause an infinite redirect loop. You should use <code>req.headers['x-forwarded-proto'] !== 'https'</code> instead in this case.</p>
        ","If your Express-powered Node app sends user information over the wire it's recommended to use a secure connection. Because of this you might want to force all your requests to run over https, even when someone explicitly opens your app using http. Luckily this is pretty easy using a Express middleware function.
server.use('/path', function(req, res, next) {  
  if(!req.secure) {
    var secureUrl = ""https://"" + req.headers['host'] + req.url; 
    res.writeHead(301, { ""Location"":  secureUrl });
    res.end();
  }
  next();
});
If the current protocol is not https, the function stops the request-response cycle and returns a 301 response that redirects to the secure version of the current url. If the current url is secure, the function calls next() to pass control to the next middleware. Without this the app will be left hanging.
When you want to mount a middleware for every request, you can ommit the first argument:
server.use(function(req, res, next) {  
 ...
});
When working locally, your server may not have a SSL certificate installed. I think it's a good idea to add a environment conditional too.
if(!req.secure && process.env.NODE_ENV === 'production') {  
...
}
Middleware functions are executed sequentially, therefore the order of middleware inclusion is important. Make sure your HTTPS middleware function is included before all the other middleware functions and routes.
Update: If you use a DNS redirect, req.secure may not work correctly and cause an infinite redirect loop. You should use req.headers['x-forwarded-proto'] !== 'https' instead in this case.",[Code]
97,Migrating to Google Tag Manager without changing hard coded _gaq.push() events,/migrating-to-google-tag-manager-without-changing-hard-coded-_gaq-push-events/,"
            <p>Imagine the following situation: you want to migrate a website's tracking code form Google Analytics - <code>_gaq.push()</code> - to Universal Analytics - <code>ga('send')</code>. And while you're at it, you also want to implement Google Tag Manager to run the new code. After a quick scan of the website, you notice that it has a lot of hardcoded events. You start thinking about the development hours it would take to change all of them. After discussing it with the client, you'd wish there was another way.</p>

<p>There is. Here's how:</p>

<h3 id=""firinggtmeventswithgoogleanalytics_gaqpushtag"">Firing GTM events with Google Analytics' _gaq.push() tag</h3>

<p>If you want to migrate to Universal Analytics, you'll have to change the code. Basically all <code>_gaq.push()</code> function calls have to be changed to <code>ga('send')</code>. Some tags are easy to move to GTM. For example basic page tracking, you only have to change the tag in GTM from Classic tracking, to Universal tracking:</p>

<p><img src=""http://i58.tinypic.com/2mz7v5.png"" alt=""Google Analytics Tag in Google Tag Manager""></p>

<p>Once you've done this, you're all set as far as basic page tracking goes. If however, you have <code>_gaq.push()</code> tags left on your page, there's an issue. If one of these events or pageviews is triggered after this change, the developer console will show an error:</p>

<p><img src=""http://i61.tinypic.com/j62wdi.png"" alt=""_gaq.push() error""></p>

<p>The best way to fix this, is by changing all onsite event code to new GTM events. But if there are a lot of hardcoded events, this will take a lot of time. </p>

<p>So to make life easy for you, here's a fix that automatically captures the hardcoded <code>_gaq.push()</code> events and sends them to GTM:</p>

<pre><code class=""language- prettyprint lang-js prettyprinted""><span class=""com"">//check if _gaq is not available as it should be when traditional analytics is running</span><span class=""pln"">
</span><span class=""kwd"">if</span><span class=""pln""> </span><span class=""pun"">(</span><span class=""kwd"">typeof</span><span class=""pln""> _gaq </span><span class=""pun"">===</span><span class=""pln""> </span><span class=""str"">'undefined'</span><span class=""pln""> </span><span class=""pun"">||</span><span class=""pln""> </span><span class=""kwd"">typeof</span><span class=""pln""> _gaq </span><span class=""pun"">!==</span><span class=""pln""> </span><span class=""str"">'object'</span><span class=""pun"">)</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">  
    </span><span class=""com"">//define _gaq yourself</span><span class=""pln"">
    </span><span class=""kwd"">var</span><span class=""pln""> _gaq </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""pun"">{};</span><span class=""pln""> 
    </span><span class=""com"">//add the push() function to the object</span><span class=""pln"">
      _gaq</span><span class=""pun"">.</span><span class=""pln"">push </span><span class=""pun"">=</span><span class=""pln"">  </span><span class=""kwd"">function</span><span class=""pun"">(</span><span class=""pln"">array</span><span class=""pun"">){</span><span class=""pln"">
      </span><span class=""com"">//check the first item in the array</span><span class=""pln"">
      </span><span class=""com"">//check type of push</span><span class=""pln"">
      </span><span class=""kwd"">if</span><span class=""pun"">(</span><span class=""pln"">array</span><span class=""pun"">[</span><span class=""lit"">0</span><span class=""pun"">]</span><span class=""pln""> </span><span class=""pun"">===</span><span class=""pln""> </span><span class=""str"">'_trackPageview'</span><span class=""pun"">){</span><span class=""pln"">
        </span><span class=""com"">//fire GTM virtual pageview</span><span class=""pln"">
      </span><span class=""pun"">}</span><span class=""kwd"">else</span><span class=""pln""> </span><span class=""kwd"">if</span><span class=""pun"">(</span><span class=""pln"">array</span><span class=""pun"">[</span><span class=""lit"">0</span><span class=""pun"">]</span><span class=""pln""> </span><span class=""pun"">===</span><span class=""pln""> </span><span class=""str"">'_trackEvent'</span><span class=""pun"">){</span><span class=""pln"">
        </span><span class=""com"">//fire GTM event</span><span class=""pln"">
      </span><span class=""pun"">}</span><span class=""pln"">
    </span><span class=""pun"">}</span><span class=""pln"">
  </span><span class=""pun"">}</span></code></pre>

<p>This code basically checks if you're not running traditional Google Analytics. If so, it translates all the function calls to a GTM call. Keep in mind that the values that you have to pass into your GTM functions are part of an array, because the standard GA trackers used an array for input. </p>

<p>To clarify, here's a full exmaple with a sample pageview and event code for GTM:  </p>

<pre><code class=""language- prettyprint lang-js prettyprinted""><span class=""pun"">&lt;</span><span class=""pln"">script type</span><span class=""pun"">=</span><span class=""str"">""text/javascript""</span><span class=""pun"">&gt;</span><span class=""pln"">  
  </span><span class=""kwd"">if</span><span class=""pln""> </span><span class=""pun"">(</span><span class=""kwd"">typeof</span><span class=""pln""> _gaq </span><span class=""pun"">===</span><span class=""pln""> </span><span class=""str"">'undefined'</span><span class=""pln""> </span><span class=""pun"">||</span><span class=""pln""> </span><span class=""kwd"">typeof</span><span class=""pln""> _gaq </span><span class=""pun"">!==</span><span class=""pln""> </span><span class=""str"">'object'</span><span class=""pun"">)</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln""> 
    </span><span class=""kwd"">var</span><span class=""pln""> _gaq </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""pun"">{};</span><span class=""pln""> 
    _gaq</span><span class=""pun"">.</span><span class=""pln"">push </span><span class=""pun"">=</span><span class=""pln"">  </span><span class=""kwd"">function</span><span class=""pun"">(</span><span class=""pln"">array</span><span class=""pun"">){</span><span class=""pln"">
      </span><span class=""kwd"">if</span><span class=""pun"">(</span><span class=""pln"">array</span><span class=""pun"">[</span><span class=""lit"">0</span><span class=""pun"">]</span><span class=""pln""> </span><span class=""pun"">===</span><span class=""pln""> </span><span class=""str"">'_trackPageview'</span><span class=""pun"">){</span><span class=""pln"">
         dataLayer</span><span class=""pun"">.</span><span class=""pln"">push</span><span class=""pun"">({</span><span class=""pln"">
          </span><span class=""str"">'event'</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""str"">'pageview'</span><span class=""pun"">,</span><span class=""pln"">
          </span><span class=""str"">'pagePath'</span><span class=""pun"">:</span><span class=""pln""> array</span><span class=""pun"">[</span><span class=""lit"">1</span><span class=""pun"">]</span><span class=""pln""> 
        </span><span class=""pun"">});</span><span class=""pln"">
      </span><span class=""pun"">}</span><span class=""kwd"">else</span><span class=""pln""> </span><span class=""kwd"">if</span><span class=""pun"">(</span><span class=""pln"">array</span><span class=""pun"">[</span><span class=""lit"">0</span><span class=""pun"">]</span><span class=""pln""> </span><span class=""pun"">===</span><span class=""pln""> </span><span class=""str"">'_trackEvent'</span><span class=""pun"">){</span><span class=""pln"">
        dataLayer</span><span class=""pun"">.</span><span class=""pln"">push</span><span class=""pun"">({</span><span class=""pln"">
          </span><span class=""str"">'event'</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""str"">'event'</span><span class=""pun"">,</span><span class=""pln"">
          </span><span class=""str"">'category'</span><span class=""pun"">:</span><span class=""pln""> array</span><span class=""pun"">[</span><span class=""lit"">1</span><span class=""pun"">],</span><span class=""pln"">
          </span><span class=""str"">'action'</span><span class=""pun"">:</span><span class=""pln""> array</span><span class=""pun"">[</span><span class=""lit"">2</span><span class=""pun"">],</span><span class=""pln""> 
          </span><span class=""str"">'label'</span><span class=""pun"">:</span><span class=""pln""> array</span><span class=""pun"">[</span><span class=""lit"">3</span><span class=""pun"">],</span><span class=""pln""> 
          </span><span class=""str"">'value'</span><span class=""pun"">:</span><span class=""pln""> array</span><span class=""pun"">[</span><span class=""lit"">4</span><span class=""pun"">]</span><span class=""pln""> 
        </span><span class=""pun"">});</span><span class=""pln"">
      </span><span class=""pun"">}</span><span class=""pln"">
    </span><span class=""pun"">}</span><span class=""pln"">
  </span><span class=""pun"">}</span><span class=""pln"">
</span><span class=""pun"">&lt;/</span><span class=""pln"">script</span><span class=""pun"">&gt;</span><span class=""pln"">  </span></code></pre>

<p>Now all you have to do is add this snippit as a custom HTML tag on all pages via GTM, and you're all set. Don't forget to change the pageview and event GTM functions to the match the structure you've set up for GTM (with triggers and variables).  </p>

<p>In this post I've focused on Google Tag Manager, but the snippet works for other tag managers as well. If you can configure a custom HTML tag, and have a function available to send events and pageviews to the tag manager, you're able to use it.</p>
        ","Imagine the following situation: you want to migrate a website's tracking code form Google Analytics - _gaq.push() - to Universal Analytics - ga('send'). And while you're at it, you also want to implement Google Tag Manager to run the new code. After a quick scan of the website, you notice that it has a lot of hardcoded events. You start thinking about the development hours it would take to change all of them. After discussing it with the client, you'd wish there was another way.
There is. Here's how:
Firing GTM events with Google Analytics' _gaq.push() tag
If you want to migrate to Universal Analytics, you'll have to change the code. Basically all _gaq.push() function calls have to be changed to ga('send'). Some tags are easy to move to GTM. For example basic page tracking, you only have to change the tag in GTM from Classic tracking, to Universal tracking:
Once you've done this, you're all set as far as basic page tracking goes. If however, you have _gaq.push() tags left on your page, there's an issue. If one of these events or pageviews is triggered after this change, the developer console will show an error:
The best way to fix this, is by changing all onsite event code to new GTM events. But if there are a lot of hardcoded events, this will take a lot of time.
So to make life easy for you, here's a fix that automatically captures the hardcoded _gaq.push() events and sends them to GTM:
//check if _gaq is not available as it should be when traditional analytics is running
if (typeof _gaq === 'undefined' || typeof _gaq !== 'object') {  
    //define _gaq yourself
    var _gaq = {}; 
    //add the push() function to the object
      _gaq.push =  function(array){
      //check the first item in the array
      //check type of push
      if(array[0] === '_trackPageview'){
        //fire GTM virtual pageview
      }else if(array[0] === '_trackEvent'){
        //fire GTM event
      }
    }
  }
This code basically checks if you're not running traditional Google Analytics. If so, it translates all the function calls to a GTM call. Keep in mind that the values that you have to pass into your GTM functions are part of an array, because the standard GA trackers used an array for input.
To clarify, here's a full exmaple with a sample pageview and event code for GTM:
<script type=""text/javascript"">  
  if (typeof _gaq === 'undefined' || typeof _gaq !== 'object') { 
    var _gaq = {}; 
    _gaq.push =  function(array){
      if(array[0] === '_trackPageview'){
         dataLayer.push({
          'event': 'pageview',
          'pagePath': array[1] 
        });
      }else if(array[0] === '_trackEvent'){
        dataLayer.push({
          'event': 'event',
          'category': array[1],
          'action': array[2], 
          'label': array[3], 
          'value': array[4] 
        });
      }
    }
  }
</script>  
Now all you have to do is add this snippit as a custom HTML tag on all pages via GTM, and you're all set. Don't forget to change the pageview and event GTM functions to the match the structure you've set up for GTM (with triggers and variables).
In this post I've focused on Google Tag Manager, but the snippet works for other tag managers as well. If you can configure a custom HTML tag, and have a function available to send events and pageviews to the tag manager, you're able to use it.",[Analytics]
98,Creating a SHA-2 encrypted certificate signing request for Heroku,/create-a-sha-256-encrypted/,"
            <p>The other day we needed to purchase a SSL certificate for one of our Heroku apps to ensure all information is transmitted securely. Luckily there is <a href=""https://addons.heroku.com/ssl"">SSL Endpoint</a>, which is a paid add-on service to get your certificate up and running on Heroku. Also, they have a pretty nice <a href=""https://devcenter.heroku.com/articles/ssl-endpoint"">guide</a> on how to exactly purchase an certificate and install it on a specific Heroku app. So far so good.  </p>

<p>But when they explain how to create a CSR (certificate signing request), they use this command:</p>

<pre><code>openssl req -nodes -new -key server.key -out server.csr  
</code></pre>

<p>Ajj. This creates a CSR with the SHA-1 hash function. For us (and probably for you) this is not secure enough anymore. We want to use the newer and more secure SHA-2. And we set an explicit RSA key size of 4096 bits, since we don't want to depend on whatever our openssl command might have as a default. Setting it to 2048 bits is also <a href=""https://developers.google.com/web/fundamentals/discovery-and-distribution/security-with-https/generating-keys-and-csr?hl=en"">still considered secure nowadays</a>. But you definately don't want a 1024 bits RSA key anymore!</p>

<p>The following openssl command generates a new 4096-bit private key and a CSR with SHA-2 hash:  </p>

<pre><code>openssl req -new -newkey rsa:4096 -sha256 -nodes -keyout server.key -out server.csr  
</code></pre>

<p>When answering the signing questions, make sure the The Common Name field matches your secure domain and the Country Name is a two letter code, in ISO 3166-1 format, of the country in which your organization is based. You can now use your 4096-bit server.csr to request your certificate. </p>

<h4 id=""verifyyourcsr"">Verify your CSR</h4>

<p>If you want to read (decode) the contents of your CSR to confirm it's OK, you can use this command:</p>

<pre><code>openssl req -in server.csr -noout -text  
</code></pre>

<p>Once again, for a detailed instruction on how to use SSL on Heroku check out their <a href=""https://devcenter.heroku.com/articles/ssl-endpoint"">SSL Endpoint guide</a>.</p>
        ","The other day we needed to purchase a SSL certificate for one of our Heroku apps to ensure all information is transmitted securely. Luckily there is SSL Endpoint, which is a paid add-on service to get your certificate up and running on Heroku. Also, they have a pretty nice guide on how to exactly purchase an certificate and install it on a specific Heroku app. So far so good.
But when they explain how to create a CSR (certificate signing request), they use this command:
openssl req -nodes -new -key server.key -out server.csr  
Ajj. This creates a CSR with the SHA-1 hash function. For us (and probably for you) this is not secure enough anymore. We want to use the newer and more secure SHA-2. And we set an explicit RSA key size of 4096 bits, since we don't want to depend on whatever our openssl command might have as a default. Setting it to 2048 bits is also still considered secure nowadays. But you definately don't want a 1024 bits RSA key anymore!
The following openssl command generates a new 4096-bit private key and a CSR with SHA-2 hash:
openssl req -new -newkey rsa:4096 -sha256 -nodes -keyout server.key -out server.csr  
When answering the signing questions, make sure the The Common Name field matches your secure domain and the Country Name is a two letter code, in ISO 3166-1 format, of the country in which your organization is based. You can now use your 4096-bit server.csr to request your certificate.
Verify your CSR
If you want to read (decode) the contents of your CSR to confirm it's OK, you can use this command:
openssl req -in server.csr -noout -text  
Once again, for a detailed instruction on how to use SSL on Heroku check out their SSL Endpoint guide.","[Code, Heroku, SSL]"
99,Heap Analytics: an adhoc analysis dream come true,/heap-analytics-an-adhoc-analysis-dream-come-true/,"
            <p>Measurement has a key role in every project. It allows you to measure your KPI's and user behavior. Doing an adhoc analysis on the later can be hard. Sometimes you don't track every interaction a user has on your website. With most packages, if you didn't implement any tracking, your out of options. There is no data.</p>

<p>Luckily, there's a solution for this, and it's called <a href=""https://heapanalytics.com/"">Heap Analytics</a>.</p>

<h3 id=""instantretroactiveanalytics"">Instant, Retroactive Analytics</h3>

<p>After you place the Heap tag on your website, it captures every user action on your website (they also have support for iOS apps). Based on the data logs, you can run retroactive analysis. So consider this: you find out that people drop off on your product selection page, but you only have standard pageviews enabled in your analytics tool. Normally, our analysis stops here. But with heap, you can select any item on that page, set is an event, and all the events that happened since you've placed the Heap tag are available in your analysis. </p>

<h3 id=""theeventvisualizer"">The event visualizer</h3>

<p>The easiest way to add Heap events is with the event visualizer. If you're logged in to Heap, all you have to do is add <strong>?heap-event-visualizer=on</strong> to your URL and your good to go.</p>

<p><img src=""http://i58.tinypic.com/2qvs8q9.png"" alt=""event visualizer"">
<em>a sample event on generateiban.com</em></p>

<p>As you can see on the image above, you can click any element on the site, and set it as an event. When you hit 'Define Event', Heap adds it to the interface. While using the event visualizer, you have the option to switch between Definition mode, to set events, and Normal mode, to browse the website. They also have an API that allows you to add extra data to the Heap data set, for exmaple: set user id's and send e-commerce data to Heap. </p>

<h3 id=""theeventvisualizer"">The event visualizer</h3>

<p>When you log into Heap Analytics, you have the following reports available:</p>

<ul>
<li><strong>Events:</strong> this lists your custom events (set up with the event visualizer) and suggested events, based on URLs and frequent clicks. You edit both of them by name, make combinations, and add filters based on country, query parameters (like utm_source) and other meta data.
<img src=""http://i61.tinypic.com/33wxtmt.png"" alt=""event report"" class=""full-img""></li>
<li><strong>Segment:</strong> the place to set up and manage your segments. A Heap segment is a group of users that has completed a task or a set of tasks. For example: landed on the website from an newsletter and visited the basket page. Optionally, you can add a time dimension in the mix that filters the segment to actions that happened within a day, week or month.
<img src=""http://i62.tinypic.com/wjxax1.png"" alt=""segment report""></li>
<li><strong>Funnels:</strong> in my opinion, this is where Heap really shines. You can set up a funnel report based on your own events and you can add segments to your liking. A retroactive funnel report that supports segments, awesome don't you think? 
<img src=""http://i58.tinypic.com/28l7tdi.png"" alt=""funnel report""></li>
<li><strong>Retention:</strong> this report allows you to generate a retention table based a first event and a second event (the retention moment). A nice feature is the option to in- or exclude retention events that had the first event before the selected date range.
<img src=""http://i61.tinypic.com/245lhso.png"" alt=""retention report""></li>
<li><strong>Lists:</strong> generates a list of users and shows how many events they've had. A click on a user plots the events on a timeline. 
<img src=""http://i60.tinypic.com/xaulbt.png"" alt=""list report""></li>
</ul>

<h3 id=""useitforfreesortof"">Use it for free, sort of</h3>

<p>You can start using Heap for free, if you have less than 5.000 visits a month. After that, pricing starts at $99 per month (<a href=""https://heapanalytics.com/pricing"">more info over here</a>). If you're willing to promote the tool, you can add an image link to Heap on your page an up your monthly allowed visits to 25.000, for free. A nice option.</p>

<h3 id=""aclickanalysisdreamcometrue"">A click analysis dream come true</h3>

<p>In the end, Heap is great tool to do adhoc analysis. You install the tag, and don't have to worry about special event tags that should fire onclick. You just trust Heap, and get the data when you need it. This makes it a great tool for young startups that want quick insights without a big analytics implementation and a great way for conversion specialists that need to find out where you're users are dropping out.</p>
        ","Measurement has a key role in every project. It allows you to measure your KPI's and user behavior. Doing an adhoc analysis on the later can be hard. Sometimes you don't track every interaction a user has on your website. With most packages, if you didn't implement any tracking, your out of options. There is no data.
Luckily, there's a solution for this, and it's called Heap Analytics.
Instant, Retroactive Analytics
After you place the Heap tag on your website, it captures every user action on your website (they also have support for iOS apps). Based on the data logs, you can run retroactive analysis. So consider this: you find out that people drop off on your product selection page, but you only have standard pageviews enabled in your analytics tool. Normally, our analysis stops here. But with heap, you can select any item on that page, set is an event, and all the events that happened since you've placed the Heap tag are available in your analysis.
The event visualizer
The easiest way to add Heap events is with the event visualizer. If you're logged in to Heap, all you have to do is add ?heap-event-visualizer=on to your URL and your good to go.
a sample event on generateiban.com
As you can see on the image above, you can click any element on the site, and set it as an event. When you hit 'Define Event', Heap adds it to the interface. While using the event visualizer, you have the option to switch between Definition mode, to set events, and Normal mode, to browse the website. They also have an API that allows you to add extra data to the Heap data set, for exmaple: set user id's and send e-commerce data to Heap.
The event visualizer
When you log into Heap Analytics, you have the following reports available:
Events: this lists your custom events (set up with the event visualizer) and suggested events, based on URLs and frequent clicks. You edit both of them by name, make combinations, and add filters based on country, query parameters (like utm_source) and other meta data.
Segment: the place to set up and manage your segments. A Heap segment is a group of users that has completed a task or a set of tasks. For example: landed on the website from an newsletter and visited the basket page. Optionally, you can add a time dimension in the mix that filters the segment to actions that happened within a day, week or month.
Funnels: in my opinion, this is where Heap really shines. You can set up a funnel report based on your own events and you can add segments to your liking. A retroactive funnel report that supports segments, awesome don't you think?
Retention: this report allows you to generate a retention table based a first event and a second event (the retention moment). A nice feature is the option to in- or exclude retention events that had the first event before the selected date range.
Lists: generates a list of users and shows how many events they've had. A click on a user plots the events on a timeline.
Use it for free, sort of
You can start using Heap for free, if you have less than 5.000 visits a month. After that, pricing starts at $99 per month (more info over here). If you're willing to promote the tool, you can add an image link to Heap on your page an up your monthly allowed visits to 25.000, for free. A nice option.
A click analysis dream come true
In the end, Heap is great tool to do adhoc analysis. You install the tag, and don't have to worry about special event tags that should fire onclick. You just trust Heap, and get the data when you need it. This makes it a great tool for young startups that want quick insights without a big analytics implementation and a great way for conversion specialists that need to find out where you're users are dropping out.",[Analytics]
100,Getting started with Sequential Messaging: the 4 main stages,/getting-started-with-sequential-messaging-the-4-main-stages/,"
            <p>Not all the visitors coming to your website are ready to convert. Each user will pass through a process before they have the intention to purchase a certain product or service. At Blue Mango we call such process ""the customer journey"". </p>

<p>The journey is unique for every user and consumer. It can be more or less complex depending on the product, the personality of the user or the situation in which the user is for example.</p>

<p>From a marketing prospective is important to recognize that every user faces different situations through this journey towards conversion, and a standard message won't be as effective in different situations. <br>
That's why here at Blue Mango we are working on a way of making advertising smarter and more efficient based on where the users stand in their journey towards the conversion.</p>

<h2 id=""thefourmainstages"">The four main stages</h2>

<p>Inspired by <a href=""http://www.amazon.com/Conversion-Optimization-Converting-Prospects-Customers/dp/1449377564"">Khalid Saleh &amp; Aya Shukairy</a> (2011) we identified 4 main steps of this journey.</p>

<h3 id=""stage1earlystage"">Stage 1: Early stage</h3>

<p>The early phase user doesn't recognize that he is interested in buying something. He doesn't know exactly what he needs or wants yet. The need is recognized based on:</p>

<ul>
<li>an <em>Internal stimulus</em> when the stimulus is based on the customer's internal system. For example, when a consumer is hungry or sleepy, the consumer recognizes that he must do something to satisfy his need.</li>
<li>an <em>External stimulus</em> when the stimulus is based on external factors which trigger the customer to recognize a specific need or want. </li>
</ul>

<p>In this case the advertising (the external stimulus) is effective when it helps the user to recognize a need.</p>

<p>To reach this goal it is necessary that we <em>catch the user's attention</em> and <em>create his interest</em> with the content shown. In such a phase it can be effective to persuade on an irrational level, since most likely the user is in an unconscious state of mind, or as Kahneman would say, he is thinking fast. </p>

<h3 id=""stage2middlestage"">Stage 2: Middle stage</h3>

<p>The middle stage buyer is in a more complex situation. </p>

<p>He is aware of his need, and he is in a more rational state of mind, or as Kanehman would say he is thinking slow. So he knows what he wants to buy, but he is not sure from where or who he should purchase. He is carefully evaluating the alternatives he has in order to satisfy his need.</p>

<p>Life is hard for the Middle Stage buyer, and he is taking a lot of effort to take the ""right"" decision and have the best value for money. For users in this stage, a relevant advertising should answer the question: ""Why is your company/product best suited to meet my needs?""</p>

<p>That's why an effective advertising is helping the user to compare, providing the information needed. Then the content for an effective advertising is more rational and focus on information.</p>

<h3 id=""stage3latestage"">Stage 3: Late stage</h3>

<p>Only 10% to 20% of website visitors are in the purchase stage. Such visitors are highly motivated to complete the buying process. The Late stage buyer knows exactly what will satisfy his need. He has narrowed the field of competitors, he looked into detail for terms and conditions and looked into the reputation of your brand. </p>

<p>Since these visitors require the least amount of persuasion to convert, they should have the highest conversion rate. For an ecommerce website, this visitor will add item(s) to the shopping cart and go through the checkout process. However they might leave your website due to usability issues, lack of trust, increased levels of uncertainties or doubts. </p>

<p>Therefore, effective advertising for a user in the Late stage should be brief and straight to the point, without providing much information that the user has already, but mainly focusing on the ""call to action"".</p>

<h3 id=""stage4postpurchasestage"">Stage 4: Post-purchase stage</h3>

<p>The user in the post-purchase stage has already purchased a certain product or service, and now the customer evaluates the purchase. Although this stage of the buying process does not have impact directly on the conversion action that just took place, it does impact the possibility of the customer placing future orders on your website. The unhappy customer could also cancel or return the order, or spread bad word-of-mouth feedback. This is a very delicate phase, and it is important to pay close attention to this stage. <br>
Therefore, it is important to consider the converted users in you online advertising strategy.</p>

<p>The goal of this targeted advertising is to keep the satisfaction of your customer, for example by providing tips and tricks about his new product, information about your client service or ask for their feedback. <br>
A happy loyal customer is very valuable, that's why specific advertising for this target needs to be in place to show them how important they are for your business.</p>

<h2 id=""keepevaluatingandevolvingyourstrategy"">Keep evaluating and evolving your strategy</h2>

<p>Customers go through these steps in a sequential process, moving from one stage to the next until they reach the loyalty stage. Creating a loyal customer is the goal of conversion optimization; it is about keeping customers happy, engaged, and coming back to your website.</p>

<p>Since these 4 stages are so different in terms of needs and information that customers are looking for, it is very important that you keep evaluating and evolving your strategy. Although you provide different types of information for each buying stage, how this information is created and presented must appeal to the various personas.</p>
        ","Not all the visitors coming to your website are ready to convert. Each user will pass through a process before they have the intention to purchase a certain product or service. At Blue Mango we call such process ""the customer journey"".
The journey is unique for every user and consumer. It can be more or less complex depending on the product, the personality of the user or the situation in which the user is for example.
From a marketing prospective is important to recognize that every user faces different situations through this journey towards conversion, and a standard message won't be as effective in different situations.
That's why here at Blue Mango we are working on a way of making advertising smarter and more efficient based on where the users stand in their journey towards the conversion.
The four main stages
Inspired by Khalid Saleh & Aya Shukairy (2011) we identified 4 main steps of this journey.
Stage 1: Early stage
The early phase user doesn't recognize that he is interested in buying something. He doesn't know exactly what he needs or wants yet. The need is recognized based on:
an Internal stimulus when the stimulus is based on the customer's internal system. For example, when a consumer is hungry or sleepy, the consumer recognizes that he must do something to satisfy his need.
an External stimulus when the stimulus is based on external factors which trigger the customer to recognize a specific need or want.
In this case the advertising (the external stimulus) is effective when it helps the user to recognize a need.
To reach this goal it is necessary that we catch the user's attention and create his interest with the content shown. In such a phase it can be effective to persuade on an irrational level, since most likely the user is in an unconscious state of mind, or as Kahneman would say, he is thinking fast.
Stage 2: Middle stage
The middle stage buyer is in a more complex situation.
He is aware of his need, and he is in a more rational state of mind, or as Kanehman would say he is thinking slow. So he knows what he wants to buy, but he is not sure from where or who he should purchase. He is carefully evaluating the alternatives he has in order to satisfy his need.
Life is hard for the Middle Stage buyer, and he is taking a lot of effort to take the ""right"" decision and have the best value for money. For users in this stage, a relevant advertising should answer the question: ""Why is your company/product best suited to meet my needs?""
That's why an effective advertising is helping the user to compare, providing the information needed. Then the content for an effective advertising is more rational and focus on information.
Stage 3: Late stage
Only 10% to 20% of website visitors are in the purchase stage. Such visitors are highly motivated to complete the buying process. The Late stage buyer knows exactly what will satisfy his need. He has narrowed the field of competitors, he looked into detail for terms and conditions and looked into the reputation of your brand.
Since these visitors require the least amount of persuasion to convert, they should have the highest conversion rate. For an ecommerce website, this visitor will add item(s) to the shopping cart and go through the checkout process. However they might leave your website due to usability issues, lack of trust, increased levels of uncertainties or doubts.
Therefore, effective advertising for a user in the Late stage should be brief and straight to the point, without providing much information that the user has already, but mainly focusing on the ""call to action"".
Stage 4: Post-purchase stage
The user in the post-purchase stage has already purchased a certain product or service, and now the customer evaluates the purchase. Although this stage of the buying process does not have impact directly on the conversion action that just took place, it does impact the possibility of the customer placing future orders on your website. The unhappy customer could also cancel or return the order, or spread bad word-of-mouth feedback. This is a very delicate phase, and it is important to pay close attention to this stage.
Therefore, it is important to consider the converted users in you online advertising strategy.
The goal of this targeted advertising is to keep the satisfaction of your customer, for example by providing tips and tricks about his new product, information about your client service or ask for their feedback.
A happy loyal customer is very valuable, that's why specific advertising for this target needs to be in place to show them how important they are for your business.
Keep evaluating and evolving your strategy
Customers go through these steps in a sequential process, moving from one stage to the next until they reach the loyalty stage. Creating a loyal customer is the goal of conversion optimization; it is about keeping customers happy, engaged, and coming back to your website.
Since these 4 stages are so different in terms of needs and information that customers are looking for, it is very important that you keep evaluating and evolving your strategy. Although you provide different types of information for each buying stage, how this information is created and presented must appeal to the various personas.",[cro]
101,IE8 is no longer worth our client's money,/ie8-users-are-no-longer-worth-our-clients-money/,"
            <p>I have always been convinced we should stop blaming Internet Explorer and make sure everyone on the web can at least use the bare minimum of our products and see our ads using graceful degredation. But there are limits. The discussion whether one should support Internet Explorer 8 may be obsolete for most of you, but for us it was still a present-day topic. Until today.  </p>

<h4 id=""thenumbers"">The numbers</h4>

<p>The percentage of IE8 users has been pretty low for years now, and after Microsoft dropped the support for Windows XP, the number dropped even further. In June 2015, the number of people in The Netherlands using IE8 is only <strong><a href=""http://gs.statcounter.com/#desktop-browser_version_partially_combined-NL-monthly-201406-201506"">0.95%</a></strong>. We also have ads running in Eastern Europe, Russia and Kazachstan, and we see the same figures there. To me, that number is negligible. According to our own statistics (both RTB and reserved display) the actual percentage is even lower: <strong>0.82%</strong> (3.5 million of 426 million impressions). </p>

<h4 id=""whydowestillcareaboutie8"">Why do we still care about IE8?</h4>

<p>We are in the online marketing industry, and to our clients and to us as an agency, every conversion matters. And although 0.82% is a very small number, it is still an extra 0.82% chance for a click, a shopping basket placement and eventually an order.  </p>

<h4 id=""whythesenumberslie"">Why these numbers lie</h4>

<p>The chance that someone using IE8 sees one of our ads and clicks it, is actually far below 0.82%. We dived into our clients' analytics and what I discovered met my expectations. Not only do IE8 visitors have less interaction with our ads, it turns out that <strong>IE8 visitors convert over 44% less</strong> over visitors using other browsers*. </p>

<p>I think one of the main causes for this is that the websites of most of our clients are no longer working (properly) on the 7 year old browser. Another possible cause is that IE8 is mostly used in corporate environments, and people are less likely to buy something during their boss' time.</p>

<h4 id=""supportingie8isexpensive"">Supporting IE8 is expensive</h4>

<p>Although the media costs for serving ads to IE8 users is pretty low because of the small numbers, supporting IE8 is expensive. We can't use new technologies that enable richer and faster animation like Canvas or CSS3 animations, because IE8 does not support them. Unless, of course, we build fallbacks for IE8 and other legacy browsers. This is something we do everyday. We show image stills or simplified versions of our ads. In our case this takes up <em>over an avarage of 4% of the total time</em> designers and developers put in the proces. That's quite a lot, compared to the number of people that we put all the effort in for, right? I'd rather see the designers use this time to come up with and implement things that improve the performance on browsers that matter.</p>

<h4 id=""thedayhascome"">The day has come</h4>

<p>When we read those numbers it didn't make sense to keep supporting IE8. Therefore I'm glad to say that <strong>we stop developing for and testing on Internet Explorer 8 as of today</strong>. We will exclude IE8 from all our new media campaigns, so users that run IE8 will never, ever see the ads we develop.</p>

<p>Does this mean mean we can start using modern fancy things without keeping other legacy browsers in mind? No, unfortunately not. For example we still have to test on Android 2.3, because it is still pretty big. Graceful degradation stays key in everything we design and develop, so we can have the largest possible audience for our ads.</p>

<h4 id=""whatiftheclientusesie8themselves"">What if the client uses IE8 themselves?</h4>

<p>Every now and then I find myself in one more discussion about supporting IE8 and the <em>counterparty</em> comes up with this argument:</p>

<blockquote>
  <p>""[..] our clients are still using IE8 and cannot upgrade. So they can't preview their own creatives""</p>
</blockquote>

<p>So this person is suggesting we should put man-hours in making a creatives work in IE8 just for this? I think this is complete madness and the issue should be fixed at the client's side, not at the designer's side. The reason clients are still using IE8 is likely because they do not have the permissions to upgrade because of 'security reasons'. Microsoft revealed end of support for the browser, saying IE8 will <a href=""http://www.alphr.com/news/enterprise/390217/still-on-ie8-you-ve-got-18-months-to-upgrade"">no longer receive updates from 12 January 2016.</a>. How about that for security? The clock is ticking, your client should start updating. If you really can't get your client to update, you can suggest to preview your products using a service like <a href=""https://www.browserstack.com"">BrowserStack</a>, or their mobile phones.</p>

<p>I personally don't think our clients even care about what specific browsers or platforms their ads are visible on. The most important thing is that their ads convert. This is achieved by showing them to the right user, at the right moment, on the right device with the right message. It is our responsibility to serve our ads on the browsers that help achieve that.</p>

<h4 id=""awordofcaution"">A word of caution</h4>

<p>In our case it is no longer worth it to support IE8. But before you decide to also ditch IE8 (or any other browser), you should always check your analytics.</p>

<p>* We gathered this data from over 500.000 visits. The metric for a conversion was <em>shopping cart placement</em>. </p>
        ","I have always been convinced we should stop blaming Internet Explorer and make sure everyone on the web can at least use the bare minimum of our products and see our ads using graceful degredation. But there are limits. The discussion whether one should support Internet Explorer 8 may be obsolete for most of you, but for us it was still a present-day topic. Until today.
The numbers
The percentage of IE8 users has been pretty low for years now, and after Microsoft dropped the support for Windows XP, the number dropped even further. In June 2015, the number of people in The Netherlands using IE8 is only 0.95%. We also have ads running in Eastern Europe, Russia and Kazachstan, and we see the same figures there. To me, that number is negligible. According to our own statistics (both RTB and reserved display) the actual percentage is even lower: 0.82% (3.5 million of 426 million impressions).
Why do we still care about IE8?
We are in the online marketing industry, and to our clients and to us as an agency, every conversion matters. And although 0.82% is a very small number, it is still an extra 0.82% chance for a click, a shopping basket placement and eventually an order.
Why these numbers lie
The chance that someone using IE8 sees one of our ads and clicks it, is actually far below 0.82%. We dived into our clients' analytics and what I discovered met my expectations. Not only do IE8 visitors have less interaction with our ads, it turns out that IE8 visitors convert over 44% less over visitors using other browsers*.
I think one of the main causes for this is that the websites of most of our clients are no longer working (properly) on the 7 year old browser. Another possible cause is that IE8 is mostly used in corporate environments, and people are less likely to buy something during their boss' time.
Supporting IE8 is expensive
Although the media costs for serving ads to IE8 users is pretty low because of the small numbers, supporting IE8 is expensive. We can't use new technologies that enable richer and faster animation like Canvas or CSS3 animations, because IE8 does not support them. Unless, of course, we build fallbacks for IE8 and other legacy browsers. This is something we do everyday. We show image stills or simplified versions of our ads. In our case this takes up over an avarage of 4% of the total time designers and developers put in the proces. That's quite a lot, compared to the number of people that we put all the effort in for, right? I'd rather see the designers use this time to come up with and implement things that improve the performance on browsers that matter.
The day has come
When we read those numbers it didn't make sense to keep supporting IE8. Therefore I'm glad to say that we stop developing for and testing on Internet Explorer 8 as of today. We will exclude IE8 from all our new media campaigns, so users that run IE8 will never, ever see the ads we develop.
Does this mean mean we can start using modern fancy things without keeping other legacy browsers in mind? No, unfortunately not. For example we still have to test on Android 2.3, because it is still pretty big. Graceful degradation stays key in everything we design and develop, so we can have the largest possible audience for our ads.
What if the client uses IE8 themselves?
Every now and then I find myself in one more discussion about supporting IE8 and the counterparty comes up with this argument:
""[..] our clients are still using IE8 and cannot upgrade. So they can't preview their own creatives""
So this person is suggesting we should put man-hours in making a creatives work in IE8 just for this? I think this is complete madness and the issue should be fixed at the client's side, not at the designer's side. The reason clients are still using IE8 is likely because they do not have the permissions to upgrade because of 'security reasons'. Microsoft revealed end of support for the browser, saying IE8 will no longer receive updates from 12 January 2016.. How about that for security? The clock is ticking, your client should start updating. If you really can't get your client to update, you can suggest to preview your products using a service like BrowserStack, or their mobile phones.
I personally don't think our clients even care about what specific browsers or platforms their ads are visible on. The most important thing is that their ads convert. This is achieved by showing them to the right user, at the right moment, on the right device with the right message. It is our responsibility to serve our ads on the browsers that help achieve that.
A word of caution
In our case it is no longer worth it to support IE8. But before you decide to also ditch IE8 (or any other browser), you should always check your analytics.
* We gathered this data from over 500.000 visits. The metric for a conversion was shopping cart placement.",[opinion]
102,4 immediate improvements in our Scrum process,/4-improvements-in-scrum/,"
            <p>Attending a two day Scrum Master training by Jeff Sutherland &amp; Serge Beaumont , I was drawn a very thorough picture of the success of the Scrum way of working. They proved from international &amp; large longitudinal studies that teams doing Scrum were <em>hyper-productive, swarming around topics and were feeling awesome</em>. Who wouldn’t want that in their team? So back to the office and do better Scrum, now!</p>

<p>Of course, the question is: <br>
<strong>How to improve the Scrum in an existing team?</strong></p>

<p>I will share 4 insights and subjects in which we improved our way of working. If you want a short reminder about what Scrum is and it’s terminology, read this:&nbsp;<a href=""http://www.scrumguides.org/scrum-guide.html"">http://www.scrumguides.org/scrum-guide.html</a> first.</p>

<h4 id=""1estimatingatfirstnobodyknowswhatastorypointmeansanditsok"">1. Estimating : At first nobody knows what a storypoint means. And it's OK.</h4>

<p>So how do you start estimating your tickets? Nobody knows the value of a storypoint ( <em>SP</em> ) and/or the team’s velocity. I think many teams feel uncomfortable with not delivering by underestimating stuff ( we do, I know for sure). So how to improve it? </p>

<p>One of the essentials of Scrum is that you don’t look at what a team <em>thinks</em> it can do, but <em>measure</em> what it does. So starting out is simple: assign a value of 3 points to a simple, known task A. The the question for estimation will be : “<em>how much more work is this task compared to task A</em>”. After a few sprints, the teams knows pretty much how much points they can do in a sprint and it will be based on their actual capacity. I carefully say ""work"", because the work is something else as the time it takes to complete the work itself.</p>

<h4 id=""2whyitshardthinkinginstorypointsinsteadofhourswhenestimatingwork"">2. Why it’s hard thinking in Storypoints instead of hours when estimating work</h4>

<p>People tend to think about work and hours in a 1 to 1 relation. We work 40 hours a week eg. But the amount of work I get done in this time will be different then the work my direct colleague get's done. The difference in our skills will make me do a task slower then him, but the work is the same. The purpose of SP is to define the <em>amount of work</em>, so later on, the team can measure the <em>time</em> it will take them to complete the work as a team.</p>

<p>There is a nice comparison of SP with the height and width of an elephant : In general, a taller elephant may be heavier than a shorter one, but this is not always the case. There is no biological proof of a weight-versus-height formula, despite the common perception that more height means more weight. The same explanation applies to story points versus task hours: In general, a more complex user story (higher points) should take more hours to complete, but there are always exceptions.</p>

<p>In short: SP’s gives an indication of the <em>amount of work</em>, which does <strong>not</strong> say anything about <em>the time it takes</em> to do this work.  We still have to repeat this truth to each other now and then.</p>

<h4 id=""3protectyoursprintgoalswhoevertellsyouwhatever"">3. Protect your sprintgoals, whoever tells you whatever</h4>

<p>One of the reasons&nbsp;Scrum is successful is because it puts control in the hands of the people who will actually do the work. <br>
In Scrum the Stakeholders suggest topics/features and their value to the Productowner(PO), who will fill and prioritize the backlog. <br>
During planning, <em>the team decides</em> how many tickets they will take up from the backlog in the next sprint. Of course they help the PO to estimate the amount of work beforehand and do suggestions about which stories add most value. </p>

<p>Management is usually a stakeholder, and thus, in a classical way, not able to directly influence the work the team does. It makes the team ""own their work"" and thus commit to it. It also comes with to responsibility to protect the sprint and make it successfull.  The challenge here is to gently tell stakeholders &amp; management to go to the PO when they come in to reinforce importance on a not scheduled or unclear user-story...</p>

<h4 id=""4whataboutbugsandnewtickets"">4. What about bugs and new tickets?</h4>

<p>The most disturbing thing for a team is ‘tickets showing up’ during a sprint. So we agreed on only adding a ticket to a sprint if: </p>

<ul>
<li>It’s a blocker, a non functional product for our users. </li>
<li>All tickets from the sprint are done. </li>
</ul>

<p>This means saying ""<em>nope</em>"" again to all those lovely users, dropping in and asking ""could you do A and B for me?"". It’s a challenge since our primary users are in the same office as the team.</p>

<p>The simple rule: <strong>never work on something that's not on a ticket</strong>, does give some rest and guides to the team. So if it’s not in the first day of the sprint: it’s out.</p>

<h6 id=""butwhataboutbugs"">But what about Bugs?</h6>

<p>To deal with bugs &amp; blockers, we have a bug-buffer on our board, that has no subtasks at the beginning of the sprint but has some points assigned. It’s used to log the bugs on. So even when we're fixing bugs, we don't work without an already added a ticket with a value. </p>

<p>Because of this, we’re able to track time and follow our burndown properly. The big advantage is that if we’ve used our buffer, we can already update the PO that the scope of the sprint is probably being affected. Protecting our sprintgoals as a team.</p>

<p>That’s it for now. <br>
<em>What improvements or tips do you have for successfully improving Scrum?</em></p>
        ","Attending a two day Scrum Master training by Jeff Sutherland & Serge Beaumont , I was drawn a very thorough picture of the success of the Scrum way of working. They proved from international & large longitudinal studies that teams doing Scrum were hyper-productive, swarming around topics and were feeling awesome. Who wouldn’t want that in their team? So back to the office and do better Scrum, now!
Of course, the question is:
How to improve the Scrum in an existing team?
I will share 4 insights and subjects in which we improved our way of working. If you want a short reminder about what Scrum is and it’s terminology, read this: http://www.scrumguides.org/scrum-guide.html first.
1. Estimating : At first nobody knows what a storypoint means. And it's OK.
So how do you start estimating your tickets? Nobody knows the value of a storypoint ( SP ) and/or the team’s velocity. I think many teams feel uncomfortable with not delivering by underestimating stuff ( we do, I know for sure). So how to improve it?
One of the essentials of Scrum is that you don’t look at what a team thinks it can do, but measure what it does. So starting out is simple: assign a value of 3 points to a simple, known task A. The the question for estimation will be : “how much more work is this task compared to task A”. After a few sprints, the teams knows pretty much how much points they can do in a sprint and it will be based on their actual capacity. I carefully say ""work"", because the work is something else as the time it takes to complete the work itself.
2. Why it’s hard thinking in Storypoints instead of hours when estimating work
People tend to think about work and hours in a 1 to 1 relation. We work 40 hours a week eg. But the amount of work I get done in this time will be different then the work my direct colleague get's done. The difference in our skills will make me do a task slower then him, but the work is the same. The purpose of SP is to define the amount of work, so later on, the team can measure the time it will take them to complete the work as a team.
There is a nice comparison of SP with the height and width of an elephant : In general, a taller elephant may be heavier than a shorter one, but this is not always the case. There is no biological proof of a weight-versus-height formula, despite the common perception that more height means more weight. The same explanation applies to story points versus task hours: In general, a more complex user story (higher points) should take more hours to complete, but there are always exceptions.
In short: SP’s gives an indication of the amount of work, which does not say anything about the time it takes to do this work. We still have to repeat this truth to each other now and then.
3. Protect your sprintgoals, whoever tells you whatever
One of the reasons Scrum is successful is because it puts control in the hands of the people who will actually do the work.
In Scrum the Stakeholders suggest topics/features and their value to the Productowner(PO), who will fill and prioritize the backlog.
During planning, the team decides how many tickets they will take up from the backlog in the next sprint. Of course they help the PO to estimate the amount of work beforehand and do suggestions about which stories add most value.
Management is usually a stakeholder, and thus, in a classical way, not able to directly influence the work the team does. It makes the team ""own their work"" and thus commit to it. It also comes with to responsibility to protect the sprint and make it successfull. The challenge here is to gently tell stakeholders & management to go to the PO when they come in to reinforce importance on a not scheduled or unclear user-story...
4. What about bugs and new tickets?
The most disturbing thing for a team is ‘tickets showing up’ during a sprint. So we agreed on only adding a ticket to a sprint if:
It’s a blocker, a non functional product for our users.
All tickets from the sprint are done.
This means saying ""nope"" again to all those lovely users, dropping in and asking ""could you do A and B for me?"". It’s a challenge since our primary users are in the same office as the team.
The simple rule: never work on something that's not on a ticket, does give some rest and guides to the team. So if it’s not in the first day of the sprint: it’s out.
But what about Bugs?
To deal with bugs & blockers, we have a bug-buffer on our board, that has no subtasks at the beginning of the sprint but has some points assigned. It’s used to log the bugs on. So even when we're fixing bugs, we don't work without an already added a ticket with a value.
Because of this, we’re able to track time and follow our burndown properly. The big advantage is that if we’ve used our buffer, we can already update the PO that the scope of the sprint is probably being affected. Protecting our sprintgoals as a team.
That’s it for now.
What improvements or tips do you have for successfully improving Scrum?","[opinion, process, scrum]"
103,Setting up a cookie law compliant Google Analytics tracker,/setting-up-a-cookie-law-compliant-google-analytics-tracker/,"
            <p>The European Union has set up a cookie law that forces websites to inform their users about the cookies they set. The Dutch cookie law is even stricter, not allowing websites to set tracking cookies (e.g. for retargeting) without a cookie consent from the user. Google Analytics also has some features that should only be activated after a cookie consent, and features to protect the users that don't give one. </p>

<p>This post shows you how to modify the standard Google Analytics tracking code in a way that is compliant with the Dutch law. With this setup, your tracking code should be ready for upcoming cookie law changes in Europe.</p>

<h3 id=""thechanges"">The changes</h3>

<p>Google Analytics (GA) tracks a lot of information by default. And that's great for web analysts and online marketeers. But GA has some features that allow you to set up this code with the user in mind, changing the data it collects if a user doesn't give a cookie consent. The following two features are optional and protect the data that is collected about the user:</p>

<ul>
<li><strong>Anonymize IP:</strong> this changes the last number of the IP address to 0, anonymizing it; </li>
<li><strong>Force SSL:</strong> ensures all data is sent to Google via a secured connection;</li>
</ul>

<p>Other features require a cookie consent. In this post, we'll use these two features as an example:</p>

<ul>
<li><strong>DoubleClick display features:</strong> for collecting extra demographics data based on the user's DoubleClick data (only after cookie consent);</li>
<li><strong>Set user id:</strong> using a user specific identifier for cross device reporting.</li>
</ul>

<h3 id=""thecode"">The code</h3>

<p>To implement the changes correctly, you'll need access to the user's cookie opt-in choice in the code. In this example, we'll use a sample function <em>getUserConsentState()</em> that returns true (for consent given) or false (for no consent given). </p>

<p><strong>Default code</strong></p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""tag"">&lt;script&gt;</span><span class=""pln"">  
  </span><span class=""pun"">(</span><span class=""kwd"">function</span><span class=""pun"">(</span><span class=""pln"">i</span><span class=""pun"">,</span><span class=""pln"">s</span><span class=""pun"">,</span><span class=""pln"">o</span><span class=""pun"">,</span><span class=""pln"">g</span><span class=""pun"">,</span><span class=""pln"">r</span><span class=""pun"">,</span><span class=""pln"">a</span><span class=""pun"">,</span><span class=""pln"">m</span><span class=""pun"">){</span><span class=""pln"">i</span><span class=""pun"">[</span><span class=""str"">'GoogleAnalyticsObject'</span><span class=""pun"">]=</span><span class=""pln"">r</span><span class=""pun"">;</span><span class=""pln"">i</span><span class=""pun"">[</span><span class=""pln"">r</span><span class=""pun"">]=</span><span class=""pln"">i</span><span class=""pun"">[</span><span class=""pln"">r</span><span class=""pun"">]||</span><span class=""kwd"">function</span><span class=""pun"">(){</span><span class=""pln"">
  </span><span class=""pun"">(</span><span class=""pln"">i</span><span class=""pun"">[</span><span class=""pln"">r</span><span class=""pun"">].</span><span class=""pln"">q</span><span class=""pun"">=</span><span class=""pln"">i</span><span class=""pun"">[</span><span class=""pln"">r</span><span class=""pun"">].</span><span class=""pln"">q</span><span class=""pun"">||[]).</span><span class=""pln"">push</span><span class=""pun"">(</span><span class=""pln"">arguments</span><span class=""pun"">)},</span><span class=""pln"">i</span><span class=""pun"">[</span><span class=""pln"">r</span><span class=""pun"">].</span><span class=""pln"">l</span><span class=""pun"">=</span><span class=""lit"">1</span><span class=""pun"">*</span><span class=""kwd"">new</span><span class=""pln""> </span><span class=""typ"">Date</span><span class=""pun"">();</span><span class=""pln"">a</span><span class=""pun"">=</span><span class=""pln"">s</span><span class=""pun"">.</span><span class=""pln"">createElement</span><span class=""pun"">(</span><span class=""pln"">o</span><span class=""pun"">),</span><span class=""pln"">
  m</span><span class=""pun"">=</span><span class=""pln"">s</span><span class=""pun"">.</span><span class=""pln"">getElementsByTagName</span><span class=""pun"">(</span><span class=""pln"">o</span><span class=""pun"">)[</span><span class=""lit"">0</span><span class=""pun"">];</span><span class=""pln"">a</span><span class=""pun"">.</span><span class=""pln"">async</span><span class=""pun"">=</span><span class=""lit"">1</span><span class=""pun"">;</span><span class=""pln"">a</span><span class=""pun"">.</span><span class=""pln"">src</span><span class=""pun"">=</span><span class=""pln"">g</span><span class=""pun"">;</span><span class=""pln"">m</span><span class=""pun"">.</span><span class=""pln"">parentNode</span><span class=""pun"">.</span><span class=""pln"">insertBefore</span><span class=""pun"">(</span><span class=""pln"">a</span><span class=""pun"">,</span><span class=""pln"">m</span><span class=""pun"">)</span><span class=""pln"">
  </span><span class=""pun"">})(</span><span class=""pln"">window</span><span class=""pun"">,</span><span class=""pln"">document</span><span class=""pun"">,</span><span class=""str"">'script'</span><span class=""pun"">,</span><span class=""str"">'//www.google-analytics.com/analytics.js'</span><span class=""pun"">,</span><span class=""str"">'ga'</span><span class=""pun"">);</span><span class=""pln"">

  ga</span><span class=""pun"">(</span><span class=""str"">'create'</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""str"">'UA-123456-1'</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""str"">'auto'</span><span class=""pun"">);</span><span class=""pln"">
  ga</span><span class=""pun"">(</span><span class=""str"">'send'</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""str"">'pageview'</span><span class=""pun"">);</span><span class=""pln"">
</span><span class=""tag"">&lt;/script&gt;</span><span class=""pln"">  </span></code></pre>

<p><em>The starting point is the standard tracking code snippet that you'll get from GA, as shown in the example above.</em></p>

<p><strong>The updated default code</strong></p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""tag"">&lt;script&gt;</span><span class=""pln"">  
  </span><span class=""pun"">(</span><span class=""kwd"">function</span><span class=""pun"">(</span><span class=""pln"">i</span><span class=""pun"">,</span><span class=""pln"">s</span><span class=""pun"">,</span><span class=""pln"">o</span><span class=""pun"">,</span><span class=""pln"">g</span><span class=""pun"">,</span><span class=""pln"">r</span><span class=""pun"">,</span><span class=""pln"">a</span><span class=""pun"">,</span><span class=""pln"">m</span><span class=""pun"">){</span><span class=""pln"">i</span><span class=""pun"">[</span><span class=""str"">'GoogleAnalyticsObject'</span><span class=""pun"">]=</span><span class=""pln"">r</span><span class=""pun"">;</span><span class=""pln"">i</span><span class=""pun"">[</span><span class=""pln"">r</span><span class=""pun"">]=</span><span class=""pln"">i</span><span class=""pun"">[</span><span class=""pln"">r</span><span class=""pun"">]||</span><span class=""kwd"">function</span><span class=""pun"">(){</span><span class=""pln"">
  </span><span class=""pun"">(</span><span class=""pln"">i</span><span class=""pun"">[</span><span class=""pln"">r</span><span class=""pun"">].</span><span class=""pln"">q</span><span class=""pun"">=</span><span class=""pln"">i</span><span class=""pun"">[</span><span class=""pln"">r</span><span class=""pun"">].</span><span class=""pln"">q</span><span class=""pun"">||[]).</span><span class=""pln"">push</span><span class=""pun"">(</span><span class=""pln"">arguments</span><span class=""pun"">)},</span><span class=""pln"">i</span><span class=""pun"">[</span><span class=""pln"">r</span><span class=""pun"">].</span><span class=""pln"">l</span><span class=""pun"">=</span><span class=""lit"">1</span><span class=""pun"">*</span><span class=""kwd"">new</span><span class=""pln""> </span><span class=""typ"">Date</span><span class=""pun"">();</span><span class=""pln"">a</span><span class=""pun"">=</span><span class=""pln"">s</span><span class=""pun"">.</span><span class=""pln"">createElement</span><span class=""pun"">(</span><span class=""pln"">o</span><span class=""pun"">),</span><span class=""pln"">
  m</span><span class=""pun"">=</span><span class=""pln"">s</span><span class=""pun"">.</span><span class=""pln"">getElementsByTagName</span><span class=""pun"">(</span><span class=""pln"">o</span><span class=""pun"">)[</span><span class=""lit"">0</span><span class=""pun"">];</span><span class=""pln"">a</span><span class=""pun"">.</span><span class=""pln"">async</span><span class=""pun"">=</span><span class=""lit"">1</span><span class=""pun"">;</span><span class=""pln"">a</span><span class=""pun"">.</span><span class=""pln"">src</span><span class=""pun"">=</span><span class=""pln"">g</span><span class=""pun"">;</span><span class=""pln"">m</span><span class=""pun"">.</span><span class=""pln"">parentNode</span><span class=""pun"">.</span><span class=""pln"">insertBefore</span><span class=""pun"">(</span><span class=""pln"">a</span><span class=""pun"">,</span><span class=""pln"">m</span><span class=""pun"">)</span><span class=""pln"">
  </span><span class=""pun"">})(</span><span class=""pln"">window</span><span class=""pun"">,</span><span class=""pln"">document</span><span class=""pun"">,</span><span class=""str"">'script'</span><span class=""pun"">,</span><span class=""str"">'//www.google-analytics.com/analytics.js'</span><span class=""pun"">,</span><span class=""str"">'ga'</span><span class=""pun"">);</span><span class=""pln"">

  ga</span><span class=""pun"">(</span><span class=""str"">'create'</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""str"">'UA-123456-1'</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""str"">'auto'</span><span class=""pun"">);</span><span class=""pln"">
  ga</span><span class=""pun"">(</span><span class=""str"">'set'</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""str"">'forceSSL'</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""kwd"">true</span><span class=""pun"">);</span><span class=""pln"">
  </span><span class=""kwd"">if</span><span class=""pun"">(</span><span class=""pln"">getUserConsentState</span><span class=""pun"">()</span><span class=""pln""> </span><span class=""pun"">===</span><span class=""pln""> </span><span class=""kwd"">true</span><span class=""pun"">){</span><span class=""pln"">
    ga</span><span class=""pun"">(</span><span class=""str"">'require'</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""str"">'displayfeatures'</span><span class=""pun"">);</span><span class=""pln"">
    ga</span><span class=""pun"">(</span><span class=""str"">'set'</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""str"">'anonymizeIp'</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""kwd"">undefined</span><span class=""pun"">);</span><span class=""pln"">
    ga</span><span class=""pun"">(</span><span class=""str"">'set'</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""str"">'&amp;uid'</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""str"">'1234567'</span><span class=""pun"">);</span><span class=""pln"">
  </span><span class=""pun"">}</span><span class=""pln""> </span><span class=""kwd"">else</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">
    ga</span><span class=""pun"">(</span><span class=""str"">'set'</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""str"">'displayFeaturesTask'</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""kwd"">null</span><span class=""pun"">);</span><span class=""pln"">
    ga</span><span class=""pun"">(</span><span class=""str"">'set'</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""str"">'anonymizeIp'</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""kwd"">true</span><span class=""pun"">);</span><span class=""pln"">
  </span><span class=""pun"">}</span><span class=""pln"">
  ga</span><span class=""pun"">(</span><span class=""str"">'send'</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""str"">'pageview'</span><span class=""pun"">);</span><span class=""pln"">
</span><span class=""tag"">&lt;/script&gt;</span><span class=""pln"">  </span></code></pre>

<p><em>The updated example above shows the standard tracking code snippet, modified to comply with the cookie law. Keep in mind that both <strong>display features and setting the '&amp;uid' parameter are optional</strong>, but if you use them they do require a cookie consent. Note that the line <code>ga('set', 'displayFeaturesTask', null);</code> explicitly turns off the display features when the there's no consent.</em></p>

<h3 id=""otheractions"">Other actions</h3>

<p>This post only shows you how to set up the tracking code correctly. Besides this, you'll need to make sure the settings in GA's admin area are also compliant:</p>

<ul>
<li>Agree with Data Processing Amendment;</li>
<li>Disable data sharing settings;</li>
</ul>

<p>With the code ready, and these settings checked, your GA implementation will be compliant.</p>

<p><strong><em>Update - 16 November 2016</em></strong><em>: it turns out you can only disable anonymize ip by setting it to undefined. Setting it to false or not sending it when it was active before won't work . The code snippet is updated accordingly.</em></p>
        ","The European Union has set up a cookie law that forces websites to inform their users about the cookies they set. The Dutch cookie law is even stricter, not allowing websites to set tracking cookies (e.g. for retargeting) without a cookie consent from the user. Google Analytics also has some features that should only be activated after a cookie consent, and features to protect the users that don't give one.
This post shows you how to modify the standard Google Analytics tracking code in a way that is compliant with the Dutch law. With this setup, your tracking code should be ready for upcoming cookie law changes in Europe.
The changes
Google Analytics (GA) tracks a lot of information by default. And that's great for web analysts and online marketeers. But GA has some features that allow you to set up this code with the user in mind, changing the data it collects if a user doesn't give a cookie consent. The following two features are optional and protect the data that is collected about the user:
Anonymize IP: this changes the last number of the IP address to 0, anonymizing it;
Force SSL: ensures all data is sent to Google via a secured connection;
Other features require a cookie consent. In this post, we'll use these two features as an example:
DoubleClick display features: for collecting extra demographics data based on the user's DoubleClick data (only after cookie consent);
Set user id: using a user specific identifier for cross device reporting.
The code
To implement the changes correctly, you'll need access to the user's cookie opt-in choice in the code. In this example, we'll use a sample function getUserConsentState() that returns true (for consent given) or false (for no consent given).
Default code
<script>  
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-123456-1', 'auto');
  ga('send', 'pageview');
</script>  
The starting point is the standard tracking code snippet that you'll get from GA, as shown in the example above.
The updated default code
<script>  
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-123456-1', 'auto');
  ga('set', 'forceSSL', true);
  if(getUserConsentState() === true){
    ga('require', 'displayfeatures');
    ga('set', 'anonymizeIp', undefined);
    ga('set', '&uid', '1234567');
  } else {
    ga('set', 'displayFeaturesTask', null);
    ga('set', 'anonymizeIp', true);
  }
  ga('send', 'pageview');
</script>  
The updated example above shows the standard tracking code snippet, modified to comply with the cookie law. Keep in mind that both display features and setting the '&uid' parameter are optional, but if you use them they do require a cookie consent. Note that the line ga('set', 'displayFeaturesTask', null); explicitly turns off the display features when the there's no consent.
Other actions
This post only shows you how to set up the tracking code correctly. Besides this, you'll need to make sure the settings in GA's admin area are also compliant:
Agree with Data Processing Amendment;
Disable data sharing settings;
With the code ready, and these settings checked, your GA implementation will be compliant.
Update - 16 November 2016: it turns out you can only disable anonymize ip by setting it to undefined. Setting it to false or not sending it when it was active before won't work . The code snippet is updated accordingly.",[Analytics]
104,How npm 3 solves WebStorm's performance issues,/how-npm3-will-saves-webstorms-life/,"
            <p>WebStorm is our IDE of choice and we love it. It's brilliant. This blog's title might be a bit of a hyperbole, but WebStorm has a serious issue. There is one big fuckup and that is the way WebStorm indexes stuff. With version 10 the <a href=""http://geek.bluemangointeractive.com/webstorm-10-improves-the-performance-of-indexing-files/"">indexing process improved already</a>, but it is still far from perfect. Once in a while I switch to Sublime just because of Webstorm's performance issue. That's not good.</p>

<p>When you are using a lot of Node dependencies in your project, WebStorm tend to get <del>a bit</del> very slow. We can't actually blame WebStorm for it, it is NPM's fault. Of course we can change WebStorm's settings and disable indexing of the /node_modules folder, but to me that's crazy. We want indexing and autocompletion on those files!</p>

<h6 id=""sowhatseemstobetheproblem"">So, what seems to be the problem?</h6>

<p>Let's say we have already installed jQuery. Our Node modules folder looks like this:</p>

<pre><code>/jquery
</code></pre>

<p>Fine. Next we decide you also need Bootstrap. One of Bootstrap's dependencies is jQuery. After installing Bootstrap our modules folder looks something like this:</p>

<pre><code>/jquery
/bootstrap/...
/bootstrap/node_modules/jquery
</code></pre>

<h6 id=""wtfdoubledependencies"">Wtf? Double dependencies?</h6>

<p>Yeah. And this is a problem. Installing Bootstrap only gives us only one doubled dependency, but what if we use 10 modules that all use jQuery? Exactly. This sucks. Of course, the tools that handle our build process will make sure we do not load jQuery 10 times, but during development things will slow down. </p>

<h6 id=""npm3totherescue"">NPM 3 to the rescue</h6>

<p>The <a href=""http://blog.npmjs.org/post/122450408965/npm-weekly-20-npm-3-is-here-ish"">beta of npm 3</a> has been released, and I'm running it right now. Let's reproduce our prevous example and install jQuery:</p>

<pre><code>npm install jquery  
</code></pre>

<p>Bam. Our folder node_modules folder looks like this:  </p>

<pre><code>/jquery
</code></pre>

<p>Now let's install Bootstrap using <code>npm install bootstrap</code>. Now our node_modules folder looks like this:  </p>

<pre><code>/jquery
/bootstrap/...
</code></pre>

<p><strong>Wow!</strong> In node_modules/bootstrap there is no longer a reference to jQuery because we already one of our previously installed components already needed it or we've already installed it manually. And it's obvious that WebStorm only needs to index jQuery once, which will increase its performancy incredibility. Yeah!</p>

<h6 id=""tryit"">Try it!</h6>

<p>If you want to try it, use npm install <code>-g npm@3.0-latest</code> but be careful: not only is it a beta, but you can also break your npm installation if the global install fails.</p>
        ","WebStorm is our IDE of choice and we love it. It's brilliant. This blog's title might be a bit of a hyperbole, but WebStorm has a serious issue. There is one big fuckup and that is the way WebStorm indexes stuff. With version 10 the indexing process improved already, but it is still far from perfect. Once in a while I switch to Sublime just because of Webstorm's performance issue. That's not good.
When you are using a lot of Node dependencies in your project, WebStorm tend to get a bit very slow. We can't actually blame WebStorm for it, it is NPM's fault. Of course we can change WebStorm's settings and disable indexing of the /node_modules folder, but to me that's crazy. We want indexing and autocompletion on those files!
So, what seems to be the problem?
Let's say we have already installed jQuery. Our Node modules folder looks like this:
/jquery
Fine. Next we decide you also need Bootstrap. One of Bootstrap's dependencies is jQuery. After installing Bootstrap our modules folder looks something like this:
/jquery
/bootstrap/...
/bootstrap/node_modules/jquery
Wtf? Double dependencies?
Yeah. And this is a problem. Installing Bootstrap only gives us only one doubled dependency, but what if we use 10 modules that all use jQuery? Exactly. This sucks. Of course, the tools that handle our build process will make sure we do not load jQuery 10 times, but during development things will slow down.
NPM 3 to the rescue
The beta of npm 3 has been released, and I'm running it right now. Let's reproduce our prevous example and install jQuery:
npm install jquery  
Bam. Our folder node_modules folder looks like this:
/jquery
Now let's install Bootstrap using npm install bootstrap. Now our node_modules folder looks like this:
/jquery
/bootstrap/...
Wow! In node_modules/bootstrap there is no longer a reference to jQuery because we already one of our previously installed components already needed it or we've already installed it manually. And it's obvious that WebStorm only needs to index jQuery once, which will increase its performancy incredibility. Yeah!
Try it!
If you want to try it, use npm install -g npm@3.0-latest but be careful: not only is it a beta, but you can also break your npm installation if the global install fails.",[Code]
105,3 conversion rate optimisation trends for 2015,/3-conversion-rate-optimisation-trends-for-2015/,"
            <p>At Blue Mango Interactive we are constantly focusing on optimizing the campaign of our clients, in particular Conversion Rate Optimization (CRO) department is working on content and UX optimization. The CRO department is focusing on increasing the conversion rate of brand's media, their website and several touch points through the whole customer journey. Conversion rate optimization is focused on increasing the percentage of visitors that  ""convert"" into buyers for a marketing campaign.</p>

<p>Based on qualitative and quantitative pre-research (analytics, heatmaps, surveys, interviews, usability researach, insights from neuroscience and social psychology) we conduct experiments providing marking insight and optimized performance to our clients.</p>

<p>As conversion optimization is in the DNA of Bluemango, in particular we are focusing on the following trends:</p>

<h5 id=""trend1crossdevicetesting"">Trend 1: Cross-device testing</h5>

<p>Due to the increase of the devices used mobile traffic has increase. However we measured that desktop conversion is increased, which suggests that orientation phase is shifted to mobile and conversion increase on desktop devices. That's why is necessary to consider different devices to increase online performance in advertising and onsite. Nowadays there are tools which made it possible to test across devices.</p>

<h5 id=""trend2personalization"">Trend 2: Personalization</h5>

<p>Dynamic advertising: Research showed that personalized ads convert 10 time better than general ads, that's why at Blue Mango we developed LemonPI, a dynamic advertising solution which allow us to adapt advertising to customer preference.</p>

<h5 id=""trend3cromovestobeinganessentialongoingpractice"">Trend 3: CRO Moves to Being an Essential Ongoing Practice</h5>

<p>In 2014, there’s was a tipping point that moved CRO to being a mandatory practice for all high-traffic websites and this trend will only be magnified in 2015. Most brands now understand that optimizing their website and user experiences are just as important as optimizing their advertising.</p>

<p>Of course there is a lot more going on in the world of CRO this year and there will be in years yet to come, so start <a href=""https://twitter.com/bluemango_geek"">following</a> us and we will keep you updated.</p>
        ","At Blue Mango Interactive we are constantly focusing on optimizing the campaign of our clients, in particular Conversion Rate Optimization (CRO) department is working on content and UX optimization. The CRO department is focusing on increasing the conversion rate of brand's media, their website and several touch points through the whole customer journey. Conversion rate optimization is focused on increasing the percentage of visitors that ""convert"" into buyers for a marketing campaign.
Based on qualitative and quantitative pre-research (analytics, heatmaps, surveys, interviews, usability researach, insights from neuroscience and social psychology) we conduct experiments providing marking insight and optimized performance to our clients.
As conversion optimization is in the DNA of Bluemango, in particular we are focusing on the following trends:
Trend 1: Cross-device testing
Due to the increase of the devices used mobile traffic has increase. However we measured that desktop conversion is increased, which suggests that orientation phase is shifted to mobile and conversion increase on desktop devices. That's why is necessary to consider different devices to increase online performance in advertising and onsite. Nowadays there are tools which made it possible to test across devices.
Trend 2: Personalization
Dynamic advertising: Research showed that personalized ads convert 10 time better than general ads, that's why at Blue Mango we developed LemonPI, a dynamic advertising solution which allow us to adapt advertising to customer preference.
Trend 3: CRO Moves to Being an Essential Ongoing Practice
In 2014, there’s was a tipping point that moved CRO to being a mandatory practice for all high-traffic websites and this trend will only be magnified in 2015. Most brands now understand that optimizing their website and user experiences are just as important as optimizing their advertising.
Of course there is a lot more going on in the world of CRO this year and there will be in years yet to come, so start following us and we will keep you updated.","[cro, Analytics, trends]"
106,5 key takeaways from the Google Analytics User Conference in Amsterdam,/5-takeaways-from-the-google-analytics-user-conference-in-amsterdam/,"
            <p>Last Wednesday I attended the conference day of the ninth Google Analytics User Conference (GAUC) in Amsterdam. It's the biggest Google Analytics event in Europe. There were a lot of interesting speakers. This post contains my five key takeaways from the conference. For more details about the speakers, visit <a href=""www.gauc.nl"">http://www.gauc.nl/programma-conference-day/</a>.</p>

<h3 id=""1dimensionlabeling"">1: Dimension labeling</h3>

<p>With Google's Universal Analytics, Google gave us the option to add custom data (custom dimensions &amp; metrics) to the standard analytics data. As a web analyst, you are responsible for the data people act on. You are also the one that should make this as easy as possible for them. The example given at the GAUC was about a blog. Say you want to see if a read article is old. You'll probably do this by adding a custom dimension with a publication date. But you shouldn't stop there:</p>

<ul>
<li><strong>Publication date</strong>: a great way to see when a article was published. But you'll need to calculate the actual days an article is old. </li>
<li><strong>Amount of days an article is live</strong>: you instantly see how many days an article is old, but you don't really know if this is a new, old or even archived one.</li>
<li><strong>New, old or archived</strong>: your data instantly shows if read articles are new, old or archived. No thinking required.</li>
</ul>

<p>The examples above show that you should always think about what the data is for, and how to make it useful in the easiest way. Keep that in mind when adding data to any report.</p>

<h3 id=""2setyourkpiscarefully"">2: Set your KPIs carefully</h3>

<p>KPIs add great value to any data set. In Google Analytics, you can set goals to track important events and pageviews, or use e-commerce data to analyse your website's performance. When setting up your KPIs, make sure that they have a quantitative and qualitative part. </p>

<p><strong>A bad KPI example</strong></p>

<p>Conversion rate (CR) is not really useable as a KPI. For example: your website's CR increased from 10% to 15%. A nice increase of 50%. In the same period, website traffic is down with 50%. What does this give us?</p>

<ul>
<li><strong>Period 1</strong>: 2000 visits, 10% CR, 200 conversions</li>
<li><strong>Period 2</strong>: 1000 visits, 15% CR, 150 conversions</li>
</ul>

<p>The image is clear: a better CR does not result in more conversions.</p>

<p><strong>A good KPI example</strong></p>

<p>Conversions are great for KPIs. They have a quantitative and a qualitative part. Both are hidden in the bad example above:</p>

<ul>
<li><strong>Visits (quantitative)</strong>: how large was the volume of visits where conversions could have happened?</li>
<li><strong>Conversion Rate (qualitative)</strong>: how well did these visits perform?</li>
</ul>

<p>In the same two periods as before, you would have seen that conversions were down. Looking deeper into the data (the quantitative and qualitative part), the drop in visits and better CR would have appeared. Less conversions, better performance of your website. Interesting if you'd ask me.</p>

<h3 id=""3diyapis"">3: DIY APIs</h3>

<p>One talk was about adding API data to analytics data. A frequently used example is weather data. Add this information to see how sunny or cloudy weather impacts your website's visitors. But the most awesome thing was the mention of <a href=""https://www.kimonolabs.com/"">Kimonolabs</a> (or the alternative <a href=""https://import.io/"">import.io</a>). This tool allows you to turn any website into a data feed. Their video does a great job of explaining how it works: <br>
<div class=""fluid-width-video-wrapper"" style=""padding-top: 56.2%;""><iframe src=""https://player.vimeo.com/video/82849382?title=0&amp;byline=0&amp;portrait=0"" frameborder=""0"" webkitallowfullscreen="""" mozallowfullscreen="""" allowfullscreen="""" id=""fitvid888910""></iframe></div> </p><p><a href=""https://vimeo.com/82849382"">kimono: a 60 second introduction</a> from <a href=""https://vimeo.com/user23724399"">Kimono Labs</a> on <a href=""https://vimeo.com"">Vimeo</a>.</p><p></p>

<p>Awesome right?</p>

<h3 id=""4measuringjobinterviewswithuasmeasurementprotocol"">4: Measuring job interviews with UA's Measurement Protocol</h3>

<p><a href=""https://developers.google.com/analytics/devguides/collection/protocol/v1/devguide"">The Measurement Protocol</a> may be the most powerful feature of Universal Analytics. It basically allows you to setup a URL to send data to Google Analytics. This way, any system that can send GET or POST requests, can also send data to  Google Analytics. A great example was given during the GAUC. It was about a vacancy website. </p>

<p>Normally, you'd  optimize your data by website behaviour, for example job applications. This is nice and all, but the real question is: do these people get invited for a job interview? You can measure this with the Measurement Protocol:</p>

<ul>
<li><strong>Step 1:</strong> Google Analytics is loaded on a webpage and generates a client id.</li>
<li><strong>Step 2:</strong> The user sends an application by filling out a form.</li>
<li><strong>Step 3:</strong> Add a hidden input field to the form and store the Google Analytics client id (step 1) in it. You can read how to het the client id in <a href=""https://developers.google.com/analytics/devguides/collection/analyticsjs/domains"">the google analytics guide</a>.</li>
<li><strong>Step 4:</strong> Store the client id with the job application on the server.</li>
<li><strong>Step 5:</strong> As soon as an agent changes the state of an application to 'invited for job interview', send an event to Google Analytics via the Measurement Protocol and use the client id of the application (step 4) as the client id in this request.</li>
<li><strong>Step 6:</strong> Because of the <a href=""https://support.google.com/analytics/answer/1665189?hl=en"">last non-direct attribution model</a>, and the same client id, the <em>invited for job interview</em> event will be attributed to the source the user had when applying for the job. Keep in mind that if a user had a new source between these two actions (the application and the <em>invited for job interview</em> event) the newer source will be attributed.</li>
</ul>

<p>Looking at a sample data set:</p>

<ul>
<li><strong>Adwords traffic:</strong> 10.000 visits, 200 applications, 25 interviews</li>
<li><strong>LinkedIn traffic:</strong> 10.000 visits, 100 applications, 50 interviews</li>
</ul>

<p>Without the Measurement Protocol solution in place, you'd think Adwords had the best performing ads. With the new insights, LinkedIn is the better one. A great use of the possibilities of Universal Analytics. </p>

<p>It really got the analytics geek inside of me thinking. Potentially, you could use this to setup your own measurement system for apps (Google has a different way of tracking for apps and websites) to send the app behavior data to analytics just as website data. Make the app's data mimic the website data, merging the data, and the insights, into one Google Analytics property. </p>

<h3 id=""5enhancedecommerceforblogs"">5: Enhanced e-commerce for blogs</h3>

<p>Yes, enhanced e-commerce reporting for a non e-commerce website. It's possible. A recurring theme throughout several talks was creativity. Be creative when implementing analytics into any site. This is how <a href=""https://twitter.com/SimoAhava"">Simo Ahava</a> came up with the <em>crazy</em> idea of using Universal Analytics (awesome) enhanced e-commerce report features for his own blog. He basically looked at the available product fields and used them to measure his blogs performance:</p>

<ul>
<li><strong>Product:</strong> a blog article.</li>
<li><strong>Product view:</strong> a view of an article title.</li>
<li><strong>Product add to cart:</strong> a minor scroll on an article page.</li>
<li><strong>Product checkout:</strong> 
<ul><li><strong>Step 1:</strong> a 25% read.</li>
<li><strong>Step 2:</strong> a 50% read.</li>
<li><strong>Step 3:</strong> a 75% read.</li></ul></li>
<li><strong>Transaction:</strong> a 100% read and time on article of 60 seconds.</li>
<li><strong>Revenue:</strong> word count.</li>
</ul>

<p>A great and very creative use of Universal Analytics. You can read the details on <a href=""http://www.simoahava.com/analytics/track-content-enhanced-ecommerce/"">his blog post</a>. And yes, we'll be implementing this on GEEK.</p>

<p>These were my 5 takeaways from the Google Analytics User Conference in Amsterdam. There was a lot more awesome stuff. It was an inspiring day, and I hope you're inspired as well now you've read this article. If you have any questions or suggestions, don't be afraid to ask.</p>
        ","Last Wednesday I attended the conference day of the ninth Google Analytics User Conference (GAUC) in Amsterdam. It's the biggest Google Analytics event in Europe. There were a lot of interesting speakers. This post contains my five key takeaways from the conference. For more details about the speakers, visit http://www.gauc.nl/programma-conference-day/.
1: Dimension labeling
With Google's Universal Analytics, Google gave us the option to add custom data (custom dimensions & metrics) to the standard analytics data. As a web analyst, you are responsible for the data people act on. You are also the one that should make this as easy as possible for them. The example given at the GAUC was about a blog. Say you want to see if a read article is old. You'll probably do this by adding a custom dimension with a publication date. But you shouldn't stop there:
Publication date: a great way to see when a article was published. But you'll need to calculate the actual days an article is old.
Amount of days an article is live: you instantly see how many days an article is old, but you don't really know if this is a new, old or even archived one.
New, old or archived: your data instantly shows if read articles are new, old or archived. No thinking required.
The examples above show that you should always think about what the data is for, and how to make it useful in the easiest way. Keep that in mind when adding data to any report.
2: Set your KPIs carefully
KPIs add great value to any data set. In Google Analytics, you can set goals to track important events and pageviews, or use e-commerce data to analyse your website's performance. When setting up your KPIs, make sure that they have a quantitative and qualitative part.
A bad KPI example
Conversion rate (CR) is not really useable as a KPI. For example: your website's CR increased from 10% to 15%. A nice increase of 50%. In the same period, website traffic is down with 50%. What does this give us?
Period 1: 2000 visits, 10% CR, 200 conversions
Period 2: 1000 visits, 15% CR, 150 conversions
The image is clear: a better CR does not result in more conversions.
A good KPI example
Conversions are great for KPIs. They have a quantitative and a qualitative part. Both are hidden in the bad example above:
Visits (quantitative): how large was the volume of visits where conversions could have happened?
Conversion Rate (qualitative): how well did these visits perform?
In the same two periods as before, you would have seen that conversions were down. Looking deeper into the data (the quantitative and qualitative part), the drop in visits and better CR would have appeared. Less conversions, better performance of your website. Interesting if you'd ask me.
3: DIY APIs
One talk was about adding API data to analytics data. A frequently used example is weather data. Add this information to see how sunny or cloudy weather impacts your website's visitors. But the most awesome thing was the mention of Kimonolabs (or the alternative import.io). This tool allows you to turn any website into a data feed. Their video does a great job of explaining how it works:
kimono: a 60 second introduction from Kimono Labs on Vimeo.
Awesome right?
4: Measuring job interviews with UA's Measurement Protocol
The Measurement Protocol may be the most powerful feature of Universal Analytics. It basically allows you to setup a URL to send data to Google Analytics. This way, any system that can send GET or POST requests, can also send data to Google Analytics. A great example was given during the GAUC. It was about a vacancy website.
Normally, you'd optimize your data by website behaviour, for example job applications. This is nice and all, but the real question is: do these people get invited for a job interview? You can measure this with the Measurement Protocol:
Step 1: Google Analytics is loaded on a webpage and generates a client id.
Step 2: The user sends an application by filling out a form.
Step 3: Add a hidden input field to the form and store the Google Analytics client id (step 1) in it. You can read how to het the client id in the google analytics guide.
Step 4: Store the client id with the job application on the server.
Step 5: As soon as an agent changes the state of an application to 'invited for job interview', send an event to Google Analytics via the Measurement Protocol and use the client id of the application (step 4) as the client id in this request.
Step 6: Because of the last non-direct attribution model, and the same client id, the invited for job interview event will be attributed to the source the user had when applying for the job. Keep in mind that if a user had a new source between these two actions (the application and the invited for job interview event) the newer source will be attributed.
Looking at a sample data set:
Adwords traffic: 10.000 visits, 200 applications, 25 interviews
LinkedIn traffic: 10.000 visits, 100 applications, 50 interviews
Without the Measurement Protocol solution in place, you'd think Adwords had the best performing ads. With the new insights, LinkedIn is the better one. A great use of the possibilities of Universal Analytics.
It really got the analytics geek inside of me thinking. Potentially, you could use this to setup your own measurement system for apps (Google has a different way of tracking for apps and websites) to send the app behavior data to analytics just as website data. Make the app's data mimic the website data, merging the data, and the insights, into one Google Analytics property.
5: Enhanced e-commerce for blogs
Yes, enhanced e-commerce reporting for a non e-commerce website. It's possible. A recurring theme throughout several talks was creativity. Be creative when implementing analytics into any site. This is how Simo Ahava came up with the crazy idea of using Universal Analytics (awesome) enhanced e-commerce report features for his own blog. He basically looked at the available product fields and used them to measure his blogs performance:
Product: a blog article.
Product view: a view of an article title.
Product add to cart: a minor scroll on an article page.
Product checkout:
Step 1: a 25% read.
Step 2: a 50% read.
Step 3: a 75% read.
Transaction: a 100% read and time on article of 60 seconds.
Revenue: word count.
A great and very creative use of Universal Analytics. You can read the details on his blog post. And yes, we'll be implementing this on GEEK.
These were my 5 takeaways from the Google Analytics User Conference in Amsterdam. There was a lot more awesome stuff. It was an inspiring day, and I hope you're inspired as well now you've read this article. If you have any questions or suggestions, don't be afraid to ask.",[Analytics]
107,Google OAuth 2: access data from a Google user in your application,/google-oauth-2-enable-your-application-to-access-data-from-a-google-user/,"
            <p>So, you want to access data from a Google user in your application. To accomplish this the user must allow your application to operate as the user. Although it looks simple by clicking “accept” on the login prompt there is a lot more going on than just that. Of course Google has documentation about how this authorization works, but it gets complicated when they start talking about refresh tokens, access tokens, expiration time, client secret, etc. </p>

<p>First things first, let’s start with looking at a Google account. Google has a lot of different services. <br>
<img src=""http://oi61.tinypic.com/or2ctw.jpg"" alt=""google services icon cloud"" class=""full-img""></p>

<p>When you create a Google account you are automatically can use your account for Gmail, Chrome, Youtube, Google Maps, Google Play and Google+. Eventually you can make use of other Google services by just signing in with your Google account. </p>

<h6 id=""tosetupaprojectfollowthenextsteps"">To set up a project, follow the next steps:</h6>

<p>Keep the following things in mind: </p>

<ul>
<li>We want to access data from a unknown user</li>
<li><p>We are building a web application</p>

<ol><li>Create a developer account at <a href=""https://console.developers.google.com/"">https://console.developers.google.com/</a>  </li>
<li>Login with your developer account at <a href=""https://console.developers.google.com/"">https://console.developers.google.com/</a>  </li>
<li>Click ""Create Project""  </li>
<li>Fill in your project name <br>
<img src=""http://i59.tinypic.com/2yor8qu.jpg"" alt=""google create"" title="""">.
Optional: below “”Show advanced options” you can choose where the project will run. Either in the US or Europe  </li>
<li>Click ""Create"". <br>
Google will now create your project this will take a minute.  </li>
<li>In the left menu select “APIs &amp; auth” &gt; “Consent screen” <img src=""http://i57.tinypic.com/n13zbk.png"" alt=""menu consent screen"" title=""""> <br>
The consent screen is the authentication prompt for the user when it wants to use your application.</li>
<li>Select for email address your developer account and fill in a product name. All other fields are optional.  </li>
<li>Click ""Save"".  </li>
<li>Select in the left menu “APIs &amp; auth” &gt; “APIs” <br>
<img src=""http://i59.tinypic.com/11jlnxh.png"" alt=""menu APIS"">
Here you can add APIs to your project to access different google services.</li>
<li>Select or search for an API you want to use.  </li>
<li>Click ""Enable API"",  </li>
<li>Select in the left menu “APIs &amp; auth” &gt; “Credentials” <br>
<img src=""http://i57.tinypic.com/52fcw5.png"" alt=""menu credentials"">
Here you will create the authentication tokens for your application.</li>
<li>Click ""Create new Client ID"".  </li>
<li>Select “Web application”.  </li>
<li>At “Authorized JavaScript origins” fill in the host address of your web application. This can only be one URL.  </li>
<li>At “Authorized redirect URIs” fill in the URL path where you want to handle the authentication. <img src=""http://i57.tinypic.com/123w9d4.png"" alt=""Google oAuth authorization"" title="""">  </li>
<li>Click ""Create Client ID""</li></ol></li>
</ul>

<p>What happened now is that we create credentials for our application to access the API project. <br>
<img src=""http://i62.tinypic.com/34z0dwy.png"" alt=""Google oAuth app credentials"">
With the Client ID and Client Secret your application can connect to your Google Project <br>
<img src=""http://i57.tinypic.com/1zx5ztc.png"" alt=""google app authorization"">
At this point it is useless since there are no users that have authorized your application to access their data.</p>

<h4 id=""oauthflow"">oAuth Flow</h4>

<p>The next step is to make it possible for users to authorize your application to access their data. This is where the oAuth2.0 flow will kick in. <br>
The flow exists of 2 steps: <br>
1. Generate the login URL <br>
2. Exchange login code  </p>

<h5 id=""step1generatetheloginurl"">Step 1: generate the login URL</h5>

<p>The user must login to your application. Therefor you must use a specific URL for the user to login. Google oAuth has its own structure for it. The endpoint for a Google login is <a href=""https://accounts.google.com/o/oauth2/auth"">https://accounts.google.com/o/oauth2/auth</a> <br>
The URL itself is not enough. It will require some parameters. All parameters can be found at <a href=""https://developers.google.com/identity/protocols/OAuth2WebServer#formingtheurl"">https://developers.google.com/identity/protocols/OAuth2WebServer#formingtheurl</a> <br>
For your web application it is sufficient to use the following parameters:</p>

<ul>
<li><code>redirect_uri</code>: The URL where the user will be redirected to after he signed in. This URL must be registered in your Google Project in your Google developers console. If this is not the case your will get an error “redirect<em>uri</em>mismatch”</li>
<li><code>response_type</code>: Determines if Google Oauth will return a code. Web application must use this option.</li>
<li><code>client_id</code>: The Client ID that is generated for your web application in the Google developers console.</li>
<li><code>scope</code>: Determines for what API your application wants access to. For a full list of scopes visit <a href=""https://developers.google.com/oauthplayground/"">https://developers.google.com/oauthplayground/</a> Note: Adwords is not in the list. The adwords scope is <a href=""https://www.googleapis.com/auth/adwords"">https://www.googleapis.com/auth/adwords</a></li>
<li><code>approval_prompt</code>: Indicates if the user should always accept your application before continuing. </li>
<li><code>access_type</code>: Indicates if the application wants to access the user's account if the user is logged in or not.</li>
</ul>

<p>For example here below you will see an authentication URL to allow my web application to read out all of the Google analytics data and email address from a user. For my web application I also want to access his data even when the user isn’t online.</p>

<p><strong>Note:</strong> Remember to add Google analytics as an API to your project</p>

<p>Example URL with breaklines for readability:  </p>

<pre><code>https://accounts.google.com/o/oauth2/auth?  
redirect_uri=http://www.mywebapp.com/oauth2callback&amp;  
response_type=code&amp;  
client_id=104608secret-secret-secret-secret.apps.googleusercontent.com&amp;  
scope=https://www.googleapis.com/auth/analytics.readonly+https://www.googleapis.com/auth/userinfo.email&amp;  
approval_prompt=force&amp;  
access_type=offline  
</code></pre>

<p>When visiting the URL the user first have to login with his Google account. When the user is logged in the user will see your consent screen (in out case, it is in Dutch). <br>
<img src=""http://i58.tinypic.com/2qbhtl1.png"" alt=""google oath consent screen""></p>

<p>When the user clicks on “Accept” google will redirect the user to your given redirect_url with a code in the GET. The URL will look like this:  </p>

<pre><code>http://www.mywebapp.com/oauth2callback?code=4/gXCN77EWLDCOfakep2tvfakezOg6Mn0fakej2vA.giyP3fakejxeAeYFZr95uygvU3j0dumQI  
</code></pre>

<h5 id=""step2exchangelogincode"">Step 2 Exchange login code</h5>

<p>In your application you must get the code from the request. The next step is to send a POST request to the Google server to retrieve the credentials of the user and verify that your application can access the user's data. <br>
The POST request must contain the following parameters</p>

<ul>
<li><code>Code</code>: The authorization code from the GET request</li>
<li><code>Client_id</code>: The Client ID that is generated for your web application in the Google developers console.</li>
<li><code>Client_secret</code>: The client secret that is generated for your web application in the Google developers console.</li>
<li><code>Redirect_uri</code>: The URL where the user will be redirected to after he signed in. This URL must be registered in your Google Project in your Goolge developers console. </li>
<li><code>Grant_type</code>: Identifies the type of token returned. For this example the value is always authorization_code</li>
</ul>

<p>The POST address should be <a href=""https://accounts.google.com/o/oauth2/token"">https://accounts.google.com/o/oauth2/token</a> and also will need a header with Content-Type: application/x-www-form-urlencoded </p>

<p>A POST request will look like as followed:</p>

<pre><code>POST /o/oauth2/token HTTP/1.1  
Host: accounts.google.com  
Content-Type: application/x-www-form-urlencoded

code=4/gXCN77EWLDCO_fake_p2tvfakezOg6Mn0fakej2vA.giyP3fakejxeAeYFZr95uygvU3j0dumQI&amp;  
client_id=104608secret-secret-secret-secret.apps.googleusercontent.com&amp;  
client_secret=90V0FAKE_WkFAKExrHCZti&amp;  
redirect_uri=http://www.mywebapp.com/oauth2callback&amp;  
grant_type=authorization_code  
</code></pre>

<p>The response of the request will return a JSON array. The JSON array will contain the following fields.</p>

<ul>
<li><code>Access_token</code>: The token that will grant you access to user account</li>
<li><code>Refresh_token</code>: A token that can be used to obtain a new access token. This field is only present if access_type=offline is include in the authorization code request.</li>
<li><code>Expires_in</code>: The remaining lifetime of the access token in seconds</li>
<li><code>Token_type</code>: Identifies the type of token returned. In this example it always will have the value Bearer</li>
<li><code>Id_token</code>: Will contain the user's account information. Will only be present if your application wants to know information about the user. Depends on the scope|</li>
</ul>

<p>The example request will return something like this:  </p>

<pre><code>{
    “access_token”:”1/fjfKDe9irjnfAJDFkjFDKjreoijf”,
    “expires_in”: 3920,
    “token_type”: “Bearer”
    “refresh_token”:”4/NdkereoIjnrOIKJMNFI39KEJRNF”
    “id_token”: {“email”:”googletest@bluemango.nl”}
}
</code></pre>

<p>Your application is now ready to access the data from the user. When you try to connect to a API you always have to pass the oAuth 2 credentials to the API. The user can go to <a href=""https://myaccount.google.com"">https://myaccount.google.com</a> and login. The user can scroll down to “Connected apps and services” and click on “Account permissions”. <br>
<img src=""http://i57.tinypic.com/339j41g.png"" alt=""google apps connected""></p>

<p>The user can now see a list of apps that are allowed to access his data. By selecting a app and clicks in the right panel on ""Revoke access"" the user will disable the app to access his data. Now that we have logged in and gained access to the user its data we now have the following completed. <br>
<img src=""http://i57.tinypic.com/vrrdcj.png"" alt=""google app authentication"" class=""full-img""></p>

<p>The access token will allow your application to access data from the user.</p>

<h5 id=""tokens"">Tokens</h5>

<p>So now that we can access the users data let’s take a look at the different tokens that came by and what they mean.</p>

<p>Basically we need an access<em>token to access the user data. The downside of an access</em>token is that it will expire after about one hour. This means that storing the access<em>token  is useless. You will only use the access</em>token for a one time session. To solve this problem Google created the refresh<em>token. This token allows you to generate a new access</em>token for your application. This allows your application to create a new session to access the user data at any time.</p>

<p>To retrieve the refresh<em>token you have to include access</em>type=offline in the authorization request in step 1 of the Oauth2 flow. The refresh<em>token will only be generated at the first time the user accepts your application to access your data. If a user already accepted your application and will come back another time and accepts your application again. The refresh</em>token is not generated. The user must first revoke your application. After the user has revoked your application and then logs in and accepts your application, a new refresh_token is generated.</p>

<p>It is also possible to revoke the application access by sending a GET request.  The GET request must be send to <a href=""https://accounts.google.com/o/oauth2/revoke"">https://accounts.google.com/o/oauth2/revoke</a> with a token as parameter. This token can either be an refresh<em>token or access</em>token.</p>

<p>Example: <br>
<a href=""https://accounts.google.com/o/oauth2/revoke?token=1/fjfKDe9irjnfAJDFkjFDKjreoijf"">https://accounts.google.com/o/oauth2/revoke?token=1/fjfKDe9irjnfAJDFkjFDKjreoijf</a> <br>
By executing the request it will revoke the application from the user where the token belongs to.</p>

<h6 id=""generatingtokens"">Generating tokens</h6>

<p>A access<em>token or refresh</em>token are specifically generated for your application. This means that you can’t use these tokens to access a user account with another application or for different services. That means if you add new APIs to your application the user must reaccept your application.</p>

<h6 id=""sowhatisthebestworkaroundwiththesetokens"">So what is the best workaround with these tokens?</h6>

<p>The refresh<em>token with the correct client</em>id and client_secret will allow other people to use your application to access services of an user account. Some Google APIs allows you to programmatically add or delete stuff from a user. The first thing to prevent this is that you must consider what you want to do with the account. Like for Google analytics when you only want to retrieve data you should use <a href=""https://www.googleapis.com/auth/analytics.readonly"">https://www.googleapis.com/auth/analytics.readonly</a> as scope, just in case your parameters are leaked, other people can only retrieve Google analytics data.</p>

<p>Second thing is that you store your client<em>id and client</em>secret on different locations than your source code. Also do not store them at the same place as the retrieved refresh_tokens. </p>

<p>The third thing you should do is to reset your client_secret once a while. This can be done in the developer console at the credential area. </p>
        ","So, you want to access data from a Google user in your application. To accomplish this the user must allow your application to operate as the user. Although it looks simple by clicking “accept” on the login prompt there is a lot more going on than just that. Of course Google has documentation about how this authorization works, but it gets complicated when they start talking about refresh tokens, access tokens, expiration time, client secret, etc.
First things first, let’s start with looking at a Google account. Google has a lot of different services.
When you create a Google account you are automatically can use your account for Gmail, Chrome, Youtube, Google Maps, Google Play and Google+. Eventually you can make use of other Google services by just signing in with your Google account.
To set up a project, follow the next steps:
Keep the following things in mind:
We want to access data from a unknown user
We are building a web application
Create a developer account at https://console.developers.google.com/
Login with your developer account at https://console.developers.google.com/
Click ""Create Project""
Fill in your project name
. Optional: below “”Show advanced options” you can choose where the project will run. Either in the US or Europe
Click ""Create"".
Google will now create your project this will take a minute.
In the left menu select “APIs & auth” > “Consent screen”

The consent screen is the authentication prompt for the user when it wants to use your application.
Select for email address your developer account and fill in a product name. All other fields are optional.
Click ""Save"".
Select in the left menu “APIs & auth” > “APIs”
Here you can add APIs to your project to access different google services.
Select or search for an API you want to use.
Click ""Enable API"",
Select in the left menu “APIs & auth” > “Credentials”
Here you will create the authentication tokens for your application.
Click ""Create new Client ID"".
Select “Web application”.
At “Authorized JavaScript origins” fill in the host address of your web application. This can only be one URL.
At “Authorized redirect URIs” fill in the URL path where you want to handle the authentication.
Click ""Create Client ID""
What happened now is that we create credentials for our application to access the API project.
With the Client ID and Client Secret your application can connect to your Google Project
At this point it is useless since there are no users that have authorized your application to access their data.
oAuth Flow
The next step is to make it possible for users to authorize your application to access their data. This is where the oAuth2.0 flow will kick in.
The flow exists of 2 steps:
1. Generate the login URL
2. Exchange login code
Step 1: generate the login URL
The user must login to your application. Therefor you must use a specific URL for the user to login. Google oAuth has its own structure for it. The endpoint for a Google login is https://accounts.google.com/o/oauth2/auth
The URL itself is not enough. It will require some parameters. All parameters can be found at https://developers.google.com/identity/protocols/OAuth2WebServer#formingtheurl
For your web application it is sufficient to use the following parameters:
redirect_uri: The URL where the user will be redirected to after he signed in. This URL must be registered in your Google Project in your Google developers console. If this is not the case your will get an error “redirecturimismatch”
response_type: Determines if Google Oauth will return a code. Web application must use this option.
client_id: The Client ID that is generated for your web application in the Google developers console.
scope: Determines for what API your application wants access to. For a full list of scopes visit https://developers.google.com/oauthplayground/ Note: Adwords is not in the list. The adwords scope is https://www.googleapis.com/auth/adwords
approval_prompt: Indicates if the user should always accept your application before continuing.
access_type: Indicates if the application wants to access the user's account if the user is logged in or not.
For example here below you will see an authentication URL to allow my web application to read out all of the Google analytics data and email address from a user. For my web application I also want to access his data even when the user isn’t online.
Note: Remember to add Google analytics as an API to your project
Example URL with breaklines for readability:
https://accounts.google.com/o/oauth2/auth?  
redirect_uri=http://www.mywebapp.com/oauth2callback&  
response_type=code&  
client_id=104608secret-secret-secret-secret.apps.googleusercontent.com&  
scope=https://www.googleapis.com/auth/analytics.readonly+https://www.googleapis.com/auth/userinfo.email&  
approval_prompt=force&  
access_type=offline  
When visiting the URL the user first have to login with his Google account. When the user is logged in the user will see your consent screen (in out case, it is in Dutch).
When the user clicks on “Accept” google will redirect the user to your given redirect_url with a code in the GET. The URL will look like this:
http://www.mywebapp.com/oauth2callback?code=4/gXCN77EWLDCOfakep2tvfakezOg6Mn0fakej2vA.giyP3fakejxeAeYFZr95uygvU3j0dumQI  
Step 2 Exchange login code
In your application you must get the code from the request. The next step is to send a POST request to the Google server to retrieve the credentials of the user and verify that your application can access the user's data.
The POST request must contain the following parameters
Code: The authorization code from the GET request
Client_id: The Client ID that is generated for your web application in the Google developers console.
Client_secret: The client secret that is generated for your web application in the Google developers console.
Redirect_uri: The URL where the user will be redirected to after he signed in. This URL must be registered in your Google Project in your Goolge developers console.
Grant_type: Identifies the type of token returned. For this example the value is always authorization_code
The POST address should be https://accounts.google.com/o/oauth2/token and also will need a header with Content-Type: application/x-www-form-urlencoded
A POST request will look like as followed:
POST /o/oauth2/token HTTP/1.1  
Host: accounts.google.com  
Content-Type: application/x-www-form-urlencoded

code=4/gXCN77EWLDCO_fake_p2tvfakezOg6Mn0fakej2vA.giyP3fakejxeAeYFZr95uygvU3j0dumQI&  
client_id=104608secret-secret-secret-secret.apps.googleusercontent.com&  
client_secret=90V0FAKE_WkFAKExrHCZti&  
redirect_uri=http://www.mywebapp.com/oauth2callback&  
grant_type=authorization_code  
The response of the request will return a JSON array. The JSON array will contain the following fields.
Access_token: The token that will grant you access to user account
Refresh_token: A token that can be used to obtain a new access token. This field is only present if access_type=offline is include in the authorization code request.
Expires_in: The remaining lifetime of the access token in seconds
Token_type: Identifies the type of token returned. In this example it always will have the value Bearer
Id_token: Will contain the user's account information. Will only be present if your application wants to know information about the user. Depends on the scope|
The example request will return something like this:
{
    “access_token”:”1/fjfKDe9irjnfAJDFkjFDKjreoijf”,
    “expires_in”: 3920,
    “token_type”: “Bearer”
    “refresh_token”:”4/NdkereoIjnrOIKJMNFI39KEJRNF”
    “id_token”: {“email”:”googletest@bluemango.nl”}
}
Your application is now ready to access the data from the user. When you try to connect to a API you always have to pass the oAuth 2 credentials to the API. The user can go to https://myaccount.google.com and login. The user can scroll down to “Connected apps and services” and click on “Account permissions”.
The user can now see a list of apps that are allowed to access his data. By selecting a app and clicks in the right panel on ""Revoke access"" the user will disable the app to access his data. Now that we have logged in and gained access to the user its data we now have the following completed.
The access token will allow your application to access data from the user.
Tokens
So now that we can access the users data let’s take a look at the different tokens that came by and what they mean.
Basically we need an accesstoken to access the user data. The downside of an accesstoken is that it will expire after about one hour. This means that storing the accesstoken is useless. You will only use the accesstoken for a one time session. To solve this problem Google created the refreshtoken. This token allows you to generate a new accesstoken for your application. This allows your application to create a new session to access the user data at any time.
To retrieve the refreshtoken you have to include accesstype=offline in the authorization request in step 1 of the Oauth2 flow. The refreshtoken will only be generated at the first time the user accepts your application to access your data. If a user already accepted your application and will come back another time and accepts your application again. The refreshtoken is not generated. The user must first revoke your application. After the user has revoked your application and then logs in and accepts your application, a new refresh_token is generated.
It is also possible to revoke the application access by sending a GET request. The GET request must be send to https://accounts.google.com/o/oauth2/revoke with a token as parameter. This token can either be an refreshtoken or accesstoken.
Example:
https://accounts.google.com/o/oauth2/revoke?token=1/fjfKDe9irjnfAJDFkjFDKjreoijf
By executing the request it will revoke the application from the user where the token belongs to.
Generating tokens
A accesstoken or refreshtoken are specifically generated for your application. This means that you can’t use these tokens to access a user account with another application or for different services. That means if you add new APIs to your application the user must reaccept your application.
So what is the best workaround with these tokens?
The refreshtoken with the correct clientid and client_secret will allow other people to use your application to access services of an user account. Some Google APIs allows you to programmatically add or delete stuff from a user. The first thing to prevent this is that you must consider what you want to do with the account. Like for Google analytics when you only want to retrieve data you should use https://www.googleapis.com/auth/analytics.readonly as scope, just in case your parameters are leaked, other people can only retrieve Google analytics data.
Second thing is that you store your clientid and clientsecret on different locations than your source code. Also do not store them at the same place as the retrieved refresh_tokens.
The third thing you should do is to reset your client_secret once a while. This can be done in the developer console at the credential area.","[Code, oauth]"
108,Export preview urls in Optimizely without starting a test,/export-preview-urls-in-optimizely-without-starting-a-test/,"
            <p>Today I discoverd that it's possible to export preview urls in Optimizely without even starting the test!</p>

<p>You can do this by following these 3 steps:</p>

<h4 id=""1openatestinoptimizely"">1. Open a test in Optimizely</h4>

<p>That should be easy, you can do it!  </p>

<h4 id=""2openthepreview"">2. Open the preview</h4>

<p><img src=""http://oi57.tinypic.com/p2a3c.jpg"" alt=""Open variant preview""></p>

<h4 id=""3changetheurl"">3. Change the url</h4>

<p>When the preview link opens copy the url and change the <code>optimizely_show_preview</code> querystring parameter from from <code>true</code> to <code>false</code>.</p>

<p>So from: <br>
<a href="""">https://www.your-page-under-test.com/?optimizely<em>show</em>preview=true&amp;optimizely<em>token=d7a7ad7ffggfda790bb1c3fb57570169a5bb&amp;optimizely</em>x3029150007=1</a></p>

<p>To: <br>
<a href="""">https://www.your-page-under-test.com/?optimizely<em>show</em>preview=false&amp;optimizely<em>token=d7a7ad7ffggfda790bb1c3fb57570169a5bb&amp;optimizely</em>x3029150007=1</a></p>

<h4 id=""4changevariantbonus"">4. Change variant (bonus)</h4>

<p>The number in the end of the url refers to the variation you are previewing. Changing this number thus allows you to quickly switch between two variants. So let's say we have this url to preview variant #1:</p>

<pre><code>https://www.your-page-under-test.com/?optimizely_show_preview=true&amp;optimizely_token=d7a7ad7ffggfda790bb1c3fb57570169a5bb&amp;optimizely_x3029150007=1  
</code></pre>

<p>We only need to replace the 1 with a 2 to preview variant #2:</p>

<pre><code>https://www.your-page-under-test.com/?optimizely_show_preview=true&amp;optimizely_token=d7a7ad7ffggfda790bb1c3fb57570169a5bb&amp;optimizely_x3029150007=2  
</code></pre>

<p>I hope this helps! Good luck!</p>
        ","Today I discoverd that it's possible to export preview urls in Optimizely without even starting the test!
You can do this by following these 3 steps:
1. Open a test in Optimizely
That should be easy, you can do it!
2. Open the preview
3. Change the url
When the preview link opens copy the url and change the optimizely_show_preview querystring parameter from from true to false.
So from:
https://www.your-page-under-test.com/?optimizelyshowpreview=true&optimizelytoken=d7a7ad7ffggfda790bb1c3fb57570169a5bb&optimizelyx3029150007=1
To:
https://www.your-page-under-test.com/?optimizelyshowpreview=false&optimizelytoken=d7a7ad7ffggfda790bb1c3fb57570169a5bb&optimizelyx3029150007=1
4. Change variant (bonus)
The number in the end of the url refers to the variation you are previewing. Changing this number thus allows you to quickly switch between two variants. So let's say we have this url to preview variant #1:
https://www.your-page-under-test.com/?optimizely_show_preview=true&optimizely_token=d7a7ad7ffggfda790bb1c3fb57570169a5bb&optimizely_x3029150007=1  
We only need to replace the 1 with a 2 to preview variant #2:
https://www.your-page-under-test.com/?optimizely_show_preview=true&optimizely_token=d7a7ad7ffggfda790bb1c3fb57570169a5bb&optimizely_x3029150007=2  
I hope this helps! Good luck!",[Analytics]
109,"Angular 2: Where have my factories, services, constants and values gone?",/where-have-my-factories-services-constants-and-values-gone-in-angular-2/,"
            <p>Let's cut to the chase: they are gone. And in my humble opionion that's a good thing. The concepts of factories, services, constants, values and providers in Angular 1.x have been critized a lot and caused a lot of confusion for people new to Angular. Even after four years of Angular development, I'm not always sure which one to choose. Also why are constants called constants when they do not act like a constant. Ugh. </p>

<p>Fortunately things get a lot easier in Angular 2. Although it is still in alpha (I'm using 2.0.0-alpha.26 for this post) it is already starting to look pretty awesome. </p>

<h4 id=""asimpleserviceinangular2"">A simple service in Angular 2</h4>

<p>Let’s create a service to find photos at Flickr. A service in Angular 2 is simply a ES6 class. </p>

<pre><code class=""language- prettyprint prettyprinted""><span class=""kwd"">class</span><span class=""pln""> </span><span class=""typ"">Flickr</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">  
</span><span class=""pun"">}</span></code></pre>

<p>We do not need to extend any built-in class or add annotation, this is it. </p>

<p>The $http service also dissapeared, and there is no replacement (yet, but there will be). Fortunately there are some great http utilities out there (like <a href=""https://github.com/visionmedia/superagent"">Superagent</a>), and we could even start to use the <a href=""https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API"">Fetch API</a>. Let's use <code>fetch</code> for simplicity sake.</p>

<pre><code class=""language- prettyprint prettyprinted""><span class=""kwd"">class</span><span class=""pln""> </span><span class=""typ"">Flickr</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">  
  searchPhotos</span><span class=""pun"">(</span><span class=""pln"">query</span><span class=""pun"">)</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">
    </span><span class=""kwd"">return</span><span class=""pln""> fetch</span><span class=""pun"">(</span><span class=""str"">`http://api.flickr.com/services/rest/?&amp;method=flickr.photos.search&amp;api_key=[your api key here]&amp;texts=${query}&amp;format=json`</span><span class=""pun"">).</span><span class=""kwd"">then</span><span class=""pun"">(</span><span class=""kwd"">function</span><span class=""pun"">(</span><span class=""pln"">response</span><span class=""pun"">)</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">
      </span><span class=""kwd"">return</span><span class=""pln""> response</span><span class=""pun"">;</span><span class=""pln"">
    </span><span class=""pun"">});</span><span class=""pln"">
  </span><span class=""pun"">}</span><span class=""pln"">
</span><span class=""pun"">}</span></code></pre>

<p>All this does is getting some photos by a given query and returns the data as JSON. </p>

<p>So, how do we use this Flickr class in a application? Luckily the Angular team did not drop dependency injection, as it was one of the greatest features in Angular 1.x. Let's inject our Flickr class into our component:</p>

<pre><code class=""language- prettyprint prettyprinted""><span class=""com"">// Assuming the class exists in a different file</span><span class=""pln"">
</span><span class=""kwd"">import</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""typ"">Flickr</span><span class=""pun"">}</span><span class=""pln""> </span><span class=""kwd"">from</span><span class=""pln""> </span><span class=""str"">'flickr'</span><span class=""pun"">;</span><span class=""pln"">

</span><span class=""lit"">@Component</span><span class=""pun"">({</span><span class=""pln"">
  selector</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""str"">'my-photos-component'</span><span class=""pun"">,</span><span class=""pln"">
  appInjector</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""pun"">[</span><span class=""typ"">Flickr</span><span class=""pun"">]</span><span class=""pln"">
</span><span class=""pun"">})</span></code></pre>

<p>Then in the constructor of that component we simply inject it:</p>

<pre><code class=""language- prettyprint prettyprinted""><span class=""pln"">constructor</span><span class=""pun"">(</span><span class=""pln"">flickr</span><span class=""pun"">:</span><span class=""typ"">Flickr</span><span class=""pun"">)</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">  
  </span><span class=""kwd"">this</span><span class=""pun"">.</span><span class=""pln"">flickr </span><span class=""pun"">=</span><span class=""pln""> flickr</span><span class=""pun"">;</span><span class=""pln"">
</span><span class=""pun"">}</span></code></pre>

<p>This beautiful syntax is made possible by <a href=""http://www.typescriptlang.org/"">TypeScript</a>. Every time the application runs into a variable of type Flickr, it will create an instance of the Flickr class for us. Please note that this <strong>is</strong> a singleton. </p>

<p>Now we have a Flickr instance injected and saved in our component, let's see how we can use this instance to get some photos:</p>

<pre><code class=""language- prettyprint prettyprinted""><span class=""pln"">searchPhotos</span><span class=""pun"">(</span><span class=""pln"">query</span><span class=""pun"">)</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">  
  </span><span class=""kwd"">this</span><span class=""pun"">.</span><span class=""pln"">flickr</span><span class=""pun"">.</span><span class=""pln"">searchPhotos</span><span class=""pun"">(</span><span class=""pln"">query</span><span class=""pun"">).</span><span class=""kwd"">then</span><span class=""pun"">((</span><span class=""pln"">photos</span><span class=""pun"">)</span><span class=""pln""> </span><span class=""pun"">=&gt;</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">
    </span><span class=""kwd"">this</span><span class=""pun"">.</span><span class=""pln"">photos </span><span class=""pun"">=</span><span class=""pln""> photos</span><span class=""pun"">;</span><span class=""pln"">
  </span><span class=""pun"">});</span><span class=""pln"">
</span><span class=""pun"">}</span></code></pre>

<p>That will call our service and assign the photos to <code>this.photos</code>. Did you see we do not even have to call <code>$apply</code> anymore, even though fetch is not part of Angular? Mindblowing.</p>

<p>We still need to call the searchPhotos method from the DOM:</p>

<pre><code>&lt;input type=""search"" #photoQuery /&gt;  
&lt;button (click)=""searchPhotos(photoQuery.value)""&gt;Search photos&lt;/button&gt;  
</code></pre>

<p>One last thing! Of course we want to show the results to our users:  </p>

<pre><code class=""language- prettyprint prettyprinted""><span class=""tag"">&lt;ul&gt;</span><span class=""pln"">  
  </span><span class=""tag"">&lt;li</span><span class=""pln""> *</span><span class=""atn"">ng-for</span><span class=""pun"">=</span><span class=""atv"">""#photo of photos""</span><span class=""tag"">&gt;</span><span class=""pln"">
    </span><span class=""tag"">&lt;img</span><span class=""pln""> [</span><span class=""atn"">src</span><span class=""pln"">]</span><span class=""pun"">=</span><span class=""atv"">""createFlickrImagePath(photo.id, photo.owner)""</span><span class=""pln""> </span><span class=""atn"">width</span><span class=""pun"">=</span><span class=""atv"">""250px""</span><span class=""tag"">&gt;</span><span class=""pln"">
  </span><span class=""tag"">&lt;/li&gt;</span><span class=""pln"">
</span><span class=""tag"">&lt;/ul&gt;</span><span class=""pln"">  </span></code></pre>

<p>The <code>ng-for</code> is a new directive in Angular 2 and it replaced <code>ng-repeat</code>. Also the * in front of it is part of <a href=""http://victorsavkin.com/post/119943127151/angular-2-template-syntax"">Angular's new template syntax</a>. In order for the ngFor - and every other directive - to work, you will need to import it and add it to the directives array of your component before it works. </p>

<p>Our complete component now looks like this:</p>

<pre><code class=""language- prettyprint prettyprinted""><span class=""kwd"">import</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">  
  </span><span class=""typ"">ComponentAnnotation</span><span class=""pln""> </span><span class=""kwd"">as</span><span class=""pln""> </span><span class=""typ"">Component</span><span class=""pun"">,</span><span class=""pln"">
  </span><span class=""typ"">ViewAnnotation</span><span class=""pln""> </span><span class=""kwd"">as</span><span class=""pln""> </span><span class=""typ"">View</span><span class=""pun"">,</span><span class=""pln"">
  </span><span class=""typ"">NgFor</span><span class=""pln"">
</span><span class=""pun"">}</span><span class=""pln""> </span><span class=""kwd"">from</span><span class=""pln""> </span><span class=""str"">'angular2/angular2'</span><span class=""pun"">;</span><span class=""pln"">

</span><span class=""kwd"">import</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""typ"">Flickr</span><span class=""pun"">}</span><span class=""pln""> </span><span class=""kwd"">from</span><span class=""pln""> </span><span class=""str"">'flickr'</span><span class=""pun"">;</span><span class=""pln"">

</span><span class=""lit"">@Component</span><span class=""pun"">({</span><span class=""pln"">
  selector</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""str"">'my-photos-component'</span><span class=""pun"">,</span><span class=""pln"">
  appInjector</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""pun"">[</span><span class=""typ"">Flickr</span><span class=""pun"">]</span><span class=""pln"">
</span><span class=""pun"">})</span><span class=""pln"">
</span><span class=""lit"">@View</span><span class=""pun"">({</span><span class=""pln"">
  </span><span class=""kwd"">template</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""str"">`
  &lt;input type=""search"" #photoQuery /&gt;
  &lt;button (click)=""searchPhotos(photoQuery.value)""&gt;Search photos&lt;/button&gt;
  &lt;ul&gt;
    &lt;li *ng-for=""#photo of photos""&gt;
      &lt;img [src]=""createFlickrImagePath(photo.id, photo.owner)"" width=""250px""&gt;
    &lt;/li&gt;
  &lt;/ul&gt;
  `</span><span class=""pun"">,</span><span class=""pln"">
  directives</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""pun"">[</span><span class=""typ"">NgFor</span><span class=""pun"">]</span><span class=""pln"">
</span><span class=""pun"">})</span><span class=""pln"">

</span><span class=""kwd"">export</span><span class=""pln""> </span><span class=""kwd"">class</span><span class=""pln""> photosView </span><span class=""pun"">{</span><span class=""pln"">

  constructor</span><span class=""pun"">(</span><span class=""pln"">flickr</span><span class=""pun"">:</span><span class=""typ"">Flickr</span><span class=""pun"">)</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">
    </span><span class=""kwd"">this</span><span class=""pun"">.</span><span class=""pln"">flickr </span><span class=""pun"">=</span><span class=""pln""> flickr</span><span class=""pun"">;</span><span class=""pln"">
  </span><span class=""pun"">}</span><span class=""pln"">

  searchPhotos</span><span class=""pun"">(</span><span class=""pln"">query</span><span class=""pun"">)</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">
    </span><span class=""kwd"">this</span><span class=""pun"">.</span><span class=""pln"">flickr</span><span class=""pun"">.</span><span class=""pln"">searchPhotos</span><span class=""pun"">(</span><span class=""pln"">query</span><span class=""pun"">).</span><span class=""kwd"">then</span><span class=""pun"">((</span><span class=""pln"">photos</span><span class=""pun"">)</span><span class=""pln""> </span><span class=""pun"">=&gt;</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">
      </span><span class=""kwd"">this</span><span class=""pun"">.</span><span class=""pln"">photos </span><span class=""pun"">=</span><span class=""pln""> photos</span><span class=""pun"">;</span><span class=""pln"">
    </span><span class=""pun"">});</span><span class=""pln"">
  </span><span class=""pun"">}</span><span class=""pln"">
</span><span class=""pun"">}</span></code></pre>

<p><em>Please note that you will need a ES6 transpiler like <a href=""https://babeljs.io/docs/learn-es2015/"">Babel</a> to run this example.</em></p>
        ","Let's cut to the chase: they are gone. And in my humble opionion that's a good thing. The concepts of factories, services, constants, values and providers in Angular 1.x have been critized a lot and caused a lot of confusion for people new to Angular. Even after four years of Angular development, I'm not always sure which one to choose. Also why are constants called constants when they do not act like a constant. Ugh.
Fortunately things get a lot easier in Angular 2. Although it is still in alpha (I'm using 2.0.0-alpha.26 for this post) it is already starting to look pretty awesome.
A simple service in Angular 2
Let’s create a service to find photos at Flickr. A service in Angular 2 is simply a ES6 class.
class Flickr {  
}
We do not need to extend any built-in class or add annotation, this is it.
The $http service also dissapeared, and there is no replacement (yet, but there will be). Fortunately there are some great http utilities out there (like Superagent), and we could even start to use the Fetch API. Let's use fetch for simplicity sake.
class Flickr {  
  searchPhotos(query) {
    return fetch(`http://api.flickr.com/services/rest/?&method=flickr.photos.search&api_key=[your api key here]&texts=${query}&format=json`).then(function(response) {
      return response;
    });
  }
}
All this does is getting some photos by a given query and returns the data as JSON.
So, how do we use this Flickr class in a application? Luckily the Angular team did not drop dependency injection, as it was one of the greatest features in Angular 1.x. Let's inject our Flickr class into our component:
// Assuming the class exists in a different file
import {Flickr} from 'flickr';

@Component({
  selector: 'my-photos-component',
  appInjector: [Flickr]
})
Then in the constructor of that component we simply inject it:
constructor(flickr:Flickr) {  
  this.flickr = flickr;
}
This beautiful syntax is made possible by TypeScript. Every time the application runs into a variable of type Flickr, it will create an instance of the Flickr class for us. Please note that this is a singleton.
Now we have a Flickr instance injected and saved in our component, let's see how we can use this instance to get some photos:
searchPhotos(query) {  
  this.flickr.searchPhotos(query).then((photos) => {
    this.photos = photos;
  });
}
That will call our service and assign the photos to this.photos. Did you see we do not even have to call $apply anymore, even though fetch is not part of Angular? Mindblowing.
We still need to call the searchPhotos method from the DOM:
<input type=""search"" #photoQuery />  
<button (click)=""searchPhotos(photoQuery.value)"">Search photos</button>  
One last thing! Of course we want to show the results to our users:
<ul>  
  <li *ng-for=""#photo of photos"">
    <img [src]=""createFlickrImagePath(photo.id, photo.owner)"" width=""250px"">
  </li>
</ul>  
The ng-for is a new directive in Angular 2 and it replaced ng-repeat. Also the * in front of it is part of Angular's new template syntax. In order for the ngFor - and every other directive - to work, you will need to import it and add it to the directives array of your component before it works.
Our complete component now looks like this:
import {  
  ComponentAnnotation as Component,
  ViewAnnotation as View,
  NgFor
} from 'angular2/angular2';

import {Flickr} from 'flickr';

@Component({
  selector: 'my-photos-component',
  appInjector: [Flickr]
})
@View({
  template: `
  <input type=""search"" #photoQuery />
  <button (click)=""searchPhotos(photoQuery.value)"">Search photos</button>
  <ul>
    <li *ng-for=""#photo of photos"">
      <img [src]=""createFlickrImagePath(photo.id, photo.owner)"" width=""250px"">
    </li>
  </ul>
  `,
  directives: [NgFor]
})

export class photosView {

  constructor(flickr:Flickr) {
    this.flickr = flickr;
  }

  searchPhotos(query) {
    this.flickr.searchPhotos(query).then((photos) => {
      this.photos = photos;
    });
  }
}
Please note that you will need a ES6 transpiler like Babel to run this example.",[Code]
110,Xebicon Amsterdam 2015: our notes (1/2),/xebicon-amsterdam-2015-our-notes/,"
            <p>8 tracks ranging from Microservices, Datacenter Automation, Test Automation, Internet of Things, Big Data, Continuous Delivery, Agile to Software Development; <a href=""https://xebicon.nl/"">Xebicon 2015</a> took place in the Westergastfabriek in Amsterdam last week. Whole day long you could participate in Handson labs on Go, Docker and the Internet of Things! We went there with a couple of collegues and we would like to share our notes with you.</p>

<h4 id=""godatadrivennow"">Go Data Driven NOW!</h4>

<h6 id=""byfrisovanvollenhoven"">By Friso van Vollenhoven</h6>

<p>Big Data and Data Driven are some of the biggest buzzwords in the tech industry.  Either you have it or you want it, but few actually know what it entails.  Large companies are jumping on the bandwagon and investing millions in Big Data and data science departments and often missing the crucial points required for being a ""data driven"" organization.  </p>

<p>During the GoDataDrivenNow presentation at Xebicon, Friso van Vollenhoven mentioned a few things in order to simplify the whole ""data driven"" hype.  Everything is data nowadays: tweets, posts, weather logs, maps, images, articles, etc. are just a few of the millions of data sources available today.  To be ""data driven"" you must understand that there's a lot more at play than just having / collecting a lot of data.  If it can be distilled into a term, it is more of a mindset than anything else, meaning that the first thing you think of when posed with a question or problem is: ""What kind of data is available to help me make a decision"".  This already is a change in approach for most.</p>

<p>In addition, data is like an optical illusion, the image you see depends on you perspective and expectations.  The realization that data is only useful if you look at it the right way is crucial, you have to ask the right questions.  But finding relevant insights is easier said than done and requires the room to experiment quickly and efficiently.  </p>

<p>Finally one of the biggest challenges in being ""data driven"" is learning to trust in the quantified priors rather than gut feelings or instinct.  Or simply put start making informed decisions instead of gambling on instinct.  </p>

<p><em>- Visited by Jan</em></p>

<h4 id=""beyondsinglepageapplicationswithisomorphism"">Beyond Single Page Applications with Isomorphism</h4>

<h6 id=""bygerthengeveld"">By Gert Hengeveld</h6>

<p>The presentation began with a short story of how web development evolved over time. We start out with just server responses to enhancing the experience through JavaScript to full blown single page web applications.</p>

<p>There is a large gap from going from single page web application which is controlled in the browser to a solution which also works on the server side. Gert explained how isomorphism can solve this issue by creating shareable code which is used in both front and back-end.</p>

<p>It would be great if developers don't have to write code two times in different systems and get the same results on both systems. This will save the developer a lot of time and adds the advantage of maintaining a single code base for different concerns.</p>

<p>At the time of writing is only possible using JavaScript, since that is the only language that works in the browser. For the server was can use Node.js for handling JavaScript. This way, in theory, code can work on both platforms. Gert showed a few frameworks which power the concepts of isomorphism, but sadly didn't go any deeper into them. It would have been interesting to see code both working on front and back-end.</p>

<p>All in all a good talk to get people familiar with the concepts of isomorphism, but not as indepth as I was hoping for because the concepts of isomorphic code were already clear.</p>

<p><em>- Visited by Gaya</em></p>

<h4 id=""theagileengineeringandleadershipcultureatspotify"">The Agile Engineering and Leadership Culture at Spotify</h4>

<h6 id=""bykristianlindwallandersivarsson"">By Kristian Lindwall &amp; Anders Ivarsson</h6>

<p>The keynote was by Spotify’s agile coaches Kristian Lindwall &amp; Anders Ivarsson and started with the statement: It starts with why! </p>

<p>Everybody should always know why the solution he’s working on, brings success to the team and company. </p>

<p>They pinpoint 4 behaviours that indicate a strong team:</p>

<ol>
<li><p><strong>Explore your context actively!</strong> <br>
You’re actions/solutions affect others by definitions, you need to be able to find out what the best solution is within the context of your team, department and company. A leading vision (explainable by everyone) is needed.</p></li>
<li><p><strong>Deliver value early and often</strong> <br>
Define success with the team. Organise your work to accomplish that. Deliver early and prototype: you’re rarely right the first time.</p></li>
<li><p><strong>So keep improving</strong> <br>
Embrace failure by creating an environment where it’s safe to fail. Do retrospectives (or whatever non-scrum name you use to reflect on work) and do them well. It’s the ultimate time for learning.</p></li>
<li><p><strong>Do kickass engineering</strong> <br>
Create a collective ownership of code and make it easy to do great engineering. Hence: automate all boring things —&gt; continuous delivery and automate all possible tasks.</p></li>
</ol>

<p>To behave like these 4 bulletpoints, you need to know why you’re doing your job. If you know, you’ll rock.  </p>

<p><em>- Visited by Wouter</em></p>

<h3 id=""howtostopdrowninginmonitoringdata"">How to stop drowning in monitoring data</h3>

<h5 id=""bymarkbakkerandlodewijkbogaards"">By Mark Bakker and Lodewijk Bogaards</h5>

<p>More components being added to the system every day, more (specialistic) monitoring solutions are added to keep a tab on the system. This starts to create an information overflow, where important errors threaten to get lost in the masses of notifications. Secondly drawing conclusions and finding the source of the problem from the generated errors and warnings is becoming harder and more time consuming due to the dependencies and complexity of the system. <br>
In other words there is the need for a dashboard, where components in error and their relations with other components can be visualized, so it is clear what actions should be taken.</p>

<p>The actual monitoring is only the first step. Automating actions based on errors would be the next step, but this is only feasible for some types of problems / components. When humans need to take action, we need to automate the problem finding process. The third step therefore is to create a chart of the components and their dependencies and color them if they are in a state of warning or in error. Automatically processing information from various monitoring systems and converting them to state changes on the chart is done by StackState, a open source tool developed by a new startup that is part of the Xebia Group. The components are also visually categorised in business processes and infrastructure, so problem components and their impact seen in one view. </p>

<p>The 4th and last step in monitoring heaven would be that you can go back to any moment in time and view the states of the stack at that moment and this will also be possible in the next version of StackState to analyse for instance in what order components started failing.</p>

<p>I think that it is crucial to develop these kind of insights and this session gave me the inspiration to start working on this. Implementation can start with a simple map of components and their dependencies, where you can manually set the states. From there you can start automating the updating of the states monitoring service by monitoring service. So even if you do not implement the whole StackState solution, there are ideas you can put into practice easily. <br>
<em>- visited by Hanneke Gieles</em></p>

<h3 id=""acceptancetestingforcontinuousdelivery"">Acceptance Testing for Continuous Delivery</h3>

<h5 id=""bydavefarley"">By Dave Farley</h5>

<p>To start off Dave Farley explicity explained he wasn't going to talk about Continous Delivery but only a small, but very important part of it. Acceptance Testing is very important in the Continous Delivery proces. Mainly he talked about Acceptance Testing in enterprise systems. In his talk, Dave described approaches to acceptance testing to allow teams to: Work quickly and effectively; Build excellent functional coverage for complex enterprise-scale systems;Manage and maintain those tests in the face of change.</p>

<p>To be clear he; was talking about automated test cases.</p>

<p>The first thing he pointed out was that you have to describe ""What"" you are going to test, and not ""How"". Don't describe step by step what will happen. Teammates probably won't understand what you are willing to do with the test. In complex systems there are numberous of tests. If one module is changed you have to figure out where this module is used in all of your tests and rewrite them.  </p>

<p>To maintain scalability of your program you will have to add an extra abstract layer. In this layer you will write test cases for a certain goal. For example the name of a testcase could be <code>createAnInvoice()</code>. The test contains steps for creating an invoice. Also these steps will also contain by the businness understandable functions, describing what will happen step by step. Like <code>prioritiseNewlyAddedOrders()</code> and <code>getLatestProductPrice()</code>. Eventually when you change a module, the abstract testcase will stay the same. You will have to change the underlying function, but the function is reusable in the different testcases.</p>

<p>Before you start your test, determine what you want to test. If a testcase contains step A-&gt;B-&gt;C and your only made changes to step B, then only test step B.</p>

<p>For development I think this means that you have to develop in an abstract way. Creating reusable code for different cases. <br>
<em>- Visited by Erwin</em> </p>

<h3 id=""videosslidesandphotos"">Videos, slides and photos</h3>

<p>Presentation videos, slides and photos of all tracks of XebiCon 2015 will be available for download soon at <a href=""https://xebicon.nl"">https://xebicon.nl</a>.</p>
        ","8 tracks ranging from Microservices, Datacenter Automation, Test Automation, Internet of Things, Big Data, Continuous Delivery, Agile to Software Development; Xebicon 2015 took place in the Westergastfabriek in Amsterdam last week. Whole day long you could participate in Handson labs on Go, Docker and the Internet of Things! We went there with a couple of collegues and we would like to share our notes with you.
Go Data Driven NOW!
By Friso van Vollenhoven
Big Data and Data Driven are some of the biggest buzzwords in the tech industry. Either you have it or you want it, but few actually know what it entails. Large companies are jumping on the bandwagon and investing millions in Big Data and data science departments and often missing the crucial points required for being a ""data driven"" organization.
During the GoDataDrivenNow presentation at Xebicon, Friso van Vollenhoven mentioned a few things in order to simplify the whole ""data driven"" hype. Everything is data nowadays: tweets, posts, weather logs, maps, images, articles, etc. are just a few of the millions of data sources available today. To be ""data driven"" you must understand that there's a lot more at play than just having / collecting a lot of data. If it can be distilled into a term, it is more of a mindset than anything else, meaning that the first thing you think of when posed with a question or problem is: ""What kind of data is available to help me make a decision"". This already is a change in approach for most.
In addition, data is like an optical illusion, the image you see depends on you perspective and expectations. The realization that data is only useful if you look at it the right way is crucial, you have to ask the right questions. But finding relevant insights is easier said than done and requires the room to experiment quickly and efficiently.
Finally one of the biggest challenges in being ""data driven"" is learning to trust in the quantified priors rather than gut feelings or instinct. Or simply put start making informed decisions instead of gambling on instinct.
- Visited by Jan
Beyond Single Page Applications with Isomorphism
By Gert Hengeveld
The presentation began with a short story of how web development evolved over time. We start out with just server responses to enhancing the experience through JavaScript to full blown single page web applications.
There is a large gap from going from single page web application which is controlled in the browser to a solution which also works on the server side. Gert explained how isomorphism can solve this issue by creating shareable code which is used in both front and back-end.
It would be great if developers don't have to write code two times in different systems and get the same results on both systems. This will save the developer a lot of time and adds the advantage of maintaining a single code base for different concerns.
At the time of writing is only possible using JavaScript, since that is the only language that works in the browser. For the server was can use Node.js for handling JavaScript. This way, in theory, code can work on both platforms. Gert showed a few frameworks which power the concepts of isomorphism, but sadly didn't go any deeper into them. It would have been interesting to see code both working on front and back-end.
All in all a good talk to get people familiar with the concepts of isomorphism, but not as indepth as I was hoping for because the concepts of isomorphic code were already clear.
- Visited by Gaya
The Agile Engineering and Leadership Culture at Spotify
By Kristian Lindwall & Anders Ivarsson
The keynote was by Spotify’s agile coaches Kristian Lindwall & Anders Ivarsson and started with the statement: It starts with why!
Everybody should always know why the solution he’s working on, brings success to the team and company.
They pinpoint 4 behaviours that indicate a strong team:
Explore your context actively!
You’re actions/solutions affect others by definitions, you need to be able to find out what the best solution is within the context of your team, department and company. A leading vision (explainable by everyone) is needed.
Deliver value early and often
Define success with the team. Organise your work to accomplish that. Deliver early and prototype: you’re rarely right the first time.
So keep improving
Embrace failure by creating an environment where it’s safe to fail. Do retrospectives (or whatever non-scrum name you use to reflect on work) and do them well. It’s the ultimate time for learning.
Do kickass engineering
Create a collective ownership of code and make it easy to do great engineering. Hence: automate all boring things —> continuous delivery and automate all possible tasks.
To behave like these 4 bulletpoints, you need to know why you’re doing your job. If you know, you’ll rock.
- Visited by Wouter
How to stop drowning in monitoring data
By Mark Bakker and Lodewijk Bogaards
More components being added to the system every day, more (specialistic) monitoring solutions are added to keep a tab on the system. This starts to create an information overflow, where important errors threaten to get lost in the masses of notifications. Secondly drawing conclusions and finding the source of the problem from the generated errors and warnings is becoming harder and more time consuming due to the dependencies and complexity of the system.
In other words there is the need for a dashboard, where components in error and their relations with other components can be visualized, so it is clear what actions should be taken.
The actual monitoring is only the first step. Automating actions based on errors would be the next step, but this is only feasible for some types of problems / components. When humans need to take action, we need to automate the problem finding process. The third step therefore is to create a chart of the components and their dependencies and color them if they are in a state of warning or in error. Automatically processing information from various monitoring systems and converting them to state changes on the chart is done by StackState, a open source tool developed by a new startup that is part of the Xebia Group. The components are also visually categorised in business processes and infrastructure, so problem components and their impact seen in one view.
The 4th and last step in monitoring heaven would be that you can go back to any moment in time and view the states of the stack at that moment and this will also be possible in the next version of StackState to analyse for instance in what order components started failing.
I think that it is crucial to develop these kind of insights and this session gave me the inspiration to start working on this. Implementation can start with a simple map of components and their dependencies, where you can manually set the states. From there you can start automating the updating of the states monitoring service by monitoring service. So even if you do not implement the whole StackState solution, there are ideas you can put into practice easily.
- visited by Hanneke Gieles
Acceptance Testing for Continuous Delivery
By Dave Farley
To start off Dave Farley explicity explained he wasn't going to talk about Continous Delivery but only a small, but very important part of it. Acceptance Testing is very important in the Continous Delivery proces. Mainly he talked about Acceptance Testing in enterprise systems. In his talk, Dave described approaches to acceptance testing to allow teams to: Work quickly and effectively; Build excellent functional coverage for complex enterprise-scale systems;Manage and maintain those tests in the face of change.
To be clear he; was talking about automated test cases.
The first thing he pointed out was that you have to describe ""What"" you are going to test, and not ""How"". Don't describe step by step what will happen. Teammates probably won't understand what you are willing to do with the test. In complex systems there are numberous of tests. If one module is changed you have to figure out where this module is used in all of your tests and rewrite them.
To maintain scalability of your program you will have to add an extra abstract layer. In this layer you will write test cases for a certain goal. For example the name of a testcase could be createAnInvoice(). The test contains steps for creating an invoice. Also these steps will also contain by the businness understandable functions, describing what will happen step by step. Like prioritiseNewlyAddedOrders() and getLatestProductPrice(). Eventually when you change a module, the abstract testcase will stay the same. You will have to change the underlying function, but the function is reusable in the different testcases.
Before you start your test, determine what you want to test. If a testcase contains step A->B->C and your only made changes to step B, then only test step B.
For development I think this means that you have to develop in an abstract way. Creating reusable code for different cases.
- Visited by Erwin
Videos, slides and photos
Presentation videos, slides and photos of all tracks of XebiCon 2015 will be available for download soon at https://xebicon.nl.",[Events]
111,How we keep our mobile device lab small and simple,/how-we-keep-our-mobile-device-lab-small-and-simple/,"
            <p>In the responsive and adaptive web design movement a need for real world testing came along. There is and always will be a very wide variation of devices and operating systems. It’s important to think about what mobile devices should and should not be on your mobile device wall to maintain fast development. Without doubt, testing on unnecessary devices has a negative impact on your project's timing and budget.</p>

<p><strong>Supporting legacy stuff is time consuming. Gathering some insights on your client’s user analytics is a valuable investment.</strong></p>

<p>A lot of mobile test labs are popping up around the world and that’s a good thing. Of course, an ideal situation is an app that is tested on all possible circumstances. But in a real world clients should only have to pay for actual beneficial improvement. Don’t just assume your app needs to work well on that Samsung Galaxy SII that’s sitting there on your mobile device wall.</p>

<p><img src=""https://c1.staticflickr.com/3/2838/12674230513_85ca410e4e_b.jpg"" alt=""image by Jeremy Keith"" title="""" class=""full-img""><em>Image by Jeremy Keith</em></p>

<h2 id=""howtodecidewhatmobiledevicesyoushouldbetestingon"">How to decide what mobile devices you should be testing on</h2>

<blockquote>
  <p>How you manage your mobile device wall really depends on the kind of end user you support and the budget you have. Our approach is an inspiration on how to go about it yourself. Keep in mind that our decisions are made on how we think it's best for us, it might be different for your situation. In most cases the key to success is to know who you are developing for.</p>
</blockquote>

<p>We are an online marketing agency and develop products that vary from small banners and simple landing pages to full functional web apps and heavily animated webpages.</p>

<p>Our clients often need visually appealing promotional websites. At the same time we like to raise the bar to incorporate new techniques into our toolset. On the other hand we care a lot about supporting legacy browsers, since we are an online marketing company, everything is about getting the highest conversion ratio.</p>

<p>This means that we need to find a balance in what we use to test our products.</p>

<h3 id=""relyonyourclientsuseranalytics"">Rely on your client's user analytics</h3>

<p>We already assumed we were testing on obsolete devices. So we decided to start a research on how to cover the largest part of devices, and later on to keep our testing hours as low as possible. In our analytics results we found out that there is a is a large fragmentation of devices and OS versions, especially for Android (obviously). For example, this is a list of Samsung Galaxy S3 usage in our clients user base:</p>

<p><img src=""http://i.imgur.com/5VdVMhC.png"" alt=""List of Samsung Galaxy S3 usage""></p>

<p>It’s not hard to conclude that this is impossible to test in a real life project, let alone doing this for all types of devices. We wanted to make decisions based on a more narrowed down analysis. The results below is an analysis our 5 biggest client’s websites gathered from a total visitor session amount of 35 million in the first 4 months of 2015. </p>

<p><img src=""http://i.imgur.com/vraOts0.png"" alt=""User OS and device statistics""></p>

<h3 id=""considerdeviceselectiononsimilaritiesbetweenoperatingsystemsanddevices"">Consider device selection on similarities between operating systems and devices</h3>

<p>We didn’t filter on specific iOS devices since Google Analytics doesn’t supply this information by default. Honestly we feel quite confident that there are no significant differences (device performance and resolution not considered) in website functionality behaviour between an iPad with (for instance) iOS8 or an iPhone with iOS8. </p>

<h3 id=""beconfidenttousedevelopertoolsforviewportsizeimpression"">Be confident to use developer tools for viewport size impression</h3>

<p>For responsive testing we use our devices and the Google Developer viewport rendering tool. It doesn’t beat checking your layout on actual devices but if you do this carefully it gives you a quick and reliable impression. It even offers simulation of network speed throttling. And if you aim to have your design fluid with content determined breakpoints then different screen sizes should give you no big surprises. </p>

<p><img src=""http://i.imgur.com/gZezUav.png"" alt=""Simulate specific mobile devices and speed throttling in your Chrome Developer Tool"" class=""full-img""></p>

<h2 id=""ourcarefullycomposedlistofmobiledevices"">Our carefully composed list of mobile devices</h2>

<p><strong>This is our list of devices as we have it today. It is pretty basic but we feel there is no need to have more than this. We only cover the top 5 user scenarios. Every test device added is x amount of time added to project turnaround time. So we tried to be very selective.</strong></p>

<ul>
<li><strong>iPhone 4</strong> (iOS 7)
<br>Small resolution and slow device. If it works on this iPhone it works on all similar and higher versions of iPhone. iPhone 4 updates no higher than iOS7 and that's what we wanted.  </li>
<li><strong>iPad 4</strong> (iOS 8)
<br>Most used iOS version on a tablet of all. Retina screen. No need for newer tablets like the iPad Air.  </li>
<li><strong>iPad 2</strong> (iOS 7)<br>
An older iOS tablet for performance testing.  </li>
<li><strong>Samsung Galaxy Tab 3 Mini</strong> (Android 4.4.2)
<br>Most used Android version on tablets and a smaller variation of resolution.  </li>
<li><strong>Samsung Galaxy Tab 2</strong> (Android 4.2)
<br>Second most used Android version on tablets and still a widely used device. We expected that we didn’t have to support this device anymore but sadly our analytics proved us wrong.  </li>
<li><strong>Samsung Galaxy S4</strong> (Android 4.4.2)
<br>Most used smartphone with the most used Android version of all.  </li>
<li><strong>Samsung Galaxy S3</strong> (Android 4.3)
<br>Largely used Android device with second most used Android version.</li>
</ul>

<p><strong>These are devices we ditched:</strong></p>

<ul>
<li>Samsung Galaxy SII (Android 3.1)</li>
<li>iPhone 4S (This iPhone 4S had iOS6 and we needed iOS7. Updating an iPhone to a specific iOS version is not possible so we changed it with an iPhone 4 on iOS7)</li>
<li>Samsung Galaxy Tab 1</li>
<li>Some Ericsson</li>
<li>Some Blackberry</li>
</ul>

<h2 id=""howtoharvestusefuldeviceandosdatafromyourclientsanalytics"">How to harvest useful device and OS data from your client's analytics</h2>

<p>When you're using data for your mobile wall, you'll want to automate that data. For a basic check, we look at the Google Analytics data of five big clients. For the dataset, we use the following three statistics:</p>

<ul>
<li><strong>Browser + browser version</strong>: to see what browsers are popular. Keep in mind that Chrome has a lot of different versions and because of that has quite some fragmentation.</li>
<li><strong>Device info + OS version</strong>: for insights on which devices and OS version combinations visit the websites.</li>
<li><strong>OS + OS Version</strong>: to see what OS's are used.</li>
</ul>

<p>For each of these dimensions, we have a dashboard. A dashboard lists the amount of sessions for each website, the total amount of sessions, and the percentage of total sessions. The last one is very useful if you want to have an x percentage (e.g. 95%) of all traffic covered with your device testing. </p>

<p>We currently set up these dashboards with <a href=""http://www.tableau.com/"">Tableau</a>. Tableau is a tool that helps anyone quickly analyze, visualize and share information. With a tool like this, it's easy to add powerful filters. We use two of them:</p>

<ul>
<li><strong>Date filter</strong>: filter on the date period you want to review (maybe you'll only want to look at the past 2 months, or see how the data changed over time). </li>
<li><strong>Device category filter</strong>: filter the dashboard to show only desktop, mobile, or tablet data.</li>
</ul>

<p>These two filters make it easy to only show you the data you need.</p>

<h2 id=""whatifthereisnoclientdataavailable"">What if there is no client data available?</h2>

<p>In our case we develop products for clients that already have websites of which we have user data. When you create a product for a new client it is likely that you don’t have any data yet. In that case I would suggest to search for other <a href=""http://developer.android.com/about/dashboards/index.html"">relevant</a> <a href=""http://www.w3schools.com/browsers/browsers_mobile.asp"">statistics</a>.</p>

<h2 id=""keepitleanandkillyourdarlings"">Keep it lean and kill your darlings</h2>

<p>Once you have your clients user data insightful you can keep a close eye on any future changes and if necessary adapt your device list. We already keep in mind that high resolution devices like the iPhone 6 plus and Android Lollipop devices like the Samsung Galaxy S6 will soon have a large user base. We expect to remove the iPhone 4 in the near future, and even if it still looks nice on our mobile wall we will have to say goodbye. But only when we see changes in our statistics, we will act on it.</p>

<p>Special thanks to <a href=""http://geek.bluemangointeractive.com/author/erik-driessen/"">Erik Driessen</a>.</p>
        ","In the responsive and adaptive web design movement a need for real world testing came along. There is and always will be a very wide variation of devices and operating systems. It’s important to think about what mobile devices should and should not be on your mobile device wall to maintain fast development. Without doubt, testing on unnecessary devices has a negative impact on your project's timing and budget.
Supporting legacy stuff is time consuming. Gathering some insights on your client’s user analytics is a valuable investment.
A lot of mobile test labs are popping up around the world and that’s a good thing. Of course, an ideal situation is an app that is tested on all possible circumstances. But in a real world clients should only have to pay for actual beneficial improvement. Don’t just assume your app needs to work well on that Samsung Galaxy SII that’s sitting there on your mobile device wall.
Image by Jeremy Keith
How to decide what mobile devices you should be testing on
How you manage your mobile device wall really depends on the kind of end user you support and the budget you have. Our approach is an inspiration on how to go about it yourself. Keep in mind that our decisions are made on how we think it's best for us, it might be different for your situation. In most cases the key to success is to know who you are developing for.
We are an online marketing agency and develop products that vary from small banners and simple landing pages to full functional web apps and heavily animated webpages.
Our clients often need visually appealing promotional websites. At the same time we like to raise the bar to incorporate new techniques into our toolset. On the other hand we care a lot about supporting legacy browsers, since we are an online marketing company, everything is about getting the highest conversion ratio.
This means that we need to find a balance in what we use to test our products.
Rely on your client's user analytics
We already assumed we were testing on obsolete devices. So we decided to start a research on how to cover the largest part of devices, and later on to keep our testing hours as low as possible. In our analytics results we found out that there is a is a large fragmentation of devices and OS versions, especially for Android (obviously). For example, this is a list of Samsung Galaxy S3 usage in our clients user base:
It’s not hard to conclude that this is impossible to test in a real life project, let alone doing this for all types of devices. We wanted to make decisions based on a more narrowed down analysis. The results below is an analysis our 5 biggest client’s websites gathered from a total visitor session amount of 35 million in the first 4 months of 2015.
Consider device selection on similarities between operating systems and devices
We didn’t filter on specific iOS devices since Google Analytics doesn’t supply this information by default. Honestly we feel quite confident that there are no significant differences (device performance and resolution not considered) in website functionality behaviour between an iPad with (for instance) iOS8 or an iPhone with iOS8.
Be confident to use developer tools for viewport size impression
For responsive testing we use our devices and the Google Developer viewport rendering tool. It doesn’t beat checking your layout on actual devices but if you do this carefully it gives you a quick and reliable impression. It even offers simulation of network speed throttling. And if you aim to have your design fluid with content determined breakpoints then different screen sizes should give you no big surprises.
Our carefully composed list of mobile devices
This is our list of devices as we have it today. It is pretty basic but we feel there is no need to have more than this. We only cover the top 5 user scenarios. Every test device added is x amount of time added to project turnaround time. So we tried to be very selective.
iPhone 4 (iOS 7)
Small resolution and slow device. If it works on this iPhone it works on all similar and higher versions of iPhone. iPhone 4 updates no higher than iOS7 and that's what we wanted.
iPad 4 (iOS 8)
Most used iOS version on a tablet of all. Retina screen. No need for newer tablets like the iPad Air.
iPad 2 (iOS 7)
An older iOS tablet for performance testing.
Samsung Galaxy Tab 3 Mini (Android 4.4.2)
Most used Android version on tablets and a smaller variation of resolution.
Samsung Galaxy Tab 2 (Android 4.2)
Second most used Android version on tablets and still a widely used device. We expected that we didn’t have to support this device anymore but sadly our analytics proved us wrong.
Samsung Galaxy S4 (Android 4.4.2)
Most used smartphone with the most used Android version of all.
Samsung Galaxy S3 (Android 4.3)
Largely used Android device with second most used Android version.
These are devices we ditched:
Samsung Galaxy SII (Android 3.1)
iPhone 4S (This iPhone 4S had iOS6 and we needed iOS7. Updating an iPhone to a specific iOS version is not possible so we changed it with an iPhone 4 on iOS7)
Samsung Galaxy Tab 1
Some Ericsson
Some Blackberry
How to harvest useful device and OS data from your client's analytics
When you're using data for your mobile wall, you'll want to automate that data. For a basic check, we look at the Google Analytics data of five big clients. For the dataset, we use the following three statistics:
Browser + browser version: to see what browsers are popular. Keep in mind that Chrome has a lot of different versions and because of that has quite some fragmentation.
Device info + OS version: for insights on which devices and OS version combinations visit the websites.
OS + OS Version: to see what OS's are used.
For each of these dimensions, we have a dashboard. A dashboard lists the amount of sessions for each website, the total amount of sessions, and the percentage of total sessions. The last one is very useful if you want to have an x percentage (e.g. 95%) of all traffic covered with your device testing.
We currently set up these dashboards with Tableau. Tableau is a tool that helps anyone quickly analyze, visualize and share information. With a tool like this, it's easy to add powerful filters. We use two of them:
Date filter: filter on the date period you want to review (maybe you'll only want to look at the past 2 months, or see how the data changed over time).
Device category filter: filter the dashboard to show only desktop, mobile, or tablet data.
These two filters make it easy to only show you the data you need.
What if there is no client data available?
In our case we develop products for clients that already have websites of which we have user data. When you create a product for a new client it is likely that you don’t have any data yet. In that case I would suggest to search for other relevant statistics.
Keep it lean and kill your darlings
Once you have your clients user data insightful you can keep a close eye on any future changes and if necessary adapt your device list. We already keep in mind that high resolution devices like the iPhone 6 plus and Android Lollipop devices like the Samsung Galaxy S6 will soon have a large user base. We expect to remove the iPhone 4 in the near future, and even if it still looks nice on our mobile wall we will have to say goodbye. But only when we see changes in our statistics, we will act on it.
Special thanks to Erik Driessen.","[mobile, rwd, Analytics]"
112,Viewability: No Guarantee for Success,/viewability-does-not-guarantee-success/,"
            <p>Viewability is a hot topic in the advertising industry. Both advertisers and publishers have their eyes on this matter and not a week goes by without someone writing about it. Although viewability is fundamental for further growth of the display ecosystem, it is far from being the holy grail. Like a click on a banner, viewability is just a raw indicator for success and nothing more than a starting point to help guide your advertising decisions. It rarely is an end goal by itself. An advertiser who optimizes all of his marketing efforts to reach 100% viewability is making a fool's bargain.</p>

<p>A lot of gaming websites, video websites or sites like <a href=""http://www.speedtest.net"">Speedtest.net</a> have the best viewability rates in the market. And then there are websites that do everything to get their viewability as high as possible, like the Sticky/Scroll ads we have been seeing a lot lately. But do they also provide the quality, engagement and sales you desire in your campaigns?</p>

<h4 id=""thediscrepancybetweenviewabilityandonlinesuccess"">The discrepancy between viewability and online success</h4>

<p>I often see large and remarkable discrepancies between viewability rate and the actual on-site interaction and conversion. Let's take the Dutch newspaper <a href=""http://www.telegraaf.nl"">telegraaf.nl</a> as an example: although the viewability of the ad positions above the fold is generally over 70 percent, I see the best results for the rectangle position far below the fold, embedded within the news articles. Positioned where users are focused on the content with an higher time spent on page. Although this position has a viewability rate of “only” 35 percent, it outperforms on CPA by 300% for both sales and brand metrics.</p>

<p>When the industry keeps treating viewability as a goal in itself, the gates will be far open for fraudulent parties that will do anything to get the highest viewability as possible. Similar to click bots, pop-ups and clickbait websites manipulating the click-through rate, we will start seeing 'viewability bots' and malicious websites that will try to fake viewability as much as possible, using a variety of technical tricks.</p>

<h4 id=""viewabilityasatoolnotagoal"">Viewability as a tool, not a goal</h4>

<p>But what should we do with viewability instead? Use viewability as a valuable and efficient way to run smarter advertising and to provide valuable insights, but not as a goal in itself. Think of use cases like:</p>

<ul>
<li><p><strong>Post-viewable conversion tracking.</strong> By using viewability as a touch point in conversion measurement you can get insights on whether the high viewability also resulted in increased on-site behavior, like a qualitative visit (over x seconds on the website), or an interaction on the website like putting a product in the shopping basket.</p></li>
<li><p><strong>DSP's &amp; viewability.</strong> Enrich your RTB platform with viewability data to learn and bid smarter and faster on inventory, but don’t forget to check the relation with other KPI's, like creative interaction and on-site engagement. Various DSP's are investing heavily in the enrichment of their their platform with viewability data. Although this currently mostly concerns reporting, it will find its way to bidding algorithms and smart targeting in the near future.</p></li>
<li><p><strong>Viewability &amp; creative.</strong> Create smarter creatives by dynamically integrating viewability duration to change the creative content after x seconds. For example, you can change the showed offer after a certain viewability time. Or move your call to action forward as soon as you know you are serving on a page with a low viewability duration. For example the Dutch weather website <a href=""http://www.buienradar.nl"">buienradar.nl</a> typically has a viewability rate of over 80 percent, but the time spent on the page is often just 10 seconds. You can also implement this non-dynamically by bundling domains and placements with equivalent viewability durations and serve separate creatives for those groups.</p></li>
<li><p><strong>Don’t limit yourself to the default IAB viewability ratio of 1 second and 50 percent of the surface,</strong> but try to see viewability in relation to creative size and creative duration. A billboard with a viewability of 50 percent will probably have more post-viewable impact than a rectangle with a viewability of 75 percent. Try to find a correlation between the number of pixels that was viewable and the number of interactions it resulted in. </p></li>
<li><p><strong>Finally, don’t forget the price you pay for the inventory.</strong> Why wouldn’t you want to run on an advertising slot with only 20 percent viewability when the CPM you pay is in line with your goals? </p></li>
</ul>

<p>Try out these use cases of viewability and try not to focus solely on 100 percent viewability as the only KPI for your display advertising. Otherwise we are doomed to follow the same path as we did with the click-through rate 10 years ago. </p>

<p><mark>This post has been <a href=""http://www.marketingfacts.nl/berichten/viewability-geen-garantie-voor-succes"">previously published</a> on Marketingfacts. </mark></p>

<p><em>This post was translated by <a href=""http://geek.bluemangointeractive.com/author/siebe/"">Siebe Hiemstra</a></em></p>
        ","Viewability is a hot topic in the advertising industry. Both advertisers and publishers have their eyes on this matter and not a week goes by without someone writing about it. Although viewability is fundamental for further growth of the display ecosystem, it is far from being the holy grail. Like a click on a banner, viewability is just a raw indicator for success and nothing more than a starting point to help guide your advertising decisions. It rarely is an end goal by itself. An advertiser who optimizes all of his marketing efforts to reach 100% viewability is making a fool's bargain.
A lot of gaming websites, video websites or sites like Speedtest.net have the best viewability rates in the market. And then there are websites that do everything to get their viewability as high as possible, like the Sticky/Scroll ads we have been seeing a lot lately. But do they also provide the quality, engagement and sales you desire in your campaigns?
The discrepancy between viewability and online success
I often see large and remarkable discrepancies between viewability rate and the actual on-site interaction and conversion. Let's take the Dutch newspaper telegraaf.nl as an example: although the viewability of the ad positions above the fold is generally over 70 percent, I see the best results for the rectangle position far below the fold, embedded within the news articles. Positioned where users are focused on the content with an higher time spent on page. Although this position has a viewability rate of “only” 35 percent, it outperforms on CPA by 300% for both sales and brand metrics.
When the industry keeps treating viewability as a goal in itself, the gates will be far open for fraudulent parties that will do anything to get the highest viewability as possible. Similar to click bots, pop-ups and clickbait websites manipulating the click-through rate, we will start seeing 'viewability bots' and malicious websites that will try to fake viewability as much as possible, using a variety of technical tricks.
Viewability as a tool, not a goal
But what should we do with viewability instead? Use viewability as a valuable and efficient way to run smarter advertising and to provide valuable insights, but not as a goal in itself. Think of use cases like:
Post-viewable conversion tracking. By using viewability as a touch point in conversion measurement you can get insights on whether the high viewability also resulted in increased on-site behavior, like a qualitative visit (over x seconds on the website), or an interaction on the website like putting a product in the shopping basket.
DSP's & viewability. Enrich your RTB platform with viewability data to learn and bid smarter and faster on inventory, but don’t forget to check the relation with other KPI's, like creative interaction and on-site engagement. Various DSP's are investing heavily in the enrichment of their their platform with viewability data. Although this currently mostly concerns reporting, it will find its way to bidding algorithms and smart targeting in the near future.
Viewability & creative. Create smarter creatives by dynamically integrating viewability duration to change the creative content after x seconds. For example, you can change the showed offer after a certain viewability time. Or move your call to action forward as soon as you know you are serving on a page with a low viewability duration. For example the Dutch weather website buienradar.nl typically has a viewability rate of over 80 percent, but the time spent on the page is often just 10 seconds. You can also implement this non-dynamically by bundling domains and placements with equivalent viewability durations and serve separate creatives for those groups.
Don’t limit yourself to the default IAB viewability ratio of 1 second and 50 percent of the surface, but try to see viewability in relation to creative size and creative duration. A billboard with a viewability of 50 percent will probably have more post-viewable impact than a rectangle with a viewability of 75 percent. Try to find a correlation between the number of pixels that was viewable and the number of interactions it resulted in.
Finally, don’t forget the price you pay for the inventory. Why wouldn’t you want to run on an advertising slot with only 20 percent viewability when the CPM you pay is in line with your goals?
Try out these use cases of viewability and try not to focus solely on 100 percent viewability as the only KPI for your display advertising. Otherwise we are doomed to follow the same path as we did with the click-through rate 10 years ago.
This post has been previously published on Marketingfacts.
This post was translated by Siebe Hiemstra","[Analytics, programmatic buying, viewability, Code]"
113,How to update out of date npm dependencies,/how-to-update-an-out-of-date-package-json/,"
            <p>As projects get older, the dependencies used at the time get older too. If you don't update these packages regularly they could get out of date and might cease to work over time.</p>

<p>Most modern web projects contain dependency management through npm. When the project is setup and worked on by its regular developers not much might change on their machines. Imagine stepping in later on in the process and having to install ancient versions of the chosen tools. This could give quite some problems.</p>

<h2 id=""concerns"">Concerns</h2>

<p>When trying to run <code>npm install</code> on older projects the most occuring problem is that the local version of node and npm do not meet the required versions of the declared dependencies. The local versions might be way too new for the old dependencies to work or even install.</p>

<p>Old versions of dependencies might also contain older unsecure code which has been improved over time. The chance that bugs have been solved since is also quite large.</p>

<h2 id=""thingstoconsiderbeforeupdating"">Things to consider before updating</h2>

<p>Be careful when updating major versions of a dependency. This might change the API altogether, possibly breaking the programatic use of the dependency.</p>

<p>For plugins used in Grunt or Gulp this is not the case most of the time since the plugin itself is a layer between your code and the actual library.</p>

<p>Some stuff might break, don't be afraid to do some work. Not everything is backwards compatible, so take precaution when you're about to update. Store your previous work and begin updating in a seperate branch. Make your work at least reversible.</p>

<h2 id=""updatingthequicklazyway"">Updating the quick (lazy) way</h2>

<p>The quick way to update your project is to open the <code>package.json</code> file in the project and replace all the version numbers with <code>""*""</code>.</p>

<p>This will force all dependencies to get the latest versions. After this, update all the version numbers in the <code>package.json</code> file so they match the latest again. The following command does exactly that.</p>

<pre><code>npm install --save  
</code></pre>

<p>Or use the <code>--save-dev</code> flag if you dependencies are in <code>devDependencies</code>.</p>

<blockquote>
  <p><strong>Be careful:</strong> This will update all dependencies in one go, so you might not know which dependency broke your project later on.</p>
</blockquote>

<h2 id=""updatingthesaferway"">Updating the safer way</h2>

<p>A safer way to update your project is go over all the dependencies declared in <code>package.json</code> one by one.</p>

<p>This will give you the opportunity to take a look at all the dependencies. You might find some unused or dead projects on your way. It's better to have maintained dependencies in your project so they keep getting improved.</p>

<p>Depending on the type of dependency (<code>--save-dev</code> or <code>--save</code>) execute the following per existing dependency:</p>

<pre><code>npm install package-name --save  
</code></pre>

<p>This will update the <code>package.json</code> file with the latest version as well as update the dependency in <code>node_modules</code>.</p>

<h2 id=""testyouroutcome"">Test your outcome</h2>

<p>Finally you can run your tests and see if everything is doing what it did before. Check your builds for errors and if you have a development workflow: give it a quick spin to see if it still functions.</p>

<p>Now you can sleep safely again with a better, improved and safer project. People will thank you for your effort.</p>
        ","As projects get older, the dependencies used at the time get older too. If you don't update these packages regularly they could get out of date and might cease to work over time.
Most modern web projects contain dependency management through npm. When the project is setup and worked on by its regular developers not much might change on their machines. Imagine stepping in later on in the process and having to install ancient versions of the chosen tools. This could give quite some problems.
Concerns
When trying to run npm install on older projects the most occuring problem is that the local version of node and npm do not meet the required versions of the declared dependencies. The local versions might be way too new for the old dependencies to work or even install.
Old versions of dependencies might also contain older unsecure code which has been improved over time. The chance that bugs have been solved since is also quite large.
Things to consider before updating
Be careful when updating major versions of a dependency. This might change the API altogether, possibly breaking the programatic use of the dependency.
For plugins used in Grunt or Gulp this is not the case most of the time since the plugin itself is a layer between your code and the actual library.
Some stuff might break, don't be afraid to do some work. Not everything is backwards compatible, so take precaution when you're about to update. Store your previous work and begin updating in a seperate branch. Make your work at least reversible.
Updating the quick (lazy) way
The quick way to update your project is to open the package.json file in the project and replace all the version numbers with ""*"".
This will force all dependencies to get the latest versions. After this, update all the version numbers in the package.json file so they match the latest again. The following command does exactly that.
npm install --save  
Or use the --save-dev flag if you dependencies are in devDependencies.
Be careful: This will update all dependencies in one go, so you might not know which dependency broke your project later on.
Updating the safer way
A safer way to update your project is go over all the dependencies declared in package.json one by one.
This will give you the opportunity to take a look at all the dependencies. You might find some unused or dead projects on your way. It's better to have maintained dependencies in your project so they keep getting improved.
Depending on the type of dependency (--save-dev or --save) execute the following per existing dependency:
npm install package-name --save  
This will update the package.json file with the latest version as well as update the dependency in node_modules.
Test your outcome
Finally you can run your tests and see if everything is doing what it did before. Check your builds for errors and if you have a development workflow: give it a quick spin to see if it still functions.
Now you can sleep safely again with a better, improved and safer project. People will thank you for your effort.",[Code]
114,5 reasons why you need a JavaScript style guide,/5-reasons-why-you-need-a-javascript-style-guide/,"
            <p>JavaScript style guides have been a hot topic lately. At Blue Mango we’ve also been talking about using one for a long time, and we even got started writing one ourselves a few years ago. I don't know why, but it never really took off.</p>

<p>When recently I got a little distressed about the different coding styles across projects (and departments) and variable naming went crazy again, I decided I would not rest before we implemented a style guide, and this time I would really push through!</p>

<h3 id=""whatisajavascriptstyleguide"">What is a JavaScript style guide?</h3>

<p>A JavaScript style guide is a collection of rules on how you and your team should write your JavaScript code. A style guide consists of guidelines on topics like using single quotes or double quotes, doing indentation using tabs or spaces, and what letter case to use with constants, variables, functions etcetera. </p>

<h3 id=""whyisthisimportant"">Why is this important?</h3>

<p>Why is having a style guide so important to me? I started writing down the reasons why I think having a style guide is a good thing (next to keeping me from being annoyed). I shared them with my team and I would like to share 'm with you too:</p>

<h4 id=""1forconsistency"">1. For consistency</h4>

<p>This is the most obvious one, but we should write and style code in a consistent manner. Not only in your department, but across all departments in your company that produce code. This way we enforce a readable style that everyone in the company (and third parties) can understand and read. I really like the idea that when someone reads code, he or she should not be able to identify who wrote it. </p>

<h4 id=""2forthenewkid"">2. For the new kid</h4>

<p>When a new employee starts in your team, a new style guide will give him or her a guideline on how the team writes code. This will get the newbie up and running fast. </p>

<h4 id=""3tolearnhowtobeabettercoder"">3. To learn how to be a better coder</h4>

<p>Style guides are not just an opinion on code style, but they also (in most cases) follow best practises. As a basic example, most style guides state that you should always use semicolons at the end of every statement. This has a very good reason, and every developer writing JavaScript should understand that reason. So style guides can be used to learn best practices and learn new kids and juniors to write proper JavaScript.</p>

<h4 id=""4forsimplicityfocus"">4. For simplicity &amp; focus</h4>

<p>When having a style guide in place, we all know how to style our code, and we don’t have to think about styling and basic stuff anymore. We should not be thinking about whether we should write our functions <a href=""http://www.reddit.com/r/javascript/comments/v9uzg/the_different_ways_to_write_a_function/"">as an expression or as a declaration</a>. Let's focus on the interesting parts!</p>

<h4 id=""5asaguidelineforcodereviews"">5.    As a guideline for code reviews</h4>

<p>Although code reviews are not solely about style, to me styling is important when reviewing one's code. I truly believe that 'x' is not a proper variable name, but some people think it is. I've had a lot of these discussion on coding style during code reviews. When you have all agreed on a style guide, there is not a lot to discuss anymore. When you’ve documented that <code>x</code> is not a descriptive and clear name for a variable, we do not have to discuss this over and over again during code reviews. </p>

<h3 id=""igotitnowwhatstyleguideshouldweuse"">I got it. Now what style guide should we use?</h3>

<p>After communicating these reasons to the team, we all agreed that we really need a JavaScript style guide, and that we should stick to it as well. Writing one ourselves takes a lot of time, and also there are a lot of great guides available (like <a href=""https://contribute.jquery.org/style-guide/js/"">this one from jQuery</a> or <a href=""https://google-styleguide.googlecode.com/svn/trunk/javascriptguide.xml"">this guide from Google</a>). So we decided to pick one of the existing ones. </p>

<p>I am a big fan of the <a href=""https://github.com/airbnb/javascript"">Airbnb Style Guide</a>, because it's well-documented and they seem to have thought everything through. Plus we can <a href=""http://geek.bluemangointeractive.com/how-to-get-airbnbs-javascript-code-style-working-in-webstorm/"">integrate the settings directly into our Webstorm</a>  (or Pycharm) environment and automate stuff using JSCS. They also updated their beloved guide to <a href=""https://github.com/airbnb/javascript/tree/es6"">ES6</a>, so that's cool. But in the end, choosing a style guide is just a matter of taste. You can't choose a wrong one, as long as you choose.</p>

<p>When I first suggested the Airbnb style guide to the team, the tabs vs. spaces indentation discussion started all over again. People asked whether we could do some exceptions on the given style guide. Well, of course we could. We'd just have to fork Airbnb's repo and change it to our liking. </p>

<p>But I strongly recommended to stick with their guide, because I don't want our guide to get outdated when they choose to update theirs. We had to give in one some small issues, but the team agreed!</p>

<h3 id=""inclosing"">In closing</h3>

<p>I hope you agree that using a style guide for your JavaScript development is important. It does not matter which one you choose, as long as you pick one (or write one yourself) and stick to it. If you haven't already done so, start today! I promise you'll like it. </p>

<p>Of course discussing certain styling decisions is always educative and interesting, so you should never stop questioning your style guide. </p>

<p>Btw, if you are serious about styling and clean code, you should check out <a href=""http://www.amazon.com/Clean-Code-Handbook-Software-Craftsmanship/dp/0132350882"">Clean Code: A Handbook of Agile Software Craftsmanship</a> by Robert C. Martin.  </p>
        ","JavaScript style guides have been a hot topic lately. At Blue Mango we’ve also been talking about using one for a long time, and we even got started writing one ourselves a few years ago. I don't know why, but it never really took off.
When recently I got a little distressed about the different coding styles across projects (and departments) and variable naming went crazy again, I decided I would not rest before we implemented a style guide, and this time I would really push through!
What is a JavaScript style guide?
A JavaScript style guide is a collection of rules on how you and your team should write your JavaScript code. A style guide consists of guidelines on topics like using single quotes or double quotes, doing indentation using tabs or spaces, and what letter case to use with constants, variables, functions etcetera.
Why is this important?
Why is having a style guide so important to me? I started writing down the reasons why I think having a style guide is a good thing (next to keeping me from being annoyed). I shared them with my team and I would like to share 'm with you too:
1. For consistency
This is the most obvious one, but we should write and style code in a consistent manner. Not only in your department, but across all departments in your company that produce code. This way we enforce a readable style that everyone in the company (and third parties) can understand and read. I really like the idea that when someone reads code, he or she should not be able to identify who wrote it.
2. For the new kid
When a new employee starts in your team, a new style guide will give him or her a guideline on how the team writes code. This will get the newbie up and running fast.
3. To learn how to be a better coder
Style guides are not just an opinion on code style, but they also (in most cases) follow best practises. As a basic example, most style guides state that you should always use semicolons at the end of every statement. This has a very good reason, and every developer writing JavaScript should understand that reason. So style guides can be used to learn best practices and learn new kids and juniors to write proper JavaScript.
4. For simplicity & focus
When having a style guide in place, we all know how to style our code, and we don’t have to think about styling and basic stuff anymore. We should not be thinking about whether we should write our functions as an expression or as a declaration. Let's focus on the interesting parts!
5. As a guideline for code reviews
Although code reviews are not solely about style, to me styling is important when reviewing one's code. I truly believe that 'x' is not a proper variable name, but some people think it is. I've had a lot of these discussion on coding style during code reviews. When you have all agreed on a style guide, there is not a lot to discuss anymore. When you’ve documented that x is not a descriptive and clear name for a variable, we do not have to discuss this over and over again during code reviews.
I got it. Now what style guide should we use?
After communicating these reasons to the team, we all agreed that we really need a JavaScript style guide, and that we should stick to it as well. Writing one ourselves takes a lot of time, and also there are a lot of great guides available (like this one from jQuery or this guide from Google). So we decided to pick one of the existing ones.
I am a big fan of the Airbnb Style Guide, because it's well-documented and they seem to have thought everything through. Plus we can integrate the settings directly into our Webstorm (or Pycharm) environment and automate stuff using JSCS. They also updated their beloved guide to ES6, so that's cool. But in the end, choosing a style guide is just a matter of taste. You can't choose a wrong one, as long as you choose.
When I first suggested the Airbnb style guide to the team, the tabs vs. spaces indentation discussion started all over again. People asked whether we could do some exceptions on the given style guide. Well, of course we could. We'd just have to fork Airbnb's repo and change it to our liking.
But I strongly recommended to stick with their guide, because I don't want our guide to get outdated when they choose to update theirs. We had to give in one some small issues, but the team agreed!
In closing
I hope you agree that using a style guide for your JavaScript development is important. It does not matter which one you choose, as long as you pick one (or write one yourself) and stick to it. If you haven't already done so, start today! I promise you'll like it.
Of course discussing certain styling decisions is always educative and interesting, so you should never stop questioning your style guide.
Btw, if you are serious about styling and clean code, you should check out Clean Code: A Handbook of Agile Software Craftsmanship by Robert C. Martin.",[Code]
115,Transforming an online marketing agency talk at Nextbuild: the slides,/transforming-an-online-marketing-agency-talk-at-nexbuild-the-slides/,"
            <p>This afternoon I did a talk on how we used cutting edge technology and the right mindset to transform an online marketing agency at software dev conference Nextbuild in Eindhoven. </p>

<p><a href=""http://nielswijers.github.io/nextbuild-presentation/"">Check the slides here</a>. We will soon tell you a lot more about this subject in an upcoming post. Stay tuned!</p>

<p>A big thanks to those who attended my session at Nextbuild today!</p>
        ","This afternoon I did a talk on how we used cutting edge technology and the right mindset to transform an online marketing agency at software dev conference Nextbuild in Eindhoven.
Check the slides here. We will soon tell you a lot more about this subject in an upcoming post. Stay tuned!
A big thanks to those who attended my session at Nextbuild today!",[marketing]
116,Using String.fromCharCode when encoding gets out of control,/using-string-fromcharcode-when-encoding-gets-out-of-control/,"
            <p>While character encoding and character sets are <a href=""http://htmlpurifier.org/docs/enduser-utf8.html"">pretty easy to understand</a>, it used to be pretty tough back in the old days. Luckily modern browsers handle encoding pretty well, and we can even state that when you are using UTF-8 in your document and in your database, nothing can go wrong really. </p>

<p>But the shit really hits the fan when you are using a third party application that does not use UTF-8 in its database but you need to use the content from that database in an HTML file that does use UTF-8 or when that party rewrites stuff on the fly. That's what happened to us recently when we were working with a HTML creative in the console of the adserver <a href=""http://www.appnexus.com"">Appnexus</a>. </p>

<p>In our creative HTML that we uploaded to Appnexus we had the literal ""€"" sign. </p>

<pre><code>var price = '€ 5.00';  
</code></pre>

<p>This should not cause any problems using UTF-8, but it did. It rendered a weird 'a' character, indicating the character is not available in the current character set for the set encoding type. </p>

<p>So we tried to change it to the <a href=""http://www.w3.org/TR/html4/sgml/entities.html"">HTML entity</a> of the Euro sign. </p>

<pre><code>var price = '&amp;amp;euro; 5.00';  
</code></pre>

<p>Unfortunately this got rendered as <code>&amp;euro;</code>. This sucks. It looks like Appnexus translated this to <code>&amp;amp;euro;</code> or something. Same thing with the numeric reference <code>&amp;#8364;</code>. </p>

<p>This was driving us crazy and felt like we've had already spent far too many time on this weird specific case. </p>

<h4 id=""stringfromcharcodetotherescue"">String.fromCharCode to the rescue</h4>

<p>This is when we got our hands dirty and reached for our friend from the past: fromCharCode. <a href=""http://www.w3schools.com/jsref/jsref_fromcharcode.asp"">String.fromCharCode</a> is a static method of the String object and converts a Unicode number into a character. Using this method we can get round Appnexus rewriting magic. </p>

<p>First we need to find out what the Unicode number is for the Euro sign. Let's open our console and write:</p>

<pre><code>'€'.charCodeAt(0)  
// 8364
</code></pre>

<p>Ok, so now we know the Unicode number for the Euro sign is 8364. Cool. Now in our creative, let's replace the literal ""€"" with the fromCharCode method call.</p>

<pre><code>var euroSign = String.fromCharCode(8364);  
var price = euroSign + ' 5.00';  
</code></pre>

<p>That's it, the Euro sign now renders correctly in our HTML output. We totally tricked you there, Appnexus console!</p>

<p>Of course this is only an edge case, and Appnexus will come with a better solution for this. But still I think it's a neat way to go round crazy encoding problems when you don't have total control over the environment. </p>
        ","While character encoding and character sets are pretty easy to understand, it used to be pretty tough back in the old days. Luckily modern browsers handle encoding pretty well, and we can even state that when you are using UTF-8 in your document and in your database, nothing can go wrong really.
But the shit really hits the fan when you are using a third party application that does not use UTF-8 in its database but you need to use the content from that database in an HTML file that does use UTF-8 or when that party rewrites stuff on the fly. That's what happened to us recently when we were working with a HTML creative in the console of the adserver Appnexus.
In our creative HTML that we uploaded to Appnexus we had the literal ""€"" sign.
var price = '€ 5.00';  
This should not cause any problems using UTF-8, but it did. It rendered a weird 'a' character, indicating the character is not available in the current character set for the set encoding type.
So we tried to change it to the HTML entity of the Euro sign.
var price = '&amp;euro; 5.00';  
Unfortunately this got rendered as &euro;. This sucks. It looks like Appnexus translated this to &amp;euro; or something. Same thing with the numeric reference &#8364;.
This was driving us crazy and felt like we've had already spent far too many time on this weird specific case.
String.fromCharCode to the rescue
This is when we got our hands dirty and reached for our friend from the past: fromCharCode. String.fromCharCode is a static method of the String object and converts a Unicode number into a character. Using this method we can get round Appnexus rewriting magic.
First we need to find out what the Unicode number is for the Euro sign. Let's open our console and write:
'€'.charCodeAt(0)  
// 8364
Ok, so now we know the Unicode number for the Euro sign is 8364. Cool. Now in our creative, let's replace the literal ""€"" with the fromCharCode method call.
var euroSign = String.fromCharCode(8364);  
var price = euroSign + ' 5.00';  
That's it, the Euro sign now renders correctly in our HTML output. We totally tricked you there, Appnexus console!
Of course this is only an edge case, and Appnexus will come with a better solution for this. But still I think it's a neat way to go round crazy encoding problems when you don't have total control over the environment.",[Code]
117,How to add VWO test variants to your Snowplow Analytics data,/how-to-add-vwo-test-variants-to-your-snowplow-analytics-data/,"
            <p><a href=""http://snowplowanalytics.com/"">Snowplow</a> is an open source datalogger that gives you access to event level data. This makes the data very powerful and allows you to make complex analysis with your data. At Blue Mango, we currently use it for advanced analysis like Conversion Attribution. </p>

<p>One of the new uses we want to explore is analysing A/B tests with Snowplow data. To do this, it would be nice to add A/B test data to all of our Snowplow data logs. To make this as convenient as possible, the code should be automated. </p>

<p>For this post, we use <a href=""https://vwo.com/"">Visual Website Optimizer (VWO)</a> as the source for our A/B testing data. Getting the VWO data automatically is quite easy.</p>

<h2 id=""gettingthevwodata"">Getting the VWO data</h2>

<p>To get all of the VWO data available on a webpage, we take the following two things into account:</p>

<ul>
<li>Get the available VWO cookies based on a regular expression; and</li>
<li>Transform the cookie strings into short test ID's.</li>
</ul>

<p>We used a regular expression to automatically get all the VWO experiment cookies from the list of the website's cookies. We chose to transform the cookie strings into shorter test ID's to maximize readability. For example: a test cookie contains the following information: <em>_vis_opt_exp_21_combi=2</em>. This means that for VWO test 21 the current variant is 2. So to only push the information we need, we transform this string to <em>vwo-21-2</em>. </p>

<p><strong>Get VWO test info code</strong></p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""pln""> </span><span class=""com"">/**
  * get Visual Website Optimizer cookies
  * @returns {array}
  */</span><span class=""pln"">
  </span><span class=""kwd"">function</span><span class=""pln""> getVwoTests</span><span class=""pun"">(){</span><span class=""pln"">
      </span><span class=""kwd"">var</span><span class=""pln""> cs</span><span class=""pun"">=</span><span class=""pln"">document</span><span class=""pun"">.</span><span class=""pln"">cookie</span><span class=""pun"">.</span><span class=""pln"">split</span><span class=""pun"">(</span><span class=""str"">/;\s*/</span><span class=""pun"">),</span><span class=""pln""> vwoCookies</span><span class=""pun"">=[],</span><span class=""pln""> i</span><span class=""pun"">;</span><span class=""pln"">
      </span><span class=""kwd"">for</span><span class=""pln""> </span><span class=""pun"">(</span><span class=""pln"">i</span><span class=""pun"">=</span><span class=""lit"">0</span><span class=""pun"">;</span><span class=""pln""> i</span><span class=""pun"">&lt;</span><span class=""pln"">cs</span><span class=""pun"">.</span><span class=""pln"">length</span><span class=""pun"">;</span><span class=""pln""> i</span><span class=""pun"">++)</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">
        </span><span class=""com"">//check if cookie matches VWO cookie regex</span><span class=""pln"">
        </span><span class=""kwd"">if</span><span class=""pln""> </span><span class=""pun"">(</span><span class=""pln"">cs</span><span class=""pun"">[</span><span class=""pln"">i</span><span class=""pun"">].</span><span class=""pln"">match</span><span class=""pun"">(</span><span class=""str"">/^_vis_opt_exp_.*_combi/</span><span class=""pun"">))</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">
          </span><span class=""com"">//modify the cookie string to VWO-TESTID-VARIANT and push to array</span><span class=""pln"">
          vwoCookies</span><span class=""pun"">.</span><span class=""pln"">push</span><span class=""pun"">(</span><span class=""str"">'vwo-'</span><span class=""pun"">+</span><span class=""pln"">cs</span><span class=""pun"">[</span><span class=""pln"">i</span><span class=""pun"">].</span><span class=""pln"">match</span><span class=""pun"">(</span><span class=""str"">/\d+/</span><span class=""pln"">g</span><span class=""pun"">)[</span><span class=""lit"">0</span><span class=""pun"">]+</span><span class=""str"">'-'</span><span class=""pun"">+</span><span class=""pln"">cs</span><span class=""pun"">[</span><span class=""pln"">i</span><span class=""pun"">].</span><span class=""pln"">slice</span><span class=""pun"">(-</span><span class=""lit"">1</span><span class=""pun"">));</span><span class=""pln"">
        </span><span class=""pun"">}</span><span class=""pln"">
      </span><span class=""pun"">}</span><span class=""pln"">
      </span><span class=""kwd"">return</span><span class=""pln""> vwoCookies</span><span class=""pun"">;</span><span class=""pln"">
  </span><span class=""pun"">};</span></code></pre>

<p>The code above returns the following array:  </p>

<pre><code>[""vwo-69-3"", ""vwo-72-4""]
</code></pre>

<p>The next step is to add this array to your Snowplow data logs. And personally, I think the <a href=""https://github.com/snowplow/snowplow/wiki/2-Specific-event-tracking-with-the-Javascript-tracker#custom-contexts"">custom context</a> is the best place to put this.</p>

<h2 id=""snowplowcustomcontext"">Snowplow custom context</h2>

<p>The custom context is the place to add custom meta data to your data logs. It's a good place to add cookie opt-in information (true/false) to all Snowplow logs. At Blue Mango, we do this to make sure we only use user data of users that have given a cookie consent. Here's an example of how to add custom context data to a standard pageview:</p>

<p><strong>Snowplow Pageview with custom context example</strong></p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""pln"">  snaq</span><span class=""pun"">(</span><span class=""str"">'trackPageView'</span><span class=""pun"">,</span><span class=""pln""> document</span><span class=""pun"">.</span><span class=""pln"">title</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">
    </span><span class=""str"">'sp-settings'</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">
      </span><span class=""com"">//addcustom data</span><span class=""pln"">
    </span><span class=""pun"">}</span><span class=""pln"">
  </span><span class=""pun"">});</span></code></pre>

<p>Note that the second parameter is document.title, this mimics the default value Snowplow uses for a pagename. After this, it's quite easy to add the testing information with the <em>getVwoTests</em> function:</p>

<p><strong>Snowplow Pageview with Custom Context &amp; VWO info example</strong></p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""pln""> </span><span class=""com"">/**
  * get Visual Website Optimizer cookies
  * @returns {array}
  */</span><span class=""pln"">
  </span><span class=""kwd"">function</span><span class=""pln""> getVwoTests</span><span class=""pun"">(){</span><span class=""pln"">
    </span><span class=""kwd"">var</span><span class=""pln""> cs</span><span class=""pun"">=</span><span class=""pln"">document</span><span class=""pun"">.</span><span class=""pln"">cookie</span><span class=""pun"">.</span><span class=""pln"">split</span><span class=""pun"">(</span><span class=""str"">/;\s*/</span><span class=""pun"">),</span><span class=""pln""> vwoCookies</span><span class=""pun"">=[],</span><span class=""pln""> i</span><span class=""pun"">;</span><span class=""pln"">
    </span><span class=""kwd"">for</span><span class=""pln""> </span><span class=""pun"">(</span><span class=""pln"">i</span><span class=""pun"">=</span><span class=""lit"">0</span><span class=""pun"">;</span><span class=""pln""> i</span><span class=""pun"">&lt;</span><span class=""pln"">cs</span><span class=""pun"">.</span><span class=""pln"">length</span><span class=""pun"">;</span><span class=""pln""> i</span><span class=""pun"">++)</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">
    </span><span class=""com"">//check if cookie matches VWO cookie regex</span><span class=""pln"">
    </span><span class=""kwd"">if</span><span class=""pln""> </span><span class=""pun"">(</span><span class=""pln"">cs</span><span class=""pun"">[</span><span class=""pln"">i</span><span class=""pun"">].</span><span class=""pln"">match</span><span class=""pun"">(</span><span class=""str"">/^_vis_opt_exp_.*_combi/</span><span class=""pun"">))</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">
      </span><span class=""com"">//modify the cookie string to VWO-TESTID-VARIANT and push to array</span><span class=""pln"">
      vwoCookies</span><span class=""pun"">.</span><span class=""pln"">push</span><span class=""pun"">(</span><span class=""str"">'vwo-'</span><span class=""pun"">+</span><span class=""pln"">cs</span><span class=""pun"">[</span><span class=""pln"">i</span><span class=""pun"">].</span><span class=""pln"">match</span><span class=""pun"">(</span><span class=""str"">/\d+/</span><span class=""pln"">g</span><span class=""pun"">)[</span><span class=""lit"">0</span><span class=""pun"">]+</span><span class=""str"">'-'</span><span class=""pun"">+</span><span class=""pln"">cs</span><span class=""pun"">[</span><span class=""pln"">i</span><span class=""pun"">].</span><span class=""pln"">slice</span><span class=""pun"">(-</span><span class=""lit"">1</span><span class=""pun"">));</span><span class=""pln"">
      </span><span class=""pun"">}</span><span class=""pln"">
    </span><span class=""pun"">}</span><span class=""pln"">
    </span><span class=""kwd"">return</span><span class=""pln""> vwoCookies</span><span class=""pun"">;</span><span class=""pln"">
  </span><span class=""pun"">};</span><span class=""pln"">

  snaq</span><span class=""pun"">(</span><span class=""str"">'trackPageView'</span><span class=""pun"">,</span><span class=""pln""> document</span><span class=""pun"">.</span><span class=""pln"">title</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">
    </span><span class=""str"">'sp-settings'</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">
      </span><span class=""str"">'abtests'</span><span class=""pun"">:</span><span class=""pln""> getVwoTests</span><span class=""pun"">()</span><span class=""pln"">
    </span><span class=""pun"">}</span><span class=""pln"">
  </span><span class=""pun"">})</span></code></pre>

<p>By using a function to return this information, you make sure that every time you  log a pageview, the VWO information is updated automatically. </p>

<p>To take this a step further, you can create a global function to return the custom context. After that, you can use that function to add custom context information to all Snowplow data logs (<em>note that this is always the last parameter to add in your event scripts</em>).</p>

<p><strong>Snowplow events with return custom context examples</strong></p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""pln""> </span><span class=""com"">/**
  * get Snowplow custom context object
  * @returns {json}
  */</span><span class=""pln"">
  </span><span class=""kwd"">function</span><span class=""pln""> getSnowplowCustomContextObject</span><span class=""pun"">(){</span><span class=""pln"">
    </span><span class=""kwd"">var</span><span class=""pln""> snowplowCustomContextObject </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">
      </span><span class=""str"">'sp-settings'</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">
        </span><span class=""com"">//add snowplow custom data</span><span class=""pln"">
      </span><span class=""pun"">}</span><span class=""pln"">
    </span><span class=""pun"">};</span><span class=""pln"">
    </span><span class=""kwd"">return</span><span class=""pln""> snowplowCustomContextObject</span><span class=""pun"">;</span><span class=""pln"">
  </span><span class=""pun"">};</span><span class=""pln"">

  </span><span class=""com"">//pageview example</span><span class=""pln"">
  snaq</span><span class=""pun"">(</span><span class=""str"">'trackPageView'</span><span class=""pun"">,</span><span class=""pln""> document</span><span class=""pun"">.</span><span class=""pln"">title</span><span class=""pun"">,</span><span class=""pln""> getSnowplowCustomContextObject</span><span class=""pun"">());</span><span class=""pln"">

  </span><span class=""com"">//event example</span><span class=""pln"">
  snaq</span><span class=""pun"">(</span><span class=""str"">'trackStructEvent'</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""str"">'category'</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""str"">'action'</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""str"">'label'</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""kwd"">property</span><span class=""pun"">,</span><span class=""pln""> value</span><span class=""pun"">,</span><span class=""pln""> getSnowplowCustomContextObject</span><span class=""pun"">());</span><span class=""pln"">

  </span><span class=""com"">//transaction item exmample</span><span class=""pln"">
  snaq</span><span class=""pun"">(</span><span class=""str"">'addItem'</span><span class=""pun"">,</span><span class=""pln"">
     </span><span class=""str"">'orderid'</span><span class=""pun"">,</span><span class=""pln"">           
     </span><span class=""str"">'sku'</span><span class=""pun"">,</span><span class=""pln"">        
     </span><span class=""str"">'productname'</span><span class=""pun"">,</span><span class=""pln"">         
     </span><span class=""str"">'product category'</span><span class=""pun"">,</span><span class=""pln"">  
     product price</span><span class=""pun"">,</span><span class=""pln"">      
     product quantity</span><span class=""pun"">,</span><span class=""pln"">
     </span><span class=""str"">'EUR'</span><span class=""pun"">,</span><span class=""pln"">
     getSnowplowCustomContextObject</span><span class=""pun"">()</span><span class=""pln"">
  </span><span class=""pun"">);</span></code></pre>

<p>As a final example, here's the complete code snippet with the VWO information, the custom context function and a Snowplow pageview:</p>

<p><strong>Snowplow Pageview with return custom context function &amp; VWO info</strong></p>

<pre><code class=""language-prettyprint lang-js prettyprinted""><span class=""pln""> </span><span class=""com"">/**
  * get Visual Website Optimizer cookies
  * @returns {array}
  */</span><span class=""pln"">
  </span><span class=""kwd"">function</span><span class=""pln""> getVwoTests</span><span class=""pun"">(){</span><span class=""pln"">
      </span><span class=""kwd"">var</span><span class=""pln""> cs</span><span class=""pun"">=</span><span class=""pln"">document</span><span class=""pun"">.</span><span class=""pln"">cookie</span><span class=""pun"">.</span><span class=""pln"">split</span><span class=""pun"">(</span><span class=""str"">/;\s*/</span><span class=""pun"">),</span><span class=""pln""> vwoCookies</span><span class=""pun"">=[],</span><span class=""pln""> i</span><span class=""pun"">;</span><span class=""pln"">
      </span><span class=""kwd"">for</span><span class=""pln""> </span><span class=""pun"">(</span><span class=""pln"">i</span><span class=""pun"">=</span><span class=""lit"">0</span><span class=""pun"">;</span><span class=""pln""> i</span><span class=""pun"">&lt;</span><span class=""pln"">cs</span><span class=""pun"">.</span><span class=""pln"">length</span><span class=""pun"">;</span><span class=""pln""> i</span><span class=""pun"">++)</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">
        </span><span class=""kwd"">if</span><span class=""pln""> </span><span class=""pun"">(</span><span class=""pln"">cs</span><span class=""pun"">[</span><span class=""pln"">i</span><span class=""pun"">].</span><span class=""pln"">match</span><span class=""pun"">(</span><span class=""str"">/^_vis_opt_exp_.*_combi/</span><span class=""pun"">))</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">
            vwoCookies</span><span class=""pun"">.</span><span class=""pln"">push</span><span class=""pun"">(</span><span class=""str"">'vwo-'</span><span class=""pun"">+</span><span class=""pln"">cs</span><span class=""pun"">[</span><span class=""pln"">i</span><span class=""pun"">].</span><span class=""pln"">match</span><span class=""pun"">(</span><span class=""str"">/\d+/</span><span class=""pln"">g</span><span class=""pun"">)[</span><span class=""lit"">0</span><span class=""pun"">]+</span><span class=""str"">'-'</span><span class=""pun"">+</span><span class=""pln"">cs</span><span class=""pun"">[</span><span class=""pln"">i</span><span class=""pun"">].</span><span class=""pln"">slice</span><span class=""pun"">(-</span><span class=""lit"">1</span><span class=""pun"">));</span><span class=""pln"">
        </span><span class=""pun"">}</span><span class=""pln"">
      </span><span class=""pun"">}</span><span class=""pln"">
      </span><span class=""kwd"">return</span><span class=""pln""> vwoCookies</span><span class=""pun"">;</span><span class=""pln"">
  </span><span class=""pun"">};</span><span class=""pln"">

 </span><span class=""com"">/**
  * get Snowplow custom context object
  * @returns {json}
  */</span><span class=""pln"">
  </span><span class=""kwd"">function</span><span class=""pln""> getSnowplowCustomContextObject</span><span class=""pun"">(){</span><span class=""pln""> 
    </span><span class=""kwd"">var</span><span class=""pln""> snowplowCustomContextObject </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">
      </span><span class=""str"">'sp-settings'</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">
        </span><span class=""str"">'abtests'</span><span class=""pun"">:</span><span class=""pln""> getVwoTests</span><span class=""pun"">()</span><span class=""pln"">
      </span><span class=""pun"">}</span><span class=""pln"">
    </span><span class=""pun"">};</span><span class=""pln"">
    </span><span class=""kwd"">return</span><span class=""pln""> snowplowCustomContextObject</span><span class=""pun"">;</span><span class=""pln"">
  </span><span class=""pun"">};</span><span class=""pln"">

  </span><span class=""com"">/*pageview example*/</span><span class=""pln"">
  snaq</span><span class=""pun"">(</span><span class=""str"">'trackPageView'</span><span class=""pun"">,</span><span class=""pln""> document</span><span class=""pun"">.</span><span class=""pln"">title</span><span class=""pun"">,</span><span class=""pln""> getSnowplowCustomContextObject</span><span class=""pun"">());</span></code></pre>

<h2 id=""pitfall"">Pitfall</h2>

<p>There's a pitfall to this code. If you use a tag manager (like Google Tag Manager), the following user behaviour may occur:</p>

<ul>
<li>User visits website;</li>
<li>Snowplow is loaded (with pageview &amp; no VWO cookies);</li>
<li>VWO is loaded and sets the cookies;</li>
<li>The user leaves the website.</li>
</ul>

<p>In this scenario, there won't be any VWO related user data in Snowplow. Though the user is a 'bounce', you still want to add this information to the Snowplow data. To fix this, make sure to fire a Snowplow event (with custom context) after VWO loads.</p>

<p><strong>Basic example to track each VWO visit</strong></p>

<pre><code>  /*start VWO Code*/
  // put your VWO code here
  /*end VWO Code*/

  /*track VWO Visit*/
  snaq('trackStructEvent', 'VWO', 'Visit', '', '', 0, getSnowplowCustomContextObject());
</code></pre>

<h2 id=""nextsteps"">Next Steps</h2>

<p>With VWO information and custom context in place, you are ready for analysis. We'll also be working on a similar script for Optimizely. We'll create post for that once we have it ready. </p>
        ","Snowplow is an open source datalogger that gives you access to event level data. This makes the data very powerful and allows you to make complex analysis with your data. At Blue Mango, we currently use it for advanced analysis like Conversion Attribution.
One of the new uses we want to explore is analysing A/B tests with Snowplow data. To do this, it would be nice to add A/B test data to all of our Snowplow data logs. To make this as convenient as possible, the code should be automated.
For this post, we use Visual Website Optimizer (VWO) as the source for our A/B testing data. Getting the VWO data automatically is quite easy.
Getting the VWO data
To get all of the VWO data available on a webpage, we take the following two things into account:
Get the available VWO cookies based on a regular expression; and
Transform the cookie strings into short test ID's.
We used a regular expression to automatically get all the VWO experiment cookies from the list of the website's cookies. We chose to transform the cookie strings into shorter test ID's to maximize readability. For example: a test cookie contains the following information: _vis_opt_exp_21_combi=2. This means that for VWO test 21 the current variant is 2. So to only push the information we need, we transform this string to vwo-21-2.
Get VWO test info code
 /**
  * get Visual Website Optimizer cookies
  * @returns {array}
  */
  function getVwoTests(){
      var cs=document.cookie.split(/;\s*/), vwoCookies=[], i;
      for (i=0; i<cs.length; i++) {
        //check if cookie matches VWO cookie regex
        if (cs[i].match(/^_vis_opt_exp_.*_combi/)) {
          //modify the cookie string to VWO-TESTID-VARIANT and push to array
          vwoCookies.push('vwo-'+cs[i].match(/\d+/g)[0]+'-'+cs[i].slice(-1));
        }
      }
      return vwoCookies;
  };
The code above returns the following array:
[""vwo-69-3"", ""vwo-72-4""]
The next step is to add this array to your Snowplow data logs. And personally, I think the custom context is the best place to put this.
Snowplow custom context
The custom context is the place to add custom meta data to your data logs. It's a good place to add cookie opt-in information (true/false) to all Snowplow logs. At Blue Mango, we do this to make sure we only use user data of users that have given a cookie consent. Here's an example of how to add custom context data to a standard pageview:
Snowplow Pageview with custom context example
  snaq('trackPageView', document.title, {
    'sp-settings': {
      //addcustom data
    }
  });
Note that the second parameter is document.title, this mimics the default value Snowplow uses for a pagename. After this, it's quite easy to add the testing information with the getVwoTests function:
Snowplow Pageview with Custom Context & VWO info example
 /**
  * get Visual Website Optimizer cookies
  * @returns {array}
  */
  function getVwoTests(){
    var cs=document.cookie.split(/;\s*/), vwoCookies=[], i;
    for (i=0; i<cs.length; i++) {
    //check if cookie matches VWO cookie regex
    if (cs[i].match(/^_vis_opt_exp_.*_combi/)) {
      //modify the cookie string to VWO-TESTID-VARIANT and push to array
      vwoCookies.push('vwo-'+cs[i].match(/\d+/g)[0]+'-'+cs[i].slice(-1));
      }
    }
    return vwoCookies;
  };

  snaq('trackPageView', document.title, {
    'sp-settings': {
      'abtests': getVwoTests()
    }
  })
By using a function to return this information, you make sure that every time you log a pageview, the VWO information is updated automatically.
To take this a step further, you can create a global function to return the custom context. After that, you can use that function to add custom context information to all Snowplow data logs (note that this is always the last parameter to add in your event scripts).
Snowplow events with return custom context examples
 /**
  * get Snowplow custom context object
  * @returns {json}
  */
  function getSnowplowCustomContextObject(){
    var snowplowCustomContextObject = {
      'sp-settings': {
        //add snowplow custom data
      }
    };
    return snowplowCustomContextObject;
  };

  //pageview example
  snaq('trackPageView', document.title, getSnowplowCustomContextObject());

  //event example
  snaq('trackStructEvent', 'category', 'action', 'label', property, value, getSnowplowCustomContextObject());

  //transaction item exmample
  snaq('addItem',
     'orderid',           
     'sku',        
     'productname',         
     'product category',  
     product price,      
     product quantity,
     'EUR',
     getSnowplowCustomContextObject()
  );
As a final example, here's the complete code snippet with the VWO information, the custom context function and a Snowplow pageview:
Snowplow Pageview with return custom context function & VWO info
 /**
  * get Visual Website Optimizer cookies
  * @returns {array}
  */
  function getVwoTests(){
      var cs=document.cookie.split(/;\s*/), vwoCookies=[], i;
      for (i=0; i<cs.length; i++) {
        if (cs[i].match(/^_vis_opt_exp_.*_combi/)) {
            vwoCookies.push('vwo-'+cs[i].match(/\d+/g)[0]+'-'+cs[i].slice(-1));
        }
      }
      return vwoCookies;
  };

 /**
  * get Snowplow custom context object
  * @returns {json}
  */
  function getSnowplowCustomContextObject(){ 
    var snowplowCustomContextObject = {
      'sp-settings': {
        'abtests': getVwoTests()
      }
    };
    return snowplowCustomContextObject;
  };

  /*pageview example*/
  snaq('trackPageView', document.title, getSnowplowCustomContextObject());
Pitfall
There's a pitfall to this code. If you use a tag manager (like Google Tag Manager), the following user behaviour may occur:
User visits website;
Snowplow is loaded (with pageview & no VWO cookies);
VWO is loaded and sets the cookies;
The user leaves the website.
In this scenario, there won't be any VWO related user data in Snowplow. Though the user is a 'bounce', you still want to add this information to the Snowplow data. To fix this, make sure to fire a Snowplow event (with custom context) after VWO loads.
Basic example to track each VWO visit
  /*start VWO Code*/
  // put your VWO code here
  /*end VWO Code*/

  /*track VWO Visit*/
  snaq('trackStructEvent', 'VWO', 'Visit', '', '', 0, getSnowplowCustomContextObject());
Next Steps
With VWO information and custom context in place, you are ready for analysis. We'll also be working on a similar script for Optimizely. We'll create post for that once we have it ready.","[Code, Analytics]"
118,Debugging Adobe Generator plugins for Photoshop in production,/debugging-adobe-generator-plugins-for-photoshop/,"
            <p>We heavily rely on <a href=""https://github.com/adobe-photoshop/generator-core"">Adobe Generator</a> in our in-house HTML animation tool called Splash.Create. When developing Photoshop plugins locally, debugging really is a breeze because you can test directly against Photoshop just like an end-user would. You can debug the plugin the way you would debug any other Node application. Perfect. </p>

<p>But as soon as you release your plugin and users start to install your plugin, things get a bit more complicated. Everytime your plugin crashes, and this is likely to happen in the first iterations because of the different versions of Photoshop out there, the user gets this message:</p>

<p><img src=""http://oi61.tinypic.com/333y71u.jpg"" alt=""""></p>

<p>Hmm, that's not a lot of context, right? You can only guess what went wrong here. Luckily Photoshop Generator writes logs to the following location:</p>

<h6 id=""mac"">Mac</h6>

<p><code>/Users/[your username]/Library/Logs/Adobe/Adobe Photoshop CC/Generator</code></p>

<h6 id=""windows"">Windows</h6>

<p><code>C:\Users\[your username]\AppData\Roaming\Adobe\Adobe Photoshop CC\Generator\logs</code></p>

<p>These logs contain all the debug information that normally would see in your console when running the plugin locally. This really saved me hours of headache.</p>

<p>Please note the log folders are hidden by default on a Mac. So in OSX, you need to hit <code>Cmd + Shift + G</code> in Finder, and  type <code>~/Library/Logs/Adobe/Adobe Photoshop CC/Generator</code>. </p>
        ","We heavily rely on Adobe Generator in our in-house HTML animation tool called Splash.Create. When developing Photoshop plugins locally, debugging really is a breeze because you can test directly against Photoshop just like an end-user would. You can debug the plugin the way you would debug any other Node application. Perfect.
But as soon as you release your plugin and users start to install your plugin, things get a bit more complicated. Everytime your plugin crashes, and this is likely to happen in the first iterations because of the different versions of Photoshop out there, the user gets this message:
Hmm, that's not a lot of context, right? You can only guess what went wrong here. Luckily Photoshop Generator writes logs to the following location:
Mac
/Users/[your username]/Library/Logs/Adobe/Adobe Photoshop CC/Generator
Windows
C:\Users\[your username]\AppData\Roaming\Adobe\Adobe Photoshop CC\Generator\logs
These logs contain all the debug information that normally would see in your console when running the plugin locally. This really saved me hours of headache.
Please note the log folders are hidden by default on a Mac. So in OSX, you need to hit Cmd + Shift + G in Finder, and type ~/Library/Logs/Adobe/Adobe Photoshop CC/Generator.","[Code, Photoshop, Debugging]"
119,Fixing the T-date bug in Safari mobile,/fixing-the-t-date-bug-in-safari-mobile/,"
            <p>Every now and then, you run into a bug that is better described as a 'pain in the ass'. This is one of these bugs, and it has to do with handling dates in iOS. </p>

<p>The story starts with the API of the Dutch Railways (or NS). They have a decent API in place and return dates in following format:</p>

<pre><code class=""language- prettyprint lang-js prettyprinted""><span class=""lit"">2015</span><span class=""pun"">-</span><span class=""lit"">05</span><span class=""pun"">-</span><span class=""lit"">18T15</span><span class=""pun"">:</span><span class=""lit"">02</span><span class=""pun"">:</span><span class=""lit"">00</span><span class=""pun"">+</span><span class=""lit"">0200</span><span class=""pln"">  </span></code></pre>

<p>At first sight, it's just a simple date string. Parsing this date in a desktop, Android or Windows Phone browser works like a charm. Sadly, on iOS, it doesn't. It turns out to be the T right in the middle that causes the issues in iOS. So, my first guess was to remove that T with a simple regex. </p>

<pre><code class=""language- prettyprint lang-js prettyprinted""><span class=""kwd"">var</span><span class=""pln""> departureTime </span><span class=""pun"">=</span><span class=""pln""> advies</span><span class=""pun"">.</span><span class=""typ"">ActueleVertrekTijd</span><span class=""pun"">,</span><span class=""pln"">  
    departureTimeRegex </span><span class=""pun"">=</span><span class=""pln""> vertrekTijd</span><span class=""pun"">.</span><span class=""pln"">replace</span><span class=""pun"">(</span><span class=""kwd"">new</span><span class=""pln""> </span><span class=""typ"">RegExp</span><span class=""pun"">(</span><span class=""str"">""\\T""</span><span class=""pun"">,</span><span class=""str"">""g""</span><span class=""pun"">),</span><span class=""str"">' '</span><span class=""pun"">);</span></code></pre>

<p>This returns the date string as follows:  </p>

<pre><code class=""language- prettyprint lang-js prettyprinted""><span class=""lit"">2015</span><span class=""pun"">-</span><span class=""lit"">05</span><span class=""pun"">-</span><span class=""lit"">18</span><span class=""pln""> </span><span class=""lit"">15</span><span class=""pun"">:</span><span class=""lit"">02</span><span class=""pun"">:</span><span class=""lit"">00</span><span class=""pun"">+</span><span class=""lit"">0200</span><span class=""pln"">  </span></code></pre>

<p>Sadly, iOS doesn't like this one eather. After some online searches, the best solution seemed to be converting this string into an array:  </p>

<pre><code class=""language- prettyprint lang-js prettyprinted""><span class=""kwd"">var</span><span class=""pln""> departureTime </span><span class=""pun"">=</span><span class=""pln""> advies</span><span class=""pun"">.</span><span class=""typ"">ActueleVertrekTijd</span><span class=""pun"">,</span><span class=""pln"">  
    departureTimeRegex </span><span class=""pun"">=</span><span class=""pln""> departureTime</span><span class=""pun"">.</span><span class=""pln"">replace</span><span class=""pun"">(</span><span class=""kwd"">new</span><span class=""pln""> </span><span class=""typ"">RegExp</span><span class=""pun"">(</span><span class=""str"">""\\T""</span><span class=""pun"">,</span><span class=""str"">""g""</span><span class=""pun"">),</span><span class=""str"">' '</span><span class=""pun"">),</span><span class=""pln"">
    departureTimeArray </span><span class=""pun"">=</span><span class=""pln""> departureTimeRegex</span><span class=""pun"">.</span><span class=""pln"">split</span><span class=""pun"">(</span><span class=""str"">/[- :]/</span><span class=""pun"">);</span></code></pre>

<p>The array that we have now:  </p>

<pre><code class=""language- prettyprint lang-js prettyprinted""><span class=""pun"">[</span><span class=""str"">""2015""</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""str"">""05""</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""str"">""18""</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""str"">""15""</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""str"">""02""</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""str"">""00+0200""</span><span class=""pun"">]</span></code></pre>

<p>Finally, you'll need to create a date object using this array:  </p>

<pre><code class=""language- prettyprint lang-js prettyprinted""><span class=""kwd"">var</span><span class=""pln""> departureTime </span><span class=""pun"">=</span><span class=""pln""> advies</span><span class=""pun"">.</span><span class=""typ"">ActueleVertrekTijd</span><span class=""pun"">,</span><span class=""pln"">  
    departureTimeRegex </span><span class=""pun"">=</span><span class=""pln""> departureTime</span><span class=""pun"">.</span><span class=""pln"">replace</span><span class=""pun"">(</span><span class=""kwd"">new</span><span class=""pln""> </span><span class=""typ"">RegExp</span><span class=""pun"">(</span><span class=""str"">""\\T""</span><span class=""pun"">,</span><span class=""str"">""g""</span><span class=""pun"">),</span><span class=""str"">' '</span><span class=""pun"">),</span><span class=""pln"">
    departureTimeArray </span><span class=""pun"">=</span><span class=""pln""> departureTimeRegex</span><span class=""pun"">.</span><span class=""pln"">split</span><span class=""pun"">(</span><span class=""str"">/[- :]/</span><span class=""pun"">),</span><span class=""pln"">
    departureTimeNew </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""kwd"">new</span><span class=""pln""> </span><span class=""typ"">Date</span><span class=""pun"">(</span><span class=""pln"">departureTimeArray</span><span class=""pun"">[</span><span class=""lit"">0</span><span class=""pun"">],</span><span class=""pln""> departureTimeArray</span><span class=""pun"">[</span><span class=""lit"">1</span><span class=""pun"">]-</span><span class=""lit"">1</span><span class=""pun"">,</span><span class=""pln""> departureTimeArray</span><span class=""pun"">[</span><span class=""lit"">2</span><span class=""pun"">],</span><span class=""pln""> departureTimeArray</span><span class=""pun"">[</span><span class=""lit"">3</span><span class=""pun"">],</span><span class=""pln""> departureTimeArray</span><span class=""pun"">[</span><span class=""lit"">4</span><span class=""pun"">]);</span></code></pre>

<p>Et voila, we have a date object that works on iOS (and all other operating systems and browsers):</p>

<pre><code class=""language- prettyprint lang-js prettyprinted""><span class=""typ"">Mon</span><span class=""pln""> </span><span class=""typ"">May</span><span class=""pln""> </span><span class=""lit"">18</span><span class=""pln""> </span><span class=""lit"">2015</span><span class=""pln""> </span><span class=""lit"">15</span><span class=""pun"">:</span><span class=""lit"">02</span><span class=""pun"">:</span><span class=""lit"">00</span><span class=""pln""> GMT</span><span class=""pun"">+</span><span class=""lit"">0200</span><span class=""pln""> </span><span class=""pun"">(</span><span class=""pln"">W</span><span class=""pun"">.</span><span class=""pln""> </span><span class=""typ"">Europe</span><span class=""pln""> </span><span class=""typ"">Daylight</span><span class=""pln""> </span><span class=""typ"">Time</span><span class=""pun"">)</span><span class=""pln"">  </span></code></pre>

<p>It's still unclear to me what causes this T-date bug, but at least there is a fix. If you have any thoughts on this, please comment below.</p>

<p>As a bonus, here's the final code for handling T-dates wrapped in a function:  </p>

<pre><code class=""language- prettyprint lang-js prettyprinted""><span class=""kwd"">function</span><span class=""pln""> returnDateObject</span><span class=""pun"">(</span><span class=""pln"">date</span><span class=""pun"">){</span><span class=""pln"">  
    </span><span class=""kwd"">var</span><span class=""pln""> dt </span><span class=""pun"">=</span><span class=""pln""> date</span><span class=""pun"">,</span><span class=""pln"">
        dtr </span><span class=""pun"">=</span><span class=""pln""> dt</span><span class=""pun"">.</span><span class=""pln"">replace</span><span class=""pun"">(</span><span class=""kwd"">new</span><span class=""pln""> </span><span class=""typ"">RegExp</span><span class=""pun"">(</span><span class=""str"">""\\T""</span><span class=""pun"">,</span><span class=""str"">""g""</span><span class=""pun"">),</span><span class=""str"">' '</span><span class=""pun"">),</span><span class=""pln"">
        dta </span><span class=""pun"">=</span><span class=""pln""> dtr</span><span class=""pun"">.</span><span class=""pln"">split</span><span class=""pun"">(</span><span class=""str"">/[- :]/</span><span class=""pun"">),</span><span class=""pln"">
        dtn </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""kwd"">new</span><span class=""pln""> </span><span class=""typ"">Date</span><span class=""pun"">(</span><span class=""pln"">dta</span><span class=""pun"">[</span><span class=""lit"">0</span><span class=""pun"">],</span><span class=""pln""> dta</span><span class=""pun"">[</span><span class=""lit"">1</span><span class=""pun"">]-</span><span class=""lit"">1</span><span class=""pun"">,</span><span class=""pln""> dta</span><span class=""pun"">[</span><span class=""lit"">2</span><span class=""pun"">],</span><span class=""pln""> dta</span><span class=""pun"">[</span><span class=""lit"">3</span><span class=""pun"">],</span><span class=""pln""> dta</span><span class=""pun"">[</span><span class=""lit"">4</span><span class=""pun"">]);</span><span class=""pln"">
    </span><span class=""kwd"">return</span><span class=""pln""> dtn</span><span class=""pun"">;</span><span class=""pln"">
</span><span class=""pun"">}</span></code></pre>
        ","Every now and then, you run into a bug that is better described as a 'pain in the ass'. This is one of these bugs, and it has to do with handling dates in iOS.
The story starts with the API of the Dutch Railways (or NS). They have a decent API in place and return dates in following format:
2015-05-18T15:02:00+0200  
At first sight, it's just a simple date string. Parsing this date in a desktop, Android or Windows Phone browser works like a charm. Sadly, on iOS, it doesn't. It turns out to be the T right in the middle that causes the issues in iOS. So, my first guess was to remove that T with a simple regex.
var departureTime = advies.ActueleVertrekTijd,  
    departureTimeRegex = vertrekTijd.replace(new RegExp(""\\T"",""g""),' ');
This returns the date string as follows:
2015-05-18 15:02:00+0200  
Sadly, iOS doesn't like this one eather. After some online searches, the best solution seemed to be converting this string into an array:
var departureTime = advies.ActueleVertrekTijd,  
    departureTimeRegex = departureTime.replace(new RegExp(""\\T"",""g""),' '),
    departureTimeArray = departureTimeRegex.split(/[- :]/);
The array that we have now:
[""2015"", ""05"", ""18"", ""15"", ""02"", ""00+0200""]
Finally, you'll need to create a date object using this array:
var departureTime = advies.ActueleVertrekTijd,  
    departureTimeRegex = departureTime.replace(new RegExp(""\\T"",""g""),' '),
    departureTimeArray = departureTimeRegex.split(/[- :]/),
    departureTimeNew = new Date(departureTimeArray[0], departureTimeArray[1]-1, departureTimeArray[2], departureTimeArray[3], departureTimeArray[4]);
Et voila, we have a date object that works on iOS (and all other operating systems and browsers):
Mon May 18 2015 15:02:00 GMT+0200 (W. Europe Daylight Time)  
It's still unclear to me what causes this T-date bug, but at least there is a fix. If you have any thoughts on this, please comment below.
As a bonus, here's the final code for handling T-dates wrapped in a function:
function returnDateObject(date){  
    var dt = date,
        dtr = dt.replace(new RegExp(""\\T"",""g""),' '),
        dta = dtr.split(/[- :]/),
        dtn = new Date(dta[0], dta[1]-1, dta[2], dta[3], dta[4]);
    return dtn;
}","[Code, ios, Debugging]"
120,Higher conversion rates with a shopping cart abandonment program,/higher-conversion-rates-with-an-shopping-cart-abandonment-program/,"
            <p>Many online marketeers will recognise this problem: despite filling up their digital shopping cart, more than half of customers leave without making it through checkout. Besides remarketing with search and display advertising, a so called Abandoned Shopping Cart (ASC) email program can persuade a portion of those visitors to return to your webshop and finish their order. Last year, Vodafone started a pilot with an ASC program. I’d like to share the most important learnings. </p>

<h3 id=""customercentricity"">Customer centricity</h3>

<p>The principle behind the abandoned shopping cart program is simple: visitors who leave the website during checkout and who already filled in their email adress, will receive an email. This email will give them the opportunity to finish their order. Like any conversion optimalisation process, the ASC-program moves over 2 effect-axis:</p>

<ul>
<li>Increase desire</li>
<li>Decrease effort</li>
</ul>

<p><img src=""http://cms.tig.nl/media/blog/fec_desire.png"" alt="""">
<em>Source: <a href=""http://www.lukew.com"">http://www.lukew.com</a></em></p>

<p>A no-brainer, but as we soon discovered during the pilot, personalising e-mail templates increases success. Not just using a personal title, but also showing the abandoned shopping cart and auto-filling the previously entered data, can positively affect the results.  </p>

<p>With this extensive personalisation, you make it as easy as possible for your customers to finish their order. Well executed, an ASC email can be interpreted as a service by your customers. A clear example of how decreasing effort can lead to higher conversions. </p>

<p><img src=""http://oi60.tinypic.com/2whf49k.jpg"" alt=""""></p>

<p>However, it’s wise to keep a generic template at hand. Despite the higher conversion ratio for personalised e-mails during the Vodafone pilot, you can increase your reach considerably when you add ‘plain’ emails. If you can’t capture all dynamic content correctly, it’s desirable to send a generic e-mail, instead of an email with missing data. That is killing. </p>

<h3 id=""alwaysbetesting"">Always be testing</h3>

<p>A strong ASC program is more than just personalising emails. There’s a great deal to be gained by structurally testing emails. A test with two subject lines showed that <a href=""http://www.marketingfacts.nl/topic/de-be%C3%AFnvloedingsprincipes-van-cialdini"">Cialdini’s 6 principles of persuasion</a> are of added value in email marketing as well. The scarcity-based subject line ‘Don’t miss this temporary offer’ increased the open rate by 7 percent, compared to the more service-minded default line “Is there something we can help you with?”. </p>

<p>This principle of scarcity was applied to the content of the email in a subsequent test. In the visuals below, you’ll see how we created a sense or urgency by changing the text and using a more striking visual in the header. This lead to a 30 per cent increase in customers returning to their shopping cart. </p>

<p>In this example the other effect axis, increasing desire, is used effectively. But the goal remains the same: increasing the amount of customers that return to their shopping cart to finish their order. </p>

<p><img src=""http://www.marketingfacts.nl/images/blogimages/Afbeelding3.JPG"" alt=""""></p>

<h3 id=""timingisessential"">Timing is essential</h3>

<p>Not just the content of the email determines the result; the frequency and timing also have an impact. To prevent sending an email while the visitor is still in the checkout process, it’s smart to wait an hour before sendin the first email. If this email doesn’t persuade the customer to return, sending a reminder can pay off. </p>

<p>Although the conversion ratio of the first email is, on average, significantly higher compared to the second email, the reminder still contributes substantially in absolute numbers. It would be a shame to waste that opportunity. But track pixel loads to make sure that the customer didn’t finish their order in the meantime. This prevents unneccessary calls to the customer center by customers who worry that their order didn’t come through. </p>

<p><img src=""http://oi58.tinypic.com/2mca79l.jpg"" alt=""""></p>

<h3 id=""volumeforimpact"">Volume for impact</h3>

<p>In order to contribute to the sales results notably, you can’t limit your efforts to optimising open and conversion ratios. You should also focus on volume. To succesfully set up a high-impact ASC-program, you need email addresses. </p>

<p>The amount of email addresses you can collect depends on the location of the email field in the checkout process. For instance, don’t put this field in the final step. But here again, start testing! Collecting emails isn’t supposed to comprimise your initial conversion. </p>

<p>This is literally what happened during an a/b-test for another customer. Bringing forward the email field resulted in a 4 per cent increase of collected emails, but it also decreased the amount of orders by 7 per cent. That’s why it’s always important to look for the right balance: get as many email adresses without decreasing your website’s conversion ratio. </p>

<h3 id=""conclusion"">Conclusion</h3>

<p>By aiding your customers, finding the right dosis of personalisation and thoroughly testing your email templates, it’s possible to convert a considerable part of previous dropouts. During the end of our pilot, the conversion ratio of visitors from the ASC program was 3 times higher than the average website conversion.</p>

<p>A downside of ASC campaigns is the limited scalability. You can increase the percentage of email adresses that you collect by optimising, but you will always be dependent on the amount of check out dropouts. </p>

<p>Of course there are other important things to note. For instance the cannibalisation of organic and other campaign traffic. But down the line, ASC programming still offers a significant contribution. “A strong ASC program provides us with extra orders on an ongoing basis. An absolute must for every e-commerce website.” – Christiaan Holman, senior manager consumer online sales &amp; crm at Vodafone. </p>

<p><mark>This post has been <a href=""http://www.marketingfacts.nl/berichten/maximaal-resultaat-uit-elke-shopping-cart"">previously published</a> on Marketingfacts. </mark></p>

<p><em>This post was translated by <a href=""http://geek.bluemangointeractive.com/author/siebe/"">Siebe Hiemstra</a> and written together with <a href=""http://geek.bluemangointeractive.com/author/luuk-janssen/"">Luuk Janssen</a>.</em>.</p>
        ","Many online marketeers will recognise this problem: despite filling up their digital shopping cart, more than half of customers leave without making it through checkout. Besides remarketing with search and display advertising, a so called Abandoned Shopping Cart (ASC) email program can persuade a portion of those visitors to return to your webshop and finish their order. Last year, Vodafone started a pilot with an ASC program. I’d like to share the most important learnings.
Customer centricity
The principle behind the abandoned shopping cart program is simple: visitors who leave the website during checkout and who already filled in their email adress, will receive an email. This email will give them the opportunity to finish their order. Like any conversion optimalisation process, the ASC-program moves over 2 effect-axis:
Increase desire
Decrease effort
Source: http://www.lukew.com
A no-brainer, but as we soon discovered during the pilot, personalising e-mail templates increases success. Not just using a personal title, but also showing the abandoned shopping cart and auto-filling the previously entered data, can positively affect the results.
With this extensive personalisation, you make it as easy as possible for your customers to finish their order. Well executed, an ASC email can be interpreted as a service by your customers. A clear example of how decreasing effort can lead to higher conversions.
However, it’s wise to keep a generic template at hand. Despite the higher conversion ratio for personalised e-mails during the Vodafone pilot, you can increase your reach considerably when you add ‘plain’ emails. If you can’t capture all dynamic content correctly, it’s desirable to send a generic e-mail, instead of an email with missing data. That is killing.
Always be testing
A strong ASC program is more than just personalising emails. There’s a great deal to be gained by structurally testing emails. A test with two subject lines showed that Cialdini’s 6 principles of persuasion are of added value in email marketing as well. The scarcity-based subject line ‘Don’t miss this temporary offer’ increased the open rate by 7 percent, compared to the more service-minded default line “Is there something we can help you with?”.
This principle of scarcity was applied to the content of the email in a subsequent test. In the visuals below, you’ll see how we created a sense or urgency by changing the text and using a more striking visual in the header. This lead to a 30 per cent increase in customers returning to their shopping cart.
In this example the other effect axis, increasing desire, is used effectively. But the goal remains the same: increasing the amount of customers that return to their shopping cart to finish their order.
Timing is essential
Not just the content of the email determines the result; the frequency and timing also have an impact. To prevent sending an email while the visitor is still in the checkout process, it’s smart to wait an hour before sendin the first email. If this email doesn’t persuade the customer to return, sending a reminder can pay off.
Although the conversion ratio of the first email is, on average, significantly higher compared to the second email, the reminder still contributes substantially in absolute numbers. It would be a shame to waste that opportunity. But track pixel loads to make sure that the customer didn’t finish their order in the meantime. This prevents unneccessary calls to the customer center by customers who worry that their order didn’t come through.
Volume for impact
In order to contribute to the sales results notably, you can’t limit your efforts to optimising open and conversion ratios. You should also focus on volume. To succesfully set up a high-impact ASC-program, you need email addresses.
The amount of email addresses you can collect depends on the location of the email field in the checkout process. For instance, don’t put this field in the final step. But here again, start testing! Collecting emails isn’t supposed to comprimise your initial conversion.
This is literally what happened during an a/b-test for another customer. Bringing forward the email field resulted in a 4 per cent increase of collected emails, but it also decreased the amount of orders by 7 per cent. That’s why it’s always important to look for the right balance: get as many email adresses without decreasing your website’s conversion ratio.
Conclusion
By aiding your customers, finding the right dosis of personalisation and thoroughly testing your email templates, it’s possible to convert a considerable part of previous dropouts. During the end of our pilot, the conversion ratio of visitors from the ASC program was 3 times higher than the average website conversion.
A downside of ASC campaigns is the limited scalability. You can increase the percentage of email adresses that you collect by optimising, but you will always be dependent on the amount of check out dropouts.
Of course there are other important things to note. For instance the cannibalisation of organic and other campaign traffic. But down the line, ASC programming still offers a significant contribution. “A strong ASC program provides us with extra orders on an ongoing basis. An absolute must for every e-commerce website.” – Christiaan Holman, senior manager consumer online sales & crm at Vodafone.
This post has been previously published on Marketingfacts.
This post was translated by Siebe Hiemstra and written together with Luuk Janssen..",[marketing]
121,Optimizing media spends using S-response curves,/optimize-media-spends-using-s-response-curves/,"
            <p>A key focus of our Data Science team is to help our clients understand how their marketing spend affects their KPIs. In particular, we create models to understand the effect of individual marketing channels such as television or paid search ads on KPIs such as sales, visits and footfall.  Knowing how marketing spend affects KPIs enables us to optimize the clients marketing spend for maximal result. In this Geek post I will give a fictional example on how we use S-curves to optimize radio spend. The final code of this fictional example can be found on <a href=""https://github.com/thomhopmans/themarketingtechnologist"">Github</a> as well.</p>

<h4 id=""thedata"">The data</h4>

<p>Let’s consider a simple example where we have the number of sales and the number of radio <a href=""http://en.wikipedia.org/wiki/Gross_rating_point"">GRPs</a> on a daily basis. We assume that radio only has an immediate positive effect on the number of sales the same day. This is obviously not true in real-life, because radio still shows a positive effect after several days. For now, such delayed effects are out of the scope of this post. The figure below provides an overview of how our fictional sales and radio dataset looks like. We see that in this example dataset there are two important factors that determine the number of sales on a day: the day of the week (seasonality) and the number of radio GRPs on that day (marketing spend). </p>

<p><img src=""http://i62.tinypic.com/20pppfs.png"" alt=""FIGURE 1"" class=""full-img""></p>

<h4 id=""simpleolsregression"">Simple OLS regression</h4>

<p>As we are interested in how radio affects the number of sales, we run a simple OLS regression to capture the relationship of marketing spend and seasonality on sales. The results of this regression are shown in the table below.</p>

<pre><code>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                  sales   R-squared:                       0.920  
Model:                            OLS   Adj. R-squared:                  0.896  
Method:                 Least Squares   F-statistic:                     38.01  
Date:                Thu, 23 Apr 2015   Prob (F-statistic):           3.62e-11  
Time:                        15:00:50   Log-Likelihood:                -47.424  
No. Observations:                  31   AIC:                             110.8  
Df Residuals:                      23   BIC:                             122.3  
Df Model:                           7  
Covariance Type:            nonrobust  
========================================================================================
                           coef    std err          t      P&gt;|t|      [95.0% Conf. Int.]
----------------------------------------------------------------------------------------
const                    7.4642      0.583     12.793      0.000         6.257     8.671  
radio_grp                1.1423      0.096     11.854      0.000         0.943     1.342  
seasonality_monday      -2.7826      0.820     -3.392      0.003        -4.480    -1.085  
seasonality_tuesday     -0.4115      0.869     -0.473      0.640        -2.210     1.387  
seasonality_thursday    -4.8585      0.903     -5.381      0.000        -6.726    -2.991  
seasonality_friday      -3.7702      0.886     -4.256      0.000        -5.603    -1.938  
seasonality_saturday    -4.9362      0.886     -5.572      0.000        -6.769    -3.104  
seasonality_sunday      -5.3039      0.872     -6.080      0.000        -7.109    -3.499  
==============================================================================
Omnibus:                        0.646   Durbin-Watson:                   1.188  
Prob(Omnibus):                  0.724   Jarque-Bera (JB):                0.739  
Skew:                          -0.265   Prob(JB):                        0.691  
Kurtosis:                       2.461   Cond. No.                         24.1  
==============================================================================
</code></pre>

<p><img src=""http://i62.tinypic.com/2s9yr6v.png"" alt=""FIGURE 2"" class=""full-img""></p>

<p>The result of this regression shows that the day of the week is a very important factor for the number of sales. We used Wednesday as reference day and added binary dummy variables for all other days. Note Wednesday is taken arbitrarily. The coefficients of these day-dummies can be interpreted as the number of sales this day has more (or less) compared to the number of sales on Wednesday (the reference day). For example, the Monday coefficient of –2.8 implies that there are 2.8 less absolute sales on Monday than on Wednesday. So, as all day dummy coefficients are negative, it follows that Wednesday is the best day of the week for the sales. </p>

<p>More interestingly, the result of the regression also gives a positive coefficient of 1.14 for the radio variable. This coefficient can be interpreted as a positive effect of 1.14 additional sales for each radio GRP used. Note that this also implies that the effect of radio is <em>linear</em>, i.e. 1 GRP results in 1.14 additional sales and 5 GRPs results in 5 times 1.14 (=5.7) additional sales. </p>

<h4 id=""thesresponsecurve"">The S-response curve</h4>

<p>In reality, it is not often the case that radio has a linear effect on sales. KPI drivers such as television and radio, but also display and search ads tend to have diminishing returns. <a href=""http://en.wikipedia.org/wiki/Diminishing_returns"">Wikipedia</a> provides the following example about diminishing returns:  </p>

<blockquote>
  <p>“A common sort of example is adding more workers to a job, such as assembling a car on a factory floor. At some point, adding more workers causes problems such as workers getting in each other's way or frequently finding themselves waiting for access to a part. Producing one more unit of output per unit of time will eventually cost increasingly more, due to inputs being used less and less effectively.”</p>
</blockquote>

<p>In a similar manner, research has shown that initial advertising budget has little impact on sales. One possible reason for this is that a low advertising budget might result in your marketing not being noticable between all of the competitors marketing campaigns. Only after a certain budget spend threshold results are noticeable in the form of improved KPIs. Hence, the result is that the effect of marketing spend on KPI drivers such as radio typically follows an S-shaped curve. The figure below provides an example of an S-curve for radio spend.</p>

<p><img src=""http://i60.tinypic.com/2ykbz2v.png"" alt=""FIGURE 3"" class=""full-img""></p>

<h4 id=""introducingdummyvariables"">Introducing dummy variables</h4>

<p>The notion of the S-shaped curve conflicts with our earlier calculated linear effect of radio on sales. Therefore, it is very likely that we get a much more accurate model when we can account for the effect of the S-curve. One possible approach is to just try all possible transformations of our radio dataset to an S-curve. However, as the S-curve is defined by three parameter values, this implies trying a lot of different transformations. Therefore, we use a smarter approach to find the S-curve. That is, instead of adding a continuous variable that denotes the number of radio GRPs on a given day, we add binary dummies where each dummy represents an interval of GRPs. Given our example dataset where the maximum number of GRPs is 10, we add four binary dummies representing the GRP intervals [0.1-2.4], [2.5-4.9], [5-7.4] and [7.5-10]. Now, the dummy value of an interval is 1 if the number of GRPs of that day is in that interval, otherwise the value is zero. Below is a short snippet of the radio dataset we then obtain:</p>

<table>  
  <tbody><tr align=""center"">
    <th>Date time</th>
    <th>Radio GRP</th>
    <th>Radio dummy<br>0.1 - 2.4</th>
    <th>Radio dummy<br>2.5 - 4.9</th>
    <th>Radio dummy<br>5.0 - 7.4</th>
    <th>Radio dummy<br>7.5 - 10</th>
  </tr>
  <tr>
      <td>2015-06-01</td>
    <td>4</td>
    <td>0</td>
    <td>1</td>
    <td>0</td>
    <td>0</td>
  </tr>
  <tr>
      <td>2015-06-02</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
  </tr>
  <tr>
      <td>2015-06-03</td>
    <td>1</td>
    <td>1</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
  </tr>
  <tr>
      <td>2015-06-04</td>
    <td>2</td>
    <td>1</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
  </tr>
  <tr>
      <td>2015-06-05</td>
    <td>3</td>
    <td>0</td>
    <td>1</td>
    <td>0</td>
    <td>0</td>
  </tr>
</tbody></table>

<p>Using the new radio dummy variables we again run a standard OLS regression. The results of this regression are shown below:</p>

<pre><code>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                  sales   R-squared:                       0.985  
Model:                            OLS   Adj. R-squared:                  0.978  
Method:                 Least Squares   F-statistic:                     134.6  
Date:                Thu, 23 Apr 2015   Prob (F-statistic):           4.28e-16  
Time:                        15:42:39   Log-Likelihood:                -21.183  
No. Observations:                  31   AIC:                             64.37  
Df Residuals:                      20   BIC:                             80.14  
Df Model:                          10  
Covariance Type:            nonrobust  
========================================================================================
                           coef    std err          t      P&gt;|t|      [95.0% Conf. Int.]
----------------------------------------------------------------------------------------
const                    8.1073      0.298     27.175      0.000         7.485     8.730  
radio_dummy_0.1_2.5      0.2485      0.334      0.745      0.465        -0.448     0.945  
radio_dummy_2.5_5        1.9554      0.407      4.806      0.000         1.107     2.804  
radio_dummy_5_7.5        8.7667      0.415     21.146      0.000         7.902     9.632  
radio_dummy_7.5_10       9.3869      0.494     19.001      0.000         8.356    10.417  
seasonality_monday      -2.9030      0.405     -7.171      0.000        -3.747    -2.059  
seasonality_tuesday     -0.5771      0.401     -1.439      0.165        -1.413     0.259  
seasonality_thursday    -4.6460      0.430    -10.793      0.000        -5.544    -3.748  
seasonality_friday      -4.3809      0.441     -9.929      0.000        -5.301    -3.461  
seasonality_saturday    -5.2752      0.419    -12.602      0.000        -6.148    -4.402  
seasonality_sunday      -5.9470      0.422    -14.096      0.000        -6.827    -5.067  
==============================================================================
Omnibus:                        1.184   Durbin-Watson:                   1.621  
Prob(Omnibus):                  0.553   Jarque-Bera (JB):                0.590  
Skew:                           0.334   Prob(JB):                        0.744  
Kurtosis:                       3.107   Cond. No.                         8.27  
==============================================================================
</code></pre>

<p>The interpretation of the coefficients of the new radio dummy variables is slightly different than in our first regression. The coefficient of each dummy represents the additional sales when the number of GRPs is in the corresponding interval. For example, consider a given day on which 6 radio GRPs are used. This implies that the value of the dummy for the interval [5 – 7.4] is one (and all other dummies are zero) and that this results in an additional 8.5 sales.</p>

<table>  
<tbody><tr>  
    <th style=""font-weight:bold"">GRP interval</th>
    <th style=""font-weight:bold"">Additional sales</th>
</tr>  
<tr>  
    <td>0.1 - 2.4</td>
    <td>0.3</td>
</tr>  
<tr>  
    <td>2.5 - 4.9</td>
    <td>2.5</td>
</tr>  
<tr>  
    <td>5 - 7.4</td>
    <td>8.5</td>
</tr>  
<tr>  
    <td>7.5 - 10</td>
    <td>10.1</td>
</tr>  
</tbody></table>

<h4 id=""findingthescurveusingthedummyvariables"">Finding the S-curve using the dummy variables</h4>

<p>When plotting the additional sales against the GRP intervals we obtain the points in Figure 4. The shape of the S-curve can already be seen in these points. It then is a simple task to find the S-curve that best fits these points. Note that in our example the S-curve we predicted fits the true S-curve (which we used for creating our fictional dataset) quite good because it is nearly the same <a href=""http://en.wikipedia.org/wiki/Logistic_function"">logistic function</a>. The small difference in curves can be explained due to the fact that we added noise to our fictional dataset to simulate reality.</p>

<p><img src=""http://i61.tinypic.com/io0ub5.png"" alt=""FIGURE 4"" class=""full-img""></p>

<h4 id=""finalresults"">Final results</h4>

<p>So, since we found our best S-curve transformation, we run the first OLS regression again but this time with our radio data transformed to fit the new S-curve. The resulted predictions of the model are plotted in Figure 5. Note that this regression now fits our dataset much better than the first regression we run. Therefore, we are able to obtain much better predictions of the effect of radio on sales! :-)</p>

<pre><code>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                  sales   R-squared:                       0.987  
Model:                            OLS   Adj. R-squared:                  0.984  
Method:                 Least Squares   F-statistic:                     258.4  
Date:                Thu, 23 Apr 2015   Prob (F-statistic):           2.56e-20  
Time:                        16:38:53   Log-Likelihood:                -18.808  
No. Observations:                  31   AIC:                             53.62  
Df Residuals:                      23   BIC:                             65.09  
Df Model:                           7  
Covariance Type:            nonrobust  
========================================================================================
                           coef    std err          t      P&gt;|t|      [95.0% Conf. Int.]
----------------------------------------------------------------------------------------
const                    8.1540      0.230     35.381      0.000         7.677     8.631  
radio_grp                0.9904      0.031     31.828      0.000         0.926     1.055  
seasonality_monday      -2.9477      0.326     -9.040      0.000        -3.622    -2.273  
seasonality_tuesday     -0.7462      0.347     -2.153      0.042        -1.463    -0.029  
seasonality_thursday    -4.8023      0.357    -13.463      0.000        -5.540    -4.064  
seasonality_friday      -4.0407      0.353    -11.454      0.000        -4.771    -3.311  
seasonality_saturday    -5.3139      0.353    -15.034      0.000        -6.045    -4.583  
seasonality_sunday      -6.0026      0.346    -17.364      0.000        -6.718    -5.287  
==============================================================================
Omnibus:                        0.371   Durbin-Watson:                   1.502  
Prob(Omnibus):                  0.831   Jarque-Bera (JB):                0.525  
Skew:                          -0.047   Prob(JB):                        0.769  
Kurtosis:                       2.370   Cond. No.                         27.0  
==============================================================================
</code></pre>

<p><img src=""http://i62.tinypic.com/juvaqp.png"" alt=""FIGURE 5"" class=""full-img""></p>

<h4 id=""optimizemarketingspend"">Optimize marketing spend</h4>

<p>A particular useful property of the S-curve is that it has several useful characteristics for optimization. For one, it is easier to find the point where your return on investment is maximized. The inflection point of the S-curve helps to find this optimal spend value. At the inflection point the derivative value of the S-curve is maximized. This implies that at this point the S-curve changes from increasing returns (i.e. increasing spend by 1% leads to an &gt;1% increase in sales) into diminishing returns (i.e.  increasing spend by 1% leads to an &lt;1% increase in sales). </p>

<p>The inflection point is therefore used as the minimum spend value because all spends below this value imply <em>underspending</em> as you can easily increase your ROI if you increase the spend up to the inflection point. In our example, the inflection point lies at 5 GRPs. Using less than 5 GRPs implies underspending because you can get more sales per euro spend if you use 5 GRPs. In a similar manner we can also find the <em>overspending</em> value. Recall that above the inflection point the S-curve shows diminishing returns. This implies that for every additional euro you spend more, fewer absolute additional sales are generated. In our example, using more than 7 or 8 GRPs is obvious overspending as the additional sales hardly increase when using more GRPs. </p>

<p>Finally, when we have response curves for each of the individual KPI drivers (such as TV, radio, display, paid search, etc.) it is possible to find the optimal spend for each individual driver using an easy-to-solve optimization problem. The result is an optimal marketing mix that maximizes the chosen KPIs.</p>

<h4 id=""finalremarks"">Final remarks</h4>

<p>This post provided a simple illustration of how we use S-curves to optimize the marketing spends of our clients. In practice however, the datasets are not as simple as in this illustration. For example, in reality various media channels show lagged effects (<a href=""http://upload.wikimedia.org/wikipedia/commons/2/24/Adstock1.png"">ad-stocks</a>) or only show <a href=""http://vignette2.wikia.nocookie.net/economics/images/d/dd/Marginal_Utility.JPG/revision/latest?cb=20060726191218"">diminishing returns</a>. We use advanced modelling and time series techniques such as ARIMA and VAR models to create Marketing Mix Models (MMM) that capture these effects and help our clients understand how their marketing spend can be optimized. We will elaborate more on the advanced techniques in future Geek posts!</p>
        ","A key focus of our Data Science team is to help our clients understand how their marketing spend affects their KPIs. In particular, we create models to understand the effect of individual marketing channels such as television or paid search ads on KPIs such as sales, visits and footfall. Knowing how marketing spend affects KPIs enables us to optimize the clients marketing spend for maximal result. In this Geek post I will give a fictional example on how we use S-curves to optimize radio spend. The final code of this fictional example can be found on Github as well.
The data
Let’s consider a simple example where we have the number of sales and the number of radio GRPs on a daily basis. We assume that radio only has an immediate positive effect on the number of sales the same day. This is obviously not true in real-life, because radio still shows a positive effect after several days. For now, such delayed effects are out of the scope of this post. The figure below provides an overview of how our fictional sales and radio dataset looks like. We see that in this example dataset there are two important factors that determine the number of sales on a day: the day of the week (seasonality) and the number of radio GRPs on that day (marketing spend).
Simple OLS regression
As we are interested in how radio affects the number of sales, we run a simple OLS regression to capture the relationship of marketing spend and seasonality on sales. The results of this regression are shown in the table below.
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                  sales   R-squared:                       0.920  
Model:                            OLS   Adj. R-squared:                  0.896  
Method:                 Least Squares   F-statistic:                     38.01  
Date:                Thu, 23 Apr 2015   Prob (F-statistic):           3.62e-11  
Time:                        15:00:50   Log-Likelihood:                -47.424  
No. Observations:                  31   AIC:                             110.8  
Df Residuals:                      23   BIC:                             122.3  
Df Model:                           7  
Covariance Type:            nonrobust  
========================================================================================
                           coef    std err          t      P>|t|      [95.0% Conf. Int.]
----------------------------------------------------------------------------------------
const                    7.4642      0.583     12.793      0.000         6.257     8.671  
radio_grp                1.1423      0.096     11.854      0.000         0.943     1.342  
seasonality_monday      -2.7826      0.820     -3.392      0.003        -4.480    -1.085  
seasonality_tuesday     -0.4115      0.869     -0.473      0.640        -2.210     1.387  
seasonality_thursday    -4.8585      0.903     -5.381      0.000        -6.726    -2.991  
seasonality_friday      -3.7702      0.886     -4.256      0.000        -5.603    -1.938  
seasonality_saturday    -4.9362      0.886     -5.572      0.000        -6.769    -3.104  
seasonality_sunday      -5.3039      0.872     -6.080      0.000        -7.109    -3.499  
==============================================================================
Omnibus:                        0.646   Durbin-Watson:                   1.188  
Prob(Omnibus):                  0.724   Jarque-Bera (JB):                0.739  
Skew:                          -0.265   Prob(JB):                        0.691  
Kurtosis:                       2.461   Cond. No.                         24.1  
==============================================================================
The result of this regression shows that the day of the week is a very important factor for the number of sales. We used Wednesday as reference day and added binary dummy variables for all other days. Note Wednesday is taken arbitrarily. The coefficients of these day-dummies can be interpreted as the number of sales this day has more (or less) compared to the number of sales on Wednesday (the reference day). For example, the Monday coefficient of –2.8 implies that there are 2.8 less absolute sales on Monday than on Wednesday. So, as all day dummy coefficients are negative, it follows that Wednesday is the best day of the week for the sales.
More interestingly, the result of the regression also gives a positive coefficient of 1.14 for the radio variable. This coefficient can be interpreted as a positive effect of 1.14 additional sales for each radio GRP used. Note that this also implies that the effect of radio is linear, i.e. 1 GRP results in 1.14 additional sales and 5 GRPs results in 5 times 1.14 (=5.7) additional sales.
The S-response curve
In reality, it is not often the case that radio has a linear effect on sales. KPI drivers such as television and radio, but also display and search ads tend to have diminishing returns. Wikipedia provides the following example about diminishing returns:
“A common sort of example is adding more workers to a job, such as assembling a car on a factory floor. At some point, adding more workers causes problems such as workers getting in each other's way or frequently finding themselves waiting for access to a part. Producing one more unit of output per unit of time will eventually cost increasingly more, due to inputs being used less and less effectively.”
In a similar manner, research has shown that initial advertising budget has little impact on sales. One possible reason for this is that a low advertising budget might result in your marketing not being noticable between all of the competitors marketing campaigns. Only after a certain budget spend threshold results are noticeable in the form of improved KPIs. Hence, the result is that the effect of marketing spend on KPI drivers such as radio typically follows an S-shaped curve. The figure below provides an example of an S-curve for radio spend.
Introducing dummy variables
The notion of the S-shaped curve conflicts with our earlier calculated linear effect of radio on sales. Therefore, it is very likely that we get a much more accurate model when we can account for the effect of the S-curve. One possible approach is to just try all possible transformations of our radio dataset to an S-curve. However, as the S-curve is defined by three parameter values, this implies trying a lot of different transformations. Therefore, we use a smarter approach to find the S-curve. That is, instead of adding a continuous variable that denotes the number of radio GRPs on a given day, we add binary dummies where each dummy represents an interval of GRPs. Given our example dataset where the maximum number of GRPs is 10, we add four binary dummies representing the GRP intervals [0.1-2.4], [2.5-4.9], [5-7.4] and [7.5-10]. Now, the dummy value of an interval is 1 if the number of GRPs of that day is in that interval, otherwise the value is zero. Below is a short snippet of the radio dataset we then obtain:
Date time Radio GRP Radio dummy
0.1 - 2.4 Radio dummy
2.5 - 4.9 Radio dummy
5.0 - 7.4 Radio dummy
7.5 - 10
2015-06-01 4 0 1 0 0
2015-06-02 0 0 0 0 0
2015-06-03 1 1 0 0 0
2015-06-04 2 1 0 0 0
2015-06-05 3 0 1 0 0
Using the new radio dummy variables we again run a standard OLS regression. The results of this regression are shown below:
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                  sales   R-squared:                       0.985  
Model:                            OLS   Adj. R-squared:                  0.978  
Method:                 Least Squares   F-statistic:                     134.6  
Date:                Thu, 23 Apr 2015   Prob (F-statistic):           4.28e-16  
Time:                        15:42:39   Log-Likelihood:                -21.183  
No. Observations:                  31   AIC:                             64.37  
Df Residuals:                      20   BIC:                             80.14  
Df Model:                          10  
Covariance Type:            nonrobust  
========================================================================================
                           coef    std err          t      P>|t|      [95.0% Conf. Int.]
----------------------------------------------------------------------------------------
const                    8.1073      0.298     27.175      0.000         7.485     8.730  
radio_dummy_0.1_2.5      0.2485      0.334      0.745      0.465        -0.448     0.945  
radio_dummy_2.5_5        1.9554      0.407      4.806      0.000         1.107     2.804  
radio_dummy_5_7.5        8.7667      0.415     21.146      0.000         7.902     9.632  
radio_dummy_7.5_10       9.3869      0.494     19.001      0.000         8.356    10.417  
seasonality_monday      -2.9030      0.405     -7.171      0.000        -3.747    -2.059  
seasonality_tuesday     -0.5771      0.401     -1.439      0.165        -1.413     0.259  
seasonality_thursday    -4.6460      0.430    -10.793      0.000        -5.544    -3.748  
seasonality_friday      -4.3809      0.441     -9.929      0.000        -5.301    -3.461  
seasonality_saturday    -5.2752      0.419    -12.602      0.000        -6.148    -4.402  
seasonality_sunday      -5.9470      0.422    -14.096      0.000        -6.827    -5.067  
==============================================================================
Omnibus:                        1.184   Durbin-Watson:                   1.621  
Prob(Omnibus):                  0.553   Jarque-Bera (JB):                0.590  
Skew:                           0.334   Prob(JB):                        0.744  
Kurtosis:                       3.107   Cond. No.                         8.27  
==============================================================================
The interpretation of the coefficients of the new radio dummy variables is slightly different than in our first regression. The coefficient of each dummy represents the additional sales when the number of GRPs is in the corresponding interval. For example, consider a given day on which 6 radio GRPs are used. This implies that the value of the dummy for the interval [5 – 7.4] is one (and all other dummies are zero) and that this results in an additional 8.5 sales.
GRP interval Additional sales
0.1 - 2.4 0.3
2.5 - 4.9 2.5
5 - 7.4 8.5
7.5 - 10 10.1
Finding the S-curve using the dummy variables
When plotting the additional sales against the GRP intervals we obtain the points in Figure 4. The shape of the S-curve can already be seen in these points. It then is a simple task to find the S-curve that best fits these points. Note that in our example the S-curve we predicted fits the true S-curve (which we used for creating our fictional dataset) quite good because it is nearly the same logistic function. The small difference in curves can be explained due to the fact that we added noise to our fictional dataset to simulate reality.
Final results
So, since we found our best S-curve transformation, we run the first OLS regression again but this time with our radio data transformed to fit the new S-curve. The resulted predictions of the model are plotted in Figure 5. Note that this regression now fits our dataset much better than the first regression we run. Therefore, we are able to obtain much better predictions of the effect of radio on sales! :-)
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                  sales   R-squared:                       0.987  
Model:                            OLS   Adj. R-squared:                  0.984  
Method:                 Least Squares   F-statistic:                     258.4  
Date:                Thu, 23 Apr 2015   Prob (F-statistic):           2.56e-20  
Time:                        16:38:53   Log-Likelihood:                -18.808  
No. Observations:                  31   AIC:                             53.62  
Df Residuals:                      23   BIC:                             65.09  
Df Model:                           7  
Covariance Type:            nonrobust  
========================================================================================
                           coef    std err          t      P>|t|      [95.0% Conf. Int.]
----------------------------------------------------------------------------------------
const                    8.1540      0.230     35.381      0.000         7.677     8.631  
radio_grp                0.9904      0.031     31.828      0.000         0.926     1.055  
seasonality_monday      -2.9477      0.326     -9.040      0.000        -3.622    -2.273  
seasonality_tuesday     -0.7462      0.347     -2.153      0.042        -1.463    -0.029  
seasonality_thursday    -4.8023      0.357    -13.463      0.000        -5.540    -4.064  
seasonality_friday      -4.0407      0.353    -11.454      0.000        -4.771    -3.311  
seasonality_saturday    -5.3139      0.353    -15.034      0.000        -6.045    -4.583  
seasonality_sunday      -6.0026      0.346    -17.364      0.000        -6.718    -5.287  
==============================================================================
Omnibus:                        0.371   Durbin-Watson:                   1.502  
Prob(Omnibus):                  0.831   Jarque-Bera (JB):                0.525  
Skew:                          -0.047   Prob(JB):                        0.769  
Kurtosis:                       2.370   Cond. No.                         27.0  
==============================================================================
Optimize marketing spend
A particular useful property of the S-curve is that it has several useful characteristics for optimization. For one, it is easier to find the point where your return on investment is maximized. The inflection point of the S-curve helps to find this optimal spend value. At the inflection point the derivative value of the S-curve is maximized. This implies that at this point the S-curve changes from increasing returns (i.e. increasing spend by 1% leads to an >1% increase in sales) into diminishing returns (i.e. increasing spend by 1% leads to an <1% increase in sales).
The inflection point is therefore used as the minimum spend value because all spends below this value imply underspending as you can easily increase your ROI if you increase the spend up to the inflection point. In our example, the inflection point lies at 5 GRPs. Using less than 5 GRPs implies underspending because you can get more sales per euro spend if you use 5 GRPs. In a similar manner we can also find the overspending value. Recall that above the inflection point the S-curve shows diminishing returns. This implies that for every additional euro you spend more, fewer absolute additional sales are generated. In our example, using more than 7 or 8 GRPs is obvious overspending as the additional sales hardly increase when using more GRPs.
Finally, when we have response curves for each of the individual KPI drivers (such as TV, radio, display, paid search, etc.) it is possible to find the optimal spend for each individual driver using an easy-to-solve optimization problem. The result is an optimal marketing mix that maximizes the chosen KPIs.
Final remarks
This post provided a simple illustration of how we use S-curves to optimize the marketing spends of our clients. In practice however, the datasets are not as simple as in this illustration. For example, in reality various media channels show lagged effects (ad-stocks) or only show diminishing returns. We use advanced modelling and time series techniques such as ARIMA and VAR models to create Marketing Mix Models (MMM) that capture these effects and help our clients understand how their marketing spend can be optimized. We will elaborate more on the advanced techniques in future Geek posts!","[Data Science, python]"
122,How to get Airbnb's JavaScript style guide working in WebStorm,/how-to-get-airbnbs-javascript-code-style-working-in-webstorm/,"
            <p><strong>Update 09/2016: <a href=""https://www.themarketingtechnologist.co/eslint-with-airbnb-javascript-style-guide-in-webstorm/"">Looking to set up ESLint instead of JSCS?</a></strong></p>

<p>Having a consistent style in your code is important for maintaining readablility and makes the code easier to understand.</p>

<p>You can use different code styles, the most important thing is to stick to one code style. We at Blue Mango decided to go with <a href=""https://github.com/airbnb/javascript"">Airbnb's style guide for JavaScript</a>. Simply because it appealed to us and is a solid, maintained base which is being improved by a great community.</p>

<h2 id=""applyingcodestyles"">Applying code styles</h2>

<p>You can read the style guide a couple of times and apply it to your code, but that is nearly impossible to keep checking all the rules from the top of your head. It takes a lot of time to so manually and it would be a lot better if we had a way check our documents automatically.</p>

<p>Luckily there are tools which can check our files for consistency. <a href=""http://jscs.info/overview.html"">JSCS</a> is a Node.js based tool which can check different style guides. The great thing about JSCS is that there is an Airbnb preset that checks all the rules Airbnb enforces.</p>

<h2 id=""gettingjscstowork"">Getting JSCS to work</h2>

<p>JSCS has a plugin available for WebStorm. It will validate our open JavaScript files with the Airbnb style guide and gives appropriate warnings and errors on the parts you can improve to your code.</p>

<p>First things first, we have to install JSCS. In order to able to run JSCS you need to <a href=""https://nodejs.org/"">install Node.js</a>. If you have Node.js installed you can install JSCS. Open your terminal and execute:</p>

<pre><code>npm install jscs -g  
</code></pre>

<p>This installs JSCS globally and can be executed everywhere on your system.</p>

<h2 id=""enablingjscsinwebstorm"">Enabling JSCS in WebStorm</h2>

<p>To enable style guide checking with JSCS in WebStorm, we need to install the plugin.</p>

<ol>
<li>Go to your preferences &gt; plugins.  </li>
<li>Click ""Browse repositories..."" in the bottom of the window.  </li>
<li>Search for ""jscs"" and click ""Install"".  </li>
<li>Restart WebStorm.</li>
</ol>

<h2 id=""settingupjscsforairbnb"">Setting up JSCS for Airbnb</h2>

<p>Go to your preferences &gt; Languages &amp; Frameworks &gt; JavaScript &gt; Code Quality Tools &gt; JSCS.</p>

<p><img src=""http://i.imgur.com/m7Npz89.jpg"" alt=""JSCS settings in WebStorm"" class=""full-img""></p>

<p>Choose your Node interpreter and JSCS package if they are not filled in yet.</p>

<p>At the bottom you can choose your code style preset, choose ""Airbnb"".</p>

<p>Your files will now be checked against the Airbnb code style.</p>

<h2 id=""correctingtheautomaticcodestyling"">Correcting the automatic code styling</h2>

<p>If your preferences, go to Editor &gt; Code Style &gt; JavaScript.</p>

<p>Make sure you have <strong>Use tab character</strong> unticked in the first tab and set the <strong>Tab size</strong> to <strong>2</strong>.</p>

<p>In the tab <strong>spaces</strong>, under ""Before Parentheses"" uncheck ""In function expression"".</p>

<p>This will enforce the correct indentation.</p>

<h2 id=""onwards"">Onwards</h2>

<p>Now your files will be checked by JSCS using the Airbnb style guide. The errors will be displayed inline by red squiggly lines.</p>

<p>We're still looking for a way to enforce the <em>auto formatting of WebStorm</em> to use a apply a few rules: </p>

<ol>
<li>Single-quotes instead of double-quotes.  </li>
<li>Line breaks after curly brackets.</li>
</ol>
        ","Update 09/2016: Looking to set up ESLint instead of JSCS?
Having a consistent style in your code is important for maintaining readablility and makes the code easier to understand.
You can use different code styles, the most important thing is to stick to one code style. We at Blue Mango decided to go with Airbnb's style guide for JavaScript. Simply because it appealed to us and is a solid, maintained base which is being improved by a great community.
Applying code styles
You can read the style guide a couple of times and apply it to your code, but that is nearly impossible to keep checking all the rules from the top of your head. It takes a lot of time to so manually and it would be a lot better if we had a way check our documents automatically.
Luckily there are tools which can check our files for consistency. JSCS is a Node.js based tool which can check different style guides. The great thing about JSCS is that there is an Airbnb preset that checks all the rules Airbnb enforces.
Getting JSCS to work
JSCS has a plugin available for WebStorm. It will validate our open JavaScript files with the Airbnb style guide and gives appropriate warnings and errors on the parts you can improve to your code.
First things first, we have to install JSCS. In order to able to run JSCS you need to install Node.js. If you have Node.js installed you can install JSCS. Open your terminal and execute:
npm install jscs -g  
This installs JSCS globally and can be executed everywhere on your system.
Enabling JSCS in WebStorm
To enable style guide checking with JSCS in WebStorm, we need to install the plugin.
Go to your preferences > plugins.
Click ""Browse repositories..."" in the bottom of the window.
Search for ""jscs"" and click ""Install"".
Restart WebStorm.
Setting up JSCS for Airbnb
Go to your preferences > Languages & Frameworks > JavaScript > Code Quality Tools > JSCS.
Choose your Node interpreter and JSCS package if they are not filled in yet.
At the bottom you can choose your code style preset, choose ""Airbnb"".
Your files will now be checked against the Airbnb code style.
Correcting the automatic code styling
If your preferences, go to Editor > Code Style > JavaScript.
Make sure you have Use tab character unticked in the first tab and set the Tab size to 2.
In the tab spaces, under ""Before Parentheses"" uncheck ""In function expression"".
This will enforce the correct indentation.
Onwards
Now your files will be checked by JSCS using the Airbnb style guide. The errors will be displayed inline by red squiggly lines.
We're still looking for a way to enforce the auto formatting of WebStorm to use a apply a few rules:
Single-quotes instead of double-quotes.
Line breaks after curly brackets.",[Code]
123,Masterclass mobile? Practise what you preach!,/masterclass-mobile-practise-what-you-preach/,"
            <p><mark>This post has been <a href=""http://www.marketingfacts.nl/berichten/masterclass-mobile-fix-eerst-de-basics"">previously published</a> on Marketingfacts. </mark></p>

<p>Last week I saw a TV commercial about a master class Mobile Marketing by business university Nyenrode and RTL Z, one of the biggest Dutch TV channels. In the commercial, a lady looked me straight in the eye and asked: ""Do you want to know what kind of results online and mobile marketing can produce for you?"". Given my profession, I was intrigued. </p>

<p>I couldn’t remember the specific URL mentioned in the TVC, so I started searching on my mobile device. My first query, 'masterclass mobile marketing' (master class is written as masterclass in Dutch), produced the results below. Two ads from a different university, but none from RTL Z or Nyenrode. I had to scroll down in the organic search results to see a hit for RTL Z. </p>

<p>To make the query a bit more specific, I added ‘RTL’. The results: still two ads from competing universities. In the organic results, I find a website by RTL. Just by looking at the title and visible URL, I can't tell whether this is the page I'm looking for or not, but the text snippet leads me to believe this might be the page I'm looking for. </p>

<p><img src=""http://www.marketingfacts.nl/images/blogimages/Masterclass-mobile-Fix-eerst-de-basics!-afbeelding-1.jpg"" alt="""" class=""full-img""></p>

<p>No harm done, but these are two lost opportunities in a row. By not buying ads for the non-branded term 'masterclass mobile marketing', a portion of the people searching for the master class will end up at one of the competitors' websites, not at RTL Z's. </p>

<p>Moreover, RTL could have made the title, visible URL and text snippet more appealing, if they would have advertised with the branded search query 'RTL masterclass mobile marketing'. By doing so, they would have been able to optimize and regulate traffic for a relatively small budget. This could increase the CTR and the traffic to the website and, eventually, positively influence the amount of sign-ups. </p>

<p>If there's a connection between your TV commercials and the number of search queries on your brand, I would suggest to at least advertise on Google around and during the time your TVC is broadcasted, using branded terms. Adjusting to your TVC broadcast schedule is very easy with <em>custom ad scheduling</em>. You can find the <em>Ad schedule</em> section at the settings of your campaign. </p>

<p>Using the <em>adschedule button</em>, you can specify the day and time you want your ads to be shown. Moreover, you can adjust your bidding price for device types. Research on TV attribution indicates that synergistic effects are best achieved when you combine TV with tablet and mobile. So that's exactly where your ads should show up.</p>

<p>But hey, I found what I was looking for, or so I thought, so I clicked. Once again I got disappointed. The website was not optimized for mobile, nor was it responsive. Luckily, there was a video. A fine format for mobile devices. When I wanted to view the video, I got really sad. Instead of opening the video full screen on my iPhone, the thumbnail changed into a black box telling me to go download Silverlight. Not a hit, to be sure. </p>

<p><img src=""http://www.marketingfacts.nl/images/blogimages/Masterclass-mobile-Fix-eerst-de-basics!-afbeelding-2.jpg"" alt="""">
<em>The text next to the video in the top image roughly translates to '75% of Dutch people use a smartphone. This offers marketeers plenty of opportunities to approach them in a new way.'.</em></p>

<p>I wasn’t expecting much at this point, so I wasn’t surprised to find that the sign-up form of the <em>mobile</em> master class wasn't mobile-friendly either. </p>

<p>Of course RTL and Neyenrode aren’t the only ones that don’t have their mobile approach figured out. I see it happening all the time, and I could give you tons of examples. Even examples of big advertisers spending big on mobile advertising. But those parties aren’t advertising mobile marketing master classes on TV. You can organize master classes for all kinds of stuff, but you should really start with the basics yourself. </p>

<p>In conclusion: there is a lot to learn in the field of mobile marketing, but not from RTL and Neyenrode, so it seems.</p>

<p><strong>Update:</strong> Nyenrode commented on this blog saying they are working on a new platform, and that  the issues we pointed out will be fixed. </p>

<p><em>This post was translated by <a href=""http://geek.bluemangointeractive.com/author/siebe/"">Siebe Hiemstra</a></em>.</p>
        ","This post has been previously published on Marketingfacts.
Last week I saw a TV commercial about a master class Mobile Marketing by business university Nyenrode and RTL Z, one of the biggest Dutch TV channels. In the commercial, a lady looked me straight in the eye and asked: ""Do you want to know what kind of results online and mobile marketing can produce for you?"". Given my profession, I was intrigued.
I couldn’t remember the specific URL mentioned in the TVC, so I started searching on my mobile device. My first query, 'masterclass mobile marketing' (master class is written as masterclass in Dutch), produced the results below. Two ads from a different university, but none from RTL Z or Nyenrode. I had to scroll down in the organic search results to see a hit for RTL Z.
To make the query a bit more specific, I added ‘RTL’. The results: still two ads from competing universities. In the organic results, I find a website by RTL. Just by looking at the title and visible URL, I can't tell whether this is the page I'm looking for or not, but the text snippet leads me to believe this might be the page I'm looking for.
No harm done, but these are two lost opportunities in a row. By not buying ads for the non-branded term 'masterclass mobile marketing', a portion of the people searching for the master class will end up at one of the competitors' websites, not at RTL Z's.
Moreover, RTL could have made the title, visible URL and text snippet more appealing, if they would have advertised with the branded search query 'RTL masterclass mobile marketing'. By doing so, they would have been able to optimize and regulate traffic for a relatively small budget. This could increase the CTR and the traffic to the website and, eventually, positively influence the amount of sign-ups.
If there's a connection between your TV commercials and the number of search queries on your brand, I would suggest to at least advertise on Google around and during the time your TVC is broadcasted, using branded terms. Adjusting to your TVC broadcast schedule is very easy with custom ad scheduling. You can find the Ad schedule section at the settings of your campaign.
Using the adschedule button, you can specify the day and time you want your ads to be shown. Moreover, you can adjust your bidding price for device types. Research on TV attribution indicates that synergistic effects are best achieved when you combine TV with tablet and mobile. So that's exactly where your ads should show up.
But hey, I found what I was looking for, or so I thought, so I clicked. Once again I got disappointed. The website was not optimized for mobile, nor was it responsive. Luckily, there was a video. A fine format for mobile devices. When I wanted to view the video, I got really sad. Instead of opening the video full screen on my iPhone, the thumbnail changed into a black box telling me to go download Silverlight. Not a hit, to be sure.
The text next to the video in the top image roughly translates to '75% of Dutch people use a smartphone. This offers marketeers plenty of opportunities to approach them in a new way.'.
I wasn’t expecting much at this point, so I wasn’t surprised to find that the sign-up form of the mobile master class wasn't mobile-friendly either.
Of course RTL and Neyenrode aren’t the only ones that don’t have their mobile approach figured out. I see it happening all the time, and I could give you tons of examples. Even examples of big advertisers spending big on mobile advertising. But those parties aren’t advertising mobile marketing master classes on TV. You can organize master classes for all kinds of stuff, but you should really start with the basics yourself.
In conclusion: there is a lot to learn in the field of mobile marketing, but not from RTL and Neyenrode, so it seems.
Update: Nyenrode commented on this blog saying they are working on a new platform, and that the issues we pointed out will be fixed.
This post was translated by Siebe Hiemstra.","[marketing, mobile]"
124,Introducing Formagical.JS: form analytics without dashboarding,/introducing-formagical-form-analytics-with-dashboarding/,"
            <p>Just a few months ago I was looking for a tool to gain insights on how visitors use the forms on our websites. I only had a few requirements. The tool or service should:</p>

<ul>
<li>be easy to implement;</li>
<li>be free, or at least have a fixed price (I prefer not to pay for every <em>x</em> visitors, because the numbers on our websites can get really high);</li>
<li>have raw data available;</li>
<li>able to connect to existing analytics tools we already use (Google Analytics, Snowplow, SiteCatalyst).</li>
</ul>

<p>I came across a lot of really, really cool tools and services, and most of them had comprehensive dashboarding. Unfortunately, most of them turned out to be pretty expensive and do not share the raw data. So in the end, none of them met my requirements. </p>

<p>After a few beers on a friday night, I decided to build something myself. </p>

<h3 id=""basicconcept"">Basic concept</h3>

<p>The concept of tracking user behavior on a form is pretty easy. At least from a technical perspective. It comes down to proxying interaction events to a remote endpoint, and maybe send some extra metadata along, like time spend. And that's exactly what <a href=""https://github.com/codeorelse/formagical"">Formagical.JS</a> does. Nothing more, nothing less. </p>

<p>We (well, mostly the data scientists) are very interested in the raw data, and we are willing to do the analysis ourselves. We also don't need fancy dashboarding, because it's already included in the tools we have been using for years. I think those expensive tools and services I've been checking out are mainly charging for the dashboards, the analysis itself and storing data, not for the collection of data.</p>

<p>So, Formagical.JS only sends user events to a remote endpoint. That's all it does, no rocket science. The analysis and reporting of all the raw data is up to you.</p>

<h3 id=""whatdoesformagicaltrackexactly"">What does Formagical track exactly?</h3>

<p>Formagical tracks all of the following interactions on your form:</p>

<ul>
<li>user opened a page that has Formagical.JS on it</li>
<li>user started using form by interacting with one of its elements</li>
<li>user enters a certain form element (focus)</li>
<li>user leaves a certain form element (unfocus)</li>
<li>user starts typing in a certain element (input and text area only)</li>
<li>users changes the selection of a certain form element (dropdown, radio button and checkbox only)</li>
<li>users pauses typing and continues</li>
<li>user submits form</li>
</ul>

<p>Every interaction (event) will be individually send to the remote endpoint (Google Analytics by default). With all these interactions, Formagical keeps track of the time that each event takes. </p>

<p>With this data, you can simply answer questions like: How many percent of the users who visit the page start using the form? Which form element take most of the users' time and is most likely to be a conversion bottleneck? How many people are filling in their email address? I'm sure you get the idea. </p>

<h3 id=""outoftheboxintegrationwithgoogleanalytics"">Out-of-the-box integration with Google Analytics</h3>

<p>When you already have a Google Analytics account configured on your website, Formagical.JS will send events to that account out of the box. </p>

<p>Events show up like this, and can be treated like any other event:</p>

<p><img src=""http://oi61.tinypic.com/oa63qc.jpg"" alt=""alt""></p>

<p>All events for a specific form have a <em>category</em> value of <em>Formagical_{name of the form}</em>.  </p>

<h3 id=""howdoiuseit"">How do I use it?</h3>

<p>Using Formagical is very easy. Make sure you download the latest copy from the <a href=""https://github.com/codeorelse/formagical"">Github repository</a> and save the file somewhere in your project. Add this code to your main template file: </p>

<pre><code class=""language- prettyprint lang-js prettyprinted""><span class=""pun"">&lt;</span><span class=""pln"">script src</span><span class=""pun"">=</span><span class=""str"">""path/to/formagical.min.js""</span><span class=""pln""> </span><span class=""pun"">/&gt;</span><span class=""pln"">  
</span><span class=""pun"">&lt;</span><span class=""pln"">script</span><span class=""pun"">&gt;</span><span class=""pln"">  
    $</span><span class=""pun"">(</span><span class=""str"">'#your-form'</span><span class=""pun"">).</span><span class=""pln"">formagical</span><span class=""pun"">();</span><span class=""pln"">
</span><span class=""pun"">&lt;/</span><span class=""pln"">script</span><span class=""pun"">&gt;</span><span class=""pln"">  </span></code></pre>

<p>Unfortunately, Formagical.JS is only available as a jQuery plugin for now, so you'll need to include jQuery as well if you haven't already done so. When you've implemented Google Analytics and all of your form elements have a <em>name</em> attribute, you're good to go! </p>

<h3 id=""writingacustomtracker"">Writing a custom tracker</h3>

<p>As I mentioned before, Formagical sends events to the Google Analytics account that is implemented on the page. You might want to send the data to another endpoint, like Snowplow, Site Catalyst or your custom endpoint. Well, that's pretty easy. You can overwrite the track method when initiating the Formagical plugin.</p>

<pre><code class=""language- prettyprint lang-js prettyprinted""><span class=""pun"">(</span><span class=""kwd"">function</span><span class=""pun"">(</span><span class=""pln"">formagical</span><span class=""pun"">)</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">
formagical</span><span class=""pun"">.</span><span class=""pln"">yourTracker </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""kwd"">function</span><span class=""pun"">(</span><span class=""pln"">element</span><span class=""pun"">,</span><span class=""pln""> typeOfInteraction</span><span class=""pun"">,</span><span class=""pln""> duration</span><span class=""pun"">)</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">  
  </span><span class=""com"">// manipulate and send the data to an andpoint of choice</span><span class=""pln"">
</span><span class=""pun"">}})(</span><span class=""pln"">window</span><span class=""pun"">.</span><span class=""pln"">formagical </span><span class=""pun"">=</span><span class=""pln""> window</span><span class=""pun"">.</span><span class=""pln"">formagical </span><span class=""pun"">||</span><span class=""pln""> </span><span class=""pun"">{});</span><span class=""pln"">

</span><span class=""com"">/* Pass your custom tracker when initiating Formagical */</span><span class=""pln"">
$</span><span class=""pun"">(</span><span class=""str"">'#your-form'</span><span class=""pun"">).</span><span class=""pln"">formagical</span><span class=""pun"">({</span><span class=""pln"">track</span><span class=""pun"">:</span><span class=""pln""> window</span><span class=""pun"">.</span><span class=""pln"">formagical</span><span class=""pun"">.</span><span class=""pln"">yourTracker</span><span class=""pun"">});</span></code></pre>

<h3 id=""whatsnext"">What's next?</h3>

<p>I wrote Formagical after a few beers on a Friday night, so there are a lot of things I want to improve. These are the first things I'd like to address:</p>

<ul>
<li>Start over from scratch with proper code</li>
<li>Take error messages into account when submitting the form</li>
<li>Use vanilla JavaScript instead of a jQuery plugin</li>
</ul>

<p>My friend and co-worker Erik Driessen, who is a web analyst at Blue Mango, will soon write a post on how to analyze the data that Formagical.JS sends to Google Analytics, and how to improve your form usability based on this data.</p>

<h3 id=""howdoigetit"">How do I get it?</h3>

<p>You can go over to the <a href=""https://github.com/codeorelse/formagical"">Github repository</a> and grab the code over there. There is also some more implementation documentation available on Github. Please note that I released it under the MIT license, which permits reuse within proprietary software, but allows reuse as long as your software/service is free of charge.</p>
        ","Just a few months ago I was looking for a tool to gain insights on how visitors use the forms on our websites. I only had a few requirements. The tool or service should:
be easy to implement;
be free, or at least have a fixed price (I prefer not to pay for every x visitors, because the numbers on our websites can get really high);
have raw data available;
able to connect to existing analytics tools we already use (Google Analytics, Snowplow, SiteCatalyst).
I came across a lot of really, really cool tools and services, and most of them had comprehensive dashboarding. Unfortunately, most of them turned out to be pretty expensive and do not share the raw data. So in the end, none of them met my requirements.
After a few beers on a friday night, I decided to build something myself.
Basic concept
The concept of tracking user behavior on a form is pretty easy. At least from a technical perspective. It comes down to proxying interaction events to a remote endpoint, and maybe send some extra metadata along, like time spend. And that's exactly what Formagical.JS does. Nothing more, nothing less.
We (well, mostly the data scientists) are very interested in the raw data, and we are willing to do the analysis ourselves. We also don't need fancy dashboarding, because it's already included in the tools we have been using for years. I think those expensive tools and services I've been checking out are mainly charging for the dashboards, the analysis itself and storing data, not for the collection of data.
So, Formagical.JS only sends user events to a remote endpoint. That's all it does, no rocket science. The analysis and reporting of all the raw data is up to you.
What does Formagical track exactly?
Formagical tracks all of the following interactions on your form:
user opened a page that has Formagical.JS on it
user started using form by interacting with one of its elements
user enters a certain form element (focus)
user leaves a certain form element (unfocus)
user starts typing in a certain element (input and text area only)
users changes the selection of a certain form element (dropdown, radio button and checkbox only)
users pauses typing and continues
user submits form
Every interaction (event) will be individually send to the remote endpoint (Google Analytics by default). With all these interactions, Formagical keeps track of the time that each event takes.
With this data, you can simply answer questions like: How many percent of the users who visit the page start using the form? Which form element take most of the users' time and is most likely to be a conversion bottleneck? How many people are filling in their email address? I'm sure you get the idea.
Out-of-the-box integration with Google Analytics
When you already have a Google Analytics account configured on your website, Formagical.JS will send events to that account out of the box.
Events show up like this, and can be treated like any other event:
All events for a specific form have a category value of Formagical_{name of the form}.
How do I use it?
Using Formagical is very easy. Make sure you download the latest copy from the Github repository and save the file somewhere in your project. Add this code to your main template file:
<script src=""path/to/formagical.min.js"" />  
<script>  
    $('#your-form').formagical();
</script>  
Unfortunately, Formagical.JS is only available as a jQuery plugin for now, so you'll need to include jQuery as well if you haven't already done so. When you've implemented Google Analytics and all of your form elements have a name attribute, you're good to go!
Writing a custom tracker
As I mentioned before, Formagical sends events to the Google Analytics account that is implemented on the page. You might want to send the data to another endpoint, like Snowplow, Site Catalyst or your custom endpoint. Well, that's pretty easy. You can overwrite the track method when initiating the Formagical plugin.
(function(formagical) {
formagical.yourTracker = function(element, typeOfInteraction, duration) {  
  // manipulate and send the data to an andpoint of choice
}})(window.formagical = window.formagical || {});

/* Pass your custom tracker when initiating Formagical */
$('#your-form').formagical({track: window.formagical.yourTracker});
What's next?
I wrote Formagical after a few beers on a Friday night, so there are a lot of things I want to improve. These are the first things I'd like to address:
Start over from scratch with proper code
Take error messages into account when submitting the form
Use vanilla JavaScript instead of a jQuery plugin
My friend and co-worker Erik Driessen, who is a web analyst at Blue Mango, will soon write a post on how to analyze the data that Formagical.JS sends to Google Analytics, and how to improve your form usability based on this data.
How do I get it?
You can go over to the Github repository and grab the code over there. There is also some more implementation documentation available on Github. Please note that I released it under the MIT license, which permits reuse within proprietary software, but allows reuse as long as your software/service is free of charge.",[Analytics]
125,Webstorm 10 improves the performance of file indexing,/webstorm-10-improves-the-performance-of-indexing-files/,"
            <p>After having used Visual Studio for years, we switched to <a href=""https://www.jetbrains.com/webstorm"">Webstorm</a> in 2014. We're all a big fan of this IDE: the code assistance and debugging/testings options are truly magnificent. The only big downside of the tool is when your projects starts to grow and you've got a lot of Node dependencies in your project. The IDE freezes all the time, making it impossibile to work with. Although there are some ways to exclude your Node modules from being indexed, I choose to code in Sublime when working on large projects.</p>

<p>When I was reading up on the <a href=""https://www.jetbrains.com/webstorm/whatsnew/"">new features</a> of the recently released version 10, I was a bit dissapointed. Sure, a distraction-free mode is nice, and simultaneous HTML tag editing can save you some time, but they are not really reasons for me to upgrade.  </p>

<p>So, what was the real big improvement in this new major update? Turns out they completely rebuilt support for JavaScript. This includes the way they handle the indexing of files. This makes the IDE work much smoother with large projects. On top of that, you'll have faster code highlighting and code completion suggestions. Cool!</p>

<p>I've tried to work with some of our larger codebases that previously made Webstorm freeze, and it turns out that these projects indeed work a lot smoother in version 10! I'm not sure of how they changed the inner workings of the indexing proces, but I think they did an awesome job. There is still some room for improvement. Maybe they can <a href=""https://youtrack.jetbrains.com/issue/WEB-11419"">only index direct dependencies</a>, instead of also indexing the transitive ones?</p>

<p>There are some other neat new features in Webstorm 10 that may strike your fancy. Like built-in support for Typescript 1.4 and 1.5, V8 profiling for Node.js apps, and the possibility to trace languages compiled to JavaScript. </p>
        ","After having used Visual Studio for years, we switched to Webstorm in 2014. We're all a big fan of this IDE: the code assistance and debugging/testings options are truly magnificent. The only big downside of the tool is when your projects starts to grow and you've got a lot of Node dependencies in your project. The IDE freezes all the time, making it impossibile to work with. Although there are some ways to exclude your Node modules from being indexed, I choose to code in Sublime when working on large projects.
When I was reading up on the new features of the recently released version 10, I was a bit dissapointed. Sure, a distraction-free mode is nice, and simultaneous HTML tag editing can save you some time, but they are not really reasons for me to upgrade.
So, what was the real big improvement in this new major update? Turns out they completely rebuilt support for JavaScript. This includes the way they handle the indexing of files. This makes the IDE work much smoother with large projects. On top of that, you'll have faster code highlighting and code completion suggestions. Cool!
I've tried to work with some of our larger codebases that previously made Webstorm freeze, and it turns out that these projects indeed work a lot smoother in version 10! I'm not sure of how they changed the inner workings of the indexing proces, but I think they did an awesome job. There is still some room for improvement. Maybe they can only index direct dependencies, instead of also indexing the transitive ones?
There are some other neat new features in Webstorm 10 that may strike your fancy. Like built-in support for Typescript 1.4 and 1.5, V8 profiling for Node.js apps, and the possibility to trace languages compiled to JavaScript.",[Code]
126,Data collection and strange values in CSV format,/data-collection-and-strange-values-in-csv-format/,"
            <p>When you start a new innovation project your data is not always in a structured database. Most of the times you need to import a CSV file for a quick analysis. Thereafter you can <a href=""http://geek.bluemangointeractive.com/calculating-ad-stocks-in-a-fast-and-readable-way-in-python/"">manipulate your data</a>. Finally you can start data modelling. In one of my last projects I was working on a proof of concept for TV attribution. TV attribution determines the interaction between TV commercials and online visits. Therefore we need TV schedules and online visits on minute level. Our online data is in a database, but TV schedules were not. </p>

<h4 id=""strangevalues"">Strange values</h4>

<p>The first time I was reading the TV schedules, I saw data that I didn’t expected to see. Below you can see a small sample of the output. We see some strange values like ‘--’ instead of a date and times above 24 hours. </p>

<pre><code class=""language-prettyprint lang-python prettyprinted""><span class=""com"">######################</span><span class=""pln"">
</span><span class=""str"">""""""import libraries""""""</span><span class=""pln"">
</span><span class=""com"">######################</span><span class=""pln"">
</span><span class=""kwd"">import</span><span class=""pln""> pandas </span><span class=""kwd"">as</span><span class=""pln""> pd  
</span><span class=""kwd"">import</span><span class=""pln""> numpy </span><span class=""kwd"">as</span><span class=""pln""> np  
</span><span class=""kwd"">import</span><span class=""pln""> datetime  
</span><span class=""kwd"">import</span><span class=""pln""> calendar  
</span><span class=""kwd"">import</span><span class=""pln""> os

</span><span class=""com"">##################################</span><span class=""pln"">
</span><span class=""str"">""""""read data for the first time""""""</span><span class=""pln"">
</span><span class=""com"">##################################</span><span class=""pln"">
</span><span class=""com"">#define</span><span class=""pln""> file_path
path </span><span class=""pun"">=</span><span class=""pln""> os</span><span class=""pun"">.</span><span class=""pln"">path</span><span class=""pun"">.</span><span class=""pln"">dirname</span><span class=""pun"">(</span><span class=""pln"">os</span><span class=""pun"">.</span><span class=""pln"">path</span><span class=""pun"">.</span><span class=""pln"">realpath</span><span class=""pun"">(</span><span class=""pln"">__file__</span><span class=""pun"">))</span><span class=""pln"">  
file_name </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""str"">'tv_schedule.csv'</span><span class=""pln"">  
filepath_or_buffer </span><span class=""pun"">=</span><span class=""pln"">  path </span><span class=""pun"">+</span><span class=""pln""> </span><span class=""str"">'\\'</span><span class=""pln""> </span><span class=""pun"">+</span><span class=""pln""> file_name  
</span><span class=""com"">#define</span><span class=""pln""> </span><span class=""kwd"">if</span><span class=""pln""> head </span><span class=""kwd"">is</span><span class=""pln""> </span><span class=""kwd"">in</span><span class=""pln""> csv
header_boolean </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""kwd"">True</span><span class=""pln"">  
</span><span class=""com"">#define</span><span class=""pln""> header input
</span><span class=""kwd"">if</span><span class=""pln""> header_boolean </span><span class=""pun"">==</span><span class=""pln""> </span><span class=""kwd"">True</span><span class=""pun"">:</span><span class=""pln"">  
    header_csv </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""kwd"">int</span><span class=""pun"">(</span><span class=""lit"">0</span><span class=""pun"">)</span><span class=""pln"">
</span><span class=""kwd"">else</span><span class=""pun"">:</span><span class=""pln"">  
    header_csv </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""kwd"">None</span><span class=""pln"">
</span><span class=""com"">#read data</span><span class=""pln"">
data </span><span class=""pun"">=</span><span class=""pln""> pd</span><span class=""pun"">.</span><span class=""pln"">read_csv</span><span class=""pun"">(</span><span class=""pln"">filepath_or_buffer</span><span class=""pun"">=</span><span class=""pln"">filepath_or_buffer</span><span class=""pun"">,</span><span class=""pln"">  
                   header</span><span class=""pun"">=</span><span class=""pln"">header_csv</span><span class=""pun"">)</span><span class=""pln"">
</span><span class=""com"">#print data</span><span class=""pln"">
</span><span class=""kwd"">print</span><span class=""pln""> </span><span class=""str"">'first outcome: \n'</span><span class=""pun"">,</span><span class=""pln""> data  </span></code></pre>

<pre><code class=""language-prettyprint lang-python prettyprinted""><span class=""pln"">first outcome</span><span class=""pun"">:</span><span class=""pln"">  
          date      time     channel   grp
</span><span class=""lit"">0</span><span class=""pln"">           </span><span class=""pun"">--</span><span class=""pln"">        xx        NPO1   </span><span class=""lit"">2.0</span><span class=""pln"">  
</span><span class=""lit"">1</span><span class=""pln"">           </span><span class=""pun"">--</span><span class=""pln"">        xx        NPO1   </span><span class=""lit"">1.2</span><span class=""pln"">  
</span><span class=""lit"">2</span><span class=""pln"">   </span><span class=""lit"">30</span><span class=""pun"">-</span><span class=""lit"">11</span><span class=""pun"">-</span><span class=""lit"">2014</span><span class=""pln"">  </span><span class=""lit"">24</span><span class=""pun"">:</span><span class=""lit"">23</span><span class=""pun"">:</span><span class=""lit"">00</span><span class=""pln"">        NPO2   </span><span class=""lit"">1.8</span><span class=""pln"">  
</span><span class=""lit"">3</span><span class=""pln"">   </span><span class=""lit"">30</span><span class=""pun"">-</span><span class=""lit"">12</span><span class=""pun"">-</span><span class=""lit"">2014</span><span class=""pln"">        xx  TV </span><span class=""typ"">Gelderl</span><span class=""pln"">   </span><span class=""lit"">0.1</span><span class=""pln"">  
</span><span class=""lit"">4</span><span class=""pln"">   </span><span class=""lit"">30</span><span class=""pun"">-</span><span class=""lit"">12</span><span class=""pun"">-</span><span class=""lit"">2014</span><span class=""pln"">        xx       TV </span><span class=""typ"">Nh</span><span class=""pln"">   </span><span class=""lit"">0.1</span><span class=""pln"">  
</span><span class=""lit"">5</span><span class=""pln"">   </span><span class=""lit"">30</span><span class=""pun"">-</span><span class=""lit"">12</span><span class=""pun"">-</span><span class=""lit"">2014</span><span class=""pln"">     </span><span class=""lit"">19</span><span class=""pun"">:</span><span class=""lit"">23</span><span class=""pln"">        RTL4   </span><span class=""lit"">0.4</span><span class=""pln"">  
</span><span class=""lit"">6</span><span class=""pln"">   </span><span class=""lit"">30</span><span class=""pun"">-</span><span class=""lit"">12</span><span class=""pun"">-</span><span class=""lit"">2014</span><span class=""pln"">  </span><span class=""lit"">24</span><span class=""pun"">:</span><span class=""lit"">46</span><span class=""pun"">:</span><span class=""lit"">00</span><span class=""pln"">     TV </span><span class=""typ"">West</span><span class=""pln"">   </span><span class=""lit"">0.1</span><span class=""pln"">  
</span><span class=""lit"">7</span><span class=""pln"">   </span><span class=""lit"">30</span><span class=""pun"">-</span><span class=""lit"">12</span><span class=""pun"">-</span><span class=""lit"">2014</span><span class=""pln"">        xx   O</span><span class=""pun"">.</span><span class=""typ"">Brabant</span><span class=""pln"">   </span><span class=""lit"">0.1</span><span class=""pln"">  
</span><span class=""lit"">8</span><span class=""pln"">           </span><span class=""pun"">--</span><span class=""pln"">        xx    </span><span class=""typ"">Rijnmond</span><span class=""pln"">   </span><span class=""lit"">0.0</span><span class=""pln"">  
</span><span class=""lit"">9</span><span class=""pln"">   </span><span class=""lit"">31</span><span class=""pun"">-</span><span class=""lit"">12</span><span class=""pun"">-</span><span class=""lit"">2014</span><span class=""pln"">     </span><span class=""lit"">19</span><span class=""pun"">:</span><span class=""lit"">54</span><span class=""pln"">        SBS6   </span><span class=""lit"">0.2</span><span class=""pln"">  
</span><span class=""lit"">10</span><span class=""pln"">  </span><span class=""lit"">31</span><span class=""pun"">-</span><span class=""lit"">12</span><span class=""pun"">-</span><span class=""lit"">2014</span><span class=""pln"">     </span><span class=""lit"">20</span><span class=""pun"">:</span><span class=""lit"">21</span><span class=""pln"">        NPO1  </span><span class=""lit"">11.1</span><span class=""pln"">  
</span><span class=""lit"">11</span><span class=""pln"">  </span><span class=""lit"">31</span><span class=""pun"">-</span><span class=""lit"">12</span><span class=""pun"">-</span><span class=""lit"">2014</span><span class=""pln"">        xx  TV </span><span class=""typ"">Gelderl</span><span class=""pln"">   </span><span class=""lit"">0.1</span><span class=""pln"">  
</span><span class=""lit"">12</span><span class=""pln"">  </span><span class=""lit"">31</span><span class=""pun"">-</span><span class=""lit"">12</span><span class=""pun"">-</span><span class=""lit"">2014</span><span class=""pln"">        xx     TV </span><span class=""typ"">Oost</span><span class=""pln"">   </span><span class=""lit"">0.2</span><span class=""pln"">  
</span><span class=""lit"">13</span><span class=""pln"">  </span><span class=""lit"">31</span><span class=""pun"">-</span><span class=""lit"">12</span><span class=""pun"">-</span><span class=""lit"">2014</span><span class=""pln"">        xx  TV </span><span class=""typ"">Zeeland</span><span class=""pln"">   </span><span class=""lit"">0.1</span><span class=""pln"">  
</span><span class=""lit"">14</span><span class=""pln"">  </span><span class=""lit"">31</span><span class=""pun"">-</span><span class=""lit"">12</span><span class=""pun"">-</span><span class=""lit"">2014</span><span class=""pln"">  </span><span class=""lit"">25</span><span class=""pun"">:</span><span class=""lit"">24</span><span class=""pun"">:</span><span class=""lit"">00</span><span class=""pln"">        NPO1   </span><span class=""lit"">0.8</span><span class=""pln"">

</span><span class=""pun"">[</span><span class=""lit"">15</span><span class=""pln""> rows x </span><span class=""lit"">4</span><span class=""pln""> columns</span><span class=""pun"">]</span></code></pre>

<p>To adjust the data we need to add some intelligence. First, we have to know which rows are ‘bad’, so we can skip them. Second, we have to parse the datetime values to a datetime format. So, let’s give it a try!</p>

<h4 id=""searchbadrows"">Search bad rows</h4>

<p>The first challenge is to search which rows you don’t want to import from the CSV file. With the help of Pandas.read_csv you can add which rows you want to skip. We will use that later on. One of the issues we can see is ‘--’ in the date column and ‘xx’ in the time column. Therefore we should indicate in which column we have an issue. </p>

<pre><code class=""language-prettyprint lang-python prettyprinted""><span class=""com"">###################</span><span class=""pln"">
</span><span class=""str"">""""""define issues""""""</span><span class=""pln"">
</span><span class=""com"">###################</span><span class=""pln"">
issues </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">  
    </span><span class=""str"">""issue_0""</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">
        </span><span class=""str"">""column""</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""lit"">0</span><span class=""pun"">,#</span><span class=""pln"">integer </span><span class=""kwd"">if</span><span class=""pln""> you know which column </span><span class=""pun"">(</span><span class=""pln"">imagine you have </span><span class=""kwd"">no</span><span class=""pln""> headers</span><span class=""pun"">)</span><span class=""pln"">
        </span><span class=""str"">""indicator_of_bad_row""</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""str"">'--'</span><span class=""pun"">},</span><span class=""pln"">
    </span><span class=""str"">""issue_1""</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">
        </span><span class=""str"">""column""</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""str"">'time'</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""com"">#string if you know the name of the column</span><span class=""pln"">
        </span><span class=""str"">""indicator_of_bad_row""</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""str"">'xx'</span><span class=""pun"">}</span><span class=""pln"">
</span><span class=""pun"">}</span></code></pre>

<p>Now we know the issues, we have to determine the ‘bad’ rows. Below we define a function that returns a Numpy array including the ‘bad’ rows. For this we need to take three steps: 1) import data; 2) find ‘bad’ rows per issue and; 3) find unique ‘bad’ rows over all issues. </p>

<pre><code class=""language-prettyprint lang-python prettyprinted""><span class=""com"">##################</span><span class=""pln"">
</span><span class=""str"">""""""get bad rows""""""</span><span class=""pln"">
</span><span class=""com"">##################</span><span class=""pln"">
</span><span class=""kwd"">def</span><span class=""pln""> get_bad_rows</span><span class=""pun"">(</span><span class=""pln"">filepath_or_buffer</span><span class=""pun"">,</span><span class=""pln""> issues</span><span class=""pun"">,</span><span class=""pln""> header_csv</span><span class=""pun"">):</span><span class=""pln"">  
    </span><span class=""str"">""""""return bad rows array that include incorrect rows""""""</span><span class=""pln"">
    </span><span class=""com"">###Import data</span><span class=""pln"">
    </span><span class=""com"">#read data</span><span class=""pln"">
    data </span><span class=""pun"">=</span><span class=""pln""> pd</span><span class=""pun"">.</span><span class=""pln"">read_csv</span><span class=""pun"">(</span><span class=""pln"">filepath_or_buffer</span><span class=""pun"">=</span><span class=""pln"">filepath_or_buffer</span><span class=""pun"">,</span><span class=""pln"">
                       header</span><span class=""pun"">=</span><span class=""pln"">header_csv</span><span class=""pun"">)</span><span class=""pln"">
    </span><span class=""com"">###Find bad rows per issue</span><span class=""pln"">
    </span><span class=""com"">#define</span><span class=""pln""> column names
    column_names </span><span class=""pun"">=</span><span class=""pln""> data</span><span class=""pun"">.</span><span class=""pln"">columns
    </span><span class=""com"">#initialize empty bad_rows_array</span><span class=""pln"">
    bad_rows_total_array </span><span class=""pun"">=</span><span class=""pln""> np</span><span class=""pun"">.</span><span class=""pln"">array</span><span class=""pun"">([])</span><span class=""pln"">
    </span><span class=""com"">#find bad rows per issue</span><span class=""pln"">
    </span><span class=""kwd"">for</span><span class=""pln""> i </span><span class=""kwd"">in</span><span class=""pln""> issues</span><span class=""pun"">:</span><span class=""pln"">
        issue </span><span class=""pun"">=</span><span class=""pln"">  issues</span><span class=""pun"">[</span><span class=""pln"">i</span><span class=""pun"">]</span><span class=""pln"">
        </span><span class=""com"">#determine if column of issue is a string</span><span class=""pln"">
        string_boolean  </span><span class=""pun"">=</span><span class=""pln""> isinstance</span><span class=""pun"">(</span><span class=""pln"">issue</span><span class=""pun"">[</span><span class=""str"">'column'</span><span class=""pun"">],</span><span class=""pln""> str</span><span class=""pun"">)</span><span class=""pln"">
        </span><span class=""com"">#select data of bad rows if column is a string</span><span class=""pln"">
        </span><span class=""kwd"">if</span><span class=""pln""> string_boolean </span><span class=""pun"">==</span><span class=""pln""> </span><span class=""kwd"">True</span><span class=""pun"">:</span><span class=""pln"">
            column_name </span><span class=""pun"">=</span><span class=""pln""> issue</span><span class=""pun"">[</span><span class=""str"">'column'</span><span class=""pun"">]</span><span class=""pln"">
        </span><span class=""com"">#select data of bad rows if column is not a string but an integer or float</span><span class=""pln"">
        </span><span class=""kwd"">else</span><span class=""pun"">:</span><span class=""pln"">
            column_name </span><span class=""pun"">=</span><span class=""pln""> column_names</span><span class=""pun"">[</span><span class=""pln"">issue</span><span class=""pun"">[</span><span class=""str"">'column'</span><span class=""pun"">]]</span><span class=""pln"">
        </span><span class=""com"">#select subset of 'bad' rows</span><span class=""pln"">
        data_bad_rows </span><span class=""pun"">=</span><span class=""pln""> data</span><span class=""pun"">[</span><span class=""pln"">data</span><span class=""pun"">[</span><span class=""pln"">column_name</span><span class=""pun"">]</span><span class=""pln""> </span><span class=""pun"">==</span><span class=""pln""> issue</span><span class=""pun"">[</span><span class=""str"">'indicator_of_bad_row'</span><span class=""pun"">]]</span><span class=""pln"">
        </span><span class=""com"">#get bad rows per issue</span><span class=""pln"">
        bad_rows_issue_array </span><span class=""pun"">=</span><span class=""pln""> data_bad_rows</span><span class=""pun"">.</span><span class=""pln"">index</span><span class=""pun"">.</span><span class=""pln"">values
        </span><span class=""com"">#add bad rows to total array of bad rows</span><span class=""pln"">
        bad_rows_total_array </span><span class=""pun"">=</span><span class=""pln""> np</span><span class=""pun"">.</span><span class=""pln"">concatenate</span><span class=""pun"">((</span><span class=""pln"">bad_rows_total_array</span><span class=""pun"">,</span><span class=""pln""> bad_rows_issue_array</span><span class=""pun"">)).</span><span class=""pln"">astype</span><span class=""pun"">(</span><span class=""kwd"">int</span><span class=""pun"">)</span><span class=""pln"">
    </span><span class=""com"">###Find unique bad rows for all issues</span><span class=""pln"">
    </span><span class=""com"">#get unique bad_rows</span><span class=""pln"">
    bad_rows_total_array </span><span class=""pun"">=</span><span class=""pln""> np</span><span class=""pun"">.</span><span class=""pln"">unique</span><span class=""pun"">(</span><span class=""pln"">bad_rows_total_array</span><span class=""pun"">)</span><span class=""pln"">
    </span><span class=""com"">#if header is true than change value by +1 because of skip function in pd.read_csv</span><span class=""pln"">
    </span><span class=""kwd"">if</span><span class=""pln""> header_boolean </span><span class=""pun"">==</span><span class=""pln""> </span><span class=""kwd"">True</span><span class=""pun"">:</span><span class=""pln"">
        bad_rows_total_array </span><span class=""pun"">+=</span><span class=""lit"">1</span><span class=""pln"">
    </span><span class=""kwd"">return</span><span class=""pln""> bad_rows_total_array

</span><span class=""com"">#get bad rows</span><span class=""pln"">
bad_rows_array </span><span class=""pun"">=</span><span class=""pln""> get_bad_rows</span><span class=""pun"">(</span><span class=""pln"">filepath_or_buffer</span><span class=""pun"">=</span><span class=""pln"">filepath_or_buffer</span><span class=""pun"">,</span><span class=""pln"">  
                              issues</span><span class=""pun"">=</span><span class=""pln"">issues</span><span class=""pun"">,</span><span class=""pln"">
                              header_csv</span><span class=""pun"">=</span><span class=""pln"">header_csv</span><span class=""pun"">)</span><span class=""pln"">

</span><span class=""com"">#print bad rows array</span><span class=""pln"">
</span><span class=""kwd"">print</span><span class=""pln""> </span><span class=""str"">'bad rows: \n'</span><span class=""pun"">,</span><span class=""pln""> bad_rows_array  </span></code></pre>

<pre><code class=""language-prettyprint lang-python prettyprinted""><span class=""pln"">bad rows</span><span class=""pun"">:</span><span class=""pln"">  
</span><span class=""pun"">[</span><span class=""pln""> </span><span class=""lit"">1</span><span class=""pln"">  </span><span class=""lit"">2</span><span class=""pln"">  </span><span class=""lit"">4</span><span class=""pln"">  </span><span class=""lit"">5</span><span class=""pln"">  </span><span class=""lit"">8</span><span class=""pln"">  </span><span class=""lit"">9</span><span class=""pln""> </span><span class=""lit"">12</span><span class=""pln""> </span><span class=""lit"">13</span><span class=""pln""> </span><span class=""lit"">14</span><span class=""pun"">]</span></code></pre>

<p>The outcome looks incorrect at first sight, but it is correct. Pandas takes the header as first row(0) in the header, therefore we have to shift values by one if the CSV file has headers. </p>

<h4 id=""parsedates"">Parse dates</h4>

<p>Ok, so we know the ‘bad’ rows in the CSV file. But what about the strange datetime values? For example, we see hours that are higher than 24. From experience I know that 31-12-2014 at 25:24:00 means 1-1-2015 at 01:15:00. In this case we need to shift year, month, day and hour to return the correct datetime format. If hour is 24 or higher than we need to shift values. Thereafter we need to check if it is the last day of the month. If yes, we need to change the day value to the first day of the month. The final changes are the month and year values. This depends on the month value. In case it is the last month of the year we need to shift the year value by 1 and the month value by 1. In other cases we only need to shift the month value by 1. Datetime.timedelta can help us out in combination with one if statement (if the hour value is not right) instead of multiple if statements. Below you can see the date parser function.</p>

<pre><code class=""language-prettyprint lang-python prettyprinted""><span class=""com"">#################</span><span class=""pln"">
</span><span class=""str"">""""""date parser""""""</span><span class=""pln"">
</span><span class=""com"">#################</span><span class=""pln"">
</span><span class=""kwd"">def</span><span class=""pln""> date_parser</span><span class=""pun"">(</span><span class=""pln"">date</span><span class=""pun"">,</span><span class=""pln""> time</span><span class=""pun"">):</span><span class=""pln"">  
    </span><span class=""str"">""""""define date parsers with seperate date and time columns""""""</span><span class=""pln"">
    </span><span class=""com"">###get day, month, year as integer</span><span class=""pln"">
    date_str </span><span class=""pun"">=</span><span class=""pln""> str</span><span class=""pun"">(</span><span class=""pln"">date</span><span class=""pun"">)</span><span class=""pln"">
    day</span><span class=""pun"">,</span><span class=""pln""> month</span><span class=""pun"">,</span><span class=""pln""> year </span><span class=""pun"">=</span><span class=""pln""> map</span><span class=""pun"">(</span><span class=""kwd"">int</span><span class=""pun"">,</span><span class=""pln""> date_str</span><span class=""pun"">.</span><span class=""pln"">split</span><span class=""pun"">(</span><span class=""str"">""-""</span><span class=""pun"">))</span><span class=""pln"">
    </span><span class=""com"">###get hour, minute and second as integer</span><span class=""pln"">
    time_str </span><span class=""pun"">=</span><span class=""pln""> str</span><span class=""pun"">(</span><span class=""pln"">time</span><span class=""pun"">)</span><span class=""pln"">
    count_time_str </span><span class=""pun"">=</span><span class=""pln""> time_str</span><span class=""pun"">.</span><span class=""pln"">count</span><span class=""pun"">(</span><span class=""str"">':'</span><span class=""pun"">)</span><span class=""pln"">
    </span><span class=""kwd"">if</span><span class=""pln""> count_time_str </span><span class=""pun"">==</span><span class=""pln""> </span><span class=""lit"">1</span><span class=""pun"">:</span><span class=""pln"">
        hour</span><span class=""pun"">,</span><span class=""pln""> minute </span><span class=""pun"">=</span><span class=""pln""> map</span><span class=""pun"">(</span><span class=""kwd"">int</span><span class=""pun"">,</span><span class=""pln""> time_str</span><span class=""pun"">.</span><span class=""pln"">split</span><span class=""pun"">(</span><span class=""str"">"":""</span><span class=""pun"">))</span><span class=""pln"">
        second </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""kwd"">int</span><span class=""pun"">(</span><span class=""lit"">0</span><span class=""pun"">)</span><span class=""pln"">
    </span><span class=""kwd"">if</span><span class=""pln""> count_time_str </span><span class=""pun"">==</span><span class=""pln""> </span><span class=""lit"">2</span><span class=""pun"">:</span><span class=""pln"">
        hour</span><span class=""pun"">,</span><span class=""pln""> minute</span><span class=""pun"">,</span><span class=""pln""> second </span><span class=""pun"">=</span><span class=""pln""> map</span><span class=""pun"">(</span><span class=""kwd"">int</span><span class=""pun"">,</span><span class=""pln""> time_str</span><span class=""pun"">.</span><span class=""pln"">split</span><span class=""pun"">(</span><span class=""str"">"":""</span><span class=""pun"">))</span><span class=""pln"">
    </span><span class=""com"">###correct hour, day, month and year values in case of strange hour value</span><span class=""pln"">
    </span><span class=""kwd"">if</span><span class=""pln""> hour </span><span class=""pun"">&gt;=</span><span class=""pln""> </span><span class=""lit"">23</span><span class=""pun"">:</span><span class=""pln"">
        </span><span class=""com"">#get datetime format</span><span class=""pln"">
        datetime_format </span><span class=""pun"">=</span><span class=""pln""> datetime</span><span class=""pun"">.</span><span class=""pln"">datetime</span><span class=""pun"">(</span><span class=""pln"">year</span><span class=""pun"">,</span><span class=""pln""> month</span><span class=""pun"">,</span><span class=""pln""> day</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""kwd"">int</span><span class=""pun"">(</span><span class=""lit"">23</span><span class=""pun"">),</span><span class=""pln""> minute</span><span class=""pun"">,</span><span class=""pln""> second</span><span class=""pun"">)</span><span class=""pln"">
        </span><span class=""com"">#correct datetime because of hour values </span><span class=""pln"">
        datetime_format </span><span class=""pun"">=</span><span class=""pln""> datetime_format </span><span class=""pun"">+</span><span class=""pln""> datetime</span><span class=""pun"">.</span><span class=""pln"">timedelta</span><span class=""pun"">(</span><span class=""pln"">hours</span><span class=""pun"">=(</span><span class=""pln"">hour</span><span class=""pun"">-</span><span class=""lit"">23</span><span class=""pun"">))</span><span class=""pln"">
    </span><span class=""kwd"">else</span><span class=""pun"">:</span><span class=""pln"">
        </span><span class=""com"">#get datetime format</span><span class=""pln"">
        datetime_format </span><span class=""pun"">=</span><span class=""pln""> datetime</span><span class=""pun"">.</span><span class=""pln"">datetime</span><span class=""pun"">(</span><span class=""pln"">year</span><span class=""pun"">,</span><span class=""pln""> month</span><span class=""pun"">,</span><span class=""pln""> day</span><span class=""pun"">,</span><span class=""pln""> hour</span><span class=""pun"">,</span><span class=""pln""> minute</span><span class=""pun"">,</span><span class=""pln""> second</span><span class=""pun"">)</span><span class=""pln"">
    </span><span class=""com""># return</span><span class=""pln"">
    </span><span class=""kwd"">return</span><span class=""pln""> datetime_format</span></code></pre>

<h4 id=""importfinaldata"">Import final data</h4>

<p>If we skip the ‘bad’ rows and use the date parser when we import the data than our data is ready for data manipulation. Below we can see the results and it looks perfect!</p>

<pre><code class=""language-prettyprint lang-python prettyprinted""><span class=""com"">###################################</span><span class=""pln"">
</span><span class=""str"">""""""skip bad rows and parse dates""""""</span><span class=""pln"">
</span><span class=""com"">###################################</span><span class=""pln"">
</span><span class=""com"">#define</span><span class=""pln""> positions of datetime columns
date_position </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""lit"">0</span><span class=""pln"">  
time_position </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""lit"">1</span><span class=""pln"">  
</span><span class=""com"">#import data for data manipulation</span><span class=""pln"">
data </span><span class=""pun"">=</span><span class=""pln""> pd</span><span class=""pun"">.</span><span class=""pln"">read_csv</span><span class=""pun"">(</span><span class=""pln"">  
            filepath_or_buffer</span><span class=""pun"">=</span><span class=""pln"">filepath_or_buffer</span><span class=""pun"">,</span><span class=""pln"">
            header</span><span class=""pun"">=</span><span class=""pln"">header_csv</span><span class=""pun"">,</span><span class=""pln"">
            encoding </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""str"">'iso-8859-1'</span><span class=""pun"">,</span><span class=""pln"">
            parse_dates </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""str"">'datetime'</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""pun"">[</span><span class=""pln"">date_position</span><span class=""pun"">,</span><span class=""pln""> time_position</span><span class=""pun"">]},</span><span class=""pln"">
            date_parser </span><span class=""pun"">=</span><span class=""pln""> date_parser</span><span class=""pun"">,</span><span class=""pln"">
            skiprows</span><span class=""pun"">=</span><span class=""pln"">bad_rows_array
        </span><span class=""pun"">)</span><span class=""pln"">

</span><span class=""kwd"">print</span><span class=""pln""> </span><span class=""str"">'final outcome: \n'</span><span class=""pun"">,</span><span class=""pln""> data  </span></code></pre>

<pre><code class=""language-prettyprint lang-python prettyprinted""><span class=""pln"">data outcome</span><span class=""pun"">:</span><span class=""pln"">  
             datetime  channel   grp
</span><span class=""lit"">0</span><span class=""pln""> </span><span class=""lit"">2014</span><span class=""pun"">-</span><span class=""lit"">12</span><span class=""pun"">-</span><span class=""lit"">01</span><span class=""pln""> </span><span class=""lit"">00</span><span class=""pun"">:</span><span class=""lit"">23</span><span class=""pun"">:</span><span class=""lit"">00</span><span class=""pln"">     NPO2   </span><span class=""lit"">1.8</span><span class=""pln"">  
</span><span class=""lit"">1</span><span class=""pln""> </span><span class=""lit"">2014</span><span class=""pun"">-</span><span class=""lit"">12</span><span class=""pun"">-</span><span class=""lit"">30</span><span class=""pln""> </span><span class=""lit"">19</span><span class=""pun"">:</span><span class=""lit"">23</span><span class=""pun"">:</span><span class=""lit"">00</span><span class=""pln"">     RTL4   </span><span class=""lit"">0.4</span><span class=""pln"">  
</span><span class=""lit"">2</span><span class=""pln""> </span><span class=""lit"">2014</span><span class=""pun"">-</span><span class=""lit"">12</span><span class=""pun"">-</span><span class=""lit"">31</span><span class=""pln""> </span><span class=""lit"">00</span><span class=""pun"">:</span><span class=""lit"">46</span><span class=""pun"">:</span><span class=""lit"">00</span><span class=""pln"">  TV </span><span class=""typ"">West</span><span class=""pln"">   </span><span class=""lit"">0.1</span><span class=""pln"">  
</span><span class=""lit"">3</span><span class=""pln""> </span><span class=""lit"">2014</span><span class=""pun"">-</span><span class=""lit"">12</span><span class=""pun"">-</span><span class=""lit"">31</span><span class=""pln""> </span><span class=""lit"">19</span><span class=""pun"">:</span><span class=""lit"">54</span><span class=""pun"">:</span><span class=""lit"">00</span><span class=""pln"">     SBS6   </span><span class=""lit"">0.2</span><span class=""pln"">  
</span><span class=""lit"">4</span><span class=""pln""> </span><span class=""lit"">2014</span><span class=""pun"">-</span><span class=""lit"">12</span><span class=""pun"">-</span><span class=""lit"">31</span><span class=""pln""> </span><span class=""lit"">20</span><span class=""pun"">:</span><span class=""lit"">21</span><span class=""pun"">:</span><span class=""lit"">00</span><span class=""pln"">     NPO1  </span><span class=""lit"">11.1</span><span class=""pln"">  
</span><span class=""lit"">5</span><span class=""pln""> </span><span class=""lit"">2015</span><span class=""pun"">-</span><span class=""lit"">01</span><span class=""pun"">-</span><span class=""lit"">01</span><span class=""pln""> </span><span class=""lit"">01</span><span class=""pun"">:</span><span class=""lit"">24</span><span class=""pun"">:</span><span class=""lit"">00</span><span class=""pln"">     NPO1   </span><span class=""lit"">0.8</span><span class=""pln"">

</span><span class=""pun"">[</span><span class=""lit"">6</span><span class=""pln""> rows x </span><span class=""lit"">3</span><span class=""pln""> columns</span><span class=""pun"">]</span></code></pre>

<p>As we can seen above, only the 'good' rows are imported and for example 31-12-2014 at 25:24:00 is adjusted to 1-1-2015 at 01:24:00.</p>

<h4 id=""conclusion"">Conclusion</h4>

<p>From now on it is easy to skip 'bad' rows if there any other issues by adding them to the other issues. The date parser works perfect in this case. However I think it would be a great contribution to Pandas if we will write the ultimate date parser library. There are so many formats and exceptions with datetime values that we wrote dozens of different date parsers at Blue Mango.</p>
        ","When you start a new innovation project your data is not always in a structured database. Most of the times you need to import a CSV file for a quick analysis. Thereafter you can manipulate your data. Finally you can start data modelling. In one of my last projects I was working on a proof of concept for TV attribution. TV attribution determines the interaction between TV commercials and online visits. Therefore we need TV schedules and online visits on minute level. Our online data is in a database, but TV schedules were not.
Strange values
The first time I was reading the TV schedules, I saw data that I didn’t expected to see. Below you can see a small sample of the output. We see some strange values like ‘--’ instead of a date and times above 24 hours.
######################
""""""import libraries""""""
######################
import pandas as pd  
import numpy as np  
import datetime  
import calendar  
import os

##################################
""""""read data for the first time""""""
##################################
#define file_path
path = os.path.dirname(os.path.realpath(__file__))  
file_name = 'tv_schedule.csv'  
filepath_or_buffer =  path + '\\' + file_name  
#define if head is in csv
header_boolean = True  
#define header input
if header_boolean == True:  
    header_csv = int(0)
else:  
    header_csv = None
#read data
data = pd.read_csv(filepath_or_buffer=filepath_or_buffer,  
                   header=header_csv)
#print data
print 'first outcome: \n', data  
first outcome:  
          date      time     channel   grp
0           --        xx        NPO1   2.0  
1           --        xx        NPO1   1.2  
2   30-11-2014  24:23:00        NPO2   1.8  
3   30-12-2014        xx  TV Gelderl   0.1  
4   30-12-2014        xx       TV Nh   0.1  
5   30-12-2014     19:23        RTL4   0.4  
6   30-12-2014  24:46:00     TV West   0.1  
7   30-12-2014        xx   O.Brabant   0.1  
8           --        xx    Rijnmond   0.0  
9   31-12-2014     19:54        SBS6   0.2  
10  31-12-2014     20:21        NPO1  11.1  
11  31-12-2014        xx  TV Gelderl   0.1  
12  31-12-2014        xx     TV Oost   0.2  
13  31-12-2014        xx  TV Zeeland   0.1  
14  31-12-2014  25:24:00        NPO1   0.8

[15 rows x 4 columns]
To adjust the data we need to add some intelligence. First, we have to know which rows are ‘bad’, so we can skip them. Second, we have to parse the datetime values to a datetime format. So, let’s give it a try!
Search bad rows
The first challenge is to search which rows you don’t want to import from the CSV file. With the help of Pandas.read_csv you can add which rows you want to skip. We will use that later on. One of the issues we can see is ‘--’ in the date column and ‘xx’ in the time column. Therefore we should indicate in which column we have an issue.
###################
""""""define issues""""""
###################
issues = {  
    ""issue_0"": {
        ""column"": 0,#integer if you know which column (imagine you have no headers)
        ""indicator_of_bad_row"": '--'},
    ""issue_1"": {
        ""column"": 'time', #string if you know the name of the column
        ""indicator_of_bad_row"": 'xx'}
}
Now we know the issues, we have to determine the ‘bad’ rows. Below we define a function that returns a Numpy array including the ‘bad’ rows. For this we need to take three steps: 1) import data; 2) find ‘bad’ rows per issue and; 3) find unique ‘bad’ rows over all issues.
##################
""""""get bad rows""""""
##################
def get_bad_rows(filepath_or_buffer, issues, header_csv):  
    """"""return bad rows array that include incorrect rows""""""
    ###Import data
    #read data
    data = pd.read_csv(filepath_or_buffer=filepath_or_buffer,
                       header=header_csv)
    ###Find bad rows per issue
    #define column names
    column_names = data.columns
    #initialize empty bad_rows_array
    bad_rows_total_array = np.array([])
    #find bad rows per issue
    for i in issues:
        issue =  issues[i]
        #determine if column of issue is a string
        string_boolean  = isinstance(issue['column'], str)
        #select data of bad rows if column is a string
        if string_boolean == True:
            column_name = issue['column']
        #select data of bad rows if column is not a string but an integer or float
        else:
            column_name = column_names[issue['column']]
        #select subset of 'bad' rows
        data_bad_rows = data[data[column_name] == issue['indicator_of_bad_row']]
        #get bad rows per issue
        bad_rows_issue_array = data_bad_rows.index.values
        #add bad rows to total array of bad rows
        bad_rows_total_array = np.concatenate((bad_rows_total_array, bad_rows_issue_array)).astype(int)
    ###Find unique bad rows for all issues
    #get unique bad_rows
    bad_rows_total_array = np.unique(bad_rows_total_array)
    #if header is true than change value by +1 because of skip function in pd.read_csv
    if header_boolean == True:
        bad_rows_total_array +=1
    return bad_rows_total_array

#get bad rows
bad_rows_array = get_bad_rows(filepath_or_buffer=filepath_or_buffer,  
                              issues=issues,
                              header_csv=header_csv)

#print bad rows array
print 'bad rows: \n', bad_rows_array  
bad rows:  
[ 1  2  4  5  8  9 12 13 14]
The outcome looks incorrect at first sight, but it is correct. Pandas takes the header as first row(0) in the header, therefore we have to shift values by one if the CSV file has headers.
Parse dates
Ok, so we know the ‘bad’ rows in the CSV file. But what about the strange datetime values? For example, we see hours that are higher than 24. From experience I know that 31-12-2014 at 25:24:00 means 1-1-2015 at 01:15:00. In this case we need to shift year, month, day and hour to return the correct datetime format. If hour is 24 or higher than we need to shift values. Thereafter we need to check if it is the last day of the month. If yes, we need to change the day value to the first day of the month. The final changes are the month and year values. This depends on the month value. In case it is the last month of the year we need to shift the year value by 1 and the month value by 1. In other cases we only need to shift the month value by 1. Datetime.timedelta can help us out in combination with one if statement (if the hour value is not right) instead of multiple if statements. Below you can see the date parser function.
#################
""""""date parser""""""
#################
def date_parser(date, time):  
    """"""define date parsers with seperate date and time columns""""""
    ###get day, month, year as integer
    date_str = str(date)
    day, month, year = map(int, date_str.split(""-""))
    ###get hour, minute and second as integer
    time_str = str(time)
    count_time_str = time_str.count(':')
    if count_time_str == 1:
        hour, minute = map(int, time_str.split("":""))
        second = int(0)
    if count_time_str == 2:
        hour, minute, second = map(int, time_str.split("":""))
    ###correct hour, day, month and year values in case of strange hour value
    if hour >= 23:
        #get datetime format
        datetime_format = datetime.datetime(year, month, day, int(23), minute, second)
        #correct datetime because of hour values 
        datetime_format = datetime_format + datetime.timedelta(hours=(hour-23))
    else:
        #get datetime format
        datetime_format = datetime.datetime(year, month, day, hour, minute, second)
    # return
    return datetime_format
Import final data
If we skip the ‘bad’ rows and use the date parser when we import the data than our data is ready for data manipulation. Below we can see the results and it looks perfect!
###################################
""""""skip bad rows and parse dates""""""
###################################
#define positions of datetime columns
date_position = 0  
time_position = 1  
#import data for data manipulation
data = pd.read_csv(  
            filepath_or_buffer=filepath_or_buffer,
            header=header_csv,
            encoding = 'iso-8859-1',
            parse_dates = {'datetime': [date_position, time_position]},
            date_parser = date_parser,
            skiprows=bad_rows_array
        )

print 'final outcome: \n', data  
data outcome:  
             datetime  channel   grp
0 2014-12-01 00:23:00     NPO2   1.8  
1 2014-12-30 19:23:00     RTL4   0.4  
2 2014-12-31 00:46:00  TV West   0.1  
3 2014-12-31 19:54:00     SBS6   0.2  
4 2014-12-31 20:21:00     NPO1  11.1  
5 2015-01-01 01:24:00     NPO1   0.8

[6 rows x 3 columns]
As we can seen above, only the 'good' rows are imported and for example 31-12-2014 at 25:24:00 is adjusted to 1-1-2015 at 01:24:00.
Conclusion
From now on it is easy to skip 'bad' rows if there any other issues by adding them to the other issues. The date parser works perfect in this case. However I think it would be a great contribution to Pandas if we will write the ultimate date parser library. There are so many formats and exceptions with datetime values that we wrote dozens of different date parsers at Blue Mango.","[Data Science, python]"
127,Introducing Bananalytics,/introducing-bananalytics/,"
            <p>Today, it is time to announce <strong>Bananalytics</strong>. Bananalytics is a tool that makes your website go bananas. </p>

<p>Implementation is as easy as a (banana)pie. Bananalytics hooks onto your favourite script. All you need to do is to copy and paste the code below and change the <em>bananas</em> value to your script of choice.</p>

<p><strong>Banalytics code - Google Analytics example</strong></p>

<pre><code>&lt;script type=""text/javascript""&gt;  
  var bananas = 'Google Analytics';
  /*General Bananalytics code*/
  function bananalytics(a){""undefined""!=typeof $(""span:contains(""+a+"")"").parent()[0]&amp;&amp;($(""span:contains(""+a+"")"").parent().append(""&lt;span&gt;Bananalytics&lt;/span&gt;&lt;br/&gt;""),clearInterval(myVar))}function addBananlytics(){bananalytics(b)}var b=bananas,myVar=setInterval(function(){addBananlytics()},2e3);
&lt;/script&gt;  
</code></pre>

<p>After this, bananalytics will hook onto that script and, as a result, the tool will show up in the script tracking tool <a href=""https://www.ghostery.com/nl/"">Ghostery</a>. </p>

<p><img src=""http://i58.tinypic.com/29lz3o6.png"" alt=""Bananlytics in Ghostery""></p>

<p>Keep in mind that to run Bananalytics, you'll need to have jQuery running on your website. Also keep in mind that the tool is completely bananas.</p>

<p><strong>How it really works</strong></p>

<p>Bananalytics is a piece of code that adds a span element to the container holding the Ghostery notifications. Ghostery is a browser plug-in that shows a list of the scripts running on any given website. With our script, you can trick visitors using Ghostery into thinking you're running a specific script. </p>

<p>In this post, we've used Bananalytics as an example, but you can use this for any kind of script. All you need is the title of an existing span element in the Ghostery notification.</p>

<p>If you have any other questions, don't hesitate to ask.</p>
        ","Today, it is time to announce Bananalytics. Bananalytics is a tool that makes your website go bananas.
Implementation is as easy as a (banana)pie. Bananalytics hooks onto your favourite script. All you need to do is to copy and paste the code below and change the bananas value to your script of choice.
Banalytics code - Google Analytics example
<script type=""text/javascript"">  
  var bananas = 'Google Analytics';
  /*General Bananalytics code*/
  function bananalytics(a){""undefined""!=typeof $(""span:contains(""+a+"")"").parent()[0]&&($(""span:contains(""+a+"")"").parent().append(""<span>Bananalytics</span><br/>""),clearInterval(myVar))}function addBananlytics(){bananalytics(b)}var b=bananas,myVar=setInterval(function(){addBananlytics()},2e3);
</script>  
After this, bananalytics will hook onto that script and, as a result, the tool will show up in the script tracking tool Ghostery.
Keep in mind that to run Bananalytics, you'll need to have jQuery running on your website. Also keep in mind that the tool is completely bananas.
How it really works
Bananalytics is a piece of code that adds a span element to the container holding the Ghostery notifications. Ghostery is a browser plug-in that shows a list of the scripts running on any given website. With our script, you can trick visitors using Ghostery into thinking you're running a specific script.
In this post, we've used Bananalytics as an example, but you can use this for any kind of script. All you need is the title of an existing span element in the Ghostery notification.
If you have any other questions, don't hesitate to ask.","[ghostery, aprilfools, Analytics]"
128,Live Optimizely Variant Switcher,/live-optimizely-variant-switcher/,"
            <p>Viewing the variations of your test on a live site can be a drag. Normally, you'll have to force your testing tool of choice to show you a specifiek variation. With Optimizely, you basically have three options:</p>

<ul>
<li>Preview variations in Optimizely</li>
<li>Force 100% of the traffic to one variation</li>
<li>Manually set up a URL parameter based on the experiment ID &amp; the variation you want to show</li>
</ul>

<p>Wouldn't it be great to easily switch between your variations when you're looking at the live website (with the test running)? Well, now you can. </p>

<p>Using <a href=""http://andrescholten.net/live-visual-website-optimizer-variant-switcher/"">Andre Scholten's VWO Switcher</a> as a base, I've created a JavaScript bookmark that acts as a switcher for Optimizely. All you have to do is add a bookmark and fill out the code below as the location.</p>

<p><strong>Bookmark code:</strong></p>

<pre><code>javascript:function readCookie(o){for(var e=o+""="",t=document.cookie.split("";""),n=0;n&lt;t.length;n++){for(var i=t[n];"" ""==i.charAt(0);)i=i.substring(1,i.length);if(0==i.indexOf(e))return i.substring(e.length,i.length)}return null}var names=""Fill out your Optimizely variation of choice:"";if(null!==readCookie(""optimizelyBuckets"")){if(txt=prompt(names+""\r\nChoose version: \n 0 for Default\n 1+ for Variations""),null!=txt){var oTest=decodeURIComponent(readCookie(""optimizelyBuckets""));oTest=JSON.parse(oTest);var oId="""";for(var key in oTest)oId=key;window.location.href=document.location.protocol+""//""+document.location.host+document.location.pathname+""?optimizely_x""+oId+""=""+txt}}else alert(""No live experiment on this page."");  
</code></pre>

<p>When you click the bookmark, you'll get the following pop-up: <br>
<img src=""http://i59.tinypic.com/vrwnk2.png"" alt=""Optimizely Variant Switcher Promt"">
Just type your variation of choice in the input field and press OK. That's it. If you want to know more about the technical details, read ahead.</p>

<h2 id=""howdoesitwork"">How does it work?</h2>

<p>The idea is simple: get the Optimizely experiment ID  of a running test and use <a href=""https://help.optimizely.com/hc/en-us/articles/200107480-Force-a-specific-variation-to-run-and-other-URL-parameters-"">Optimizely's variation parameter</a> to force a variation. The magic is getting the correct experiment ID automatically. Here's the code that does just that:</p>

<pre><code>        var oTest = decodeURIComponent(readCookie('optimizelyBuckets'));
        oTest = JSON.parse(oTest);
        var oId = '';
        for (var key in oTest) {oId = key;};
        window.location.href = document.location.protocol + '//' +document.location.host +                     document.location.pathname + '?optimizely_x' + oId + '=' + txt;
</code></pre>

<p>An explanation for each line:</p>

<ul>
<li><strong>var oTest</strong>: get &amp; decode the value of the optimizelyBucket cookie (with <a href=""http://www.w3schools.com/js/js_cookies.asp"">a standard cookie function</a>).</li>
<li><strong>JSON.parse(oTest)</strong>: the optimizelyBucket cookie contains a JSON object. Parse it, so you can use it as a JSON object.</li>
<li><strong>var oId</strong>: get the first key of the object (this is the experiment ID).</li>
<li><strong>window.location.href</strong>: redirect the page to the current url with the correct '?optimizely_x' parameter added to force a variation.</li>
</ul>

<p>That's it. If you're interested in the full beautified code of the bookmark, here it is:</p>

<pre><code>var names = ""Fill out your optimizely test ID:"";

function readCookie(name) {  
    var nameEQ = name + ""="";
    var ca = document.cookie.split(';');
    for (var i = 0; i &lt; ca.length; i++) {
        var c = ca[i];
        while (c.charAt(0) == ' ') c = c.substring(1, c.length);
        if (c.indexOf(nameEQ) == 0) return c.substring(nameEQ.length, c.length);
    };
    return null;
};
if (readCookie('optimizelyBuckets') !== null) {  
    txt = prompt(names + ""\r\nChoose version: \n 0 for Default\n 1+ for Variations"");
    if (txt != null) {
        var oTest = decodeURIComponent(readCookie('optimizelyBuckets'));
        oTest = JSON.parse(oTest);
        var oId = '';
        for (var key in oTest) {oId = key;};
        window.location.href = document.location.protocol + '//' +document.location.host + document.location.pathname + '?optimizely_x' + oId + '=' + txt;
     }
} else {
    alert(""No live experiment on this page."");
};
void(0);  
</code></pre>

<p>If you regularly test with Optimizely, life just became a little easier. </p>

<p>Happy switching!</p>
        ","Viewing the variations of your test on a live site can be a drag. Normally, you'll have to force your testing tool of choice to show you a specifiek variation. With Optimizely, you basically have three options:
Preview variations in Optimizely
Force 100% of the traffic to one variation
Manually set up a URL parameter based on the experiment ID & the variation you want to show
Wouldn't it be great to easily switch between your variations when you're looking at the live website (with the test running)? Well, now you can.
Using Andre Scholten's VWO Switcher as a base, I've created a JavaScript bookmark that acts as a switcher for Optimizely. All you have to do is add a bookmark and fill out the code below as the location.
Bookmark code:
javascript:function readCookie(o){for(var e=o+""="",t=document.cookie.split("";""),n=0;n<t.length;n++){for(var i=t[n];"" ""==i.charAt(0);)i=i.substring(1,i.length);if(0==i.indexOf(e))return i.substring(e.length,i.length)}return null}var names=""Fill out your Optimizely variation of choice:"";if(null!==readCookie(""optimizelyBuckets"")){if(txt=prompt(names+""\r\nChoose version: \n 0 for Default\n 1+ for Variations""),null!=txt){var oTest=decodeURIComponent(readCookie(""optimizelyBuckets""));oTest=JSON.parse(oTest);var oId="""";for(var key in oTest)oId=key;window.location.href=document.location.protocol+""//""+document.location.host+document.location.pathname+""?optimizely_x""+oId+""=""+txt}}else alert(""No live experiment on this page."");  
When you click the bookmark, you'll get the following pop-up:
Just type your variation of choice in the input field and press OK. That's it. If you want to know more about the technical details, read ahead.
How does it work?
The idea is simple: get the Optimizely experiment ID of a running test and use Optimizely's variation parameter to force a variation. The magic is getting the correct experiment ID automatically. Here's the code that does just that:
        var oTest = decodeURIComponent(readCookie('optimizelyBuckets'));
        oTest = JSON.parse(oTest);
        var oId = '';
        for (var key in oTest) {oId = key;};
        window.location.href = document.location.protocol + '//' +document.location.host +                     document.location.pathname + '?optimizely_x' + oId + '=' + txt;
An explanation for each line:
var oTest: get & decode the value of the optimizelyBucket cookie (with a standard cookie function).
JSON.parse(oTest): the optimizelyBucket cookie contains a JSON object. Parse it, so you can use it as a JSON object.
var oId: get the first key of the object (this is the experiment ID).
window.location.href: redirect the page to the current url with the correct '?optimizely_x' parameter added to force a variation.
That's it. If you're interested in the full beautified code of the bookmark, here it is:
var names = ""Fill out your optimizely test ID:"";

function readCookie(name) {  
    var nameEQ = name + ""="";
    var ca = document.cookie.split(';');
    for (var i = 0; i < ca.length; i++) {
        var c = ca[i];
        while (c.charAt(0) == ' ') c = c.substring(1, c.length);
        if (c.indexOf(nameEQ) == 0) return c.substring(nameEQ.length, c.length);
    };
    return null;
};
if (readCookie('optimizelyBuckets') !== null) {  
    txt = prompt(names + ""\r\nChoose version: \n 0 for Default\n 1+ for Variations"");
    if (txt != null) {
        var oTest = decodeURIComponent(readCookie('optimizelyBuckets'));
        oTest = JSON.parse(oTest);
        var oId = '';
        for (var key in oTest) {oId = key;};
        window.location.href = document.location.protocol + '//' +document.location.host + document.location.pathname + '?optimizely_x' + oId + '=' + txt;
     }
} else {
    alert(""No live experiment on this page."");
};
void(0);  
If you regularly test with Optimizely, life just became a little easier.
Happy switching!","[optimizely, bookmark]"
129,In the heat of the moment: Integrating Mouseflow with Visual Website Optimizer,/in-the-heat-of-the-moment-integrating-mouseflow-with-visual-website-optimizer/,"
            <p>As Conversion Specialists at Blue Mango, we don’t like to rely on our gut feeling. That’s why we tests as much as we can. The <a href=""https://whichtestwon.com/testing-awards/2015-online-testing-awards/"">success of these tests</a> is often due to looking at more than the quantitative data only. </p>

<h2 id=""thewhyofabtesting"">The ‘why’ of A/B-testing</h2>

<p>Inspiration doesn’t just fall from the sky. Before we start testing we like to do some preliminary research. For instance, heat maps can help us to determine which elements on a page get attention and which are ignored by our visitors. Let’s say, a heat map of a certain page shows us that there are hardly any mouse moves near one of the offers. With the help of <a href=""https://vwo.com/"">Visual Website Optimizer</a> (VWO) we can design a variant that contains some directional cues to the neglected offer and launch an A/B-test. <em>So far so good..</em></p>

<p>But imagine that we don’t see a significant difference in conversions between the page with and without the directional cues. The big question then is; were the cues not driving enough attention to the offer or was the offer simply not attractive enough? In both cases there is something to improve with a follow up A/B-test. But to get an answer to this question, again we need the heat maps.</p>

<h2 id=""integratingthenonintegratables"">Integrating the non-integratables</h2>

<p>Now there are a few examples of heat map tools that can easily be integrated with A/B-testing tools, using nothing more than a simple checkbox. The integration itself is quite important, because otherwise the variants will all end up in one big messy heat map. This would also be the case when using one of our favorite heat map tools: <a href=""http://www.mouseflow.com/"">Mouseflow</a>, because VWO doesn’t support an integration with them. <em>Unless..</em></p>

<p>We create our own connection! Mouseflow supports the functionality to configure your own page path via the <a href=""https://mouseflow.zendesk.com/entries/22098003-Integration-with-A-B-and-multivariate-testing"">mouseflowPath</a> variable. If we add the variant of the A/B test to the actual path we can measure all variants separately in Mouseflow. VWO stores the variant of the A/B test in a cookie called <code>_vis_opt_exp_[experiment_id]_combi</code>. By using the <a href=""http://www.w3schools.com/js/js_cookies.asp"">getCookie</a> function we retrieved the variant which we added to the <code>mouseflowPath</code> variable before the Mouseflow script was loaded.  </p>

<pre><code class=""language-prettyprint lang-javascript prettyprinted""><span class=""tag"">&lt;script&gt;</span><span class=""pln"">  
</span><span class=""kwd"">function</span><span class=""pln""> getCookie</span><span class=""pun"">(</span><span class=""pln"">cname</span><span class=""pun"">)</span><span class=""pln"">  
</span><span class=""pun"">{</span><span class=""pln"">
  </span><span class=""kwd"">var</span><span class=""pln""> name </span><span class=""pun"">=</span><span class=""pln""> cname </span><span class=""pun"">+</span><span class=""pln""> </span><span class=""str"">""=""</span><span class=""pun"">;</span><span class=""pln"">
  </span><span class=""kwd"">var</span><span class=""pln""> ca </span><span class=""pun"">=</span><span class=""pln""> document</span><span class=""pun"">.</span><span class=""pln"">cookie</span><span class=""pun"">.</span><span class=""pln"">split</span><span class=""pun"">(</span><span class=""str"">';'</span><span class=""pun"">);</span><span class=""pln"">
  </span><span class=""kwd"">for</span><span class=""pun"">(</span><span class=""kwd"">var</span><span class=""pln""> i</span><span class=""pun"">=</span><span class=""lit"">0</span><span class=""pun"">;</span><span class=""pln""> i</span><span class=""pun"">&lt;</span><span class=""pln"">ca</span><span class=""pun"">.</span><span class=""pln"">length</span><span class=""pun"">;</span><span class=""pln""> i</span><span class=""pun"">++)</span><span class=""pln"">
  </span><span class=""pun"">{</span><span class=""pln"">
    </span><span class=""kwd"">var</span><span class=""pln""> c </span><span class=""pun"">=</span><span class=""pln""> ca</span><span class=""pun"">[</span><span class=""pln"">i</span><span class=""pun"">].</span><span class=""pln"">trim</span><span class=""pun"">();</span><span class=""pln"">
    </span><span class=""kwd"">if</span><span class=""pln""> </span><span class=""pun"">(</span><span class=""pln"">c</span><span class=""pun"">.</span><span class=""pln"">indexOf</span><span class=""pun"">(</span><span class=""pln"">name</span><span class=""pun"">)==</span><span class=""lit"">0</span><span class=""pun"">)</span><span class=""pln""> </span><span class=""kwd"">return</span><span class=""pln""> c</span><span class=""pun"">.</span><span class=""pln"">substring</span><span class=""pun"">(</span><span class=""pln"">name</span><span class=""pun"">.</span><span class=""pln"">length</span><span class=""pun"">,</span><span class=""pln"">c</span><span class=""pun"">.</span><span class=""pln"">length</span><span class=""pun"">);</span><span class=""pln"">
  </span><span class=""pun"">}</span><span class=""pln"">
  </span><span class=""kwd"">return</span><span class=""pln""> </span><span class=""str"">""""</span><span class=""pun"">;</span><span class=""pln"">
</span><span class=""pun"">}</span><span class=""pln"">
</span><span class=""kwd"">var</span><span class=""pln""> experimentId </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""lit"">40</span><span class=""pun"">;</span><span class=""pln"">  
</span><span class=""kwd"">if</span><span class=""pun"">(</span><span class=""pln"">getCookie</span><span class=""pun"">(</span><span class=""str"">'_vis_opt_exp_'</span><span class=""pun"">+</span><span class=""pln"">experimentId</span><span class=""pun"">+</span><span class=""str"">'_combi'</span><span class=""pun"">)</span><span class=""pln""> </span><span class=""pun"">!=</span><span class=""pln""> </span><span class=""str"">""""</span><span class=""pun"">){</span><span class=""pln"">  
    </span><span class=""kwd"">var</span><span class=""pln""> mouseflowPath </span><span class=""pun"">=</span><span class=""pln""> location</span><span class=""pun"">.</span><span class=""pln"">protocol </span><span class=""pun"">+</span><span class=""pln""> </span><span class=""str"">'//'</span><span class=""pln""> </span><span class=""pun"">+</span><span class=""pln""> location</span><span class=""pun"">.</span><span class=""pln"">host </span><span class=""pun"">+</span><span class=""pln""> location</span><span class=""pun"">.</span><span class=""pln"">pathname </span><span class=""pun"">+</span><span class=""pln""> getCookie</span><span class=""pun"">(</span><span class=""str"">'_vis_opt_exp_'</span><span class=""pun"">+</span><span class=""pln"">experimentId</span><span class=""pun"">+</span><span class=""str"">'_combi'</span><span class=""pun"">);</span><span class=""pln""> 
</span><span class=""pun"">}</span><span class=""pln"">
</span><span class=""tag"">&lt;/script&gt;</span><span class=""pln"">  </span></code></pre>

<p><strong>Please take into account that the variable <code>experimentId</code> (40 in this example) should be changed to the actual experiment ID.</strong></p>

<p>So there you go! Separate heatmaps for each variant of you’re A/B-test. So now we also don’t have to rely on our gut feeling anymore when we’re wondering why the variant did or didn’t win the test.</p>

<p>Written together with <a href=""http://geek.bluemangointeractive.com/author/bart-persoons/"">Bart Persoons</a></p>
        ","As Conversion Specialists at Blue Mango, we don’t like to rely on our gut feeling. That’s why we tests as much as we can. The success of these tests is often due to looking at more than the quantitative data only.
The ‘why’ of A/B-testing
Inspiration doesn’t just fall from the sky. Before we start testing we like to do some preliminary research. For instance, heat maps can help us to determine which elements on a page get attention and which are ignored by our visitors. Let’s say, a heat map of a certain page shows us that there are hardly any mouse moves near one of the offers. With the help of Visual Website Optimizer (VWO) we can design a variant that contains some directional cues to the neglected offer and launch an A/B-test. So far so good..
But imagine that we don’t see a significant difference in conversions between the page with and without the directional cues. The big question then is; were the cues not driving enough attention to the offer or was the offer simply not attractive enough? In both cases there is something to improve with a follow up A/B-test. But to get an answer to this question, again we need the heat maps.
Integrating the non-integratables
Now there are a few examples of heat map tools that can easily be integrated with A/B-testing tools, using nothing more than a simple checkbox. The integration itself is quite important, because otherwise the variants will all end up in one big messy heat map. This would also be the case when using one of our favorite heat map tools: Mouseflow, because VWO doesn’t support an integration with them. Unless..
We create our own connection! Mouseflow supports the functionality to configure your own page path via the mouseflowPath variable. If we add the variant of the A/B test to the actual path we can measure all variants separately in Mouseflow. VWO stores the variant of the A/B test in a cookie called _vis_opt_exp_[experiment_id]_combi. By using the getCookie function we retrieved the variant which we added to the mouseflowPath variable before the Mouseflow script was loaded.
<script>  
function getCookie(cname)  
{
  var name = cname + ""="";
  var ca = document.cookie.split(';');
  for(var i=0; i<ca.length; i++)
  {
    var c = ca[i].trim();
    if (c.indexOf(name)==0) return c.substring(name.length,c.length);
  }
  return """";
}
var experimentId = 40;  
if(getCookie('_vis_opt_exp_'+experimentId+'_combi') != """"){  
    var mouseflowPath = location.protocol + '//' + location.host + location.pathname + getCookie('_vis_opt_exp_'+experimentId+'_combi'); 
}
</script>  
Please take into account that the variable experimentId (40 in this example) should be changed to the actual experiment ID.
So there you go! Separate heatmaps for each variant of you’re A/B-test. So now we also don’t have to rely on our gut feeling anymore when we’re wondering why the variant did or didn’t win the test.
Written together with Bart Persoons","[cro, Code]"
130,Calculating ad stocks in a fast and readable way in Python,/calculating-ad-stocks-in-a-fast-and-readable-way-in-python/,"
            <p>As Data Scientists in the world of advertising we often need to proof the direct and delayed effects of advertising. A phenomena what we see in the last years is that consumers use multiple screens when they watch television. This is called <em>screen-stacking</em>. After a television commercial they can visit the website of the brand or product if they gained interest. </p>

<p>A couple of years ago this was done with data on week or day level. Nowadays we model on minute level at Blue Mango. We found direct interaction effects in the same minute between the commercial and  website visits. In the twenty minutes after the commercial we still see an uplift in visits. To model the direct and delayed effect , we make use of ad stock variables. Therefore we first have to calculate ad stock values. On one hand the code should be easy to read. On the other hand we hate losing time on data collection, because we love models.</p>

<h5 id=""creatingadataset"">Creating a dataset</h5>

<p>Today we only talk about data collection. Later on I will post a blog about hierarchical Bayesian models to determine the attributed value of TV commercials. One of the awesome packages in Python for data collection is <a href=""pandas.pydata.org"">Pandas</a>.  Yes, I am that guy that is addicted to Pandas. In the example below, we use Pandas to create a dataset of one year of data on minute level. The length of the dataset is 525 600 rows (365 days * 24 hours * 60 minutes). For this example we add one column with GRP’s of commercials on the TV channel RTL 4. GRP’s are the percent of the target group that watched the commercial in that specific minute. For now we only consider one commercial for the entire year.</p>

<pre><code class=""language-prettyprint lang-python prettyprinted""><span class=""com"">#######################</span><span class=""pln"">
</span><span class=""str"">""""""import libraries.""""""</span><span class=""pln"">
</span><span class=""com"">#######################</span><span class=""pln"">

</span><span class=""kwd"">import</span><span class=""pln""> datetime  
</span><span class=""kwd"">import</span><span class=""pln""> numpy </span><span class=""kwd"">as</span><span class=""pln""> np  
</span><span class=""kwd"">import</span><span class=""pln""> pandas </span><span class=""kwd"">as</span><span class=""pln""> pd


</span><span class=""com"">#####################</span><span class=""pln"">
</span><span class=""str"">""""""create dataset.""""""</span><span class=""pln"">
</span><span class=""com"">#####################</span><span class=""pln"">

</span><span class=""kwd"">def</span><span class=""pln""> create_dataset</span><span class=""pun"">(</span><span class=""pln"">start_datetime</span><span class=""pun"">=</span><span class=""pln"">datetime</span><span class=""pun"">.</span><span class=""pln"">datetime</span><span class=""pun"">(</span><span class=""pln"">year</span><span class=""pun"">=</span><span class=""lit"">2014</span><span class=""pun"">,</span><span class=""pln""> month</span><span class=""pun"">=</span><span class=""lit"">1</span><span class=""pun"">,</span><span class=""pln""> day</span><span class=""pun"">=</span><span class=""lit"">1</span><span class=""pun"">),</span><span class=""pln"">  
                   end_datetime</span><span class=""pun"">=</span><span class=""pln"">datetime</span><span class=""pun"">.</span><span class=""pln"">datetime</span><span class=""pun"">(</span><span class=""pln"">year</span><span class=""pun"">=</span><span class=""lit"">2015</span><span class=""pun"">,</span><span class=""pln"">   month</span><span class=""pun"">=</span><span class=""lit"">1</span><span class=""pun"">,</span><span class=""pln""> day</span><span class=""pun"">=</span><span class=""lit"">1</span><span class=""pun"">)):</span><span class=""pln"">
    </span><span class=""str"">""""""Create data for 1 year of data on minute level and 1 column with GRP's on RTL4""""""</span><span class=""pln"">
    </span><span class=""com"">#create datetime values between start_datetime and start_datetime on minute level</span><span class=""pln"">
    time_index </span><span class=""pun"">=</span><span class=""pln""> np</span><span class=""pun"">.</span><span class=""pln"">arange</span><span class=""pun"">(</span><span class=""pln"">start_datetime</span><span class=""pun"">,</span><span class=""pln""> end_datetime</span><span class=""pun"">,</span><span class=""pln""> dtype</span><span class=""pun"">=</span><span class=""str"">'datetime64[m]'</span><span class=""pun"">)</span><span class=""pln"">
    </span><span class=""com"">#create dataframe including time_index</span><span class=""pln"">
    data </span><span class=""pun"">=</span><span class=""pln""> pd</span><span class=""pun"">.</span><span class=""typ"">DataFrame</span><span class=""pun"">(</span><span class=""pln"">index</span><span class=""pun"">=</span><span class=""pln"">time_index</span><span class=""pun"">)</span><span class=""pln"">
    </span><span class=""com"">#define</span><span class=""pln""> index name
    data</span><span class=""pun"">.</span><span class=""pln"">index</span><span class=""pun"">.</span><span class=""pln"">name </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""str"">'datetime'</span><span class=""pln"">
    </span><span class=""com"">#add column with zero's for GRP's on RTL4</span><span class=""pln"">
    data </span><span class=""pun"">[</span><span class=""str"">'grp_on_rtl4'</span><span class=""pun"">]</span><span class=""pln""> </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""kwd"">float</span><span class=""pun"">(</span><span class=""lit"">0</span><span class=""pun"">)</span><span class=""pln"">
    </span><span class=""com"">#add ten GRP's in third minute on the first of January for one channel in an entire year</span><span class=""pln"">
    data</span><span class=""pun"">.</span><span class=""pln"">ix</span><span class=""pun"">[</span><span class=""pln"">datetime</span><span class=""pun"">.</span><span class=""pln"">datetime</span><span class=""pun"">(</span><span class=""pln"">year</span><span class=""pun"">=</span><span class=""lit"">2014</span><span class=""pun"">,</span><span class=""pln""> month</span><span class=""pun"">=</span><span class=""lit"">1</span><span class=""pun"">,</span><span class=""pln""> day</span><span class=""pun"">=</span><span class=""lit"">1</span><span class=""pun"">,</span><span class=""pln""> hour</span><span class=""pun"">=</span><span class=""lit"">0</span><span class=""pun"">,</span><span class=""pln""> minute</span><span class=""pun"">=</span><span class=""lit"">2</span><span class=""pun"">),</span><span class=""pln"">
                              </span><span class=""str"">'grp_on_rtl4'</span><span class=""pun"">]</span><span class=""pln""> </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""lit"">10</span><span class=""pln"">
    </span><span class=""kwd"">return</span><span class=""pln""> data

</span><span class=""com"">#create dataset and print first and last rows of data</span><span class=""pln"">
data </span><span class=""pun"">=</span><span class=""pln""> create_dataset</span><span class=""pun"">()</span><span class=""pln"">  
</span><span class=""kwd"">print</span><span class=""pln""> </span><span class=""str"">'\n First five rows of data: \n'</span><span class=""pun"">,</span><span class=""pln""> data</span><span class=""pun"">[:</span><span class=""lit"">5</span><span class=""pun"">]</span><span class=""pln"">  
</span><span class=""kwd"">print</span><span class=""pln""> </span><span class=""str"">'\n Last five rows of data:  \n'</span><span class=""pun"">,</span><span class=""pln""> data</span><span class=""pun"">[-</span><span class=""lit"">5</span><span class=""pun"">:]</span><span class=""pln"">  </span></code></pre>

<pre><code class=""language-prettying lang-python"">First five rows of data:  
                     grp_on_rtl4
datetime  
2014-01-01 00:00:00            0  
2014-01-01 00:01:00            0  
2014-01-01 00:02:00           10  
2014-01-01 00:03:00            0  
2014-01-01 00:04:00            0

[5 rows x 1 columns]

Last five rows of data:  
                     grp_on_rtl4
datetime  
2014-12-31 23:55:00            0  
2014-12-31 23:56:00            0  
2014-12-31 23:57:00            0  
2014-12-31 23:58:00            0  
2014-12-31 23:59:00            0

[5 rows x 1 columns]
</code></pre>

<h4 id=""calculateadstockstheslowway"">Calculate ad stocks: the slow way</h4>

<p>To calculate ad stock values we make use of an engagement factor of 0.9. So one minute after a commercial 90% of the consumers are engaged through the commercial and two minutes after a commercial 81% (0.9*0.9) of the consumers are still engaged through the commercial and so forth. Below we see a simple for loop that directly adds the ad stock values to the dataset. The disadvantage is that in every iteration we have to make use of an index operation. Index operations are great for quickly setting up a small program and data manipulation. But again, I hate losing time on data collection! In this case it takes more than one minute and this is just for one variable.</p>

<pre><code class=""language-prettyprint lang-python prettyprinted""><span class=""com"">########################################################</span><span class=""pln"">
</span><span class=""str"">""""""test speed of non-vectorized ad stock calculations""""""</span><span class=""pln"">
</span><span class=""com"">########################################################</span><span class=""pln"">

</span><span class=""kwd"">def</span><span class=""pln""> calculate_ad_stocks_non_vectorized</span><span class=""pun"">(</span><span class=""pln"">data</span><span class=""pun"">,</span><span class=""pln""> engagement_factor</span><span class=""pun"">=</span><span class=""lit"">0.9</span><span class=""pun"">):</span><span class=""pln"">  
    </span><span class=""str"">""""""Calculate ad stocks on a non-vectorized and slow way""""""</span><span class=""pln"">
    </span><span class=""com"">#initialize ad stock column</span><span class=""pln"">
    data</span><span class=""pun"">[</span><span class=""str"">'grp_on_rtl4_ad_stock'</span><span class=""pun"">]</span><span class=""pln""> </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""lit"">0</span><span class=""pln"">
    </span><span class=""com"">#initialize ad_stock_value</span><span class=""pln"">
    ad_stock_value </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""lit"">0</span><span class=""pln"">
    </span><span class=""com"">#loop through dataset to calculate ad stock values</span><span class=""pln"">
    </span><span class=""kwd"">for</span><span class=""pln""> index</span><span class=""pun"">,</span><span class=""pln""> row </span><span class=""kwd"">in</span><span class=""pln""> data</span><span class=""pun"">.</span><span class=""pln"">iterrows</span><span class=""pun"">():</span><span class=""pln"">
        ad_stock_value </span><span class=""pun"">=</span><span class=""pln""> row</span><span class=""pun"">[</span><span class=""str"">'grp_on_rtl4'</span><span class=""pun"">]</span><span class=""pln""> </span><span class=""pun"">+</span><span class=""pln""> ad_stock_value </span><span class=""pun"">*</span><span class=""pln""> engagement_factor
        data</span><span class=""pun"">.</span><span class=""pln"">ix</span><span class=""pun"">[</span><span class=""pln"">index</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""str"">'grp_on_rtl4_ad_stock'</span><span class=""pun"">]</span><span class=""pln""> </span><span class=""pun"">=</span><span class=""pln""> ad_stock_value
    </span><span class=""kwd"">return</span><span class=""pln""> data

</span><span class=""com"">#calculate time spend in non-vectorized and slow way</span><span class=""pln"">
time_start </span><span class=""pun"">=</span><span class=""pln""> datetime</span><span class=""pun"">.</span><span class=""pln"">datetime</span><span class=""pun"">.</span><span class=""pln"">now</span><span class=""pun"">()</span><span class=""pln"">  
data_ad_stocks </span><span class=""pun"">=</span><span class=""pln""> calculate_ad_stocks_non_vectorized</span><span class=""pun"">(</span><span class=""pln"">data</span><span class=""pun"">=</span><span class=""pln"">data</span><span class=""pun"">,</span><span class=""pln"">  
                                                    engagement_factor</span><span class=""pun"">=</span><span class=""lit"">0.9</span><span class=""pun"">)</span><span class=""pln"">
time_end </span><span class=""pun"">=</span><span class=""pln""> datetime</span><span class=""pun"">.</span><span class=""pln"">datetime</span><span class=""pun"">.</span><span class=""pln"">now</span><span class=""pun"">()</span><span class=""pln"">  
time_spend_non_vectorized </span><span class=""pun"">=</span><span class=""pln""> time_end </span><span class=""pun"">-</span><span class=""pln""> time_start  
</span><span class=""kwd"">print</span><span class=""pln""> </span><span class=""str"">'\n Time spend for ad stock transformation in non-vectorized and slow way:  \n'</span><span class=""pun"">,</span><span class=""pln""> time_spend_non_vectorized  
</span><span class=""kwd"">print</span><span class=""pln""> </span><span class=""str"">'\n First ten rows of data including ad stock values:  \n'</span><span class=""pun"">,</span><span class=""pln""> data_ad_stocks</span><span class=""pun"">[:</span><span class=""lit"">10</span><span class=""pun"">]</span><span class=""pln"">  </span></code></pre>

<pre><code class=""language-prettying lang-python"">Time spend for ad stock transformation in non-vectorized and slow way:  
0:01:08.984000

First ten rows of data including ad stock values:  
                     grp_on_rtl4  grp_on_rtl4_ad_stock
datetime  
2014-01-01 00:00:00            0              0.000000  
2014-01-01 00:01:00            0              0.000000  
2014-01-01 00:02:00           10             10.000000  
2014-01-01 00:03:00            0              9.000000  
2014-01-01 00:04:00            0              8.100000  
2014-01-01 00:05:00            0              7.290000  
2014-01-01 00:06:00            0              6.561000  
2014-01-01 00:07:00            0              5.904900  
2014-01-01 00:08:00            0              5.314410  
2014-01-01 00:09:00            0              4.782969

[10 rows x 2 columns]
</code></pre>

<h4 id=""calculateadstocksthefastway"">Calculate ad stocks: the fast way</h4>

<p>So if we hate losing time, why not make use of vector operations that are way faster than index operations?  The first change we make below is initializing a vector for ad stock values instead of adding a new column to the dataset.  In the for loop we append the ad stock values  to the vector instead of adding the values directly to the dataset. Finally, we add the vector to the dataset as a new column after the for loop. In this way we can win a free minute, awesome!</p>

<pre><code class=""language-prettyprint lang-python prettyprinted""><span class=""com"">####################################################</span><span class=""pln"">
</span><span class=""str"">""""""test speed of vectorized ad stock calculations""""""</span><span class=""pln"">
</span><span class=""com"">####################################################</span><span class=""pln"">

</span><span class=""kwd"">def</span><span class=""pln""> calculate_ad_stocks_vectorized</span><span class=""pun"">(</span><span class=""pln"">data</span><span class=""pun"">,</span><span class=""pln""> engagement_factor</span><span class=""pun"">):</span><span class=""pln"">  
    </span><span class=""str"">""""""Calculate ad stocks in fast way""""""</span><span class=""pln"">
    </span><span class=""com"">#initialize ad stock vector</span><span class=""pln"">
    grp_on_rtl4_ad_stock_vector </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""pun"">[]</span><span class=""pln"">
    </span><span class=""com"">#initialize ad_stock_value</span><span class=""pln"">
    ad_stock_value </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""lit"">0</span><span class=""pln"">
    </span><span class=""com"">#loop through dataset to calculate ad stock values</span><span class=""pln"">
    </span><span class=""kwd"">for</span><span class=""pln""> index</span><span class=""pun"">,</span><span class=""pln""> row </span><span class=""kwd"">in</span><span class=""pln""> data</span><span class=""pun"">.</span><span class=""pln"">iterrows</span><span class=""pun"">():</span><span class=""pln"">
        ad_stock_value </span><span class=""pun"">=</span><span class=""pln""> row</span><span class=""pun"">[</span><span class=""str"">'grp_on_rtl4'</span><span class=""pun"">]</span><span class=""pln""> </span><span class=""pun"">+</span><span class=""pln""> ad_stock_value </span><span class=""pun"">*</span><span class=""pln""> engagement_factor
        grp_on_rtl4_ad_stock_vector</span><span class=""pun"">.</span><span class=""pln"">append</span><span class=""pun"">(</span><span class=""pln"">ad_stock_value</span><span class=""pun"">)</span><span class=""pln"">
    </span><span class=""com"">#add ad stock vector to dataset</span><span class=""pln"">
    data</span><span class=""pun"">[</span><span class=""str"">'grp_on_rtl4_ad_stock'</span><span class=""pun"">]</span><span class=""pln""> </span><span class=""pun"">=</span><span class=""pln""> grp_on_rtl4_ad_stock_vector
    </span><span class=""kwd"">return</span><span class=""pln""> data

</span><span class=""com"">#calculate time spend in vectorized and fast way</span><span class=""pln"">
time_start </span><span class=""pun"">=</span><span class=""pln""> datetime</span><span class=""pun"">.</span><span class=""pln"">datetime</span><span class=""pun"">.</span><span class=""pln"">now</span><span class=""pun"">()</span><span class=""pln"">  
data_ad_stocks </span><span class=""pun"">=</span><span class=""pln""> calculate_ad_stocks_vectorized</span><span class=""pun"">(</span><span class=""pln"">data</span><span class=""pun"">=</span><span class=""pln"">data</span><span class=""pun"">,</span><span class=""pln"">  
                                                engagement_factor</span><span class=""pun"">=</span><span class=""lit"">0.9</span><span class=""pun"">)</span><span class=""pln"">
time_end </span><span class=""pun"">=</span><span class=""pln""> datetime</span><span class=""pun"">.</span><span class=""pln"">datetime</span><span class=""pun"">.</span><span class=""pln"">now</span><span class=""pun"">()</span><span class=""pln"">  
time_spend_vectorized </span><span class=""pun"">=</span><span class=""pln""> time_end </span><span class=""pun"">-</span><span class=""pln""> time_start  
</span><span class=""kwd"">print</span><span class=""pln""> </span><span class=""str"">'\n Time spend for ad stock transformation in vectorized and fast way:  \n'</span><span class=""pun"">,</span><span class=""pln""> time_spend_vectorized  
</span><span class=""kwd"">print</span><span class=""pln""> </span><span class=""str"">'\n First ten rows of data including ad stock values:  \n'</span><span class=""pun"">,</span><span class=""pln""> data_ad_stocks</span><span class=""pun"">[:</span><span class=""lit"">10</span><span class=""pun"">]</span><span class=""pln"">

</span><span class=""com"">#calculate speed up factor</span><span class=""pln"">
speed_up_factor </span><span class=""pun"">=</span><span class=""pln""> time_spend_non_vectorized</span><span class=""pun"">.</span><span class=""pln"">total_seconds</span><span class=""pun"">()</span><span class=""pln""> </span><span class=""pun"">/</span><span class=""pln""> time_spend_vectorized</span><span class=""pun"">.</span><span class=""pln"">total_seconds</span><span class=""pun"">()</span><span class=""pln"">  
</span><span class=""kwd"">print</span><span class=""pln""> </span><span class=""str"">'\n Speed up factor:  \n'</span><span class=""pun"">,</span><span class=""pln""> speed_up_factor  </span></code></pre>

<pre><code class=""language-prettying lang-python"">Time spend for ad stock transformation in vectorized and fast way:  
0:00:15.557000

First ten rows of data including ad stock values:  
                     grp_on_rtl4  grp_on_rtl4_ad_stock
datetime  
2014-01-01 00:00:00            0              0.000000  
2014-01-01 00:01:00            0              0.000000  
2014-01-01 00:02:00           10             10.000000  
2014-01-01 00:03:00            0              9.000000  
2014-01-01 00:04:00            0              8.100000  
2014-01-01 00:05:00            0              7.290000  
2014-01-01 00:06:00            0              6.561000  
2014-01-01 00:07:00            0              5.904900  
2014-01-01 00:08:00            0              5.314410  
2014-01-01 00:09:00            0              4.782969

[10 rows x 2 columns]

Speed up factor:  
4.43427396028  
</code></pre>

<p>As we have seen above, you can speed up data manipulation in Pandas approximately 4.43 times by one line of code that contains a vector operation. In case of multiple variables I use a matrix manipulation instead of a vector manipulation per variable. In this way you can win approximately ten minutes if you have ten variables. </p>

<p>Next time I will blog what you can do with this data with the help of hierarchical Bayesian models. So we can determine the attributed value per TV commercial. Below you can already see a sneak preview of what we see in the actual data. </p>

<p><img src=""http://oi61.tinypic.com/2sab0wh.jpg"" alt=""Interaction between TV commercial and online website traffic"" class=""full-img""></p>

<p>To be continued..</p>
        ","As Data Scientists in the world of advertising we often need to proof the direct and delayed effects of advertising. A phenomena what we see in the last years is that consumers use multiple screens when they watch television. This is called screen-stacking. After a television commercial they can visit the website of the brand or product if they gained interest.
A couple of years ago this was done with data on week or day level. Nowadays we model on minute level at Blue Mango. We found direct interaction effects in the same minute between the commercial and website visits. In the twenty minutes after the commercial we still see an uplift in visits. To model the direct and delayed effect , we make use of ad stock variables. Therefore we first have to calculate ad stock values. On one hand the code should be easy to read. On the other hand we hate losing time on data collection, because we love models.
Creating a dataset
Today we only talk about data collection. Later on I will post a blog about hierarchical Bayesian models to determine the attributed value of TV commercials. One of the awesome packages in Python for data collection is Pandas. Yes, I am that guy that is addicted to Pandas. In the example below, we use Pandas to create a dataset of one year of data on minute level. The length of the dataset is 525 600 rows (365 days * 24 hours * 60 minutes). For this example we add one column with GRP’s of commercials on the TV channel RTL 4. GRP’s are the percent of the target group that watched the commercial in that specific minute. For now we only consider one commercial for the entire year.
#######################
""""""import libraries.""""""
#######################

import datetime  
import numpy as np  
import pandas as pd


#####################
""""""create dataset.""""""
#####################

def create_dataset(start_datetime=datetime.datetime(year=2014, month=1, day=1),  
                   end_datetime=datetime.datetime(year=2015,   month=1, day=1)):
    """"""Create data for 1 year of data on minute level and 1 column with GRP's on RTL4""""""
    #create datetime values between start_datetime and start_datetime on minute level
    time_index = np.arange(start_datetime, end_datetime, dtype='datetime64[m]')
    #create dataframe including time_index
    data = pd.DataFrame(index=time_index)
    #define index name
    data.index.name = 'datetime'
    #add column with zero's for GRP's on RTL4
    data ['grp_on_rtl4'] = float(0)
    #add ten GRP's in third minute on the first of January for one channel in an entire year
    data.ix[datetime.datetime(year=2014, month=1, day=1, hour=0, minute=2),
                              'grp_on_rtl4'] = 10
    return data

#create dataset and print first and last rows of data
data = create_dataset()  
print '\n First five rows of data: \n', data[:5]  
print '\n Last five rows of data:  \n', data[-5:]  
First five rows of data:  
                     grp_on_rtl4
datetime  
2014-01-01 00:00:00            0  
2014-01-01 00:01:00            0  
2014-01-01 00:02:00           10  
2014-01-01 00:03:00            0  
2014-01-01 00:04:00            0

[5 rows x 1 columns]

Last five rows of data:  
                     grp_on_rtl4
datetime  
2014-12-31 23:55:00            0  
2014-12-31 23:56:00            0  
2014-12-31 23:57:00            0  
2014-12-31 23:58:00            0  
2014-12-31 23:59:00            0

[5 rows x 1 columns]
Calculate ad stocks: the slow way
To calculate ad stock values we make use of an engagement factor of 0.9. So one minute after a commercial 90% of the consumers are engaged through the commercial and two minutes after a commercial 81% (0.9*0.9) of the consumers are still engaged through the commercial and so forth. Below we see a simple for loop that directly adds the ad stock values to the dataset. The disadvantage is that in every iteration we have to make use of an index operation. Index operations are great for quickly setting up a small program and data manipulation. But again, I hate losing time on data collection! In this case it takes more than one minute and this is just for one variable.
########################################################
""""""test speed of non-vectorized ad stock calculations""""""
########################################################

def calculate_ad_stocks_non_vectorized(data, engagement_factor=0.9):  
    """"""Calculate ad stocks on a non-vectorized and slow way""""""
    #initialize ad stock column
    data['grp_on_rtl4_ad_stock'] = 0
    #initialize ad_stock_value
    ad_stock_value = 0
    #loop through dataset to calculate ad stock values
    for index, row in data.iterrows():
        ad_stock_value = row['grp_on_rtl4'] + ad_stock_value * engagement_factor
        data.ix[index, 'grp_on_rtl4_ad_stock'] = ad_stock_value
    return data

#calculate time spend in non-vectorized and slow way
time_start = datetime.datetime.now()  
data_ad_stocks = calculate_ad_stocks_non_vectorized(data=data,  
                                                    engagement_factor=0.9)
time_end = datetime.datetime.now()  
time_spend_non_vectorized = time_end - time_start  
print '\n Time spend for ad stock transformation in non-vectorized and slow way:  \n', time_spend_non_vectorized  
print '\n First ten rows of data including ad stock values:  \n', data_ad_stocks[:10]  
Time spend for ad stock transformation in non-vectorized and slow way:  
0:01:08.984000

First ten rows of data including ad stock values:  
                     grp_on_rtl4  grp_on_rtl4_ad_stock
datetime  
2014-01-01 00:00:00            0              0.000000  
2014-01-01 00:01:00            0              0.000000  
2014-01-01 00:02:00           10             10.000000  
2014-01-01 00:03:00            0              9.000000  
2014-01-01 00:04:00            0              8.100000  
2014-01-01 00:05:00            0              7.290000  
2014-01-01 00:06:00            0              6.561000  
2014-01-01 00:07:00            0              5.904900  
2014-01-01 00:08:00            0              5.314410  
2014-01-01 00:09:00            0              4.782969

[10 rows x 2 columns]
Calculate ad stocks: the fast way
So if we hate losing time, why not make use of vector operations that are way faster than index operations? The first change we make below is initializing a vector for ad stock values instead of adding a new column to the dataset. In the for loop we append the ad stock values to the vector instead of adding the values directly to the dataset. Finally, we add the vector to the dataset as a new column after the for loop. In this way we can win a free minute, awesome!
####################################################
""""""test speed of vectorized ad stock calculations""""""
####################################################

def calculate_ad_stocks_vectorized(data, engagement_factor):  
    """"""Calculate ad stocks in fast way""""""
    #initialize ad stock vector
    grp_on_rtl4_ad_stock_vector = []
    #initialize ad_stock_value
    ad_stock_value = 0
    #loop through dataset to calculate ad stock values
    for index, row in data.iterrows():
        ad_stock_value = row['grp_on_rtl4'] + ad_stock_value * engagement_factor
        grp_on_rtl4_ad_stock_vector.append(ad_stock_value)
    #add ad stock vector to dataset
    data['grp_on_rtl4_ad_stock'] = grp_on_rtl4_ad_stock_vector
    return data

#calculate time spend in vectorized and fast way
time_start = datetime.datetime.now()  
data_ad_stocks = calculate_ad_stocks_vectorized(data=data,  
                                                engagement_factor=0.9)
time_end = datetime.datetime.now()  
time_spend_vectorized = time_end - time_start  
print '\n Time spend for ad stock transformation in vectorized and fast way:  \n', time_spend_vectorized  
print '\n First ten rows of data including ad stock values:  \n', data_ad_stocks[:10]

#calculate speed up factor
speed_up_factor = time_spend_non_vectorized.total_seconds() / time_spend_vectorized.total_seconds()  
print '\n Speed up factor:  \n', speed_up_factor  
Time spend for ad stock transformation in vectorized and fast way:  
0:00:15.557000

First ten rows of data including ad stock values:  
                     grp_on_rtl4  grp_on_rtl4_ad_stock
datetime  
2014-01-01 00:00:00            0              0.000000  
2014-01-01 00:01:00            0              0.000000  
2014-01-01 00:02:00           10             10.000000  
2014-01-01 00:03:00            0              9.000000  
2014-01-01 00:04:00            0              8.100000  
2014-01-01 00:05:00            0              7.290000  
2014-01-01 00:06:00            0              6.561000  
2014-01-01 00:07:00            0              5.904900  
2014-01-01 00:08:00            0              5.314410  
2014-01-01 00:09:00            0              4.782969

[10 rows x 2 columns]

Speed up factor:  
4.43427396028  
As we have seen above, you can speed up data manipulation in Pandas approximately 4.43 times by one line of code that contains a vector operation. In case of multiple variables I use a matrix manipulation instead of a vector manipulation per variable. In this way you can win approximately ten minutes if you have ten variables.
Next time I will blog what you can do with this data with the help of hierarchical Bayesian models. So we can determine the attributed value per TV commercial. Below you can already see a sneak preview of what we see in the actual data.
To be continued..","[Data Science, python]"
131,Caching $http requests in AngularJS,/caching-http-requests-in-angularjs/,"
            <p>The other day I was looking at one or two applications I've developed lately and also looked at some that some friends of mine created. I was looking at the usage of API calls in particular. The first thing I found out is that we are using a ridiculous amount of API calls to get things done. Of course this is not a suprise. In this era of SPA's we are heavily relying on API calls more than ever. </p>

<p>No, the thing I was suprised about was that we do a lot of duplicate requests. Some pages even made multiple calls to the same endpoint at the same page. Luckily in most cases we have control over the configuration of the server as well, so if we properly configure the caching headers, everything is fine. Because when it comes to knowing the state of the data and when to invalidate, the server knows best, I believe the caching is the responsability of the server and should be taken care of by the server.  </p>

<p>But sometimes you do not have control over the caching headers, for example when using a third party API. In those cases you do not want your application to make multiple of the same requests over and over again. Configuring the caching clientside might come in handy in those circumstances. </p>

<h3 id=""cachinghttprequestsclientside"">Caching HTTP requests clientside</h3>

<p>Fortunately, enable caching with Angular is very easy and straightforward. To enable cache for an HTTP request you set the <mark>cache</mark> property on the $http configuration object to true.  </p>

<pre><code class=""language-prettyprint lang-javascript prettyprinted""><span class=""pln"">$http</span><span class=""pun"">.</span><span class=""kwd"">get</span><span class=""pun"">(</span><span class=""str"">'api/path'</span><span class=""pun"">,{</span><span class=""pln"">
    cache</span><span class=""pun"">:</span><span class=""pln""> </span><span class=""kwd"">true</span><span class=""pln"">
</span><span class=""pun"">}</span></code></pre>

<p>Angular will now cache the HTTP response object in the default $http cache object. That object can be retrieved by using the get method on the $cachefactory and passing it $http as the first parameter.</p>

<pre><code class=""language-prettyprint lang-javascript prettyprinted""><span class=""kwd"">var</span><span class=""pln""> httpCache </span><span class=""pun"">=</span><span class=""pln""> $cacheFactory</span><span class=""pun"">.</span><span class=""kwd"">get</span><span class=""pun"">(</span><span class=""str"">'$http'</span><span class=""pun"">);</span><span class=""pln"">  </span></code></pre>

<p>Items on the $cachefactory service are stored as key-pair values. When you cache an $http request, the url specified on the $http configuration object is used as the key for the cached value. You can simply retrieve that object by calling the get method on the $httpCache and pass the url as the first parameter. The actual value that is cached, is the http response object, and so this value will be returned. </p>

<pre><code class=""language-prettyprint lang-javascript prettyprinted""><span class=""kwd"">var</span><span class=""pln""> cachedResponse </span><span class=""pun"">=</span><span class=""pln""> httpCache</span><span class=""pun"">.</span><span class=""kwd"">get</span><span class=""pun"">(</span><span class=""str"">'api/path'</span><span class=""pun"">);</span><span class=""pln"">  </span></code></pre>

<p>If you prefer to cache the response in your own cache object, you can specify that object as the cache property instead of simply specifying true. </p>

<pre><code class=""language-prettyprint lang-javascript prettyprinted""><span class=""kwd"">var</span><span class=""pln""> dataCache </span><span class=""pun"">=</span><span class=""pln""> $cacheFactory</span><span class=""pun"">.</span><span class=""kwd"">get</span><span class=""pun"">(</span><span class=""str"">'yourApplicationCache'</span><span class=""pun"">);</span><span class=""pln"">  
$http</span><span class=""pun"">.</span><span class=""kwd"">get</span><span class=""pun"">(</span><span class=""str"">'api/path'</span><span class=""pun"">,{</span><span class=""pln"">
    cache</span><span class=""pun"">:</span><span class=""pln""> dataCache
</span><span class=""pun"">}</span></code></pre>

<p>Of course you should make sure you defined the yourApplicationCache cache somewhere else in your application. Like this:</p>

<pre><code class=""language-prettyprint lang-javascript prettyprinted""><span class=""kwd"">var</span><span class=""pln""> applicationCache </span><span class=""pun"">=</span><span class=""pln""> $cacheFactory</span><span class=""pun"">.</span><span class=""kwd"">get</span><span class=""pun"">(</span><span class=""str"">'yourApplicationCache'</span><span class=""pun"">);</span><span class=""pln"">  </span></code></pre>

<p>It's good to know the promise returned from the $http service will be resolved with the cached reponse object. The processing of the response will behave as if the call had actually gone to the server, so you will not have to make any changes elsewhere in your application to make the caching work. Perfect.</p>

<h3 id=""invalidatehttpcache"">Invalidate HTTP cache</h3>

<p>Of course we need to invalidate the cache if the data ever changes. We do this by getting the default $http cache, and call the <code>remove</code> method and pass it the url we used in our $http configuration object.  </p>

<pre><code class=""language-prettyprint lang-javascript prettyprinted""><span class=""kwd"">var</span><span class=""pln""> httpCache </span><span class=""pun"">=</span><span class=""pln""> $cacheFactory</span><span class=""pun"">.</span><span class=""kwd"">get</span><span class=""pun"">(</span><span class=""str"">'$http'</span><span class=""pun"">);</span><span class=""pln"">  
httpCache</span><span class=""pun"">.</span><span class=""pln"">remove</span><span class=""pun"">(</span><span class=""str"">'api/path'</span><span class=""pun"">);</span><span class=""pln"">  </span></code></pre>
        ","The other day I was looking at one or two applications I've developed lately and also looked at some that some friends of mine created. I was looking at the usage of API calls in particular. The first thing I found out is that we are using a ridiculous amount of API calls to get things done. Of course this is not a suprise. In this era of SPA's we are heavily relying on API calls more than ever.
No, the thing I was suprised about was that we do a lot of duplicate requests. Some pages even made multiple calls to the same endpoint at the same page. Luckily in most cases we have control over the configuration of the server as well, so if we properly configure the caching headers, everything is fine. Because when it comes to knowing the state of the data and when to invalidate, the server knows best, I believe the caching is the responsability of the server and should be taken care of by the server.
But sometimes you do not have control over the caching headers, for example when using a third party API. In those cases you do not want your application to make multiple of the same requests over and over again. Configuring the caching clientside might come in handy in those circumstances.
Caching HTTP requests clientside
Fortunately, enable caching with Angular is very easy and straightforward. To enable cache for an HTTP request you set the cache property on the $http configuration object to true.
$http.get('api/path',{
    cache: true
}
Angular will now cache the HTTP response object in the default $http cache object. That object can be retrieved by using the get method on the $cachefactory and passing it $http as the first parameter.
var httpCache = $cacheFactory.get('$http');  
Items on the $cachefactory service are stored as key-pair values. When you cache an $http request, the url specified on the $http configuration object is used as the key for the cached value. You can simply retrieve that object by calling the get method on the $httpCache and pass the url as the first parameter. The actual value that is cached, is the http response object, and so this value will be returned.
var cachedResponse = httpCache.get('api/path');  
If you prefer to cache the response in your own cache object, you can specify that object as the cache property instead of simply specifying true.
var dataCache = $cacheFactory.get('yourApplicationCache');  
$http.get('api/path',{
    cache: dataCache
}
Of course you should make sure you defined the yourApplicationCache cache somewhere else in your application. Like this:
var applicationCache = $cacheFactory.get('yourApplicationCache');  
It's good to know the promise returned from the $http service will be resolved with the cached reponse object. The processing of the response will behave as if the call had actually gone to the server, so you will not have to make any changes elsewhere in your application to make the caching work. Perfect.
Invalidate HTTP cache
Of course we need to invalidate the cache if the data ever changes. We do this by getting the default $http cache, and call the remove method and pass it the url we used in our $http configuration object.
var httpCache = $cacheFactory.get('$http');  
httpCache.remove('api/path');  ","[angularjs, Code]"
132,We be clickin ’n tappin: adjusting a CTA based on device capabilities,/we-be-clickin-n-tappin-adjusting-cta-based-on-device-capabilities/,"
            <p>I’m a firm believer in the power of the 'Click here’ call-to-action (or CTA). It’s probably the simplest, most direct way of getting a user to perform an action in a banner. Although a lot of findings suggest the expression to be too generic, data shows time and time again that when used in the right setting, ‘Click here’ outperforms other CTA by far.</p>

<h4>What's in a name</h4>  

<p>So it was no surprise that when Blue Mango started developing banners for mobile (i.e. for in-app display), we tested the wording of our CTA buttons. After all, since we were now developing for touch-enabled devices, we felt that using the verb ‘to click’ would not be appropriate for the desired user interaction. So we pitched three CTA buttons against one another:</p>

<p><strong>Klik hier</strong> (‘click here’ in Dutch)<br>
<strong>Tik hier</strong>  (’tap here’ in Dutch)<br>
<strong>Tap hier</strong> (our own wild mix of English and Dutch…)</p>

<p>The winner was our ‘Tik hier’ (tap here) CTA, with an APMV (Action Per Million Views) rate of 110, compared to 68 for ‘Klik hier'. This proved our hunch that device-optimised wording has a positive influence on user interaction.</p>

<h4>Let's give 'em what they need</h4>  

<p>Fast forward to 2014, when Blue Mango switched from Flash to HTML bannering using our very own Splash! platform. The most obvious advantage of using HTML was without a doubt the fact that we could now start to develop banners that are truly platform independent. But with great power comes great responsibility: we needed to make sure that anytime a user interaction was referenced in our CTA, we needed to adapt our CTA to the user’s device.</p>

<p>So we decided to use vanilla Javascript to detect whether a device is touch enabled or not before adapting the CTA accordingly:</p>

<pre><code class=""language-prettyprint lang-javascript prettyprinted""><span class=""com"">// Check if device has touch capabilities</span><span class=""pln"">
</span><span class=""kwd"">var</span><span class=""pln""> isTouchDevice </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""str"">'ontouchstart'</span><span class=""pln""> </span><span class=""kwd"">in</span><span class=""pln""> document</span><span class=""pun"">.</span><span class=""pln"">documentElement</span><span class=""pun"">;</span><span class=""pln"">

</span><span class=""com"">// Determine the CTA </span><span class=""pln"">
</span><span class=""kwd"">var</span><span class=""pln""> ctaText </span><span class=""pun"">=</span><span class=""pln""> isTouchDevice </span><span class=""pun"">?</span><span class=""pln""> </span><span class=""str"">'Tap here'</span><span class=""pln""> </span><span class=""pun"">:</span><span class=""pln""> </span><span class=""str"">'Click here'</span><span class=""pun"">;</span><span class=""pln"">

</span><span class=""com"">// Set the new CTA</span><span class=""pln"">
document</span><span class=""pun"">.</span><span class=""pln"">getElementById</span><span class=""pun"">(</span><span class=""str"">'cta-button'</span><span class=""pun"">).</span><span class=""pln"">innerText </span><span class=""pun"">=</span><span class=""pln""> ctaText</span><span class=""pun"">;</span><span class=""pln"">  </span></code></pre>

<p>There you have it. A clean, simple and user-friendly solution. As an added bonus, we quickly discovered that we could finally put our newfangled touchscreen laptops to good use. After all, the novelty of tapping a banner on a computer screen is far more fun than simply clicking it...</p>

<p>This goes to show that (and I quote Ludwig Mies van der Rohe) God is in the detail.</p>

<p>Happy clicking and tapping, y’all!</p>
        ","I’m a firm believer in the power of the 'Click here’ call-to-action (or CTA). It’s probably the simplest, most direct way of getting a user to perform an action in a banner. Although a lot of findings suggest the expression to be too generic, data shows time and time again that when used in the right setting, ‘Click here’ outperforms other CTA by far.
What's in a name
So it was no surprise that when Blue Mango started developing banners for mobile (i.e. for in-app display), we tested the wording of our CTA buttons. After all, since we were now developing for touch-enabled devices, we felt that using the verb ‘to click’ would not be appropriate for the desired user interaction. So we pitched three CTA buttons against one another:
Klik hier (‘click here’ in Dutch)
Tik hier (’tap here’ in Dutch)
Tap hier (our own wild mix of English and Dutch…)
The winner was our ‘Tik hier’ (tap here) CTA, with an APMV (Action Per Million Views) rate of 110, compared to 68 for ‘Klik hier'. This proved our hunch that device-optimised wording has a positive influence on user interaction.
Let's give 'em what they need
Fast forward to 2014, when Blue Mango switched from Flash to HTML bannering using our very own Splash! platform. The most obvious advantage of using HTML was without a doubt the fact that we could now start to develop banners that are truly platform independent. But with great power comes great responsibility: we needed to make sure that anytime a user interaction was referenced in our CTA, we needed to adapt our CTA to the user’s device.
So we decided to use vanilla Javascript to detect whether a device is touch enabled or not before adapting the CTA accordingly:
// Check if device has touch capabilities
var isTouchDevice = 'ontouchstart' in document.documentElement;

// Determine the CTA 
var ctaText = isTouchDevice ? 'Tap here' : 'Click here';

// Set the new CTA
document.getElementById('cta-button').innerText = ctaText;  
There you have it. A clean, simple and user-friendly solution. As an added bonus, we quickly discovered that we could finally put our newfangled touchscreen laptops to good use. After all, the novelty of tapping a banner on a computer screen is far more fun than simply clicking it...
This goes to show that (and I quote Ludwig Mies van der Rohe) God is in the detail.
Happy clicking and tapping, y’all!","[cro, Code, Call-To-Action]"
133,Simplify iFrame tracking for Universal Analytics,/simplify-iframe-tracking-for-universal-analytics/,"
            <p>Migrating Google Analytics to Universal Analytics is a fairly straight forward process. Especially when you have a tagmanager in place. The only challange is iFrames. </p>

<p>In the <a href=""https://developers.google.com/analytics/devguides/collection/analyticsjs/cross-domain#iframe"">default documentation for iFrame tracking</a> Google advices you to use a div-element:</p>

<pre><code class=""language- prettyprint prettyprinted""><span class=""tag"">&lt;div</span><span class=""pln""> </span><span class=""atn"">id</span><span class=""pun"">=</span><span class=""atv"">""myiFrame""</span><span class=""tag"">&gt;&lt;/div&gt;</span><span class=""pln"">

</span><span class=""tag"">&lt;script&gt;</span><span class=""pln"">  
</span><span class=""kwd"">var</span><span class=""pln""> linker</span><span class=""pun"">;</span><span class=""pln"">

</span><span class=""kwd"">function</span><span class=""pln""> addiFrame</span><span class=""pun"">(</span><span class=""pln"">divId</span><span class=""pun"">,</span><span class=""pln""> url</span><span class=""pun"">,</span><span class=""pln""> opt_hash</span><span class=""pun"">)</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">  
  </span><span class=""kwd"">return</span><span class=""pln""> </span><span class=""kwd"">function</span><span class=""pun"">(</span><span class=""pln"">tracker</span><span class=""pun"">)</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">
    window</span><span class=""pun"">.</span><span class=""pln"">linker </span><span class=""pun"">=</span><span class=""pln""> window</span><span class=""pun"">.</span><span class=""pln"">linker </span><span class=""pun"">||</span><span class=""pln""> </span><span class=""kwd"">new</span><span class=""pln""> window</span><span class=""pun"">.</span><span class=""pln"">gaplugins</span><span class=""pun"">.</span><span class=""typ"">Linker</span><span class=""pun"">(</span><span class=""pln"">tracker</span><span class=""pun"">);</span><span class=""pln"">
    </span><span class=""kwd"">var</span><span class=""pln""> iFrame </span><span class=""pun"">=</span><span class=""pln""> document</span><span class=""pun"">.</span><span class=""pln"">createElement</span><span class=""pun"">(</span><span class=""str"">'iFrame'</span><span class=""pun"">);</span><span class=""pln"">
    iFrame</span><span class=""pun"">.</span><span class=""pln"">src </span><span class=""pun"">=</span><span class=""pln""> window</span><span class=""pun"">.</span><span class=""pln"">linker</span><span class=""pun"">.</span><span class=""pln"">decorate</span><span class=""pun"">(</span><span class=""pln"">url</span><span class=""pun"">,</span><span class=""pln""> opt_hash</span><span class=""pun"">);</span><span class=""pln"">
    document</span><span class=""pun"">.</span><span class=""pln"">getElementById</span><span class=""pun"">(</span><span class=""pln"">divId</span><span class=""pun"">).</span><span class=""pln"">appendChild</span><span class=""pun"">(</span><span class=""pln"">iFrame</span><span class=""pun"">);</span><span class=""pln"">
  </span><span class=""pun"">};</span><span class=""pln"">
</span><span class=""pun"">}</span><span class=""pln"">

</span><span class=""com"">// Dynamically add the iFrame to the page with proper linker parameters.</span><span class=""pln"">
ga</span><span class=""pun"">(</span><span class=""pln"">addiFrame</span><span class=""pun"">(</span><span class=""str"">'myiFrame'</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""str"">'destination.html'</span><span class=""pun"">));</span><span class=""pln"">

</span><span class=""tag"">&lt;/script&gt;</span><span class=""pln"">  </span></code></pre>

<p>If your website has several iframes, this will require updating all the onsite iframes to div's. With a tagmanager in place, you want to minimize changes that require the website's development team. Luckily, the code mentioned above can be modified to work as the standard Google Analytics iFrame tracking, removing the need for div's:</p>

<pre><code class=""language-prettyprint prettyprinted""><span class=""tag"">&lt;iframe</span><span class=""pln""> </span><span class=""atn"">id</span><span class=""pun"">=</span><span class=""atv"">""myiFrame""</span><span class=""tag"">&gt;&lt;/iframe&gt;</span><span class=""pln"">

</span><span class=""tag"">&lt;script&gt;</span><span class=""pln"">  
</span><span class=""kwd"">var</span><span class=""pln""> linker</span><span class=""pun"">;</span><span class=""pln"">

</span><span class=""kwd"">function</span><span class=""pln""> addiFrame</span><span class=""pun"">(</span><span class=""pln"">iframeId</span><span class=""pun"">,</span><span class=""pln""> iframeUrl</span><span class=""pun"">)</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">  
  </span><span class=""kwd"">return</span><span class=""pln""> </span><span class=""kwd"">function</span><span class=""pun"">(</span><span class=""pln"">tracker</span><span class=""pun"">)</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">
    window</span><span class=""pun"">.</span><span class=""pln"">linker </span><span class=""pun"">=</span><span class=""pln""> window</span><span class=""pun"">.</span><span class=""pln"">linker </span><span class=""pun"">||</span><span class=""pln""> </span><span class=""kwd"">new</span><span class=""pln""> window</span><span class=""pun"">.</span><span class=""pln"">gaplugins</span><span class=""pun"">.</span><span class=""typ"">Linker</span><span class=""pun"">(</span><span class=""pln"">tracker</span><span class=""pun"">);</span><span class=""pln"">
    </span><span class=""kwd"">var</span><span class=""pln""> iFrame </span><span class=""pun"">=</span><span class=""pln""> document</span><span class=""pun"">.</span><span class=""pln"">getElementById</span><span class=""pun"">(</span><span class=""pln"">iframeId</span><span class=""pun"">);</span><span class=""pln"">
    iFrame</span><span class=""pun"">.</span><span class=""pln"">src </span><span class=""pun"">=</span><span class=""pln""> window</span><span class=""pun"">.</span><span class=""pln"">linker</span><span class=""pun"">.</span><span class=""pln"">decorate</span><span class=""pun"">(</span><span class=""pln"">iframeUrl</span><span class=""pun"">);</span><span class=""pln"">
  </span><span class=""pun"">};</span><span class=""pln"">
</span><span class=""pun"">}</span><span class=""pln"">

</span><span class=""com"">// Dynamically add the iFrame to the page with proper linker parameters.</span><span class=""pln"">
ga</span><span class=""pun"">(</span><span class=""pln"">addiFrame</span><span class=""pun"">(</span><span class=""str"">'myiFrame'</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""str"">'destination.html'</span><span class=""pun"">));</span><span class=""pln"">

</span><span class=""tag"">&lt;/script&gt;</span><span class=""pln"">  </span></code></pre>

<p>Please note that the iFrame src attribute <strong>must be empty by default</strong>. If it is not empty when loading the code, Universal Analytics won't be able to pass the information correctly. You'll also need to add the referring domain (the page on which your iframe will be loaded) to <a href=""https://support.google.com/analytics/answer/2795830?hl=en"">the referral exclusion list</a>.</p>

<p>With this small change to the Universal Analytics iFrame tracking function, you are ready to migrate without any onsite changes.</p>
        ","Migrating Google Analytics to Universal Analytics is a fairly straight forward process. Especially when you have a tagmanager in place. The only challange is iFrames.
In the default documentation for iFrame tracking Google advices you to use a div-element:
<div id=""myiFrame""></div>

<script>  
var linker;

function addiFrame(divId, url, opt_hash) {  
  return function(tracker) {
    window.linker = window.linker || new window.gaplugins.Linker(tracker);
    var iFrame = document.createElement('iFrame');
    iFrame.src = window.linker.decorate(url, opt_hash);
    document.getElementById(divId).appendChild(iFrame);
  };
}

// Dynamically add the iFrame to the page with proper linker parameters.
ga(addiFrame('myiFrame', 'destination.html'));

</script>  
If your website has several iframes, this will require updating all the onsite iframes to div's. With a tagmanager in place, you want to minimize changes that require the website's development team. Luckily, the code mentioned above can be modified to work as the standard Google Analytics iFrame tracking, removing the need for div's:
<iframe id=""myiFrame""></iframe>

<script>  
var linker;

function addiFrame(iframeId, iframeUrl) {  
  return function(tracker) {
    window.linker = window.linker || new window.gaplugins.Linker(tracker);
    var iFrame = document.getElementById(iframeId);
    iFrame.src = window.linker.decorate(iframeUrl);
  };
}

// Dynamically add the iFrame to the page with proper linker parameters.
ga(addiFrame('myiFrame', 'destination.html'));

</script>  
Please note that the iFrame src attribute must be empty by default. If it is not empty when loading the code, Universal Analytics won't be able to pass the information correctly. You'll also need to add the referring domain (the page on which your iframe will be loaded) to the referral exclusion list.
With this small change to the Universal Analytics iFrame tracking function, you are ready to migrate without any onsite changes.","[google analytics, iframes, cross domain, Code]"
134,Thoughts on new events for Apple's new Macbook Force Touch trackpad,/hypothetical-new-events-for-the-new-macbook-touchpad/,"
            <p>For the very few who haven't heard it by now: Apple announced their <a href=""https://www.youtube.com/watch?v=hajnEpCq5SE"">new Macbook</a> last week. It has been praised and it <a href=""https://www.youtube.com/watch?v=KHZ8ek-6ccc"">has been made fun of</a>. What's really interesting for developers is the new built-in Force Touch Trackpad. Because of its four sensors, it is pressure-sensitive. </p>

<p>Apple has built-in default behavior for a deep click (or a Force Click, as they call it) in applications on their operation system. For example, when you Force Click a word in Safari, it opens the Wikipedia page on that word. A cool feature, but it would be much more interesting if we could use the possibilities of the new trackpad to change the way users interact with our applications and websites.</p>

<p>One of the most popular examples of using pressure of an input device is ""Pressure-sensitive drawing"", where the size of a brush changes with the amount of pressure. But that's just the beginning. The new Macbook lets you control the level of fast forwarding of a video with the amount of pressure you put on the trackpad. I think we can all see the potential in this for interaction and creativity.</p>

<h3 id=""detectdifferentpressurelevelsinthebrowser"">Detect different pressure levels in the browser</h3>

<p>I was wondering if pressure levels on mouse events are supported in current browsers. The first place I usually go to when I want to know if some feature exists, is the W3C specification. After a few minutes, I found out that there was nothing about pressure or click force in the <a href=""http://www.w3.org/TR/DOM-Level-2-Events/events.html#Events-MouseEvent"">W3C Specification for Mouse Events</a>. Bummer. </p>

<p>Next stop: <a href=""https://developer.mozilla.org/nl/"">MDN</a>. More luck this time: I found a non-standard property called <code>pressure</code> in the <a href=""https://developer.mozilla.org/en-US/docs/Web/API/MouseEvent"">Mouse Event API</a>. It says:</p>

<blockquote>
  <p><strong>MouseEvent.mozPressure</strong> - The amount of pressure applied to a touch or tablet device when generating the event; this value ranges between 0.0 (minimum pressure) and 1.0 (maximum pressure).</p>
</blockquote>

<p>This property has been available since Firefox 4.0, released in March 2011. However, this property is not available for mice, only for devices like drawing tablets and trackpads. So unfortunately, this property is useless on the new Macbook, even if you're using a Gecko-based browser.</p>

<p>Even though the property is not even in the current W3C specification or in one of its drafts, I think this <code>pressure</code> property would be a great fit for Apple's new trackpad. I would be really surprised if Webkit and Blink don't implement this within a few years. </p>

<h3 id=""introducingnewshorthandevents"">Introducing new shorthand events</h3>

<p>Although introducing the <code>pressure</code> property in more browsers and making it work with the new trackpad would give you great power and flexibility, it might also be a good idea to create some shorthand events:</p>

<pre><code class=""language-prettyprint lang-javascript prettyprinted""><span class=""pln"">element</span><span class=""pun"">.</span><span class=""pln"">addEventListener</span><span class=""pun"">(</span><span class=""str"">'mousedowndeep'</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""kwd"">function</span><span class=""pun"">()</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">  
    </span><span class=""com"">// User pressed hard</span><span class=""pln"">
</span><span class=""pun"">});</span><span class=""pln"">

element</span><span class=""pun"">.</span><span class=""pln"">addEventListener</span><span class=""pun"">(</span><span class=""str"">'mousedownlight'</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""kwd"">function</span><span class=""pun"">()</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">  
    </span><span class=""com"">// User pressed subtly  </span><span class=""pln"">
</span><span class=""pun"">});</span></code></pre>

<p>Of course we could do exactly the same thing with <code>onmousemove</code>. Apple calls the deeper click a Force Click, so me may want to have <code>forceclick</code> event as well. </p>

<p>Even if vendors don't choose to add these, we could easily build a naive implementation using custom elements. That could look something like this:</p>

<pre><code class=""language-prettyprint lang-javascript prettyprinted""><span class=""kwd"">var</span><span class=""pln""> elementWithPressureEvents </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""typ"">Object</span><span class=""pun"">.</span><span class=""pln"">create</span><span class=""pun"">(</span><span class=""typ"">HTMLElement</span><span class=""pun"">.</span><span class=""pln"">prototype</span><span class=""pun"">);</span><span class=""pln"">  
elementWithPressureEvents</span><span class=""pun"">.</span><span class=""pln"">createdCallback </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""kwd"">function</span><span class=""pun"">()</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">  
  </span><span class=""kwd"">this</span><span class=""pun"">.</span><span class=""pln"">addEventListener</span><span class=""pun"">(</span><span class=""str"">'mousedown'</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""kwd"">function</span><span class=""pun"">(</span><span class=""kwd"">event</span><span class=""pun"">)</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">
    </span><span class=""kwd"">event</span><span class=""pun"">.</span><span class=""pln"">preventDefault</span><span class=""pun"">();</span><span class=""pln"">
    </span><span class=""kwd"">var</span><span class=""pln""> mousePressureEvent </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""kwd"">event</span><span class=""pun"">;</span><span class=""pln"">
    </span><span class=""kwd"">if</span><span class=""pun"">(</span><span class=""pln"">e</span><span class=""pun"">.</span><span class=""pln"">pressure </span><span class=""pun"">&gt;</span><span class=""pln""> </span><span class=""pun"">.</span><span class=""lit"">75</span><span class=""pun"">)</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">
        mousePressureEvent </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""kwd"">new</span><span class=""pln""> </span><span class=""typ"">Event</span><span class=""pun"">(</span><span class=""str"">'mousedowndeep'</span><span class=""pun"">);</span><span class=""pln"">
    </span><span class=""pun"">}</span><span class=""pln""> </span><span class=""kwd"">else</span><span class=""pln""> </span><span class=""kwd"">if</span><span class=""pun"">(</span><span class=""pln"">e</span><span class=""pun"">.</span><span class=""pln"">pressure </span><span class=""pun"">&lt;</span><span class=""pln""> </span><span class=""pun"">.</span><span class=""lit"">25</span><span class=""pun"">)</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">
        mousePressureEvent </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""kwd"">new</span><span class=""pln""> </span><span class=""typ"">Event</span><span class=""pun"">(</span><span class=""str"">'mousedownlight'</span><span class=""pun"">);</span><span class=""pln"">
    </span><span class=""pun"">}</span><span class=""pln"">
    elem</span><span class=""pun"">.</span><span class=""pln"">dispatchEvent</span><span class=""pun"">(</span><span class=""pln"">mousePressureEvent</span><span class=""pun"">);</span><span class=""pln"">
  </span><span class=""pun"">});</span><span class=""pln"">
</span><span class=""pun"">};</span><span class=""pln"">

document</span><span class=""pun"">.</span><span class=""pln"">registerElement</span><span class=""pun"">(</span><span class=""str"">'with-ressure-mousedown-events'</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""pun"">{</span><span class=""pln"">  
  prototype</span><span class=""pun"">:</span><span class=""pln""> elementWithPressureEvents
</span><span class=""pun"">});</span></code></pre>

<p>In your HTML you could use this:</p>

<pre><code class=""language-prettyprint lang-html prettyprinted""><span class=""tag"">&lt;div</span><span class=""pln""> </span><span class=""atn"">id</span><span class=""pun"">=</span><span class=""atv"">""with-pressure-mousedown-events""</span><span class=""tag"">&gt;</span><span class=""pln"">your element</span><span class=""tag"">&lt;/div&gt;</span><span class=""pln"">  </span></code></pre>

<h3 id=""whataboutpressurewithtouchevents"">What about pressure with Touch Events?</h3>

<p>Just as having the pressure level on a Mouse Event would be great, having the same data available for Touch Events offers great possibilities as well. Fortunately, this is already possible. According to the <a href=""http://www.w3.org/TR/2011/WD-touch-events-20110505/#widl-Touch-force"">Touch Events W3C Specficication</a>, there is a property called <code>force</code> available in the <a href=""https://developer.mozilla.org/en-US/docs/Web/API/Touch"">Touch object</a> interface. This property holds the amount of pressure being applied to the surface by the user, as a float between 0.0 (no pressure) and 1.0 (maximum pressure). </p>

<p>Using this property is already very common. Let's look at a simple example where we loop through all touch points and read the <code>force</code> value.</p>

<pre><code class=""language-prettyprint lang-javascript prettyprinted""><span class=""pln"">document</span><span class=""pun"">.</span><span class=""pln"">body</span><span class=""pun"">.</span><span class=""pln"">addEventListener</span><span class=""pun"">(</span><span class=""str"">'touchstart'</span><span class=""pun"">,</span><span class=""pln""> </span><span class=""kwd"">function</span><span class=""pun"">(</span><span class=""pln"">e</span><span class=""pun"">){</span><span class=""pln"">  
    </span><span class=""kwd"">var</span><span class=""pln""> touchlist </span><span class=""pun"">=</span><span class=""pln""> e</span><span class=""pun"">.</span><span class=""pln"">touches
    </span><span class=""com"">// loop through all touch points currently in contact with surface</span><span class=""pln"">
    </span><span class=""kwd"">for</span><span class=""pln""> </span><span class=""pun"">(</span><span class=""kwd"">var</span><span class=""pln""> touchIndex</span><span class=""pun"">=</span><span class=""lit"">0</span><span class=""pun"">;</span><span class=""pln""> touchIndex</span><span class=""pun"">&lt;</span><span class=""pln"">touchlist</span><span class=""pun"">.</span><span class=""pln"">length</span><span class=""pun"">;</span><span class=""pln""> touchIndex</span><span class=""pun"">++){</span><span class=""pln""> 
        </span><span class=""kwd"">var</span><span class=""pln""> currentTouchPressure </span><span class=""pun"">=</span><span class=""pln""> touchlist</span><span class=""pun"">[</span><span class=""pln"">i</span><span class=""pun"">].</span><span class=""pln"">force</span><span class=""pun"">;</span><span class=""pln"">
        </span><span class=""com"">//do something with the currentTouchPressure</span><span class=""pln"">
    </span><span class=""pun"">}</span><span class=""pln"">
</span><span class=""pun"">},</span><span class=""pln""> </span><span class=""kwd"">false</span><span class=""pun"">)</span></code></pre>

<p>MDN notes that the availability of values is hardware-dependent. For example, if the device doesn't have a way to detect the amount of pressure placed on the surface, the force value will always be 1. </p>

<h3 id=""conclusion"">Conclusion</h3>

<p>The new Macbook will be available to the audience very soon, and so will the new trackpad. Unfortunately there is no way to exploit this in the browser right now. Although all suggestions in this post are purely hypothetical, I can't imagine browser vendors won't start implementing support for multiple pressure levels. At this point, it's just nothing but a speculation game. Even though Apple patented the Force Touch Trackpad, more and more hardware manufacturers are likely to implement similar features. Definitely something I'm looking forward to! </p>
        ","For the very few who haven't heard it by now: Apple announced their new Macbook last week. It has been praised and it has been made fun of. What's really interesting for developers is the new built-in Force Touch Trackpad. Because of its four sensors, it is pressure-sensitive.
Apple has built-in default behavior for a deep click (or a Force Click, as they call it) in applications on their operation system. For example, when you Force Click a word in Safari, it opens the Wikipedia page on that word. A cool feature, but it would be much more interesting if we could use the possibilities of the new trackpad to change the way users interact with our applications and websites.
One of the most popular examples of using pressure of an input device is ""Pressure-sensitive drawing"", where the size of a brush changes with the amount of pressure. But that's just the beginning. The new Macbook lets you control the level of fast forwarding of a video with the amount of pressure you put on the trackpad. I think we can all see the potential in this for interaction and creativity.
Detect different pressure levels in the browser
I was wondering if pressure levels on mouse events are supported in current browsers. The first place I usually go to when I want to know if some feature exists, is the W3C specification. After a few minutes, I found out that there was nothing about pressure or click force in the W3C Specification for Mouse Events. Bummer.
Next stop: MDN. More luck this time: I found a non-standard property called pressure in the Mouse Event API. It says:
MouseEvent.mozPressure - The amount of pressure applied to a touch or tablet device when generating the event; this value ranges between 0.0 (minimum pressure) and 1.0 (maximum pressure).
This property has been available since Firefox 4.0, released in March 2011. However, this property is not available for mice, only for devices like drawing tablets and trackpads. So unfortunately, this property is useless on the new Macbook, even if you're using a Gecko-based browser.
Even though the property is not even in the current W3C specification or in one of its drafts, I think this pressure property would be a great fit for Apple's new trackpad. I would be really surprised if Webkit and Blink don't implement this within a few years.
Introducing new shorthand events
Although introducing the pressure property in more browsers and making it work with the new trackpad would give you great power and flexibility, it might also be a good idea to create some shorthand events:
element.addEventListener('mousedowndeep', function() {  
    // User pressed hard
});

element.addEventListener('mousedownlight', function() {  
    // User pressed subtly  
});
Of course we could do exactly the same thing with onmousemove. Apple calls the deeper click a Force Click, so me may want to have forceclick event as well.
Even if vendors don't choose to add these, we could easily build a naive implementation using custom elements. That could look something like this:
var elementWithPressureEvents = Object.create(HTMLElement.prototype);  
elementWithPressureEvents.createdCallback = function() {  
  this.addEventListener('mousedown', function(event) {
    event.preventDefault();
    var mousePressureEvent = event;
    if(e.pressure > .75) {
        mousePressureEvent = new Event('mousedowndeep');
    } else if(e.pressure < .25) {
        mousePressureEvent = new Event('mousedownlight');
    }
    elem.dispatchEvent(mousePressureEvent);
  });
};

document.registerElement('with-ressure-mousedown-events', {  
  prototype: elementWithPressureEvents
});
In your HTML you could use this:
<div id=""with-pressure-mousedown-events"">your element</div>  
What about pressure with Touch Events?
Just as having the pressure level on a Mouse Event would be great, having the same data available for Touch Events offers great possibilities as well. Fortunately, this is already possible. According to the Touch Events W3C Specficication, there is a property called force available in the Touch object interface. This property holds the amount of pressure being applied to the surface by the user, as a float between 0.0 (no pressure) and 1.0 (maximum pressure).
Using this property is already very common. Let's look at a simple example where we loop through all touch points and read the force value.
document.body.addEventListener('touchstart', function(e){  
    var touchlist = e.touches
    // loop through all touch points currently in contact with surface
    for (var touchIndex=0; touchIndex<touchlist.length; touchIndex++){ 
        var currentTouchPressure = touchlist[i].force;
        //do something with the currentTouchPressure
    }
}, false)
MDN notes that the availability of values is hardware-dependent. For example, if the device doesn't have a way to detect the amount of pressure placed on the surface, the force value will always be 1.
Conclusion
The new Macbook will be available to the audience very soon, and so will the new trackpad. Unfortunately there is no way to exploit this in the browser right now. Although all suggestions in this post are purely hypothetical, I can't imagine browser vendors won't start implementing support for multiple pressure levels. At this point, it's just nothing but a speculation game. Even though Apple patented the Force Touch Trackpad, more and more hardware manufacturers are likely to implement similar features. Definitely something I'm looking forward to!","[thoughts, Code]"
135,Monitor your A/B tests with Optimizely and Pingdom,/how-we-monitor-our-ab-tests-with-pingdom/,"
            <p>Here at Blue Mango we're pretty serious about improving the performance of our products and the products of our clients. It's in our DNA. Because of this, it should not be a big suprise that we continuously run A/B and multivariate tests. We turn out to be <a href=""http://www.bluemangointeractive.com/news/blue-mango-wins-five-whichtestwon-testing-awards"">pretty good at it</a> as well. </p>

<h5 id=""ourtestskeptbreaking"">Our tests kept breaking</h5>

<p>When A/B testing, you are creating a variant based on a default. This default is the state of the page you are about to test, before applying any transformations that are part of the testing plan. In other words, it's the basis of your page under test (or <em>PUT</em>, as I like to call it). </p>

<p>For your variant to work properly, the default should not change during the testing period. And that's exactly how our tests got sabotaged. In most cases, we are not building and maintaining the PUTs of our clients, and the parties that do are continuously improving them. Because of this, the default of our PUTs can change, which ruins our tests. </p>

<p>So let's say we have a simple test where we want to change the text in a button from option A: 'to payment' to option B: 'next step' to see if it increases the number of clicks. In the default we identify the button with the unique identifier <em>main_cta</em>. We change our B variant with something like:</p>

<pre><code class=""language-prettyprint lang-javascript prettyprinted""><span class=""pln"">document</span><span class=""pun"">.</span><span class=""pln"">getElementById</span><span class=""pun"">(</span><span class=""str"">'main_cta'</span><span class=""pun"">).</span><span class=""pln"">innerText </span><span class=""pun"">=</span><span class=""pln""> </span><span class=""str"">'next step'</span><span class=""pln"">  </span></code></pre>

<p>Save. Start the test. The first data is coming in. Everything is looking good. But then, without us knowing it, the identifier of the button changes from <em>main_cta</em> to <em>secondary_cta</em>, due to third party actions. As you can guess, our variant for the <em>main_cta</em> no longer works, making the test useless. </p>

<p>We discovered most of these changes happen on specific days and times, likely because of a release after a sprint. So at those known moments, we let a few people check the PUTs to see if everything was still working. </p>

<p>For me as a developer this was a thorn in my side, especially in this era of automation. It took up too much time, and it wasn't fail proof: we were still dealing with totally random changes in our PUTs. </p>

<h5 id=""thesolutionmonitoring"">The solution: Monitoring</h5>

<p>Ideally, we want to get notified immediately when the default changes, so we can pause our tests and change the implementation. </p>

<p>When we started using <a href=""http://www.pingdom.com"">Pingdom</a> for uptime monitoring, I found out that Pingdom also has a service called <em>Transaction Monitor</em>. On their <a href=""https://www.pingdom.com/transactionmonitor/"">website</a>, they state:</p>

<blockquote>
  <p>Now it’s easy to identify broken interactions. [..] You need to be the first to know when there is an issue with the login, search, check out, or any other user interactions on your website.</p>
</blockquote>

<p>Bingo! Exactly what the doctor ordered. </p>

<p>When using Transaction Monitor, the Pingdom bot visits your website once in a (configured) while, and executes a number of defined actions ('click on this element') and assertions ('this should be visible'). The interval of checks is configurable between one minute and seversal hours. </p>

<p>If a check comes back negative, you immediately get notified. Pingdom offers a great number of ways to alert you when something's wrong, for example SMS, email, Twitter and the Pingdom app. </p>

<p>When you add a check in the Transaction Monitor, you can assemble a sequence of two types of steps: commands and validations. </p>

<p>Examples of available commands are:</p>

<ul>
<li>Go to a URL</li>
<li>Click on an element</li>
<li>Fill a text field with text</li>
<li>Wait for element to exist</li>
<li>Submit a form</li>
</ul>

<p>And a selection of possible validations:</p>

<ul>
<li>Element should exist</li>
<li>Element should contain text</li>
<li>Text field should contain text</li>
<li>Checkbox should be checked</li>
</ul>

<p>People who have used Selenium, CasperJS or any E2E testing frameworks will feel very familiar with these commands and assertions.</p>

<p>As you can imagine, you can make your checks as complex as you like. How complex your checks are going to be, depends on the number of variants and how complex your variants are. </p>

<p>To check if the default from our earlier example has not been changed, we only need two steps:</p>

<p><img src=""http://zandbak.acceptatie.bluemango.nl/geek/pingdom/check.png"" alt=""Check if #main_cta exist""></p>

<p>That's it. Now everytime the Pingdom bot visits your website, it will execute those two steps and let you know immediately when one of them failed. </p>

<p>You can use any CSS selector to identify elements on your page. I really like <a href=""http://selectorgadget.com/"">selectorgadget.com</a> to determine a selector. </p>

<h6 id=""alwayscheckyourdefault"">Always check your default</h6>

<p>Did you see the monitoring=true query parameter in the first step? This is to make sure you verify your default, not the variants. You need to add a unique URL of your page that does not try to load one of your variants. This can easily be done by excluding URLs with a specific query string parameter. </p>

<p>Because we are using <a href=""http://www.optimizely.com"">Optimizely</a> to create and manage our A/B tests, I'm going to show you how to exclude URLs using Optimizely.</p>

<p>Let's say we want to start monitoring the test that we've got running on <a href=""http://www.yourclient.com"">yourclient.com</a>. In Optimizely, go to the experiment you want to monitor and open the editor. Add a new audience using this icon:</p>

<p><img src=""http://zandbak.acceptatie.bluemango.nl/geek/pingdom/audience.png"" alt=""Add a new audience""></p>

<p>Give your new audience a descriptive name, like 'Bots for monitoring'. Drag the 'Query Parameters' condition to the 'Audience Conditions'. Pick a useful name for the query string and make sure you select 'does not match' in the dropdown. </p>

<p><img src=""http://zandbak.acceptatie.bluemango.nl/geek/pingdom/new_audience.png"" alt=""New audience"" class=""full-img""></p>

<p>Save the audience. Now we have created a URL that excludes its visitors from the experiment and shows the default: <code>http://www.yourclient.com/?monitoring=true</code>. This is the URL you should add to the first step when adding a Transaction Monitor check in Pingdom. </p>

<p><strong>Update:</strong>  There's an easier solution that will ensure you're always monitoring the default: you can just add <code>?optimizely_disable=true</code> to the url so that Optimizely doesn't run and track. Thanks to Tobias Urff for this tip in the comments. </p>

<h5 id=""conclusion"">Conclusion</h5>

<p>Pingdom is a very powerful tool and its Transaction Monitor feature is only the tip of the iceberg. In the few weeks since we started using Pingdom to monitor our A/B tests, we have already been notified about some suddenly failing experiments we would have otherwise discovered too late. So without a doubt, using automated monitoring on our tests is extremely useful for us, and it might be for you as well. </p>
        ","Here at Blue Mango we're pretty serious about improving the performance of our products and the products of our clients. It's in our DNA. Because of this, it should not be a big suprise that we continuously run A/B and multivariate tests. We turn out to be pretty good at it as well.
Our tests kept breaking
When A/B testing, you are creating a variant based on a default. This default is the state of the page you are about to test, before applying any transformations that are part of the testing plan. In other words, it's the basis of your page under test (or PUT, as I like to call it).
For your variant to work properly, the default should not change during the testing period. And that's exactly how our tests got sabotaged. In most cases, we are not building and maintaining the PUTs of our clients, and the parties that do are continuously improving them. Because of this, the default of our PUTs can change, which ruins our tests.
So let's say we have a simple test where we want to change the text in a button from option A: 'to payment' to option B: 'next step' to see if it increases the number of clicks. In the default we identify the button with the unique identifier main_cta. We change our B variant with something like:
document.getElementById('main_cta').innerText = 'next step'  
Save. Start the test. The first data is coming in. Everything is looking good. But then, without us knowing it, the identifier of the button changes from main_cta to secondary_cta, due to third party actions. As you can guess, our variant for the main_cta no longer works, making the test useless.
We discovered most of these changes happen on specific days and times, likely because of a release after a sprint. So at those known moments, we let a few people check the PUTs to see if everything was still working.
For me as a developer this was a thorn in my side, especially in this era of automation. It took up too much time, and it wasn't fail proof: we were still dealing with totally random changes in our PUTs.
The solution: Monitoring
Ideally, we want to get notified immediately when the default changes, so we can pause our tests and change the implementation.
When we started using Pingdom for uptime monitoring, I found out that Pingdom also has a service called Transaction Monitor. On their website, they state:
Now it’s easy to identify broken interactions. [..] You need to be the first to know when there is an issue with the login, search, check out, or any other user interactions on your website.
Bingo! Exactly what the doctor ordered.
When using Transaction Monitor, the Pingdom bot visits your website once in a (configured) while, and executes a number of defined actions ('click on this element') and assertions ('this should be visible'). The interval of checks is configurable between one minute and seversal hours.
If a check comes back negative, you immediately get notified. Pingdom offers a great number of ways to alert you when something's wrong, for example SMS, email, Twitter and the Pingdom app.
When you add a check in the Transaction Monitor, you can assemble a sequence of two types of steps: commands and validations.
Examples of available commands are:
Go to a URL
Click on an element
Fill a text field with text
Wait for element to exist
Submit a form
And a selection of possible validations:
Element should exist
Element should contain text
Text field should contain text
Checkbox should be checked
People who have used Selenium, CasperJS or any E2E testing frameworks will feel very familiar with these commands and assertions.
As you can imagine, you can make your checks as complex as you like. How complex your checks are going to be, depends on the number of variants and how complex your variants are.
To check if the default from our earlier example has not been changed, we only need two steps:
That's it. Now everytime the Pingdom bot visits your website, it will execute those two steps and let you know immediately when one of them failed.
You can use any CSS selector to identify elements on your page. I really like selectorgadget.com to determine a selector.
Always check your default
Did you see the monitoring=true query parameter in the first step? This is to make sure you verify your default, not the variants. You need to add a unique URL of your page that does not try to load one of your variants. This can easily be done by excluding URLs with a specific query string parameter.
Because we are using Optimizely to create and manage our A/B tests, I'm going to show you how to exclude URLs using Optimizely.
Let's say we want to start monitoring the test that we've got running on yourclient.com. In Optimizely, go to the experiment you want to monitor and open the editor. Add a new audience using this icon:
Give your new audience a descriptive name, like 'Bots for monitoring'. Drag the 'Query Parameters' condition to the 'Audience Conditions'. Pick a useful name for the query string and make sure you select 'does not match' in the dropdown.
Save the audience. Now we have created a URL that excludes its visitors from the experiment and shows the default: http://www.yourclient.com/?monitoring=true. This is the URL you should add to the first step when adding a Transaction Monitor check in Pingdom.
Update: There's an easier solution that will ensure you're always monitoring the default: you can just add ?optimizely_disable=true to the url so that Optimizely doesn't run and track. Thanks to Tobias Urff for this tip in the comments.
Conclusion
Pingdom is a very powerful tool and its Transaction Monitor feature is only the tip of the iceberg. In the few weeks since we started using Pingdom to monitor our A/B tests, we have already been notified about some suddenly failing experiments we would have otherwise discovered too late. So without a doubt, using automated monitoring on our tests is extremely useful for us, and it might be for you as well.","[testing, usability, cro]"
136,Bring on- and offline together with iBeacons and Google Analytics,/bring-on-and-offline-together-with-ibeacons-and-google-analytics/,"
            <p>Measuring the online world is what we do every day, so why not measure the offline world? To accomplish this we worked together with <a href=""http://www.fosbury.co/"">Fosbury</a> and their iBeacon Management tool. Together we build a solution to connect our offline iBeacons with Google Analytics to get real-time insights in the offline visitor behavior. This is a small first step to get Omni-Channel insights by combining online and offline analytics in one tool.</p>

<h2 id=""thebasics"">The Basics</h2>

<p>Google Analytics provides a <a href=""https://developers.google.com/analytics/devguides/collection/protocol/v1/"">Measurement Protocol</a> which can be used to send user data via HTTP requests directly to Google Analytics via almost any environment with an internet connection. So the only thing we needed to do was making an HTTP request when someone came near an iBeacon. Fosbury has a “Webhook” functionality which we used to send the Google Analytics HTTP request:</p>

<p><img src=""https://lh6.googleusercontent.com/66Fyq2yybQei0q0OdNp_wMk8914gNpJ8fEyO1zYxPbPOtWa9Ul8OnRyIrn--F9N65wSpMh6eWA8YjfEBrBm4XH6slRVb3l5AotcgDaxS50TpgzGZ3Oaayx1DJxiz5NiBGg"" alt=""Webhook fields""></p>

<p>The basic URL of the HTTP request is <a href=""http://www.google-analytics.com/collect"">http://www.google-analytics.com/collect</a>. Besides the base URL  different parameters can be added to the URL to define what you want to measure in Google Analytics. The following four parameters are required:</p>

<table>  
    <tbody><tr>
        <td>Name</td>
        <td>Parameter</td>
        <td>Example</td>
        <td>Description</td>
    </tr>
    <tr>
        <td>Protocol Version</td>
        <td>v</td>
        <td>v=1</td>
        <td>The protocol version. The value should be 1.</td>
    </tr>
    <tr>
        <td>Tracking ID</td>
        <td>tid</td>
        <td>tid=UA-123456-1</td>
        <td>The ID that distinguishes to which Google Analytics property to send data.</td>
    </tr>
    <tr>
        <td>Client ID</td>
        <td>cid</td>
        <td>cid=xxxxx</td>
        <td>An ID unique to a particular user.</td>
    </tr>
    <tr>
        <td>Hit Type</td>
        <td>t</td>
        <td>t=pageview</td>
        <td>The type of interaction collected for a particular user.</td>
    </tr>
</tbody></table>

<p>Depending on the “hit type” some more paremeters are required. The hit type “pageview” requires the Page Path parameter (dp) as a paremeter for example. So a basic pageview would look like this:</p>

<p><code>http://www.google-analytics.com/collect?v=1&amp;tid=UA-123456-1&amp;cid=12345&amp;t=pageview&amp;dp=%2Fpage1</code></p>

<p>By using the url above in the webhoop field of Fosbury a pageview will be measured everytime someone enters the range of an iBeacon. The result in Google Analytics will look like this:</p>

<p><img src=""https://lh4.googleusercontent.com/h-USTH7B_e7OQxft-iU-Tnf-A9o_5nZJzA8-XwchhWSBMQxSQzegrII9ysVPIjmgwnuVxh5YJmIn1PPJ8eaDjl-b9uRJre-surSw4YZ7fii_X5W53SV3IcS68Ie2fVCVWA"" alt=""""></p>

<h2 id=""beyondthebasics"">Beyond the basics</h2>

<p>The example mentioned above is very basic example, but when using the right parameters you can get even more offline insights out of your iBeacons. By sending an event to Google Analytics when someone enters and leaves the range of an iBeacon you can measure via Google Analytics how many people entered and left the iBeacon range and how long your offline visitors stayed in the proximity of the iBeacon:</p>

<p><img src=""https://lh5.googleusercontent.com/XSZET-UrFgP2MEy5UxzWK2eI3OBcwy7PXHXxYLN3o42WMBxxt0Huh3kYbRxhYoibervOcDs4EXO7xNoecVwxDqIVI8xiSA50MKilsE2nlBwAi2MMr84fxCwk5ErPKP-g9w"" alt="""" class=""full-img""></p>

<p><img src=""https://lh4.googleusercontent.com/yN-fqiPVI4rPkupjRDvxLcu_DoBRHKsWjhVLpnwyxztT53Doh9tm1_y1T-LD_JUehHFCATzgb-5mYU27kz_zCLPYyevfBsslJRQeE4hkiC8gleK3nyXVSvyYlC3UmLt9tg"" alt=""""></p>

<p>Example of an HTTP request when somebody enters the region of a beacon (notice the ""Hit type"", event information and session parameters):</p>

<p><code>http://www.google-analytics.com/collect?v=1&amp;tid=UA-123456-1&amp;cid=11111&amp;t=event&amp;ec=iBeacons&amp;ea=Enter_Beacon&amp;sc=start</code></p>

<p>Webhook url for when somebody exits the region of a beacon:</p>

<p><code>http://www.google-analytics.com/collect?v=1&amp;tid=UA-123456-1&amp;cid=11111&amp;t=event&amp;ec=iBeacons&amp;ea=Exit_Beacon&amp;sc=end</code></p>

<h2 id=""whatareyourideas"">What are your ideas?</h2>

<p>These are just a few examples of how you can connect your beacons to Google Analytics. We’d love to hear your ideas!</p>
        ","Measuring the online world is what we do every day, so why not measure the offline world? To accomplish this we worked together with Fosbury and their iBeacon Management tool. Together we build a solution to connect our offline iBeacons with Google Analytics to get real-time insights in the offline visitor behavior. This is a small first step to get Omni-Channel insights by combining online and offline analytics in one tool.
The Basics
Google Analytics provides a Measurement Protocol which can be used to send user data via HTTP requests directly to Google Analytics via almost any environment with an internet connection. So the only thing we needed to do was making an HTTP request when someone came near an iBeacon. Fosbury has a “Webhook” functionality which we used to send the Google Analytics HTTP request:
The basic URL of the HTTP request is http://www.google-analytics.com/collect. Besides the base URL different parameters can be added to the URL to define what you want to measure in Google Analytics. The following four parameters are required:
Name Parameter Example Description
Protocol Version v v=1 The protocol version. The value should be 1.
Tracking ID tid tid=UA-123456-1 The ID that distinguishes to which Google Analytics property to send data.
Client ID cid cid=xxxxx An ID unique to a particular user.
Hit Type t t=pageview The type of interaction collected for a particular user.
Depending on the “hit type” some more paremeters are required. The hit type “pageview” requires the Page Path parameter (dp) as a paremeter for example. So a basic pageview would look like this:
http://www.google-analytics.com/collect?v=1&tid=UA-123456-1&cid=12345&t=pageview&dp=%2Fpage1
By using the url above in the webhoop field of Fosbury a pageview will be measured everytime someone enters the range of an iBeacon. The result in Google Analytics will look like this:
Beyond the basics
The example mentioned above is very basic example, but when using the right parameters you can get even more offline insights out of your iBeacons. By sending an event to Google Analytics when someone enters and leaves the range of an iBeacon you can measure via Google Analytics how many people entered and left the iBeacon range and how long your offline visitors stayed in the proximity of the iBeacon:
Example of an HTTP request when somebody enters the region of a beacon (notice the ""Hit type"", event information and session parameters):
http://www.google-analytics.com/collect?v=1&tid=UA-123456-1&cid=11111&t=event&ec=iBeacons&ea=Enter_Beacon&sc=start
Webhook url for when somebody exits the region of a beacon:
http://www.google-analytics.com/collect?v=1&tid=UA-123456-1&cid=11111&t=event&ec=iBeacons&ea=Exit_Beacon&sc=end
What are your ideas?
These are just a few examples of how you can connect your beacons to Google Analytics. We’d love to hear your ideas!","[Analytics, google analytics, ibeacons, omnichannel]"
137,Using HTML5's device orientation to track device behaviour,/using-html5s-device-orientation-to-track-device-behaviour/,"
            <p>HTML5 offers some features that are usefull in tracking user behavior. One of those features is device orientation. Using some very basic code, we can use this feature to track: </p>

<ul>
<li>how users land on your website</li>
<li>if users change orientation on a page</li>
</ul>

<p>To track these events you can use the following javascript code:  </p>

<pre><code class=""language-javascript"">    switch(window.orientation) {  
      case -90:
      case 90:
        //track event for landscape
        break; 
      default:
        //track event for portrait
        break; 
    };
    window.addEventListener(""orientationchange"", function() {
      switch(window.orientation) {  
        case -90:
        case 90:
          //track event for change to landscape
          break; 
        default:
          //track event for change to portrait
          break; 
      }
    }, false);
</code></pre>

<p>The code above tracks two types of landing: one for landscape (with a 90° turn) and portrait (default). Theres room here to add your favourite tracking scripts.</p>

<p>Now ofcourse, this is only relevant for mobile and tablet devices (desktops don't tend to switch orientation all that often). Therefore, I advice you to use a function to check if a visit is also a mobile visit. This way you don't fire an event for each desktop visit.</p>

<p>Personally, I recommend the isMobile() function: <a href=""https://github.com/kaimallea/isMobile"">https://github.com/kaimallea/isMobile</a>. An example with isMobile() available:</p>

<pre><code> if(isMobile.any){    
   switch(window.orientation) {  
     case -90:
     case 90:
     //track event for landscape
     break; 
   default:
     //track event for portrait
     break; 
  };
  window.addEventListener(""orientationchange"",       function() {
    switch(window.orientation) {  
      case -90:
      case 90:
      //track event for change to landscape
      break; 
    default:
      //track event for change to portrait
      break; 
    }
  }, false);
};
</code></pre>

<p>If you're using Google Analytics (Universal), here's a full example:</p>

<pre><code>if(isMobile.any){    
   switch(window.orientation) {  
     case -90:
     case 90:
     ga('send', 'event', 'Device Orientation', 'Landing', 'Landscape', {'nonInteraction': 1})
     break; 
   default:
     ga('send', 'event', 'Device Orientation', 'Landing', 'Portrait', {'nonInteraction': 1})
     break; 
  };
  window.addEventListener(""orientationchange"",       function() {
    switch(window.orientation) {  
      case -90:
      case 90:
      ga('send', 'event', 'Device Orientation', 'Change', 'Landscape', {'nonInteraction': 1})
      break; 
    default:
      ga('send', 'event', 'Device Orientation', 'Change', 'Portrait', {'nonInteraction': 1})
      break; 
    }
  }, false);
};
</code></pre>

<p>Now you can view your event report to see how people land on your website and if they tend to switch orientation:</p>

<p><img src=""http://zandbak.acceptatie.bluemango.nl/geek/analytics/1.png"" alt=""device orientation1""></p>

<p>Look at the event labels for details about the actual orientation:</p>

<p><img src=""http://zandbak.acceptatie.bluemango.nl/geek/analytics/2.png"" alt=""device orientation2""></p>

<p>Any questions? Feel free to ask.</p>
        ","HTML5 offers some features that are usefull in tracking user behavior. One of those features is device orientation. Using some very basic code, we can use this feature to track:
how users land on your website
if users change orientation on a page
To track these events you can use the following javascript code:
    switch(window.orientation) {  
      case -90:
      case 90:
        //track event for landscape
        break; 
      default:
        //track event for portrait
        break; 
    };
    window.addEventListener(""orientationchange"", function() {
      switch(window.orientation) {  
        case -90:
        case 90:
          //track event for change to landscape
          break; 
        default:
          //track event for change to portrait
          break; 
      }
    }, false);
The code above tracks two types of landing: one for landscape (with a 90° turn) and portrait (default). Theres room here to add your favourite tracking scripts.
Now ofcourse, this is only relevant for mobile and tablet devices (desktops don't tend to switch orientation all that often). Therefore, I advice you to use a function to check if a visit is also a mobile visit. This way you don't fire an event for each desktop visit.
Personally, I recommend the isMobile() function: https://github.com/kaimallea/isMobile. An example with isMobile() available:
 if(isMobile.any){    
   switch(window.orientation) {  
     case -90:
     case 90:
     //track event for landscape
     break; 
   default:
     //track event for portrait
     break; 
  };
  window.addEventListener(""orientationchange"",       function() {
    switch(window.orientation) {  
      case -90:
      case 90:
      //track event for change to landscape
      break; 
    default:
      //track event for change to portrait
      break; 
    }
  }, false);
};
If you're using Google Analytics (Universal), here's a full example:
if(isMobile.any){    
   switch(window.orientation) {  
     case -90:
     case 90:
     ga('send', 'event', 'Device Orientation', 'Landing', 'Landscape', {'nonInteraction': 1})
     break; 
   default:
     ga('send', 'event', 'Device Orientation', 'Landing', 'Portrait', {'nonInteraction': 1})
     break; 
  };
  window.addEventListener(""orientationchange"",       function() {
    switch(window.orientation) {  
      case -90:
      case 90:
      ga('send', 'event', 'Device Orientation', 'Change', 'Landscape', {'nonInteraction': 1})
      break; 
    default:
      ga('send', 'event', 'Device Orientation', 'Change', 'Portrait', {'nonInteraction': 1})
      break; 
    }
  }, false);
};
Now you can view your event report to see how people land on your website and if they tend to switch orientation:
Look at the event labels for details about the actual orientation:
Any questions? Feel free to ask.","[html5, device orientation, Analytics]"
138,Overcoming blog challenges by introducing Geek.,/introducing-geek/,"
            <p>There have been several times when we at Blue Mango were about to launch a blog. Initiatives were started in every corner of the company. Data Scientists tried it. The development department tried it. I wouldn't be surprised if the girls from HR even gave it a shot. None of those initiatives ever took off. This is because blogs have one criterium: you have to fill 'm with posts. And writing takes time. Time we usually never have. </p>

<p>Just a few weeks ago I was watching a session on <a href=""http://www.pluralsight.com"">Pluralsight</a>. I'm not sure what the subject of the talk was, but at one point the presenter spoke these words:</p>

<blockquote>
  <p>When you're writing an email longer than one paragraph, you should consider blogging about it.</p>
</blockquote>

<p>That was the moment it hit me. We write emails all the time, explain things, share new ideas and talk about how we've solved certain issues. All this knowledge disappears in our inboxes. The thing is, this way of sharing knowledge and ideas takes time too. So why not share the things you write in your emails with the rest of the world?</p>

<p>People tend to think blogs are long posts that take hours to write, and even more hours to get everyone’s approval. I believe this is one of the main reasons why all of the attempts of my colleagues failed. No one was writing for those blogs, because we set the bar too high for ourselves. When we are writing for the outside world, everything needs to be nothing short of perfect.</p>

<p>Not for Geek. We want to write to the outside world as if you are our colleagues. If we think a few lines on a subject is interesting enough for a colleague, it might also be for you. That's the essence of Geek. I'm not suggesting writing a blog takes as little time as an email — I mean, you want to make sure everything is at least comprehensive and without embarrassing spelling mistakes — but by spending less time polishing every word, the time investment will easily pay off. </p>

<p>When I asked people if they'd like to write on Geek, some people told me they didn't have a clue what to write about. Then I asked them these simple questions: </p>

<blockquote>
  <p>What problem did you solve last week? What new cool stuff did you find out about? What have you been wondering about lately?</p>
</blockquote>

<p>All the sudden those people came up with tons of ideas. Ideas for blogs. Those are exactly the blogs I'd like to see on Geek in the future. I truly believe that if you can't answer any of those questions, you should question yourself. Everyone should challenge themselves every day, and share what these challenges brought them. </p>

<p>We are tech geeks and we want to write for other tech geeks. Every post falls in either one or more of these categories: code, analytics, marketing and data science. We might have backend developers writing about hardcore coding techniques, but we might also see a guy or girl from analytics share a cool new feature in Google Analytics. One of our data scientists might eventually show you some fancy tricks in R. The length of posts doesn't matter — it may vary between only one paragraph and multi-part tutorials. </p>

<h3 id=""aboutbluemango"">About Blue Mango</h3>

<p><a href=""http://www.bluemangointeractive.com"">Blue Mango</a> is an innovative Dutch online media agency, active for an exclusive group of major advertisers. Blue Mango is part of <a href=""http://www.greenhousegroup.com"">Greenhouse Group</a>. Based on knowledge, skills, and measurability, Blue Mango supports leading customers in their online strategy, creation and media procurement. We want to surpass ourselves every day. It's in our nature and it's what gets us up in the morning. We make choices on scientific evidence, not by gut feeling. Data is everything. </p>

<p>The opinions on this blog belong to the individual writers. They don’t belong to Blue Mango, Greenhouse Group or the any of any other companies mentioned in this blog, unless otherwise specified. </p>

<h3 id=""conclusion"">Conclusion</h3>

<p>Geek is a blog from the tech geeks of Blue Mango. We want to share thoughts, stories and ideas on code, analytics, marketing and data science. We expect to publish at least one blog a week, but we're hoping for more. We'd like to hear from you, so please let us know what you think. </p>

<p>We hope you'll enjoy reading our posts here at Geek, geek!</p>
        ","There have been several times when we at Blue Mango were about to launch a blog. Initiatives were started in every corner of the company. Data Scientists tried it. The development department tried it. I wouldn't be surprised if the girls from HR even gave it a shot. None of those initiatives ever took off. This is because blogs have one criterium: you have to fill 'm with posts. And writing takes time. Time we usually never have.
Just a few weeks ago I was watching a session on Pluralsight. I'm not sure what the subject of the talk was, but at one point the presenter spoke these words:
When you're writing an email longer than one paragraph, you should consider blogging about it.
That was the moment it hit me. We write emails all the time, explain things, share new ideas and talk about how we've solved certain issues. All this knowledge disappears in our inboxes. The thing is, this way of sharing knowledge and ideas takes time too. So why not share the things you write in your emails with the rest of the world?
People tend to think blogs are long posts that take hours to write, and even more hours to get everyone’s approval. I believe this is one of the main reasons why all of the attempts of my colleagues failed. No one was writing for those blogs, because we set the bar too high for ourselves. When we are writing for the outside world, everything needs to be nothing short of perfect.
Not for Geek. We want to write to the outside world as if you are our colleagues. If we think a few lines on a subject is interesting enough for a colleague, it might also be for you. That's the essence of Geek. I'm not suggesting writing a blog takes as little time as an email — I mean, you want to make sure everything is at least comprehensive and without embarrassing spelling mistakes — but by spending less time polishing every word, the time investment will easily pay off.
When I asked people if they'd like to write on Geek, some people told me they didn't have a clue what to write about. Then I asked them these simple questions:
What problem did you solve last week? What new cool stuff did you find out about? What have you been wondering about lately?
All the sudden those people came up with tons of ideas. Ideas for blogs. Those are exactly the blogs I'd like to see on Geek in the future. I truly believe that if you can't answer any of those questions, you should question yourself. Everyone should challenge themselves every day, and share what these challenges brought them.
We are tech geeks and we want to write for other tech geeks. Every post falls in either one or more of these categories: code, analytics, marketing and data science. We might have backend developers writing about hardcore coding techniques, but we might also see a guy or girl from analytics share a cool new feature in Google Analytics. One of our data scientists might eventually show you some fancy tricks in R. The length of posts doesn't matter — it may vary between only one paragraph and multi-part tutorials.
About Blue Mango
Blue Mango is an innovative Dutch online media agency, active for an exclusive group of major advertisers. Blue Mango is part of Greenhouse Group. Based on knowledge, skills, and measurability, Blue Mango supports leading customers in their online strategy, creation and media procurement. We want to surpass ourselves every day. It's in our nature and it's what gets us up in the morning. We make choices on scientific evidence, not by gut feeling. Data is everything.
The opinions on this blog belong to the individual writers. They don’t belong to Blue Mango, Greenhouse Group or the any of any other companies mentioned in this blog, unless otherwise specified.
Conclusion
Geek is a blog from the tech geeks of Blue Mango. We want to share thoughts, stories and ideas on code, analytics, marketing and data science. We expect to publish at least one blog a week, but we're hoping for more. We'd like to hear from you, so please let us know what you think.
We hope you'll enjoy reading our posts here at Geek, geek!",[Introduction]
